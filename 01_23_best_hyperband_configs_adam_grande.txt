
 Best configuration after Hyperband n: 1 
Batch Size: 40
Validation Error: 0.9761471170912767
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 2 
Batch Size: 40
Validation Error: 0.9866045907912406
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 3 
Batch Size: 40
Validation Error: 0.9878975921031738
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 4 
Batch Size: 40
Validation Error: 1.0011369907464225
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 5 
Batch Size: 1
Validation Error: 1.0273902448448067
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 6 
Batch Size: 40
Validation Error: 1.0341107348558982
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 7 
Batch Size: 1
Validation Error: 1.036889088791915
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 8 
Batch Size: 1
Validation Error: 1.0378945910252415
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 9 
Batch Size: 40
Validation Error: 1.0385486954891583
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 10 
Batch Size: 1
Validation Error: 1.0436127000888766
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 11 
Batch Size: 1
Validation Error: 1.0487167580485786
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 12 
Batch Size: 1
Validation Error: 1.051316515323459
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 13 
Batch Size: 1
Validation Error: 1.0519210251541036
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 14 
Batch Size: 40
Validation Error: 1.0567002027843828
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 15 
Batch Size: 1
Validation Error: 1.0590652074119766
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 16 
Batch Size: 40
Validation Error: 1.0643589720503024
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 17 
Batch Size: 40
Validation Error: 1.0675497893681558
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 18 
Batch Size: 1
Validation Error: 1.0710289695957371
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 19 
Batch Size: 1
Validation Error: 1.0712370385693994
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 20 
Batch Size: 40
Validation Error: 1.0749743189560719
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 21 
Batch Size: 1
Validation Error: 1.0763084060681627
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 22 
Batch Size: 1
Validation Error: 1.0799898746582055
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 23 
Batch Size: 40
Validation Error: 1.0955966328037468
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 24 
Batch Size: 1
Validation Error: 1.095870437305242
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 25 
Batch Size: 1
Validation Error: 1.1006565197717275
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 26 
Batch Size: 1
Validation Error: 1.1043828541083407
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 27 
Batch Size: 40
Validation Error: 1.1099931862112773
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 28 
Batch Size: 1
Validation Error: 1.1105783015797226
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 29 
Batch Size: 1
Validation Error: 1.1112085709703756
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 30 
Batch Size: 1
Validation Error: 1.1182181527106358
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 31 
Batch Size: 1
Validation Error: 1.1213411726916889
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 32 
Batch Size: 40
Validation Error: 1.121386286866692
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 33 
Batch Size: 1
Validation Error: 1.1218564228928407
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 34 
Batch Size: 40
Validation Error: 1.12289231045806
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 35 
Batch Size: 1
Validation Error: 1.1231401405275463
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 36 
Batch Size: 40
Validation Error: 1.124874080520484
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 37 
Batch Size: 1
Validation Error: 1.13031482583306
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 38 
Batch Size: 40
Validation Error: 1.141749170540513
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 39 
Batch Size: 40
Validation Error: 1.1428231064784875
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 40 
Batch Size: 1
Validation Error: 1.1496894060770946
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 41 
Batch Size: 1
Validation Error: 1.1510468904309061
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 42 
Batch Size: 1
Validation Error: 1.1515853835169492
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 43 
Batch Size: 1
Validation Error: 1.1533287313827307
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 44 
Batch Size: 1
Validation Error: 1.1547802738835582
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 45 
Batch Size: 1
Validation Error: 1.1573309349346559
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 46 
Batch Size: 1
Validation Error: 1.1577767275370052
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 47 
Batch Size: 1
Validation Error: 1.1581066601418533
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 48 
Batch Size: 1
Validation Error: 1.1601769747421475
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 49 
Batch Size: 1
Validation Error: 1.1687389000490733
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 50 
Batch Size: 1
Validation Error: 1.1708850117599912
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 51 
Batch Size: 1
Validation Error: 1.1729833122601034
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 52 
Batch Size: 40
Validation Error: 1.173748820520572
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 53 
Batch Size: 1
Validation Error: 1.1759478448245777
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 54 
Batch Size: 1
Validation Error: 1.1811664587821777
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 55 
Batch Size: 1
Validation Error: 1.18566819099783
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 56 
Batch Size: 1
Validation Error: 1.1866989221483564
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 57 
Batch Size: 1
Validation Error: 1.1880944445148065
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 58 
Batch Size: 1
Validation Error: 1.1881736479506582
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 59 
Batch Size: 1
Validation Error: 1.1883032674944947
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 60 
Batch Size: 1
Validation Error: 1.1896422461791971
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 61 
Batch Size: 1
Validation Error: 1.190104480407264
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 62 
Batch Size: 1
Validation Error: 1.1901800276768992
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 63 
Batch Size: 1
Validation Error: 1.191341772901914
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 64 
Batch Size: 1
Validation Error: 1.1922622173634863
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 65 
Batch Size: 1
Validation Error: 1.1925531703186267
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 66 
Batch Size: 1
Validation Error: 1.1939495093428452
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 67 
Batch Size: 1
Validation Error: 1.195007671654778
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 68 
Batch Size: 1
Validation Error: 1.1951072492840706
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 69 
Batch Size: 1
Validation Error: 1.1964400438263092
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 70 
Batch Size: 1
Validation Error: 1.1967882634655687
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 71 
Batch Size: 40
Validation Error: 1.2027506895868765
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 72 
Batch Size: 1
Validation Error: 1.203159892246624
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 73 
Batch Size: 1
Validation Error: 1.2035856485643048
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 74 
Batch Size: 1
Validation Error: 1.2043410691879717
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 75 
Batch Size: 1
Validation Error: 1.2048044171168368
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 76 
Batch Size: 1
Validation Error: 1.2068677492593116
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 77 
Batch Size: 1
Validation Error: 1.2085583710583498
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 78 
Batch Size: 1
Validation Error: 1.209637132999946
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 79 
Batch Size: 1
Validation Error: 1.2129916091193893
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 80 
Batch Size: 40
Validation Error: 1.213433766813239
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 81 
Batch Size: 1
Validation Error: 1.2135664998836542
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 82 
Batch Size: 1
Validation Error: 1.2154227130998227
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 83 
Batch Size: 1
Validation Error: 1.2163456599599285
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 84 
Batch Size: 1
Validation Error: 1.2166110636688277
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 85 
Batch Size: 1
Validation Error: 1.217040977601799
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 86 
Batch Size: 1
Validation Error: 1.2170747201348195
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 87 
Batch Size: 1
Validation Error: 1.2174207804933794
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 88 
Batch Size: 1
Validation Error: 1.219577671636225
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 89 
Batch Size: 1
Validation Error: 1.219943279103864
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 90 
Batch Size: 1
Validation Error: 1.2218980157486854
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 91 
Batch Size: 1
Validation Error: 1.222068702518034
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 92 
Batch Size: 1
Validation Error: 1.2223429641599808
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 93 
Batch Size: 1
Validation Error: 1.222563449447518
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 94 
Batch Size: 1
Validation Error: 1.2227435410289724
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 95 
Batch Size: 1
Validation Error: 1.2230099204140579
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 96 
Batch Size: 1
Validation Error: 1.2235396633975053
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 97 
Batch Size: 1
Validation Error: 1.2238241789160356
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 98 
Batch Size: 1
Validation Error: 1.2246599923353367
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 99 
Batch Size: 1
Validation Error: 1.2247348129509246
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 100 
Batch Size: 1
Validation Error: 1.2266856437097888
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 101 
Batch Size: 1
Validation Error: 1.2269764081722465
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 102 
Batch Size: 1
Validation Error: 1.2276931380426046
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 103 
Batch Size: 1
Validation Error: 1.228163293540582
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 104 
Batch Size: 1
Validation Error: 1.228628824460524
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 105 
Batch Size: 1
Validation Error: 1.229331250793177
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 106 
Batch Size: 1
Validation Error: 1.2297106759701601
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 107 
Batch Size: 1
Validation Error: 1.229751081772078
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 108 
Batch Size: 1
Validation Error: 1.2301217179305073
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 109 
Batch Size: 1
Validation Error: 1.2301575365642428
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 110 
Batch Size: 40
Validation Error: 1.2305593796879695
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 111 
Batch Size: 1
Validation Error: 1.2309360701242091
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 112 
Batch Size: 1
Validation Error: 1.231376654657238
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 113 
Batch Size: 1
Validation Error: 1.2316747485445865
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 114 
Batch Size: 1
Validation Error: 1.2335936054550998
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 115 
Batch Size: 1
Validation Error: 1.2336400372745886
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 116 
Batch Size: 1
Validation Error: 1.2351554702335084
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 117 
Batch Size: 1
Validation Error: 1.2354680933844822
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 118 
Batch Size: 1
Validation Error: 1.2364085360086616
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 119 
Batch Size: 1
Validation Error: 1.2367327762781095
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 120 
Batch Size: 1
Validation Error: 1.2368304007498687
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 121 
Batch Size: 40
Validation Error: 1.2375331172512165
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 122 
Batch Size: 1
Validation Error: 1.2381813772678048
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 123 
Batch Size: 1
Validation Error: 1.2382823570975166
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 124 
Batch Size: 1
Validation Error: 1.2385031364422772
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 125 
Batch Size: 1
Validation Error: 1.2392748162045053
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 126 
Batch Size: 1
Validation Error: 1.239296803552207
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 127 
Batch Size: 1
Validation Error: 1.2397699775219018
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 128 
Batch Size: 1
Validation Error: 1.240301839080534
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 129 
Batch Size: 1
Validation Error: 1.2434179565508814
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 130 
Batch Size: 1
Validation Error: 1.2441285139784584
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 131 
Batch Size: 1
Validation Error: 1.2452193641598022
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 132 
Batch Size: 1
Validation Error: 1.2453517809224182
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 133 
Batch Size: 1
Validation Error: 1.245446234087531
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 134 
Batch Size: 1
Validation Error: 1.2456158480877586
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 135 
Batch Size: 40
Validation Error: 1.2456593000029041
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 136 
Batch Size: 1
Validation Error: 1.2460595166347743
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 137 
Batch Size: 1
Validation Error: 1.2461025818328824
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 138 
Batch Size: 1
Validation Error: 1.2463231145331566
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 139 
Batch Size: 1
Validation Error: 1.246527254823365
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 140 
Batch Size: 1
Validation Error: 1.247156373899227
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 141 
Batch Size: 1
Validation Error: 1.2488944159294317
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 142 
Batch Size: 1
Validation Error: 1.2509537050895794
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 143 
Batch Size: 1
Validation Error: 1.2516788610644598
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 144 
Batch Size: 1
Validation Error: 1.253524716569022
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 145 
Batch Size: 1
Validation Error: 1.2542552548132115
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 146 
Batch Size: 1
Validation Error: 1.255693285224329
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 147 
Batch Size: 40
Validation Error: 1.2570319412458528
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 148 
Batch Size: 1
Validation Error: 1.2577200111963902
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 149 
Batch Size: 1
Validation Error: 1.2579735412475779
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 150 
Batch Size: 1
Validation Error: 1.2588371418837982
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 151 
Batch Size: 1
Validation Error: 1.2599073933122142
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 152 
Batch Size: 1
Validation Error: 1.2620323975639
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 153 
Batch Size: 1
Validation Error: 1.2626136170435616
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 154 
Batch Size: 1
Validation Error: 1.2636241295416606
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 155 
Batch Size: 1
Validation Error: 1.263816408527282
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 156 
Batch Size: 1
Validation Error: 1.2639716133327603
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 157 
Batch Size: 40
Validation Error: 1.2643771506937374
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 158 
Batch Size: 1
Validation Error: 1.264593799789289
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 159 
Batch Size: 1
Validation Error: 1.264922368562286
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 160 
Batch Size: 1
Validation Error: 1.2651656399428712
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 161 
Batch Size: 40
Validation Error: 1.265235182091414
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 162 
Batch Size: 1
Validation Error: 1.265558251680514
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 163 
Batch Size: 1
Validation Error: 1.2660097809750488
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 164 
Batch Size: 1
Validation Error: 1.2674622486827103
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 165 
Batch Size: 1
Validation Error: 1.267951916728628
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 166 
Batch Size: 1
Validation Error: 1.2679615995729927
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 167 
Batch Size: 1
Validation Error: 1.268298685252023
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 168 
Batch Size: 1
Validation Error: 1.268810819564505
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 169 
Batch Size: 1
Validation Error: 1.2692896516056988
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 170 
Batch Size: 1
Validation Error: 1.2710948535147066
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 171 
Batch Size: 1
Validation Error: 1.2711269170810067
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 172 
Batch Size: 1
Validation Error: 1.2715503760179097
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 173 
Batch Size: 1
Validation Error: 1.273605701785798
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 174 
Batch Size: 1
Validation Error: 1.2740556043374558
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 175 
Batch Size: 1
Validation Error: 1.2753479184547662
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 176 
Batch Size: 1
Validation Error: 1.2754778001743798
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 177 
Batch Size: 1
Validation Error: 1.276427096642246
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 178 
Batch Size: 1
Validation Error: 1.2764331165442189
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 179 
Batch Size: 1
Validation Error: 1.2764777595713923
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 180 
Batch Size: 1
Validation Error: 1.276609209817765
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 181 
Batch Size: 1
Validation Error: 1.276852712490531
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 182 
Batch Size: 1
Validation Error: 1.279246297410312
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 183 
Batch Size: 1
Validation Error: 1.2805340189044527
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 184 
Batch Size: 1
Validation Error: 1.2805751386185915
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 185 
Batch Size: 1
Validation Error: 1.2806161877317692
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 186 
Batch Size: 1
Validation Error: 1.2808728196878467
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 187 
Batch Size: 1
Validation Error: 1.2811084906376098
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 188 
Batch Size: 1
Validation Error: 1.2826112755091845
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 189 
Batch Size: 1
Validation Error: 1.2826961883020946
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 190 
Batch Size: 1
Validation Error: 1.2831753540075705
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 191 
Batch Size: 1
Validation Error: 1.284134715780367
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 192 
Batch Size: 1
Validation Error: 1.284300393667356
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 193 
Batch Size: 1
Validation Error: 1.2848857128346531
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 194 
Batch Size: 1
Validation Error: 1.2854634111557086
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 195 
Batch Size: 1
Validation Error: 1.285995931346995
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 196 
Batch Size: 1
Validation Error: 1.2873146915148008
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 197 
Batch Size: 1
Validation Error: 1.2882479428940496
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 198 
Batch Size: 1
Validation Error: 1.288989824438326
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 199 
Batch Size: 1
Validation Error: 1.2895620872335312
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 200 
Batch Size: 1
Validation Error: 1.2902789588235972
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 201 
Batch Size: 1
Validation Error: 1.2905964887257155
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 202 
Batch Size: 1
Validation Error: 1.290872358239169
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 203 
Batch Size: 1
Validation Error: 1.291094030644991
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 204 
Batch Size: 1
Validation Error: 1.2915531418452009
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 205 
Batch Size: 1
Validation Error: 1.2916583309472711
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 206 
Batch Size: 1
Validation Error: 1.292636637998795
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 207 
Batch Size: 1
Validation Error: 1.2927992681437943
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 208 
Batch Size: 1
Validation Error: 1.293951998425987
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 209 
Batch Size: 1
Validation Error: 1.2940174625567953
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 210 
Batch Size: 1
Validation Error: 1.2943741820145755
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 211 
Batch Size: 1
Validation Error: 1.2948951449350385
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 212 
Batch Size: 40
Validation Error: 1.295718400934931
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 213 
Batch Size: 1
Validation Error: 1.2964583593995254
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 214 
Batch Size: 1
Validation Error: 1.297100972404142
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 215 
Batch Size: 1
Validation Error: 1.2973886673429373
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 216 
Batch Size: 1
Validation Error: 1.298236931597369
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 217 
Batch Size: 1
Validation Error: 1.299236071454945
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 218 
Batch Size: 1
Validation Error: 1.299673037189588
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 219 
Batch Size: 1
Validation Error: 1.299968740780601
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 220 
Batch Size: 1
Validation Error: 1.3002459852070742
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 221 
Batch Size: 1
Validation Error: 1.3004147305671343
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 222 
Batch Size: 1
Validation Error: 1.3010209583945074
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 223 
Batch Size: 1
Validation Error: 1.301480581850901
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 224 
Batch Size: 1
Validation Error: 1.3018854504024961
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 225 
Batch Size: 1
Validation Error: 1.3038982094391676
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 226 
Batch Size: 1
Validation Error: 1.304882300203722
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 227 
Batch Size: 1
Validation Error: 1.3051478421184606
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 228 
Batch Size: 1
Validation Error: 1.305933080294292
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 229 
Batch Size: 1
Validation Error: 1.3079397890997764
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 230 
Batch Size: 1
Validation Error: 1.3083112476034486
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 231 
Batch Size: 1
Validation Error: 1.308627212680871
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 232 
Batch Size: 1
Validation Error: 1.3092446421487245
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 233 
Batch Size: 1
Validation Error: 1.3095828661211644
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 234 
Batch Size: 1
Validation Error: 1.3096080433458728
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 235 
Batch Size: 1
Validation Error: 1.3098679320893603
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 236 
Batch Size: 1
Validation Error: 1.3101248203514684
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 237 
Batch Size: 1
Validation Error: 1.3110521150627086
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 238 
Batch Size: 1
Validation Error: 1.3118561877568864
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 239 
Batch Size: 1
Validation Error: 1.3137459367104085
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 240 
Batch Size: 1
Validation Error: 1.3140777413004021
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 241 
Batch Size: 1
Validation Error: 1.3141958047690443
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 242 
Batch Size: 1
Validation Error: 1.3148922617049015
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 243 
Batch Size: 1
Validation Error: 1.316205970123182
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 244 
Batch Size: 1
Validation Error: 1.3167456267483655
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 245 
Batch Size: 1
Validation Error: 1.317883418483323
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 246 
Batch Size: 1
Validation Error: 1.317930889232959
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 247 
Batch Size: 1
Validation Error: 1.3184233048718685
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 248 
Batch Size: 1
Validation Error: 1.318440413585901
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 249 
Batch Size: 1
Validation Error: 1.3195238234142264
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 250 
Batch Size: 1
Validation Error: 1.319718051118046
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 251 
Batch Size: 1
Validation Error: 1.3199693726936006
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 252 
Batch Size: 1
Validation Error: 1.320552177756144
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 253 
Batch Size: 1
Validation Error: 1.3217690326109346
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 254 
Batch Size: 1
Validation Error: 1.32197331715456
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 255 
Batch Size: 1
Validation Error: 1.3233596002637475
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 256 
Batch Size: 1
Validation Error: 1.323689133308126
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 257 
Batch Size: 1
Validation Error: 1.3251876693505762
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 258 
Batch Size: 1
Validation Error: 1.325499047538931
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 259 
Batch Size: 1
Validation Error: 1.325550865568011
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 260 
Batch Size: 1
Validation Error: 1.3257867715649843
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 261 
Batch Size: 1
Validation Error: 1.3273281832350337
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 262 
Batch Size: 1
Validation Error: 1.3277844546073485
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 263 
Batch Size: 1
Validation Error: 1.3297580226572818
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 264 
Batch Size: 1
Validation Error: 1.329772939745829
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 265 
Batch Size: 1
Validation Error: 1.3308416429718044
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 266 
Batch Size: 1
Validation Error: 1.3313937175748822
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 267 
Batch Size: 1
Validation Error: 1.3322277985387783
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 268 
Batch Size: 1
Validation Error: 1.332404437608253
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 269 
Batch Size: 1
Validation Error: 1.3332182007018685
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 270 
Batch Size: 1
Validation Error: 1.3337383317678273
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 271 
Batch Size: 1
Validation Error: 1.3338457575778413
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 272 
Batch Size: 1
Validation Error: 1.335541992178773
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 273 
Batch Size: 1
Validation Error: 1.3359490886129353
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 274 
Batch Size: 1
Validation Error: 1.3381262986475084
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 275 
Batch Size: 1
Validation Error: 1.3384713015160852
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 276 
Batch Size: 1
Validation Error: 1.3388492997646262
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 277 
Batch Size: 1
Validation Error: 1.3389301008493906
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 278 
Batch Size: 1
Validation Error: 1.3391886324946862
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 279 
Batch Size: 1
Validation Error: 1.339379529338346
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 280 
Batch Size: 1
Validation Error: 1.3397269166144965
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 281 
Batch Size: 1
Validation Error: 1.3398135070788046
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 282 
Batch Size: 1
Validation Error: 1.340845677075815
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 283 
Batch Size: 1
Validation Error: 1.3409297051893727
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 284 
Batch Size: 1
Validation Error: 1.3434880803148217
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 285 
Batch Size: 1
Validation Error: 1.3436638930914484
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 286 
Batch Size: 1
Validation Error: 1.3441111937817742
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 287 
Batch Size: 1
Validation Error: 1.3453846371994673
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 288 
Batch Size: 1
Validation Error: 1.345465901932225
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 289 
Batch Size: 1
Validation Error: 1.3458042886253885
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 290 
Batch Size: 1
Validation Error: 1.3458671538942457
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 291 
Batch Size: 1
Validation Error: 1.346050829401372
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 292 
Batch Size: 1
Validation Error: 1.3460969351499172
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 293 
Batch Size: 1
Validation Error: 1.3463087809663334
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 294 
Batch Size: 1
Validation Error: 1.3477524543964479
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 295 
Batch Size: 1
Validation Error: 1.3508549062761275
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 296 
Batch Size: 1
Validation Error: 1.3525207035362912
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 297 
Batch Size: 1
Validation Error: 1.3537222036417458
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 298 
Batch Size: 1
Validation Error: 1.354191108749952
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 299 
Batch Size: 1
Validation Error: 1.354708270714508
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 300 
Batch Size: 1
Validation Error: 1.3547662154811364
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 301 
Batch Size: 1
Validation Error: 1.356221043556204
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 302 
Batch Size: 1
Validation Error: 1.3564091026841978
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 303 
Batch Size: 1
Validation Error: 1.3567428515074416
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 304 
Batch Size: 1
Validation Error: 1.3570046341744098
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 305 
Batch Size: 1
Validation Error: 1.3580180295877486
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 306 
Batch Size: 1
Validation Error: 1.3586591743778138
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 307 
Batch Size: 1
Validation Error: 1.3589441583584196
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 308 
Batch Size: 1
Validation Error: 1.3590307384740377
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 309 
Batch Size: 1
Validation Error: 1.3602674057151918
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 310 
Batch Size: 1
Validation Error: 1.3603024427296486
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 311 
Batch Size: 1
Validation Error: 1.3612788334485164
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 312 
Batch Size: 1
Validation Error: 1.3619006680910257
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 313 
Batch Size: 1
Validation Error: 1.362236058088801
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 314 
Batch Size: 1
Validation Error: 1.362592718808844
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 315 
Batch Size: 1
Validation Error: 1.3635501729577044
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 316 
Batch Size: 1
Validation Error: 1.3649298549079976
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 317 
Batch Size: 1
Validation Error: 1.365097672107319
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 318 
Batch Size: 1
Validation Error: 1.3656127108068348
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 319 
Batch Size: 1
Validation Error: 1.366025004777121
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 320 
Batch Size: 1
Validation Error: 1.3666020259085234
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 321 
Batch Size: 1
Validation Error: 1.3672466984097167
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 322 
Batch Size: 1
Validation Error: 1.3684297700406578
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 323 
Batch Size: 1
Validation Error: 1.3687887889807424
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 324 
Batch Size: 1
Validation Error: 1.3691719330434307
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 325 
Batch Size: 1
Validation Error: 1.370653161087099
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 326 
Batch Size: 1
Validation Error: 1.3716045211390753
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 327 
Batch Size: 1
Validation Error: 1.374219926678281
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 328 
Batch Size: 1
Validation Error: 1.3753879331679388
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 329 
Batch Size: 1
Validation Error: 1.3760058659646286
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 330 
Batch Size: 1
Validation Error: 1.3763443673526972
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 331 
Batch Size: 1
Validation Error: 1.3764627116110655
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 332 
Batch Size: 1
Validation Error: 1.3777466158194083
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 333 
Batch Size: 1
Validation Error: 1.3778490647737562
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 334 
Batch Size: 1
Validation Error: 1.3779380531019654
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 335 
Batch Size: 1
Validation Error: 1.3786363562785269
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 336 
Batch Size: 1
Validation Error: 1.37866353722825
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 337 
Batch Size: 1
Validation Error: 1.3802158372458309
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 338 
Batch Size: 1
Validation Error: 1.3816476744597417
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 339 
Batch Size: 1
Validation Error: 1.3828811486208583
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 340 
Batch Size: 1
Validation Error: 1.3837976388832522
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 341 
Batch Size: 1
Validation Error: 1.3842614424446902
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 342 
Batch Size: 1
Validation Error: 1.3864327012066027
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 343 
Batch Size: 1
Validation Error: 1.3866897084952872
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 344 
Batch Size: 1
Validation Error: 1.386803029222536
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 345 
Batch Size: 1
Validation Error: 1.3870511075182326
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 346 
Batch Size: 1
Validation Error: 1.3871556734591202
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 347 
Batch Size: 1
Validation Error: 1.3872821886028153
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 348 
Batch Size: 1
Validation Error: 1.3873422150267103
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 349 
Batch Size: 1
Validation Error: 1.3887109241817583
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 350 
Batch Size: 1
Validation Error: 1.3890482799622383
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 351 
Batch Size: 1
Validation Error: 1.3908772336479207
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 352 
Batch Size: 1
Validation Error: 1.3916204637677283
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 353 
Batch Size: 1
Validation Error: 1.3919563111829354
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 354 
Batch Size: 1
Validation Error: 1.3924278391739007
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 355 
Batch Size: 1
Validation Error: 1.39332550794894
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 356 
Batch Size: 1
Validation Error: 1.3939347217326674
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 357 
Batch Size: 1
Validation Error: 1.3947273039040158
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 358 
Batch Size: 1
Validation Error: 1.3957076350814854
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 359 
Batch Size: 1
Validation Error: 1.3961857725204463
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 360 
Batch Size: 1
Validation Error: 1.3965800781504087
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 361 
Batch Size: 1
Validation Error: 1.3973173515829214
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 362 
Batch Size: 1
Validation Error: 1.3977973911858883
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 363 
Batch Size: 1
Validation Error: 1.398346984015911
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 364 
Batch Size: 1
Validation Error: 1.399869899493045
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 365 
Batch Size: 1
Validation Error: 1.4009556224092408
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 366 
Batch Size: 1
Validation Error: 1.4017375428452576
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 367 
Batch Size: 1
Validation Error: 1.4019389962424722
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 368 
Batch Size: 1
Validation Error: 1.4021148131624674
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 369 
Batch Size: 1
Validation Error: 1.4022518404196784
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 370 
Batch Size: 1
Validation Error: 1.402810241896786
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 371 
Batch Size: 1
Validation Error: 1.4038135746173004
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.001
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 372 
Batch Size: 1
Validation Error: 1.4043217712932234
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 256
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 373 
Batch Size: 1
Validation Error: 1.4051731522987327
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 374 
Batch Size: 1
Validation Error: 1.405381536349616
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 375 
Batch Size: 1
Validation Error: 1.4058821782779614
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 376 
Batch Size: 1
Validation Error: 1.4070746319530403
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 256
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 256
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 256
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 377 
Batch Size: 1
Validation Error: 1.4071200849579895
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.001
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------

 Best configuration after Hyperband n: 378 
Batch Size: 1
Validation Error: 1.4077475407934161
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 2:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 3:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08
  Optimizer 4:
    opt_type: adam
    learning_rate: 0.0005
    momentum: None
    beta_1: 0.9
    beta_2: 0.999
    epsilon: 1e-08

--------------------------------------------------
