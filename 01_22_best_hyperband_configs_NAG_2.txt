
 Best configuration after Hyperband n: 1 
Batch Size: 1
Validation Error: 2.6233547755354296
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 2 
Batch Size: 1
Validation Error: 2.6398105018795768
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 3 
Batch Size: 1
Validation Error: 2.6783529386361513
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 4 
Batch Size: 1
Validation Error: 2.7614733437176233
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 5 
Batch Size: 1
Validation Error: 2.854900061647907
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 6 
Batch Size: 40
Validation Error: 2.908181568648454
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 7 
Batch Size: 1
Validation Error: 2.9157490780076163
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 8 
Batch Size: 1
Validation Error: 2.9413470149873597
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 9 
Batch Size: 40
Validation Error: 2.945144686441337
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 10 
Batch Size: 1
Validation Error: 2.9511409324393307
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 11 
Batch Size: 40
Validation Error: 3.023855500145971
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 12 
Batch Size: 40
Validation Error: 3.0288799624659695
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 13 
Batch Size: 1
Validation Error: 3.0337774397011543
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 14 
Batch Size: 40
Validation Error: 3.050897423228087
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 15 
Batch Size: 40
Validation Error: 3.061126609637488
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 16 
Batch Size: 40
Validation Error: 3.0929804081130343
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 17 
Batch Size: 1
Validation Error: 3.1164514714046003
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 18 
Batch Size: 1
Validation Error: 3.1296007506588404
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 19 
Batch Size: 40
Validation Error: 3.131406058557471
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 20 
Batch Size: 1
Validation Error: 3.1375772026247737
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 21 
Batch Size: 40
Validation Error: 3.1448701098715697
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.0001
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 22 
Batch Size: 40
Validation Error: 3.172507114121967
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 23 
Batch Size: 40
Validation Error: 3.174773246444794
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 24 
Batch Size: 1
Validation Error: 3.1811029139646734
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 25 
Batch Size: 40
Validation Error: 3.182700048586041
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 26 
Batch Size: 1
Validation Error: 3.19459531927926
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 27 
Batch Size: 40
Validation Error: 3.195233340333992
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 28 
Batch Size: 40
Validation Error: 3.2003184220996923
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 29 
Batch Size: 40
Validation Error: 3.2061609174771997
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.0001
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 30 
Batch Size: 40
Validation Error: 3.2068483796621585
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------
