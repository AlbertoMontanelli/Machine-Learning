{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zBZ8ZOoCBL"
      },
      "source": [
        "# Cose da fare\n",
        "* inserire lo shuffling per zig-zag gradient descent \n",
        "* (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
        "* inserire loss function per problemi di classificazione: BCE o altro\n",
        "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
        "* analisi training error vs validation error vs test error\n",
        "* analisi sulla scelta di K in base a variance/bias.\n",
        "### Novelties\n",
        "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
        "* early stopping\n",
        "* standarditation e normalization (FACOLTATIVO FORSE)\n",
        "# Cose da fare secondo le (!) Micheli\n",
        "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qv1wwlgWsFM8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n",
        "\n",
        "# np.vectorize returns an object that acts like pyfunc, but takes arrays as input\n",
        "d_ReLU = np.vectorize(d_ReLU)\n",
        "d_leaky_relu = np.vectorize(d_leaky_relu)\n",
        "ELU = np.vectorize(ELU)\n",
        "d_ELU = np.vectorize(d_ELU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return - 2 * (y_true - y_pred)\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return - (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return - (y_true - y_pred) if(np.abs(y_true-y_pred)<=delta) else - delta * np.sign(y_true-y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters:**\n",
        " * input_size: $k_{i-1}$;\n",
        " * output_size: $k_i$;\n",
        " * activation_function;\n",
        " * activation_derivative.\n",
        "\n",
        "**Constructor attributes:**\n",
        "* self.input_size = input_size;\n",
        "* self.output_size = output_size;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative;\n",
        "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created;\n",
        "* self.t: number of iterations for Adam.\n",
        "\n",
        "**Methods :**\n",
        "\n",
        "* **initialize_weights**: initialize weights and biases\n",
        "  * attributes:\n",
        "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
        "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
        "    * self.velocity_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Nesterov optimization;\n",
        "    * self.velocity_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Nesterov optimization;\n",
        "    * self.m_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.v_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.m_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.v_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    \n",
        "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
        "  * parameter:\n",
        "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
        "  * attributes:\n",
        "    * self.input: input_array;\n",
        "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
        "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
        "\n",
        "* **Regularization_func**: computes the regularization term using Tikhonov, Lasso or Elastic rule\n",
        "  * parameter:\n",
        "    * Lambda_t: constant used in tikhonov regularization;\n",
        "    * Lambda_l: constant used in Lasso regularization;\n",
        "    * ww: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. It is a placeholder for the kind of weights that are used in the computation of regularization: in case of NAG optimization, we use the predictions of the weights; in case of adam optimization, we use self.weights. \n",
        "    * reg_type: one of the possible string: \"tikhonov\", \"lasso\" and \"elastic\".\n",
        "  \n",
        "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        "  * parameters:\n",
        "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "    * learning_rate.\n",
        "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "H8Ap9rLxLlNU",
        "outputId": "da307f31-1bbc-4d71-826e-718b36ac131a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "        self.initialize_weights()\n",
        "        self.t = 1 \n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialization of the parameters of the network\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
        "        self.biases = np.zeros((1, self.output_size))\n",
        "\n",
        "        # Initialization of the parameters for Nesterov optimization\n",
        "        self.velocity_weights = np.zeros_like(self.weights)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        # Initialization of the parameters for Adam optimization\n",
        "        self.m_weights = np.zeros_like(self.weights)\n",
        "        self.v_weights = np.zeros_like(self.weights)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "    def regularization_func(self, Lambda_t, Lambda_l, ww, reg_type):\n",
        "        if reg_type == 'tikhonov':\n",
        "            reg_term = 2 * Lambda_t * ww # learning rule of tikhonov regularization\n",
        "        elif reg_type == 'lasso':\n",
        "            reg_term = Lambda_l * np.sign(ww) # learning rule of lasso regularization\n",
        "        elif reg_type == 'elastic':\n",
        "            reg_term = (2 * Lambda_t * ww + Lambda_l * np.sign(ww)) # lasso + tikhonov regularization\n",
        "        return reg_term\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        self.delta = - d_Ep * self.activation_derivative(self.net)\n",
        "        if opt_type == 'NAG':\n",
        "            weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights in order to use the Nesterov momentum,\n",
        "                                                                            # used to evaluate the gradient after the momentum is applied\n",
        "            bias_pred = self.biases + momentum * self.velocity_biases # same thing for the biases\n",
        "            net_pred = np.dot(self.input, weights_pred) + bias_pred  #  Net has been computed with respect to the predicted weights and the predicted biases\n",
        "            delta_pred = - d_Ep * self.activation_derivative(net_pred)  # Loss gradient with respect to net, minus sign due to the definition\n",
        "            grad_weights = learning_rate_w * np.dot(self.input.T, delta_pred)  # Loss gradient multiplied by the learning rate.\n",
        "                                                                            # The gradient has been computed with respect to the predicted weights and biases\n",
        "            \n",
        "            reg_term = self.regularization_func(Lambda_t, Lambda_l, weights_pred, reg_type)\n",
        "            self.velocity_weights = momentum * self.velocity_weights + grad_weights - reg_term  # Delta w new \n",
        "                                                                                                # the minus sign before reg_term is due to the application of gradient descent algorithm.\n",
        "            self.weights += self.velocity_weights  # Updating the weights\n",
        "            self.velocity_biases = momentum * self.velocity_biases + learning_rate_b * np.sum(delta_pred, axis=0, keepdims=True)\n",
        "            self.biases += self.velocity_biases # Updating the biases\n",
        "\n",
        "        elif opt_type == 'adam':\n",
        "            reg_term = self.regularization_func(Lambda_t, Lambda_l, self.weights, reg_type)\n",
        "            self.m_weights = beta_1 * self.m_weights + (1 - beta_1) * (- np.dot(self.input.T, self.delta) - reg_term) # np.dot(self.input.T, delta) is dLoss/dw,\n",
        "                                                                                                                        # since self.delta is defined with a minus sign\n",
        "                                                                                                                        # and the formula is with a plus sign, we put a minus sign\n",
        "                                                                                                                        # in front of np.dot(xxx)\n",
        "            self.v_weights = beta_2* self.v_weights + (1 - beta_2) * ((- np.dot(self.input.T, self.delta) - reg_term)**2) # here we have a plus sign in front of (1 - beta_2) since\n",
        "                                                                                                                        # self.delta is squared\n",
        "            m_weights_hat = self.m_weights / (1 - beta_1**self.t)\n",
        "            v_weights_hat = self.v_weights / (1 - beta_2**self.t)\n",
        "\n",
        "            self.m_biases = beta_1 * self.m_biases - (1 - beta_1) * np.sum(self.delta, axis=0, keepdims=True)\n",
        "            self.v_biases = beta_2* self.v_biases + (1 - beta_2) * np.sum(self.delta**2, axis=0, keepdims=True)\n",
        "            m_biases_hat = self.m_biases / (1 - beta_1**self.t)\n",
        "            v_biases_hat = self.v_biases / (1 - beta_2**self.t)\n",
        "\n",
        "            self.weights -= learning_rate_w * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "            self.biases -= learning_rate_b * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)\n",
        "\n",
        "        sum_delta_weights = np.dot(self.delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
        "   * parameters:\n",
        "     * x_tot: total data given as input;\n",
        "     * target: total data labels given as input;\n",
        "     * test_split: percentile of test set with respect to the total data.\n",
        "    * return->:\n",
        "      * x_train_val: training & validation set extracted from input data;\n",
        "      * target_train_val: training & validation set labels;\n",
        "      * x_test_val: test set extracted from input data;\n",
        "      * target_test_val: test set for input data labels.\n",
        "\n",
        "     \n",
        "* **add_layer**: appends a layer to the empty list self.layers\n",
        "   * parameter:\n",
        "     * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        "  * parameter:\n",
        "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        "  * return -> input = $o_i$ for layer $L_i$.\n",
        "  \n",
        "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        "  * parameters:\n",
        "    * d_Ep;\n",
        "    * learning_rate.\n",
        "\n",
        "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
        "\n",
        "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
        "  * parameters:\n",
        "    * x_train: set of the original dataset used for training;\n",
        "    * target_train: labels corresponding to the training set;\n",
        "    * x_val: set of the original dataset used for validation;\n",
        "    * target_val: labels corresponding to the validation set;\n",
        "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
        "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
        "    * loss_function: hyperparameter;\n",
        "    * loss_function_derivative: hyperparameter;\n",
        "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
        "\n",
        "* **train_val**: actual training and validation process.\n",
        "  * parameters:\n",
        "    * x_train_val;\n",
        "    * target_train_val;\n",
        "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
        "    * epochs;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def data_split(self, x_tot, target, test_split):\n",
        "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
        "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
        "\n",
        "        x_test = x_tot[:test_size]\n",
        "        target_test = target[:test_size]\n",
        "        x_train_val = x_tot[test_size:]\n",
        "        target_train_val = target[test_size:]\n",
        "\n",
        "        return x_train_val, target_train_val, x_test, target_test\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "\n",
        "    def reinitialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.initialize_weights() # does it layer-by-layer\n",
        "\n",
        "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        train_error_epoch = np.zeros(epochs)\n",
        "        val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
        "            epoch_val_loss = 0\n",
        "\n",
        "            '''\n",
        "            # shuffling training data before splitting it into batches.\n",
        "            # done in order to avoid reinforcing neurons in the same way\n",
        "            # in different epochs due to invisible patterns in the data\n",
        "            train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(train_indices)\n",
        "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
        "            target_train = target_train[train_indices]\n",
        "            '''\n",
        "\n",
        "            # if batch_size=1 we get the online version,\n",
        "            # else we get mini-batch version with batches of size batch_size\n",
        "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
        "                                                             # Even if the number of examples is not divisible\n",
        "                                                             # for batch_size the last, smaller batch is processed anyway\n",
        "\n",
        "                x_batch = x_train[i:i+batch_size]\n",
        "                target_batch = target_train[i:i+batch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
        "                # computing loss and gradient\n",
        "                loss = loss_function(target_batch, predictions)\n",
        "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "                epoch_tr_loss += np.sum(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(loss_gradient, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "                Layer.t += 1 # number of iterations for adam\n",
        "\n",
        "            # validation\n",
        "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
        "            val_loss = loss_function(target_val, val_predictions)\n",
        "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished.\n",
        "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
        "            # Store average errors for the epoch\n",
        "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
        "            val_error_epoch[epoch] = epoch_val_loss\n",
        "\n",
        "        return train_error_epoch, val_error_epoch\n",
        "\n",
        "\n",
        "    def train_val(self, x_train_val, target_train_val, train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        num_samples = x_train_val.shape[0]\n",
        "        fold_size = num_samples // K\n",
        "\n",
        "        # error storage for averaging\n",
        "        avg_train_error_epoch = np.zeros(epochs)\n",
        "        avg_val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        if K==1: # hold-out validation\n",
        "            train_indices = np.arange(0, int(train_split*num_samples)) # training set is 75% of the training & validation set\n",
        "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            ) # computation of errors via train_val_setup method\n",
        "            return train_error_epoch, val_error_epoch\n",
        "\n",
        "        for k in range(K):\n",
        "            # creating fold indices\n",
        "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
        "                                                                        # It contains the indices of the examples used in validation set for\n",
        "                                                                        # this fold.\n",
        "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples -\n",
        "                                                                              # len(val_indices). It contains the indices of all the examples\n",
        "                                                                              # but the ones used in the validation set for this fold.\n",
        "                                                                              # It corresponds to the training set for the current fold.\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
        "\n",
        "            # shuffle \n",
        "            new_train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(new_train_indices)\n",
        "            x_train = x_train[new_train_indices]\n",
        "            target_train = target_train[new_train_indices]\n",
        "\n",
        "            '''\n",
        "            new_val_indices = np.arange(x_val.shape[0])\n",
        "            np.random.shuffle(new_val_indices)\n",
        "            x_val = x_val[new_val_indices]\n",
        "            target_val = target_val[new_val_indices]\n",
        "            print(f\"x val \\n {x_val}\")\n",
        "            '''\n",
        "            \n",
        "            # re-initializing weights for each fold\n",
        "            self.reinitialize_weights()\n",
        "            Layer.t = 1\n",
        "            # training on the current fold\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            )\n",
        "\n",
        "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
        "            avg_train_error_epoch += train_error_epoch\n",
        "            avg_val_error_epoch += val_error_epoch\n",
        "            print(f\"Fold {k+1} completed, t = {Layer.t}\")\n",
        "\n",
        "        # averaging errors across all folds\n",
        "        avg_train_error_epoch /= K\n",
        "        avg_val_error_epoch /= K\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
        "\n",
        "        return avg_train_error_epoch, avg_val_error_epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "# Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "5be15b38-62a4-439a-dd86-57790daaffe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 completed, t = 12289\n",
            "Fold 2 completed, t = 12289\n",
            "Fold 3 completed, t = 12289\n",
            "Fold 4 completed, t = 12289\n",
            "Fold 5 completed, t = 12289\n",
            "Epoch 1/2048, Avg Training Loss: 0.9198747713316746, Avg Validation Loss: 38.76178825383559\n",
            "Epoch 2/2048, Avg Training Loss: 0.9054487886202647, Avg Validation Loss: 38.12900004402255\n",
            "Epoch 3/2048, Avg Training Loss: 0.8896618700146333, Avg Validation Loss: 37.51085225791679\n",
            "Epoch 4/2048, Avg Training Loss: 0.8745971790134119, Avg Validation Loss: 36.93250877485148\n",
            "Epoch 5/2048, Avg Training Loss: 0.8605646128544072, Avg Validation Loss: 36.39526125879878\n",
            "Epoch 6/2048, Avg Training Loss: 0.8475714042273071, Avg Validation Loss: 35.89812016594529\n",
            "Epoch 7/2048, Avg Training Loss: 0.8355280961248622, Avg Validation Loss: 35.43491641861492\n",
            "Epoch 8/2048, Avg Training Loss: 0.8242905349858564, Avg Validation Loss: 35.000975932851496\n",
            "Epoch 9/2048, Avg Training Loss: 0.8137703542667175, Avg Validation Loss: 34.59492915836741\n",
            "Epoch 10/2048, Avg Training Loss: 0.8038941560138233, Avg Validation Loss: 34.21086473221338\n",
            "Epoch 11/2048, Avg Training Loss: 0.7945059153144288, Avg Validation Loss: 33.84687132955501\n",
            "Epoch 12/2048, Avg Training Loss: 0.7855859521371686, Avg Validation Loss: 33.50209065672921\n",
            "Epoch 13/2048, Avg Training Loss: 0.7771212785446561, Avg Validation Loss: 33.176334235433345\n",
            "Epoch 14/2048, Avg Training Loss: 0.769089401045602, Avg Validation Loss: 32.86777922532362\n",
            "Epoch 15/2048, Avg Training Loss: 0.7614641040746365, Avg Validation Loss: 32.57568681221244\n",
            "Epoch 16/2048, Avg Training Loss: 0.7542272198720922, Avg Validation Loss: 32.29853480282978\n",
            "Epoch 17/2048, Avg Training Loss: 0.7473229350728123, Avg Validation Loss: 32.033761962837296\n",
            "Epoch 18/2048, Avg Training Loss: 0.7407103715835821, Avg Validation Loss: 31.780911038905188\n",
            "Epoch 19/2048, Avg Training Loss: 0.7343777558758967, Avg Validation Loss: 31.5389037099573\n",
            "Epoch 20/2048, Avg Training Loss: 0.7283003004866295, Avg Validation Loss: 31.30695699401608\n",
            "Epoch 21/2048, Avg Training Loss: 0.7224672706578678, Avg Validation Loss: 31.08455877559032\n",
            "Epoch 22/2048, Avg Training Loss: 0.7168604507272657, Avg Validation Loss: 30.87132280440152\n",
            "Epoch 23/2048, Avg Training Loss: 0.7114716386758771, Avg Validation Loss: 30.6665528005658\n",
            "Epoch 24/2048, Avg Training Loss: 0.7062820032972257, Avg Validation Loss: 30.4695273323747\n",
            "Epoch 25/2048, Avg Training Loss: 0.7012724320426285, Avg Validation Loss: 30.279170798618345\n",
            "Epoch 26/2048, Avg Training Loss: 0.6964191658687934, Avg Validation Loss: 30.09527672159967\n",
            "Epoch 27/2048, Avg Training Loss: 0.6917251352698813, Avg Validation Loss: 29.917887520808886\n",
            "Epoch 28/2048, Avg Training Loss: 0.687186249303929, Avg Validation Loss: 29.746373769545652\n",
            "Epoch 29/2048, Avg Training Loss: 0.6827936491259075, Avg Validation Loss: 29.5805505285139\n",
            "Epoch 30/2048, Avg Training Loss: 0.6785441876457359, Avg Validation Loss: 29.419984194168034\n",
            "Epoch 31/2048, Avg Training Loss: 0.6744235061124334, Avg Validation Loss: 29.264521750495177\n",
            "Epoch 32/2048, Avg Training Loss: 0.6704314393005292, Avg Validation Loss: 29.114175975388704\n",
            "Epoch 33/2048, Avg Training Loss: 0.6665658960630858, Avg Validation Loss: 28.96868904082042\n",
            "Epoch 34/2048, Avg Training Loss: 0.6628186710292416, Avg Validation Loss: 28.827675846848695\n",
            "Epoch 35/2048, Avg Training Loss: 0.6591815771334444, Avg Validation Loss: 28.690822569129015\n",
            "Epoch 36/2048, Avg Training Loss: 0.6556438966308304, Avg Validation Loss: 28.557506624955433\n",
            "Epoch 37/2048, Avg Training Loss: 0.6521963337400383, Avg Validation Loss: 28.427676134419404\n",
            "Epoch 38/2048, Avg Training Loss: 0.6488383510139505, Avg Validation Loss: 28.301441917172895\n",
            "Epoch 39/2048, Avg Training Loss: 0.645571565468653, Avg Validation Loss: 28.178803458046936\n",
            "Epoch 40/2048, Avg Training Loss: 0.6423936249776837, Avg Validation Loss: 28.059649469747452\n",
            "Epoch 41/2048, Avg Training Loss: 0.639303792100432, Avg Validation Loss: 27.944149396399148\n",
            "Epoch 42/2048, Avg Training Loss: 0.6363075321995988, Avg Validation Loss: 27.833399349832877\n",
            "Epoch 43/2048, Avg Training Loss: 0.6334250958416036, Avg Validation Loss: 27.726688875788852\n",
            "Epoch 44/2048, Avg Training Loss: 0.6306380943866479, Avg Validation Loss: 27.623343524605716\n",
            "Epoch 45/2048, Avg Training Loss: 0.627934503322041, Avg Validation Loss: 27.523185596729263\n",
            "Epoch 46/2048, Avg Training Loss: 0.6253079658524227, Avg Validation Loss: 27.425969356731553\n",
            "Epoch 47/2048, Avg Training Loss: 0.6227486085215278, Avg Validation Loss: 27.3313056298972\n",
            "Epoch 48/2048, Avg Training Loss: 0.6202527909715421, Avg Validation Loss: 27.239084589615032\n",
            "Epoch 49/2048, Avg Training Loss: 0.6178158113012832, Avg Validation Loss: 27.149155221120477\n",
            "Epoch 50/2048, Avg Training Loss: 0.6154349816577505, Avg Validation Loss: 27.06130283304675\n",
            "Epoch 51/2048, Avg Training Loss: 0.6131060549269142, Avg Validation Loss: 26.975530150353258\n",
            "Epoch 52/2048, Avg Training Loss: 0.6108299330342255, Avg Validation Loss: 26.891772746119898\n",
            "Epoch 53/2048, Avg Training Loss: 0.6085998826130212, Avg Validation Loss: 26.809690455552584\n",
            "Epoch 54/2048, Avg Training Loss: 0.6064124869887955, Avg Validation Loss: 26.729328399263142\n",
            "Epoch 55/2048, Avg Training Loss: 0.6042689278548329, Avg Validation Loss: 26.650594673949293\n",
            "Epoch 56/2048, Avg Training Loss: 0.6021680354031478, Avg Validation Loss: 26.573503926197994\n",
            "Epoch 57/2048, Avg Training Loss: 0.6001095976532768, Avg Validation Loss: 26.49804481816729\n",
            "Epoch 58/2048, Avg Training Loss: 0.5980941603268023, Avg Validation Loss: 26.424194843392787\n",
            "Epoch 59/2048, Avg Training Loss: 0.5961213760261385, Avg Validation Loss: 26.351898878236803\n",
            "Epoch 60/2048, Avg Training Loss: 0.5941899876396477, Avg Validation Loss: 26.28114517677708\n",
            "Epoch 61/2048, Avg Training Loss: 0.5922981400395521, Avg Validation Loss: 26.21183371783732\n",
            "Epoch 62/2048, Avg Training Loss: 0.5904440840007278, Avg Validation Loss: 26.143965421439\n",
            "Epoch 63/2048, Avg Training Loss: 0.5886281742364515, Avg Validation Loss: 26.077506302616047\n",
            "Epoch 64/2048, Avg Training Loss: 0.5868483533963964, Avg Validation Loss: 26.012402334366715\n",
            "Epoch 65/2048, Avg Training Loss: 0.5851039809290777, Avg Validation Loss: 25.948628099855775\n",
            "Epoch 66/2048, Avg Training Loss: 0.5833940846250996, Avg Validation Loss: 25.886111130372793\n",
            "Epoch 67/2048, Avg Training Loss: 0.5817174961160911, Avg Validation Loss: 25.8247816454098\n",
            "Epoch 68/2048, Avg Training Loss: 0.5800709150263706, Avg Validation Loss: 25.76427188912225\n",
            "Epoch 69/2048, Avg Training Loss: 0.5784503602339262, Avg Validation Loss: 25.70460481850428\n",
            "Epoch 70/2048, Avg Training Loss: 0.5768553996537522, Avg Validation Loss: 25.64579856764393\n",
            "Epoch 71/2048, Avg Training Loss: 0.5752858242052931, Avg Validation Loss: 25.58788029095619\n",
            "Epoch 72/2048, Avg Training Loss: 0.573740965598048, Avg Validation Loss: 25.53091852792272\n",
            "Epoch 73/2048, Avg Training Loss: 0.5722231896899321, Avg Validation Loss: 25.475015129529872\n",
            "Epoch 74/2048, Avg Training Loss: 0.5707331318305217, Avg Validation Loss: 25.42008126852241\n",
            "Epoch 75/2048, Avg Training Loss: 0.5692688086685751, Avg Validation Loss: 25.366025974871683\n",
            "Epoch 76/2048, Avg Training Loss: 0.5678265583847351, Avg Validation Loss: 25.31263166313638\n",
            "Epoch 77/2048, Avg Training Loss: 0.5664037103398041, Avg Validation Loss: 25.259962557152896\n",
            "Epoch 78/2048, Avg Training Loss: 0.5650014824451663, Avg Validation Loss: 25.208048501704894\n",
            "Epoch 79/2048, Avg Training Loss: 0.5636205928130399, Avg Validation Loss: 25.156986511745433\n",
            "Epoch 80/2048, Avg Training Loss: 0.562260251110316, Avg Validation Loss: 25.106431025439623\n",
            "Epoch 81/2048, Avg Training Loss: 0.5609168321713268, Avg Validation Loss: 25.056388627203678\n",
            "Epoch 82/2048, Avg Training Loss: 0.5595907801219827, Avg Validation Loss: 25.007044868381616\n",
            "Epoch 83/2048, Avg Training Loss: 0.5582851916389808, Avg Validation Loss: 24.958447170255802\n",
            "Epoch 84/2048, Avg Training Loss: 0.5569999452970548, Avg Validation Loss: 24.91056181067739\n",
            "Epoch 85/2048, Avg Training Loss: 0.5557338996193687, Avg Validation Loss: 24.863342272298635\n",
            "Epoch 86/2048, Avg Training Loss: 0.5544860880898776, Avg Validation Loss: 24.81675669184777\n",
            "Epoch 87/2048, Avg Training Loss: 0.5532559571236576, Avg Validation Loss: 24.770824023188524\n",
            "Epoch 88/2048, Avg Training Loss: 0.5520441843100918, Avg Validation Loss: 24.725530786554984\n",
            "Epoch 89/2048, Avg Training Loss: 0.5508497697357491, Avg Validation Loss: 24.680834871421485\n",
            "Epoch 90/2048, Avg Training Loss: 0.5496721339720425, Avg Validation Loss: 24.636751169469797\n",
            "Epoch 91/2048, Avg Training Loss: 0.5485109637278707, Avg Validation Loss: 24.59323574112044\n",
            "Epoch 92/2048, Avg Training Loss: 0.5473653846394344, Avg Validation Loss: 24.550259503257617\n",
            "Epoch 93/2048, Avg Training Loss: 0.5462341800931654, Avg Validation Loss: 24.507421578226484\n",
            "Epoch 94/2048, Avg Training Loss: 0.5451069617015963, Avg Validation Loss: 24.464521806050612\n",
            "Epoch 95/2048, Avg Training Loss: 0.5439856553386405, Avg Validation Loss: 24.421811819049687\n",
            "Epoch 96/2048, Avg Training Loss: 0.5428737489072107, Avg Validation Loss: 24.379267275001244\n",
            "Epoch 97/2048, Avg Training Loss: 0.5417680578287444, Avg Validation Loss: 24.336827107760115\n",
            "Epoch 98/2048, Avg Training Loss: 0.5406702689325644, Avg Validation Loss: 24.29452554786644\n",
            "Epoch 99/2048, Avg Training Loss: 0.5395803683018998, Avg Validation Loss: 24.252534104856444\n",
            "Epoch 100/2048, Avg Training Loss: 0.5384990608593928, Avg Validation Loss: 24.210537341000062\n",
            "Epoch 101/2048, Avg Training Loss: 0.537420064218375, Avg Validation Loss: 24.168182459971984\n",
            "Epoch 102/2048, Avg Training Loss: 0.536340983548862, Avg Validation Loss: 24.125768077089297\n",
            "Epoch 103/2048, Avg Training Loss: 0.5352669228217545, Avg Validation Loss: 24.083519377348953\n",
            "Epoch 104/2048, Avg Training Loss: 0.5342008708755934, Avg Validation Loss: 24.041549137954597\n",
            "Epoch 105/2048, Avg Training Loss: 0.5331471074457367, Avg Validation Loss: 24.000283352854744\n",
            "Epoch 106/2048, Avg Training Loss: 0.53210824862984, Avg Validation Loss: 23.959476113340763\n",
            "Epoch 107/2048, Avg Training Loss: 0.531080749621067, Avg Validation Loss: 23.918908249033176\n",
            "Epoch 108/2048, Avg Training Loss: 0.5300609011252424, Avg Validation Loss: 23.878495700884503\n",
            "Epoch 109/2048, Avg Training Loss: 0.5290463488703735, Avg Validation Loss: 23.838152882786403\n",
            "Epoch 110/2048, Avg Training Loss: 0.5280380782246309, Avg Validation Loss: 23.798017795546002\n",
            "Epoch 111/2048, Avg Training Loss: 0.5270372917468866, Avg Validation Loss: 23.757979950027522\n",
            "Epoch 112/2048, Avg Training Loss: 0.5260416932836748, Avg Validation Loss: 23.71813997613442\n",
            "Epoch 113/2048, Avg Training Loss: 0.5250526296193354, Avg Validation Loss: 23.678345718941305\n",
            "Epoch 114/2048, Avg Training Loss: 0.5240691870017983, Avg Validation Loss: 23.638740000299112\n",
            "Epoch 115/2048, Avg Training Loss: 0.5230931186904767, Avg Validation Loss: 23.599395702598414\n",
            "Epoch 116/2048, Avg Training Loss: 0.5221247596889667, Avg Validation Loss: 23.56019868515567\n",
            "Epoch 117/2048, Avg Training Loss: 0.5211618256077291, Avg Validation Loss: 23.52118415860433\n",
            "Epoch 118/2048, Avg Training Loss: 0.5202057790003409, Avg Validation Loss: 23.48242315341043\n",
            "Epoch 119/2048, Avg Training Loss: 0.5192577635104726, Avg Validation Loss: 23.443984296980734\n",
            "Epoch 120/2048, Avg Training Loss: 0.5183185963983907, Avg Validation Loss: 23.405880401369437\n",
            "Epoch 121/2048, Avg Training Loss: 0.5173882092968993, Avg Validation Loss: 23.36795476494028\n",
            "Epoch 122/2048, Avg Training Loss: 0.5164626368010653, Avg Validation Loss: 23.330158634574545\n",
            "Epoch 123/2048, Avg Training Loss: 0.5155429049813371, Avg Validation Loss: 23.29257746679486\n",
            "Epoch 124/2048, Avg Training Loss: 0.5146301402792129, Avg Validation Loss: 23.255254349188995\n",
            "Epoch 125/2048, Avg Training Loss: 0.5137248730136937, Avg Validation Loss: 23.21820944658367\n",
            "Epoch 126/2048, Avg Training Loss: 0.5128273214776835, Avg Validation Loss: 23.18145094137643\n",
            "Epoch 127/2048, Avg Training Loss: 0.5119375783630662, Avg Validation Loss: 23.14500360291264\n",
            "Epoch 128/2048, Avg Training Loss: 0.5110580903363023, Avg Validation Loss: 23.109170936113774\n",
            "Epoch 129/2048, Avg Training Loss: 0.5101906868949092, Avg Validation Loss: 23.073807608803133\n",
            "Epoch 130/2048, Avg Training Loss: 0.5093332345974457, Avg Validation Loss: 23.038516445086376\n",
            "Epoch 131/2048, Avg Training Loss: 0.5084768397488776, Avg Validation Loss: 23.00315724738827\n",
            "Epoch 132/2048, Avg Training Loss: 0.5076228599613414, Avg Validation Loss: 22.96780433585424\n",
            "Epoch 133/2048, Avg Training Loss: 0.5067715493733143, Avg Validation Loss: 22.932286313130113\n",
            "Epoch 134/2048, Avg Training Loss: 0.5059184779557998, Avg Validation Loss: 22.89666442125648\n",
            "Epoch 135/2048, Avg Training Loss: 0.5050679023914788, Avg Validation Loss: 22.861155582938462\n",
            "Epoch 136/2048, Avg Training Loss: 0.5042225073463538, Avg Validation Loss: 22.826014538988122\n",
            "Epoch 137/2048, Avg Training Loss: 0.5033858581661467, Avg Validation Loss: 22.79111100087388\n",
            "Epoch 138/2048, Avg Training Loss: 0.5025561715716999, Avg Validation Loss: 22.756494885567037\n",
            "Epoch 139/2048, Avg Training Loss: 0.5017332282482747, Avg Validation Loss: 22.722114708237015\n",
            "Epoch 140/2048, Avg Training Loss: 0.5009163876487139, Avg Validation Loss: 22.687779557385635\n",
            "Epoch 141/2048, Avg Training Loss: 0.5000954882243156, Avg Validation Loss: 22.652709159577302\n",
            "Epoch 142/2048, Avg Training Loss: 0.4992672902551513, Avg Validation Loss: 22.61739598229819\n",
            "Epoch 143/2048, Avg Training Loss: 0.4984387620625433, Avg Validation Loss: 22.58209841929068\n",
            "Epoch 144/2048, Avg Training Loss: 0.49761344657712125, Avg Validation Loss: 22.546878643295685\n",
            "Epoch 145/2048, Avg Training Loss: 0.49679158828153613, Avg Validation Loss: 22.511788706079535\n",
            "Epoch 146/2048, Avg Training Loss: 0.49597470585298126, Avg Validation Loss: 22.47690714158337\n",
            "Epoch 147/2048, Avg Training Loss: 0.4951641829868928, Avg Validation Loss: 22.44232085972124\n",
            "Epoch 148/2048, Avg Training Loss: 0.4943608833580262, Avg Validation Loss: 22.408024694107198\n",
            "Epoch 149/2048, Avg Training Loss: 0.4935646331650319, Avg Validation Loss: 22.374013405349224\n",
            "Epoch 150/2048, Avg Training Loss: 0.49277528718935965, Avg Validation Loss: 22.34028184144178\n",
            "Epoch 151/2048, Avg Training Loss: 0.4919927174737169, Avg Validation Loss: 22.30682497341459\n",
            "Epoch 152/2048, Avg Training Loss: 0.49121670492302816, Avg Validation Loss: 22.273624703223884\n",
            "Epoch 153/2048, Avg Training Loss: 0.4904470359927576, Avg Validation Loss: 22.240682683097475\n",
            "Epoch 154/2048, Avg Training Loss: 0.4896837111785094, Avg Validation Loss: 22.207997840791098\n",
            "Epoch 155/2048, Avg Training Loss: 0.4889266866653752, Avg Validation Loss: 22.175567677331074\n",
            "Epoch 156/2048, Avg Training Loss: 0.48817593388730085, Avg Validation Loss: 22.143143697389345\n",
            "Epoch 157/2048, Avg Training Loss: 0.48742360475389723, Avg Validation Loss: 22.1104867656076\n",
            "Epoch 158/2048, Avg Training Loss: 0.4866708719073692, Avg Validation Loss: 22.077832924208515\n",
            "Epoch 159/2048, Avg Training Loss: 0.4859210136828909, Avg Validation Loss: 22.045372303154327\n",
            "Epoch 160/2048, Avg Training Loss: 0.485175952848641, Avg Validation Loss: 22.012864539761466\n",
            "Epoch 161/2048, Avg Training Loss: 0.48443089698577274, Avg Validation Loss: 21.980348846516268\n",
            "Epoch 162/2048, Avg Training Loss: 0.4836879818754387, Avg Validation Loss: 21.947570340529502\n",
            "Epoch 163/2048, Avg Training Loss: 0.48294006965289504, Avg Validation Loss: 21.914504824559707\n",
            "Epoch 164/2048, Avg Training Loss: 0.4821912760462918, Avg Validation Loss: 21.881428861939956\n",
            "Epoch 165/2048, Avg Training Loss: 0.48144521747368074, Avg Validation Loss: 21.84848677756939\n",
            "Epoch 166/2048, Avg Training Loss: 0.48070387355534283, Avg Validation Loss: 21.815448985210892\n",
            "Epoch 167/2048, Avg Training Loss: 0.4799586017539659, Avg Validation Loss: 21.78208165024888\n",
            "Epoch 168/2048, Avg Training Loss: 0.4792117731364778, Avg Validation Loss: 21.74870730926034\n",
            "Epoch 169/2048, Avg Training Loss: 0.4784674466421363, Avg Validation Loss: 21.715477402177257\n",
            "Epoch 170/2048, Avg Training Loss: 0.47772546520422676, Avg Validation Loss: 21.681874205611575\n",
            "Epoch 171/2048, Avg Training Loss: 0.47697657185713604, Avg Validation Loss: 21.647923474831206\n",
            "Epoch 172/2048, Avg Training Loss: 0.47622573466316587, Avg Validation Loss: 21.613974819007073\n",
            "Epoch 173/2048, Avg Training Loss: 0.47547902208127846, Avg Validation Loss: 21.580384888543648\n",
            "Epoch 174/2048, Avg Training Loss: 0.47473709027846694, Avg Validation Loss: 21.54675823332841\n",
            "Epoch 175/2048, Avg Training Loss: 0.47399693511349705, Avg Validation Loss: 21.51315754650303\n",
            "Epoch 176/2048, Avg Training Loss: 0.47325755418732923, Avg Validation Loss: 21.479446247648998\n",
            "Epoch 177/2048, Avg Training Loss: 0.4725196657595778, Avg Validation Loss: 21.445706320118052\n",
            "Epoch 178/2048, Avg Training Loss: 0.4717819098979327, Avg Validation Loss: 21.411948137629302\n",
            "Epoch 179/2048, Avg Training Loss: 0.471047037724155, Avg Validation Loss: 21.378365043424587\n",
            "Epoch 180/2048, Avg Training Loss: 0.470317347223068, Avg Validation Loss: 21.345024936388334\n",
            "Epoch 181/2048, Avg Training Loss: 0.469593618445324, Avg Validation Loss: 21.311966498755833\n",
            "Epoch 182/2048, Avg Training Loss: 0.4688761645042006, Avg Validation Loss: 21.279179755316342\n",
            "Epoch 183/2048, Avg Training Loss: 0.46816495630445276, Avg Validation Loss: 21.246683507820112\n",
            "Epoch 184/2048, Avg Training Loss: 0.46745776981670367, Avg Validation Loss: 21.214063063500877\n",
            "Epoch 185/2048, Avg Training Loss: 0.46674937754348383, Avg Validation Loss: 21.181131090615896\n",
            "Epoch 186/2048, Avg Training Loss: 0.4660377689426821, Avg Validation Loss: 21.148127713223808\n",
            "Epoch 187/2048, Avg Training Loss: 0.46532812317588634, Avg Validation Loss: 21.115279633023523\n",
            "Epoch 188/2048, Avg Training Loss: 0.464623047512679, Avg Validation Loss: 21.082469555153473\n",
            "Epoch 189/2048, Avg Training Loss: 0.46391855981643787, Avg Validation Loss: 21.049653871429943\n",
            "Epoch 190/2048, Avg Training Loss: 0.46321731181701103, Avg Validation Loss: 21.017123898788665\n",
            "Epoch 191/2048, Avg Training Loss: 0.46252285476487753, Avg Validation Loss: 20.984928591965435\n",
            "Epoch 192/2048, Avg Training Loss: 0.46183535245810714, Avg Validation Loss: 20.95299491438024\n",
            "Epoch 193/2048, Avg Training Loss: 0.46115185536258335, Avg Validation Loss: 20.921097896347053\n",
            "Epoch 194/2048, Avg Training Loss: 0.4604704411562538, Avg Validation Loss: 20.889279847657004\n",
            "Epoch 195/2048, Avg Training Loss: 0.45979267031977233, Avg Validation Loss: 20.857667136575575\n",
            "Epoch 196/2048, Avg Training Loss: 0.45912016487914203, Avg Validation Loss: 20.826322721171874\n",
            "Epoch 197/2048, Avg Training Loss: 0.45845339078676345, Avg Validation Loss: 20.795120407082162\n",
            "Epoch 198/2048, Avg Training Loss: 0.45779165039206526, Avg Validation Loss: 20.7643475921881\n",
            "Epoch 199/2048, Avg Training Loss: 0.457137942681848, Avg Validation Loss: 20.73395246227844\n",
            "Epoch 200/2048, Avg Training Loss: 0.4564919957688646, Avg Validation Loss: 20.703916094890896\n",
            "Epoch 201/2048, Avg Training Loss: 0.45585378606885674, Avg Validation Loss: 20.674215533684084\n",
            "Epoch 202/2048, Avg Training Loss: 0.45522263082595926, Avg Validation Loss: 20.644833387080563\n",
            "Epoch 203/2048, Avg Training Loss: 0.4545980769496199, Avg Validation Loss: 20.61575746360956\n",
            "Epoch 204/2048, Avg Training Loss: 0.4539794852331178, Avg Validation Loss: 20.58690897414757\n",
            "Epoch 205/2048, Avg Training Loss: 0.45336543029402915, Avg Validation Loss: 20.558145208444763\n",
            "Epoch 206/2048, Avg Training Loss: 0.4527533689474237, Avg Validation Loss: 20.529456777051045\n",
            "Epoch 207/2048, Avg Training Loss: 0.45214360316805446, Avg Validation Loss: 20.50080720660056\n",
            "Epoch 208/2048, Avg Training Loss: 0.4515359892241022, Avg Validation Loss: 20.472170195677386\n",
            "Epoch 209/2048, Avg Training Loss: 0.45092886182145947, Avg Validation Loss: 20.443474320467597\n",
            "Epoch 210/2048, Avg Training Loss: 0.4503230575450199, Avg Validation Loss: 20.41489096932906\n",
            "Epoch 211/2048, Avg Training Loss: 0.44972089678483074, Avg Validation Loss: 20.386507387402773\n",
            "Epoch 212/2048, Avg Training Loss: 0.4491235135353522, Avg Validation Loss: 20.358363900078956\n",
            "Epoch 213/2048, Avg Training Loss: 0.4485313249834852, Avg Validation Loss: 20.330479254767287\n",
            "Epoch 214/2048, Avg Training Loss: 0.44794416990332336, Avg Validation Loss: 20.30272217066756\n",
            "Epoch 215/2048, Avg Training Loss: 0.4473590879603647, Avg Validation Loss: 20.275150610067314\n",
            "Epoch 216/2048, Avg Training Loss: 0.44677774289438565, Avg Validation Loss: 20.247592567564432\n",
            "Epoch 217/2048, Avg Training Loss: 0.446196021192707, Avg Validation Loss: 20.219925859518668\n",
            "Epoch 218/2048, Avg Training Loss: 0.44561276734422856, Avg Validation Loss: 20.191970756066862\n",
            "Epoch 219/2048, Avg Training Loss: 0.4450260803134758, Avg Validation Loss: 20.16375036530117\n",
            "Epoch 220/2048, Avg Training Loss: 0.4444365711573786, Avg Validation Loss: 20.135432523645676\n",
            "Epoch 221/2048, Avg Training Loss: 0.44384803766393044, Avg Validation Loss: 20.107231479972317\n",
            "Epoch 222/2048, Avg Training Loss: 0.44326206593300566, Avg Validation Loss: 20.079061635135677\n",
            "Epoch 223/2048, Avg Training Loss: 0.44267861419288884, Avg Validation Loss: 20.050962842431716\n",
            "Epoch 224/2048, Avg Training Loss: 0.4420972327144096, Avg Validation Loss: 20.022983640793797\n",
            "Epoch 225/2048, Avg Training Loss: 0.4415194663609557, Avg Validation Loss: 19.99521080488534\n",
            "Epoch 226/2048, Avg Training Loss: 0.44094641208203444, Avg Validation Loss: 19.967691258909493\n",
            "Epoch 227/2048, Avg Training Loss: 0.4403785983628408, Avg Validation Loss: 19.940444897252824\n",
            "Epoch 228/2048, Avg Training Loss: 0.43981633201201464, Avg Validation Loss: 19.913477803399992\n",
            "Epoch 229/2048, Avg Training Loss: 0.43925805901195353, Avg Validation Loss: 19.886513658434843\n",
            "Epoch 230/2048, Avg Training Loss: 0.43870152545318764, Avg Validation Loss: 19.859653669979334\n",
            "Epoch 231/2048, Avg Training Loss: 0.4381474384321778, Avg Validation Loss: 19.83279679689929\n",
            "Epoch 232/2048, Avg Training Loss: 0.4375948950613274, Avg Validation Loss: 19.806053714387957\n",
            "Epoch 233/2048, Avg Training Loss: 0.4370457177578108, Avg Validation Loss: 19.779508427418115\n",
            "Epoch 234/2048, Avg Training Loss: 0.43650034661080905, Avg Validation Loss: 19.75311693230402\n",
            "Epoch 235/2048, Avg Training Loss: 0.43595882941963626, Avg Validation Loss: 19.726940027193084\n",
            "Epoch 236/2048, Avg Training Loss: 0.4354218229558383, Avg Validation Loss: 19.70100693317045\n",
            "Epoch 237/2048, Avg Training Loss: 0.4348896177281542, Avg Validation Loss: 19.67533023121961\n",
            "Epoch 238/2048, Avg Training Loss: 0.4343623226777273, Avg Validation Loss: 19.64986934531728\n",
            "Epoch 239/2048, Avg Training Loss: 0.43383866142793054, Avg Validation Loss: 19.62458129394343\n",
            "Epoch 240/2048, Avg Training Loss: 0.4333188175364372, Avg Validation Loss: 19.5995055195519\n",
            "Epoch 241/2048, Avg Training Loss: 0.432803101643995, Avg Validation Loss: 19.574625220557465\n",
            "Epoch 242/2048, Avg Training Loss: 0.43229099629422596, Avg Validation Loss: 19.549837631018512\n",
            "Epoch 243/2048, Avg Training Loss: 0.43178091618262576, Avg Validation Loss: 19.525172599198534\n",
            "Epoch 244/2048, Avg Training Loss: 0.4312739437545221, Avg Validation Loss: 19.500688048712384\n",
            "Epoch 245/2048, Avg Training Loss: 0.4307705924850161, Avg Validation Loss: 19.476411988136586\n",
            "Epoch 246/2048, Avg Training Loss: 0.43027097678906173, Avg Validation Loss: 19.45212695092385\n",
            "Epoch 247/2048, Avg Training Loss: 0.4297720828788542, Avg Validation Loss: 19.428034200860125\n",
            "Epoch 248/2048, Avg Training Loss: 0.4292757073415299, Avg Validation Loss: 19.403713538708868\n",
            "Epoch 249/2048, Avg Training Loss: 0.4287756442140709, Avg Validation Loss: 19.379218959065735\n",
            "Epoch 250/2048, Avg Training Loss: 0.4282752467481189, Avg Validation Loss: 19.354780737992076\n",
            "Epoch 251/2048, Avg Training Loss: 0.42777701457411615, Avg Validation Loss: 19.33050126307058\n",
            "Epoch 252/2048, Avg Training Loss: 0.4272820707111541, Avg Validation Loss: 19.306422416847568\n",
            "Epoch 253/2048, Avg Training Loss: 0.42679098534580706, Avg Validation Loss: 19.28256353654801\n",
            "Epoch 254/2048, Avg Training Loss: 0.4263040108274078, Avg Validation Loss: 19.258932124309347\n",
            "Epoch 255/2048, Avg Training Loss: 0.42581946714511, Avg Validation Loss: 19.23521462882726\n",
            "Epoch 256/2048, Avg Training Loss: 0.42533480853376515, Avg Validation Loss: 19.21141094699678\n",
            "Epoch 257/2048, Avg Training Loss: 0.4248492111704834, Avg Validation Loss: 19.18758173938426\n",
            "Epoch 258/2048, Avg Training Loss: 0.4243648384691494, Avg Validation Loss: 19.163811791104685\n",
            "Epoch 259/2048, Avg Training Loss: 0.4238820125878218, Avg Validation Loss: 19.140127887025496\n",
            "Epoch 260/2048, Avg Training Loss: 0.42340169138928796, Avg Validation Loss: 19.116608856400244\n",
            "Epoch 261/2048, Avg Training Loss: 0.4229247327269693, Avg Validation Loss: 19.09329083627636\n",
            "Epoch 262/2048, Avg Training Loss: 0.4224507187702674, Avg Validation Loss: 19.0698470505858\n",
            "Epoch 263/2048, Avg Training Loss: 0.4219743821544245, Avg Validation Loss: 19.046284062215285\n",
            "Epoch 264/2048, Avg Training Loss: 0.4214982354813454, Avg Validation Loss: 19.022778227512635\n",
            "Epoch 265/2048, Avg Training Loss: 0.4210242988336316, Avg Validation Loss: 18.999420040733856\n",
            "Epoch 266/2048, Avg Training Loss: 0.42055318347663195, Avg Validation Loss: 18.97613056119007\n",
            "Epoch 267/2048, Avg Training Loss: 0.4200837444006767, Avg Validation Loss: 18.95294998453766\n",
            "Epoch 268/2048, Avg Training Loss: 0.4196170678284601, Avg Validation Loss: 18.929940629750824\n",
            "Epoch 269/2048, Avg Training Loss: 0.41915383357129404, Avg Validation Loss: 18.90713286704152\n",
            "Epoch 270/2048, Avg Training Loss: 0.4186944680441564, Avg Validation Loss: 18.884543723133387\n",
            "Epoch 271/2048, Avg Training Loss: 0.41823915901006914, Avg Validation Loss: 18.862174292126536\n",
            "Epoch 272/2048, Avg Training Loss: 0.4177876003542466, Avg Validation Loss: 18.839951928801664\n",
            "Epoch 273/2048, Avg Training Loss: 0.41733868706562594, Avg Validation Loss: 18.817883538852243\n",
            "Epoch 274/2048, Avg Training Loss: 0.41689289247930067, Avg Validation Loss: 18.795996811108008\n",
            "Epoch 275/2048, Avg Training Loss: 0.41645041416556844, Avg Validation Loss: 18.77419660676136\n",
            "Epoch 276/2048, Avg Training Loss: 0.4160083819822307, Avg Validation Loss: 18.752278454255947\n",
            "Epoch 277/2048, Avg Training Loss: 0.4155659670471622, Avg Validation Loss: 18.730373971475746\n",
            "Epoch 278/2048, Avg Training Loss: 0.4151251079435019, Avg Validation Loss: 18.708577657461852\n",
            "Epoch 279/2048, Avg Training Loss: 0.41468682909424015, Avg Validation Loss: 18.68693711674298\n",
            "Epoch 280/2048, Avg Training Loss: 0.414251622742152, Avg Validation Loss: 18.66547500602771\n",
            "Epoch 281/2048, Avg Training Loss: 0.41381970716414246, Avg Validation Loss: 18.64420086503395\n",
            "Epoch 282/2048, Avg Training Loss: 0.4133911540846226, Avg Validation Loss: 18.623117123523638\n",
            "Epoch 283/2048, Avg Training Loss: 0.4129659695766531, Avg Validation Loss: 18.60221821469175\n",
            "Epoch 284/2048, Avg Training Loss: 0.41254425818641555, Avg Validation Loss: 18.581497610107245\n",
            "Epoch 285/2048, Avg Training Loss: 0.41212597029965, Avg Validation Loss: 18.560955289765957\n",
            "Epoch 286/2048, Avg Training Loss: 0.41171095612718916, Avg Validation Loss: 18.54058904740148\n",
            "Epoch 287/2048, Avg Training Loss: 0.4112991015599466, Avg Validation Loss: 18.520394097742216\n",
            "Epoch 288/2048, Avg Training Loss: 0.4108901931239938, Avg Validation Loss: 18.500322803500588\n",
            "Epoch 289/2048, Avg Training Loss: 0.4104834642686994, Avg Validation Loss: 18.480331468237967\n",
            "Epoch 290/2048, Avg Training Loss: 0.4100791567504263, Avg Validation Loss: 18.460457482760244\n",
            "Epoch 291/2048, Avg Training Loss: 0.40967743438301896, Avg Validation Loss: 18.440718980909278\n",
            "Epoch 292/2048, Avg Training Loss: 0.4092783506915327, Avg Validation Loss: 18.42112377626396\n",
            "Epoch 293/2048, Avg Training Loss: 0.4088819001073111, Avg Validation Loss: 18.401674205953775\n",
            "Epoch 294/2048, Avg Training Loss: 0.4084880437513586, Avg Validation Loss: 18.382369682359993\n",
            "Epoch 295/2048, Avg Training Loss: 0.4080967329812992, Avg Validation Loss: 18.363208195903685\n",
            "Epoch 296/2048, Avg Training Loss: 0.40770791396359113, Avg Validation Loss: 18.344186988652506\n",
            "Epoch 297/2048, Avg Training Loss: 0.4073215258162327, Avg Validation Loss: 18.325302908087945\n",
            "Epoch 298/2048, Avg Training Loss: 0.4069375061304593, Avg Validation Loss: 18.30655262093392\n",
            "Epoch 299/2048, Avg Training Loss: 0.40655579843005607, Avg Validation Loss: 18.287932774007658\n",
            "Epoch 300/2048, Avg Training Loss: 0.40617639341210493, Avg Validation Loss: 18.269446346522205\n",
            "Epoch 301/2048, Avg Training Loss: 0.4057996439484544, Avg Validation Loss: 18.251117153616622\n",
            "Epoch 302/2048, Avg Training Loss: 0.4054257673874977, Avg Validation Loss: 18.232935326064144\n",
            "Epoch 303/2048, Avg Training Loss: 0.4050544235181851, Avg Validation Loss: 18.214885179082472\n",
            "Epoch 304/2048, Avg Training Loss: 0.4046853570501089, Avg Validation Loss: 18.196957046340827\n",
            "Epoch 305/2048, Avg Training Loss: 0.40431840940218383, Avg Validation Loss: 18.17914440757682\n",
            "Epoch 306/2048, Avg Training Loss: 0.4039534696166345, Avg Validation Loss: 18.161442396672978\n",
            "Epoch 307/2048, Avg Training Loss: 0.4035904580499128, Avg Validation Loss: 18.143846396915112\n",
            "Epoch 308/2048, Avg Training Loss: 0.40322932237870357, Avg Validation Loss: 18.12635060357141\n",
            "Epoch 309/2048, Avg Training Loss: 0.4028700468225847, Avg Validation Loss: 18.10895281557256\n",
            "Epoch 310/2048, Avg Training Loss: 0.4025125628640195, Avg Validation Loss: 18.091651061004022\n",
            "Epoch 311/2048, Avg Training Loss: 0.40215679747017763, Avg Validation Loss: 18.07443413300189\n",
            "Epoch 312/2048, Avg Training Loss: 0.4018019290211139, Avg Validation Loss: 18.057290516412557\n",
            "Epoch 313/2048, Avg Training Loss: 0.4014478354279967, Avg Validation Loss: 18.040192997724038\n",
            "Epoch 314/2048, Avg Training Loss: 0.40109439570562155, Avg Validation Loss: 18.023147971309143\n",
            "Epoch 315/2048, Avg Training Loss: 0.40074163399593365, Avg Validation Loss: 18.006014513415547\n",
            "Epoch 316/2048, Avg Training Loss: 0.400387729236625, Avg Validation Loss: 17.988819412477778\n",
            "Epoch 317/2048, Avg Training Loss: 0.4000338354343712, Avg Validation Loss: 17.971629767535156\n",
            "Epoch 318/2048, Avg Training Loss: 0.3996806120516074, Avg Validation Loss: 17.954481331918526\n",
            "Epoch 319/2048, Avg Training Loss: 0.3993284504988284, Avg Validation Loss: 17.93739437348914\n",
            "Epoch 320/2048, Avg Training Loss: 0.3989774511937907, Avg Validation Loss: 17.920372493688646\n",
            "Epoch 321/2048, Avg Training Loss: 0.3986274858245837, Avg Validation Loss: 17.903421801498368\n",
            "Epoch 322/2048, Avg Training Loss: 0.39827868691699997, Avg Validation Loss: 17.886545691792502\n",
            "Epoch 323/2048, Avg Training Loss: 0.39793049727879387, Avg Validation Loss: 17.869590513363764\n",
            "Epoch 324/2048, Avg Training Loss: 0.3975816375440928, Avg Validation Loss: 17.852608444888578\n",
            "Epoch 325/2048, Avg Training Loss: 0.3972330443216831, Avg Validation Loss: 17.835646094551635\n",
            "Epoch 326/2048, Avg Training Loss: 0.39688508961298147, Avg Validation Loss: 17.818720436920735\n",
            "Epoch 327/2048, Avg Training Loss: 0.3965378838319121, Avg Validation Loss: 17.801848089377092\n",
            "Epoch 328/2048, Avg Training Loss: 0.39619169322781583, Avg Validation Loss: 17.784980400300817\n",
            "Epoch 329/2048, Avg Training Loss: 0.3958449419568465, Avg Validation Loss: 17.767998087383145\n",
            "Epoch 330/2048, Avg Training Loss: 0.39549735186630197, Avg Validation Loss: 17.750942763420714\n",
            "Epoch 331/2048, Avg Training Loss: 0.39514923015308856, Avg Validation Loss: 17.733838739677374\n",
            "Epoch 332/2048, Avg Training Loss: 0.39480140647804013, Avg Validation Loss: 17.716742916158104\n",
            "Epoch 333/2048, Avg Training Loss: 0.39445431711177753, Avg Validation Loss: 17.699684685036942\n",
            "Epoch 334/2048, Avg Training Loss: 0.394108175650462, Avg Validation Loss: 17.682678713111315\n",
            "Epoch 335/2048, Avg Training Loss: 0.39376307573285463, Avg Validation Loss: 17.66573168777134\n",
            "Epoch 336/2048, Avg Training Loss: 0.39341903980444615, Avg Validation Loss: 17.64884600574156\n",
            "Epoch 337/2048, Avg Training Loss: 0.39307606055892624, Avg Validation Loss: 17.63202187237018\n",
            "Epoch 338/2048, Avg Training Loss: 0.39273411250225426, Avg Validation Loss: 17.61525826745871\n",
            "Epoch 339/2048, Avg Training Loss: 0.3923931550655834, Avg Validation Loss: 17.598553477186705\n",
            "Epoch 340/2048, Avg Training Loss: 0.39205315205175045, Avg Validation Loss: 17.581905589336323\n",
            "Epoch 341/2048, Avg Training Loss: 0.3917140651788141, Avg Validation Loss: 17.565312908326792\n",
            "Epoch 342/2048, Avg Training Loss: 0.3913758577068362, Avg Validation Loss: 17.548773961201306\n",
            "Epoch 343/2048, Avg Training Loss: 0.3910384928345442, Avg Validation Loss: 17.532286630584924\n",
            "Epoch 344/2048, Avg Training Loss: 0.3907019327062179, Avg Validation Loss: 17.515848844637368\n",
            "Epoch 345/2048, Avg Training Loss: 0.39036614420465476, Avg Validation Loss: 17.499458660450234\n",
            "Epoch 346/2048, Avg Training Loss: 0.3900310326021265, Avg Validation Loss: 17.483017808635985\n",
            "Epoch 347/2048, Avg Training Loss: 0.3896947979031222, Avg Validation Loss: 17.46650183403806\n",
            "Epoch 348/2048, Avg Training Loss: 0.3893580897383679, Avg Validation Loss: 17.449968354738708\n",
            "Epoch 349/2048, Avg Training Loss: 0.3890214632648515, Avg Validation Loss: 17.433447179080503\n",
            "Epoch 350/2048, Avg Training Loss: 0.38868519794057316, Avg Validation Loss: 17.41695331914771\n",
            "Epoch 351/2048, Avg Training Loss: 0.3883494279911638, Avg Validation Loss: 17.40049389110878\n",
            "Epoch 352/2048, Avg Training Loss: 0.3880142086063791, Avg Validation Loss: 17.38407180421438\n",
            "Epoch 353/2048, Avg Training Loss: 0.3876795474487958, Avg Validation Loss: 17.3676876142102\n",
            "Epoch 354/2048, Avg Training Loss: 0.3873454239839335, Avg Validation Loss: 17.35134050312591\n",
            "Epoch 355/2048, Avg Training Loss: 0.387011815713541, Avg Validation Loss: 17.335028934865466\n",
            "Epoch 356/2048, Avg Training Loss: 0.3866786888496692, Avg Validation Loss: 17.318748820159023\n",
            "Epoch 357/2048, Avg Training Loss: 0.38634596705299906, Avg Validation Loss: 17.30249905002615\n",
            "Epoch 358/2048, Avg Training Loss: 0.3860136482014221, Avg Validation Loss: 17.28627916143755\n",
            "Epoch 359/2048, Avg Training Loss: 0.3856817086935275, Avg Validation Loss: 17.2700836142003\n",
            "Epoch 360/2048, Avg Training Loss: 0.38534982551740554, Avg Validation Loss: 17.253906393572187\n",
            "Epoch 361/2048, Avg Training Loss: 0.3850179732209546, Avg Validation Loss: 17.23774983454149\n",
            "Epoch 362/2048, Avg Training Loss: 0.3846862615755497, Avg Validation Loss: 17.221614386071877\n",
            "Epoch 363/2048, Avg Training Loss: 0.38435481683783057, Avg Validation Loss: 17.205510615126126\n",
            "Epoch 364/2048, Avg Training Loss: 0.38402356840347324, Avg Validation Loss: 17.18940044797292\n",
            "Epoch 365/2048, Avg Training Loss: 0.3836916281309586, Avg Validation Loss: 17.17329483899079\n",
            "Epoch 366/2048, Avg Training Loss: 0.38335949233792366, Avg Validation Loss: 17.157204988275023\n",
            "Epoch 367/2048, Avg Training Loss: 0.38302749335143277, Avg Validation Loss: 17.14113004049945\n",
            "Epoch 368/2048, Avg Training Loss: 0.3826956281979104, Avg Validation Loss: 17.125068782816875\n",
            "Epoch 369/2048, Avg Training Loss: 0.38236385874127454, Avg Validation Loss: 17.10902131921943\n",
            "Epoch 370/2048, Avg Training Loss: 0.3820321005693956, Avg Validation Loss: 17.09298598698125\n",
            "Epoch 371/2048, Avg Training Loss: 0.3817003952725792, Avg Validation Loss: 17.07696536404424\n",
            "Epoch 372/2048, Avg Training Loss: 0.3813689764732195, Avg Validation Loss: 17.060959274365043\n",
            "Epoch 373/2048, Avg Training Loss: 0.3810377833554035, Avg Validation Loss: 17.04496393708294\n",
            "Epoch 374/2048, Avg Training Loss: 0.3807067062037188, Avg Validation Loss: 17.02897632876155\n",
            "Epoch 375/2048, Avg Training Loss: 0.38037566487073377, Avg Validation Loss: 17.0129941483548\n",
            "Epoch 376/2048, Avg Training Loss: 0.38004461264595246, Avg Validation Loss: 16.99701583722905\n",
            "Epoch 377/2048, Avg Training Loss: 0.3797135234060768, Avg Validation Loss: 16.9810396482875\n",
            "Epoch 378/2048, Avg Training Loss: 0.3793823766322012, Avg Validation Loss: 16.965064260428694\n",
            "Epoch 379/2048, Avg Training Loss: 0.37905114707338483, Avg Validation Loss: 16.949087618060794\n",
            "Epoch 380/2048, Avg Training Loss: 0.37871981062983584, Avg Validation Loss: 16.933109242279702\n",
            "Epoch 381/2048, Avg Training Loss: 0.3783883602137093, Avg Validation Loss: 16.917128488507167\n",
            "Epoch 382/2048, Avg Training Loss: 0.3780567856040604, Avg Validation Loss: 16.901144614548794\n",
            "Epoch 383/2048, Avg Training Loss: 0.3777252369226028, Avg Validation Loss: 16.885173980175868\n",
            "Epoch 384/2048, Avg Training Loss: 0.37739426702082807, Avg Validation Loss: 16.869231271693074\n",
            "Epoch 385/2048, Avg Training Loss: 0.3770638784506452, Avg Validation Loss: 16.853306972887662\n",
            "Epoch 386/2048, Avg Training Loss: 0.3767336885620787, Avg Validation Loss: 16.837478459152603\n",
            "Epoch 387/2048, Avg Training Loss: 0.3764049099608946, Avg Validation Loss: 16.82182356987269\n",
            "Epoch 388/2048, Avg Training Loss: 0.3760769568135077, Avg Validation Loss: 16.806254285668587\n",
            "Epoch 389/2048, Avg Training Loss: 0.3757493336556464, Avg Validation Loss: 16.79072306774511\n",
            "Epoch 390/2048, Avg Training Loss: 0.3754217742324404, Avg Validation Loss: 16.77520434450974\n",
            "Epoch 391/2048, Avg Training Loss: 0.3750942189638951, Avg Validation Loss: 16.759597595189057\n",
            "Epoch 392/2048, Avg Training Loss: 0.37476511075781205, Avg Validation Loss: 16.743896207657098\n",
            "Epoch 393/2048, Avg Training Loss: 0.37443620965793684, Avg Validation Loss: 16.728263761390544\n",
            "Epoch 394/2048, Avg Training Loss: 0.3741075590379248, Avg Validation Loss: 16.712572899540653\n",
            "Epoch 395/2048, Avg Training Loss: 0.3737774668753613, Avg Validation Loss: 16.696802590596516\n",
            "Epoch 396/2048, Avg Training Loss: 0.3734476296450047, Avg Validation Loss: 16.68110846640722\n",
            "Epoch 397/2048, Avg Training Loss: 0.3731180374896648, Avg Validation Loss: 16.665384760569122\n",
            "Epoch 398/2048, Avg Training Loss: 0.3727872775993235, Avg Validation Loss: 16.649633742880397\n",
            "Epoch 399/2048, Avg Training Loss: 0.37245689147863015, Avg Validation Loss: 16.63398564532874\n",
            "Epoch 400/2048, Avg Training Loss: 0.37212674787429545, Avg Validation Loss: 16.61838210701123\n",
            "Epoch 401/2048, Avg Training Loss: 0.371796669067674, Avg Validation Loss: 16.602705333667313\n",
            "Epoch 402/2048, Avg Training Loss: 0.3714650231355064, Avg Validation Loss: 16.58693902364212\n",
            "Epoch 403/2048, Avg Training Loss: 0.37113355351403304, Avg Validation Loss: 16.571240961554434\n",
            "Epoch 404/2048, Avg Training Loss: 0.3708022144975398, Avg Validation Loss: 16.555567894198145\n",
            "Epoch 405/2048, Avg Training Loss: 0.3704708731452958, Avg Validation Loss: 16.539811058134028\n",
            "Epoch 406/2048, Avg Training Loss: 0.37013792841761045, Avg Validation Loss: 16.52395874859388\n",
            "Epoch 407/2048, Avg Training Loss: 0.3698051334382182, Avg Validation Loss: 16.50817075507375\n",
            "Epoch 408/2048, Avg Training Loss: 0.36947244844891086, Avg Validation Loss: 16.492405129311845\n",
            "Epoch 409/2048, Avg Training Loss: 0.3691397341167513, Avg Validation Loss: 16.476553438170047\n",
            "Epoch 410/2048, Avg Training Loss: 0.36880536776252854, Avg Validation Loss: 16.460604518001265\n",
            "Epoch 411/2048, Avg Training Loss: 0.36847111311415626, Avg Validation Loss: 16.444718207481365\n",
            "Epoch 412/2048, Avg Training Loss: 0.3681369394028946, Avg Validation Loss: 16.428852826497305\n",
            "Epoch 413/2048, Avg Training Loss: 0.36780270852538477, Avg Validation Loss: 16.412900252176787\n",
            "Epoch 414/2048, Avg Training Loss: 0.36746679142424893, Avg Validation Loss: 16.39684935445349\n",
            "Epoch 415/2048, Avg Training Loss: 0.3671309538223282, Avg Validation Loss: 16.380859718736122\n",
            "Epoch 416/2048, Avg Training Loss: 0.3667951670692087, Avg Validation Loss: 16.36488970106573\n",
            "Epoch 417/2048, Avg Training Loss: 0.3664592071450924, Avg Validation Loss: 16.348916932196982\n",
            "Epoch 418/2048, Avg Training Loss: 0.36612303824392695, Avg Validation Loss: 16.332843865923103\n",
            "Epoch 419/2048, Avg Training Loss: 0.3657850834912258, Avg Validation Loss: 16.316665156320436\n",
            "Epoch 420/2048, Avg Training Loss: 0.36544717774480145, Avg Validation Loss: 16.300547580017888\n",
            "Epoch 421/2048, Avg Training Loss: 0.3651093347708649, Avg Validation Loss: 16.284449388765537\n",
            "Epoch 422/2048, Avg Training Loss: 0.3647714042745955, Avg Validation Loss: 16.268262766518923\n",
            "Epoch 423/2048, Avg Training Loss: 0.36443174442005294, Avg Validation Loss: 16.251976962349275\n",
            "Epoch 424/2048, Avg Training Loss: 0.36409214139086626, Avg Validation Loss: 16.23575161116297\n",
            "Epoch 425/2048, Avg Training Loss: 0.3637525811585088, Avg Validation Loss: 16.219545494003594\n",
            "Epoch 426/2048, Avg Training Loss: 0.363412840128793, Avg Validation Loss: 16.20333651910734\n",
            "Epoch 427/2048, Avg Training Loss: 0.36307280266743686, Avg Validation Loss: 16.187113218921702\n",
            "Epoch 428/2048, Avg Training Loss: 0.36273249476917524, Avg Validation Loss: 16.17078396200094\n",
            "Epoch 429/2048, Avg Training Loss: 0.36239035611487075, Avg Validation Loss: 16.154346427175753\n",
            "Epoch 430/2048, Avg Training Loss: 0.3620482264129082, Avg Validation Loss: 16.13796409361905\n",
            "Epoch 431/2048, Avg Training Loss: 0.3617061222878592, Avg Validation Loss: 16.121598339717696\n",
            "Epoch 432/2048, Avg Training Loss: 0.36136383268301236, Avg Validation Loss: 16.105228509225704\n",
            "Epoch 433/2048, Avg Training Loss: 0.36102132922194413, Avg Validation Loss: 16.08875856440959\n",
            "Epoch 434/2048, Avg Training Loss: 0.360677025462924, Avg Validation Loss: 16.072183824536772\n",
            "Epoch 435/2048, Avg Training Loss: 0.36033275562053235, Avg Validation Loss: 16.0556662091597\n",
            "Epoch 436/2048, Avg Training Loss: 0.3599885349013785, Avg Validation Loss: 16.03916652320257\n",
            "Epoch 437/2048, Avg Training Loss: 0.35964414829694513, Avg Validation Loss: 16.022663827070595\n",
            "Epoch 438/2048, Avg Training Loss: 0.3592994835880367, Avg Validation Loss: 16.006146983457704\n",
            "Epoch 439/2048, Avg Training Loss: 0.3589544827459295, Avg Validation Loss: 15.989610156179197\n",
            "Epoch 440/2048, Avg Training Loss: 0.35860919856382023, Avg Validation Loss: 15.972965518051952\n",
            "Epoch 441/2048, Avg Training Loss: 0.3582620834051305, Avg Validation Loss: 15.956212710081676\n",
            "Epoch 442/2048, Avg Training Loss: 0.3579150010655399, Avg Validation Loss: 15.939515627591659\n",
            "Epoch 443/2048, Avg Training Loss: 0.35756798241260973, Avg Validation Loss: 15.922836601101967\n",
            "Epoch 444/2048, Avg Training Loss: 0.3572208155967747, Avg Validation Loss: 15.906155411219855\n",
            "Epoch 445/2048, Avg Training Loss: 0.35687339217360375, Avg Validation Loss: 15.889461342277883\n",
            "Epoch 446/2048, Avg Training Loss: 0.35652565928464475, Avg Validation Loss: 15.87274878002908\n",
            "Epoch 447/2048, Avg Training Loss: 0.3561776718385786, Avg Validation Loss: 15.855930298509888\n",
            "Epoch 448/2048, Avg Training Loss: 0.3558278799080133, Avg Validation Loss: 15.8390056772621\n",
            "Epoch 449/2048, Avg Training Loss: 0.3554781231852599, Avg Validation Loss: 15.822133814391094\n",
            "Epoch 450/2048, Avg Training Loss: 0.355128277910951, Avg Validation Loss: 15.80527760810551\n",
            "Epoch 451/2048, Avg Training Loss: 0.3547782014959686, Avg Validation Loss: 15.788419002216296\n",
            "Epoch 452/2048, Avg Training Loss: 0.35442784704460245, Avg Validation Loss: 15.771548404626754\n",
            "Epoch 453/2048, Avg Training Loss: 0.35407719285320166, Avg Validation Loss: 15.75466080873827\n",
            "Epoch 454/2048, Avg Training Loss: 0.3537262305957428, Avg Validation Loss: 15.737753691075488\n",
            "Epoch 455/2048, Avg Training Loss: 0.3533750399536752, Avg Validation Loss: 15.720741599394332\n",
            "Epoch 456/2048, Avg Training Loss: 0.35302208218585157, Avg Validation Loss: 15.703625262384376\n",
            "Epoch 457/2048, Avg Training Loss: 0.3526692380638054, Avg Validation Loss: 15.686568104564863\n",
            "Epoch 458/2048, Avg Training Loss: 0.3523165540723463, Avg Validation Loss: 15.669532928536356\n",
            "Epoch 459/2048, Avg Training Loss: 0.3519638223501661, Avg Validation Loss: 15.65249995512233\n",
            "Epoch 460/2048, Avg Training Loss: 0.3516109360762232, Avg Validation Loss: 15.635458754760588\n",
            "Epoch 461/2048, Avg Training Loss: 0.35125781731050654, Avg Validation Loss: 15.618377536522477\n",
            "Epoch 462/2048, Avg Training Loss: 0.35090429476510093, Avg Validation Loss: 15.601256508104278\n",
            "Epoch 463/2048, Avg Training Loss: 0.35055044975159777, Avg Validation Loss: 15.58410565349668\n",
            "Epoch 464/2048, Avg Training Loss: 0.35019631752725533, Avg Validation Loss: 15.566930522655017\n",
            "Epoch 465/2048, Avg Training Loss: 0.34984200106090685, Avg Validation Loss: 15.549650367136774\n",
            "Epoch 466/2048, Avg Training Loss: 0.3494859747017634, Avg Validation Loss: 15.532268319806853\n",
            "Epoch 467/2048, Avg Training Loss: 0.34913012729674225, Avg Validation Loss: 15.514948394074079\n",
            "Epoch 468/2048, Avg Training Loss: 0.34877451224691913, Avg Validation Loss: 15.497654246730695\n",
            "Epoch 469/2048, Avg Training Loss: 0.34841890057171504, Avg Validation Loss: 15.480357357632972\n",
            "Epoch 470/2048, Avg Training Loss: 0.3480631597278304, Avg Validation Loss: 15.463050593481963\n",
            "Epoch 471/2048, Avg Training Loss: 0.34770711627847245, Avg Validation Loss: 15.445689011841507\n",
            "Epoch 472/2048, Avg Training Loss: 0.3473504828520435, Avg Validation Loss: 15.42828594378274\n",
            "Epoch 473/2048, Avg Training Loss: 0.3469934628504678, Avg Validation Loss: 15.410854963888063\n",
            "Epoch 474/2048, Avg Training Loss: 0.3466361689296811, Avg Validation Loss: 15.393403532067707\n",
            "Epoch 475/2048, Avg Training Loss: 0.3462786651195126, Avg Validation Loss: 15.375935838994534\n",
            "Epoch 476/2048, Avg Training Loss: 0.34592098963356743, Avg Validation Loss: 15.35845437760995\n",
            "Epoch 477/2048, Avg Training Loss: 0.34556316699205414, Avg Validation Loss: 15.340960658029896\n",
            "Epoch 478/2048, Avg Training Loss: 0.3452052144632109, Avg Validation Loss: 15.32345567234244\n",
            "Epoch 479/2048, Avg Training Loss: 0.34484722553576236, Avg Validation Loss: 15.305856804576427\n",
            "Epoch 480/2048, Avg Training Loss: 0.34448767838218836, Avg Validation Loss: 15.288165928355646\n",
            "Epoch 481/2048, Avg Training Loss: 0.34412845122753943, Avg Validation Loss: 15.270545235606669\n",
            "Epoch 482/2048, Avg Training Loss: 0.34376959630301773, Avg Validation Loss: 15.25295824985454\n",
            "Epoch 483/2048, Avg Training Loss: 0.3434109424614492, Avg Validation Loss: 15.235390554559215\n",
            "Epoch 484/2048, Avg Training Loss: 0.3430525870245824, Avg Validation Loss: 15.217856511204271\n",
            "Epoch 485/2048, Avg Training Loss: 0.34269484777384446, Avg Validation Loss: 15.200370846701617\n",
            "Epoch 486/2048, Avg Training Loss: 0.34233745879624256, Avg Validation Loss: 15.182907486100762\n",
            "Epoch 487/2048, Avg Training Loss: 0.34198017272465725, Avg Validation Loss: 15.16542399887491\n",
            "Epoch 488/2048, Avg Training Loss: 0.34162281193199845, Avg Validation Loss: 15.14792264063353\n",
            "Epoch 489/2048, Avg Training Loss: 0.3412654137229315, Avg Validation Loss: 15.130409017423979\n",
            "Epoch 490/2048, Avg Training Loss: 0.34090799496178986, Avg Validation Loss: 15.112886347289757\n",
            "Epoch 491/2048, Avg Training Loss: 0.34055056894703706, Avg Validation Loss: 15.095356562229073\n",
            "Epoch 492/2048, Avg Training Loss: 0.34019314710092846, Avg Validation Loss: 15.077820901835349\n",
            "Epoch 493/2048, Avg Training Loss: 0.33983573986147486, Avg Validation Loss: 15.060280233067827\n",
            "Epoch 494/2048, Avg Training Loss: 0.3394783571523596, Avg Validation Loss: 15.042735222471782\n",
            "Epoch 495/2048, Avg Training Loss: 0.33912100862985817, Avg Validation Loss: 15.025186428910338\n",
            "Epoch 496/2048, Avg Training Loss: 0.3387637040098752, Avg Validation Loss: 15.007634351243537\n",
            "Epoch 497/2048, Avg Training Loss: 0.33840645326051394, Avg Validation Loss: 14.990079454620103\n",
            "Epoch 498/2048, Avg Training Loss: 0.33804926582213385, Avg Validation Loss: 14.972524192334491\n",
            "Epoch 499/2048, Avg Training Loss: 0.3376922876798007, Avg Validation Loss: 14.954982753757449\n",
            "Epoch 500/2048, Avg Training Loss: 0.33733570217074543, Avg Validation Loss: 14.937450682445753\n",
            "Epoch 501/2048, Avg Training Loss: 0.33697938544706585, Avg Validation Loss: 14.9199229395538\n",
            "Epoch 502/2048, Avg Training Loss: 0.3366232567475935, Avg Validation Loss: 14.902397109100075\n",
            "Epoch 503/2048, Avg Training Loss: 0.336267280000859, Avg Validation Loss: 14.884872115140476\n",
            "Epoch 504/2048, Avg Training Loss: 0.33591144045594723, Avg Validation Loss: 14.867347642583297\n",
            "Epoch 505/2048, Avg Training Loss: 0.33555573464892663, Avg Validation Loss: 14.849823728667534\n",
            "Epoch 506/2048, Avg Training Loss: 0.3352001651215837, Avg Validation Loss: 14.832300597894024\n",
            "Epoch 507/2048, Avg Training Loss: 0.3348445447503239, Avg Validation Loss: 14.814764016524474\n",
            "Epoch 508/2048, Avg Training Loss: 0.33448861115820333, Avg Validation Loss: 14.797221280479036\n",
            "Epoch 509/2048, Avg Training Loss: 0.33413259567877435, Avg Validation Loss: 14.779676429904153\n",
            "Epoch 510/2048, Avg Training Loss: 0.3337766231418232, Avg Validation Loss: 14.762131387389607\n",
            "Epoch 511/2048, Avg Training Loss: 0.3334207631478915, Avg Validation Loss: 14.744587764862098\n",
            "Epoch 512/2048, Avg Training Loss: 0.33306506031988353, Avg Validation Loss: 14.72704662046453\n",
            "Epoch 513/2048, Avg Training Loss: 0.33270954269872105, Avg Validation Loss: 14.709508720471005\n",
            "Epoch 514/2048, Avg Training Loss: 0.33235422949833304, Avg Validation Loss: 14.691974677080237\n",
            "Epoch 515/2048, Avg Training Loss: 0.33199913522700386, Avg Validation Loss: 14.674445020874023\n",
            "Epoch 516/2048, Avg Training Loss: 0.3316442718774388, Avg Validation Loss: 14.656920232531931\n",
            "Epoch 517/2048, Avg Training Loss: 0.33128965009028644, Avg Validation Loss: 14.639400748835293\n",
            "Epoch 518/2048, Avg Training Loss: 0.33093527517253746, Avg Validation Loss: 14.621885924286978\n",
            "Epoch 519/2048, Avg Training Loss: 0.3305811446343039, Avg Validation Loss: 14.604376490820925\n",
            "Epoch 520/2048, Avg Training Loss: 0.3302272739016385, Avg Validation Loss: 14.586872536428414\n",
            "Epoch 521/2048, Avg Training Loss: 0.32987363324523533, Avg Validation Loss: 14.569373870710901\n",
            "Epoch 522/2048, Avg Training Loss: 0.32952022823308336, Avg Validation Loss: 14.55188167480075\n",
            "Epoch 523/2048, Avg Training Loss: 0.3291670918744239, Avg Validation Loss: 14.534396812835466\n",
            "Epoch 524/2048, Avg Training Loss: 0.32881424687109934, Avg Validation Loss: 14.516919945241192\n",
            "Epoch 525/2048, Avg Training Loss: 0.3284617098289588, Avg Validation Loss: 14.499451622580475\n",
            "Epoch 526/2048, Avg Training Loss: 0.32810944494602895, Avg Validation Loss: 14.481988855684007\n",
            "Epoch 527/2048, Avg Training Loss: 0.3277572346098257, Avg Validation Loss: 14.464521509178798\n",
            "Epoch 528/2048, Avg Training Loss: 0.32740485151878307, Avg Validation Loss: 14.44705494830404\n",
            "Epoch 529/2048, Avg Training Loss: 0.3270525319784098, Avg Validation Loss: 14.429593865055306\n",
            "Epoch 530/2048, Avg Training Loss: 0.3267004260110138, Avg Validation Loss: 14.412140925082472\n",
            "Epoch 531/2048, Avg Training Loss: 0.3263486174671113, Avg Validation Loss: 14.394697729644315\n",
            "Epoch 532/2048, Avg Training Loss: 0.32599715490266656, Avg Validation Loss: 14.377265321443517\n",
            "Epoch 533/2048, Avg Training Loss: 0.32564606808836505, Avg Validation Loss: 14.359844449504724\n",
            "Epoch 534/2048, Avg Training Loss: 0.3252953981815885, Avg Validation Loss: 14.342442098470578\n",
            "Epoch 535/2048, Avg Training Loss: 0.32494516616716207, Avg Validation Loss: 14.325047168574013\n",
            "Epoch 536/2048, Avg Training Loss: 0.32459528144503136, Avg Validation Loss: 14.307656346773204\n",
            "Epoch 537/2048, Avg Training Loss: 0.3242457900921419, Avg Validation Loss: 14.290274411691918\n",
            "Epoch 538/2048, Avg Training Loss: 0.3238967186456497, Avg Validation Loss: 14.272904119136513\n",
            "Epoch 539/2048, Avg Training Loss: 0.32354808502193244, Avg Validation Loss: 14.255547137151689\n",
            "Epoch 540/2048, Avg Training Loss: 0.32319990183695335, Avg Validation Loss: 14.23820371720827\n",
            "Epoch 541/2048, Avg Training Loss: 0.32285216200202804, Avg Validation Loss: 14.220874378190542\n",
            "Epoch 542/2048, Avg Training Loss: 0.3225048801236888, Avg Validation Loss: 14.203560176656337\n",
            "Epoch 543/2048, Avg Training Loss: 0.32215807056949525, Avg Validation Loss: 14.18626186237566\n",
            "Epoch 544/2048, Avg Training Loss: 0.3218117446936789, Avg Validation Loss: 14.168980019551261\n",
            "Epoch 545/2048, Avg Training Loss: 0.3214657864088804, Avg Validation Loss: 14.15170468359488\n",
            "Epoch 546/2048, Avg Training Loss: 0.3211200171435592, Avg Validation Loss: 14.134441283960168\n",
            "Epoch 547/2048, Avg Training Loss: 0.3207745963968539, Avg Validation Loss: 14.117192855684811\n",
            "Epoch 548/2048, Avg Training Loss: 0.320429618559643, Avg Validation Loss: 14.099962090636305\n",
            "Epoch 549/2048, Avg Training Loss: 0.3200851909606339, Avg Validation Loss: 14.08275035423437\n",
            "Epoch 550/2048, Avg Training Loss: 0.31974132915949244, Avg Validation Loss: 14.065557891634196\n",
            "Epoch 551/2048, Avg Training Loss: 0.3193980299614592, Avg Validation Loss: 14.04838501368382\n",
            "Epoch 552/2048, Avg Training Loss: 0.31905529514089476, Avg Validation Loss: 14.031232062866586\n",
            "Epoch 553/2048, Avg Training Loss: 0.3187131172005168, Avg Validation Loss: 14.014164848809\n",
            "Epoch 554/2048, Avg Training Loss: 0.3183723009506204, Avg Validation Loss: 13.99720020441972\n",
            "Epoch 555/2048, Avg Training Loss: 0.31803247243282695, Avg Validation Loss: 13.980298894663681\n",
            "Epoch 556/2048, Avg Training Loss: 0.3176934443910886, Avg Validation Loss: 13.963440014299486\n",
            "Epoch 557/2048, Avg Training Loss: 0.3173551206729222, Avg Validation Loss: 13.94661250808075\n",
            "Epoch 558/2048, Avg Training Loss: 0.31701755238580465, Avg Validation Loss: 13.929896301418072\n",
            "Epoch 559/2048, Avg Training Loss: 0.31668132972056307, Avg Validation Loss: 13.91327889565002\n",
            "Epoch 560/2048, Avg Training Loss: 0.3163460796065723, Avg Validation Loss: 13.896722279698475\n",
            "Epoch 561/2048, Avg Training Loss: 0.316011637977992, Avg Validation Loss: 13.880206197076708\n",
            "Epoch 562/2048, Avg Training Loss: 0.31567792051340654, Avg Validation Loss: 13.863719938578274\n",
            "Epoch 563/2048, Avg Training Loss: 0.315344885172843, Avg Validation Loss: 13.847258124177078\n",
            "Epoch 564/2048, Avg Training Loss: 0.3150125179012617, Avg Validation Loss: 13.830818140043542\n",
            "Epoch 565/2048, Avg Training Loss: 0.3146808104435552, Avg Validation Loss: 13.814398482151644\n",
            "Epoch 566/2048, Avg Training Loss: 0.3143497590676072, Avg Validation Loss: 13.797998525058217\n",
            "Epoch 567/2048, Avg Training Loss: 0.3140193644563488, Avg Validation Loss: 13.781618114686747\n",
            "Epoch 568/2048, Avg Training Loss: 0.3136896298199252, Avg Validation Loss: 13.765257350230112\n",
            "Epoch 569/2048, Avg Training Loss: 0.3133605605320292, Avg Validation Loss: 13.748916469713427\n",
            "Epoch 570/2048, Avg Training Loss: 0.3130321620014607, Avg Validation Loss: 13.73259578300795\n",
            "Epoch 571/2048, Avg Training Loss: 0.3127044462463957, Avg Validation Loss: 13.716295968636668\n",
            "Epoch 572/2048, Avg Training Loss: 0.3123774258032918, Avg Validation Loss: 13.700017273411488\n",
            "Epoch 573/2048, Avg Training Loss: 0.3120511008658064, Avg Validation Loss: 13.683760027271756\n",
            "Epoch 574/2048, Avg Training Loss: 0.3117254732546574, Avg Validation Loss: 13.667524513779552\n",
            "Epoch 575/2048, Avg Training Loss: 0.3114005460277218, Avg Validation Loss: 13.65131105642117\n",
            "Epoch 576/2048, Avg Training Loss: 0.3110763233147646, Avg Validation Loss: 13.635119998114707\n",
            "Epoch 577/2048, Avg Training Loss: 0.3107528097945512, Avg Validation Loss: 13.618951690405128\n",
            "Epoch 578/2048, Avg Training Loss: 0.3104300104167662, Avg Validation Loss: 13.60280648776841\n",
            "Epoch 579/2048, Avg Training Loss: 0.31010793043845863, Avg Validation Loss: 13.58668475539862\n",
            "Epoch 580/2048, Avg Training Loss: 0.30978657545113464, Avg Validation Loss: 13.570586879803106\n",
            "Epoch 581/2048, Avg Training Loss: 0.3094659506896226, Avg Validation Loss: 13.554513211396864\n",
            "Epoch 582/2048, Avg Training Loss: 0.30914604596534156, Avg Validation Loss: 13.538483968466604\n",
            "Epoch 583/2048, Avg Training Loss: 0.3088271609309294, Avg Validation Loss: 13.522519192787017\n",
            "Epoch 584/2048, Avg Training Loss: 0.3085091937118905, Avg Validation Loss: 13.50660033296902\n",
            "Epoch 585/2048, Avg Training Loss: 0.30819206609882865, Avg Validation Loss: 13.490717237705082\n",
            "Epoch 586/2048, Avg Training Loss: 0.3078757378327316, Avg Validation Loss: 13.47486492594291\n",
            "Epoch 587/2048, Avg Training Loss: 0.30756019261293216, Avg Validation Loss: 13.459040872165655\n",
            "Epoch 588/2048, Avg Training Loss: 0.3072454239237077, Avg Validation Loss: 13.44324387127266\n",
            "Epoch 589/2048, Avg Training Loss: 0.306931430429144, Avg Validation Loss: 13.427473427530384\n",
            "Epoch 590/2048, Avg Training Loss: 0.306618213526111, Avg Validation Loss: 13.411729425598418\n",
            "Epoch 591/2048, Avg Training Loss: 0.3063057760436303, Avg Validation Loss: 13.39601195340154\n",
            "Epoch 592/2048, Avg Training Loss: 0.3059941215513972, Avg Validation Loss: 13.380321206118573\n",
            "Epoch 593/2048, Avg Training Loss: 0.30568325399204893, Avg Validation Loss: 13.364657432658932\n",
            "Epoch 594/2048, Avg Training Loss: 0.30537317748550946, Avg Validation Loss: 13.349020913693682\n",
            "Epoch 595/2048, Avg Training Loss: 0.3050638962248046, Avg Validation Loss: 13.333411944378412\n",
            "Epoch 596/2048, Avg Training Loss: 0.30475541442050647, Avg Validation Loss: 13.31783082582431\n",
            "Epoch 597/2048, Avg Training Loss: 0.30444773627103117, Avg Validation Loss: 13.302277860780881\n",
            "Epoch 598/2048, Avg Training Loss: 0.30414086594668904, Avg Validation Loss: 13.286753351313095\n",
            "Epoch 599/2048, Avg Training Loss: 0.30383480758105136, Avg Validation Loss: 13.271257597553065\n",
            "Epoch 600/2048, Avg Training Loss: 0.3035295652662154, Avg Validation Loss: 13.255790897030728\n",
            "Epoch 601/2048, Avg Training Loss: 0.30322514305014653, Avg Validation Loss: 13.240353544316616\n",
            "Epoch 602/2048, Avg Training Loss: 0.30292154493513535, Avg Validation Loss: 13.224945830832981\n",
            "Epoch 603/2048, Avg Training Loss: 0.3026187748768515, Avg Validation Loss: 13.209568417169637\n",
            "Epoch 604/2048, Avg Training Loss: 0.3023169151785929, Avg Validation Loss: 13.194228525049226\n",
            "Epoch 605/2048, Avg Training Loss: 0.3020161663581961, Avg Validation Loss: 13.17892463248202\n",
            "Epoch 606/2048, Avg Training Loss: 0.3017164345657472, Avg Validation Loss: 13.163654280380442\n",
            "Epoch 607/2048, Avg Training Loss: 0.30141763518632625, Avg Validation Loss: 13.1484163103363\n",
            "Epoch 608/2048, Avg Training Loss: 0.30111972492496175, Avg Validation Loss: 13.13321024396743\n",
            "Epoch 609/2048, Avg Training Loss: 0.30082268247309457, Avg Validation Loss: 13.118035957484121\n",
            "Epoch 610/2048, Avg Training Loss: 0.30052629625603783, Avg Validation Loss: 13.102816878351286\n",
            "Epoch 611/2048, Avg Training Loss: 0.30023029679902075, Avg Validation Loss: 13.08757855676721\n",
            "Epoch 612/2048, Avg Training Loss: 0.2999349233592126, Avg Validation Loss: 13.072346067412834\n",
            "Epoch 613/2048, Avg Training Loss: 0.2996402992398806, Avg Validation Loss: 13.057137292112333\n",
            "Epoch 614/2048, Avg Training Loss: 0.2993465018147083, Avg Validation Loss: 13.041958267374348\n",
            "Epoch 615/2048, Avg Training Loss: 0.2990535511398058, Avg Validation Loss: 13.02681173316222\n",
            "Epoch 616/2048, Avg Training Loss: 0.2987614604689619, Avg Validation Loss: 13.011699231794829\n",
            "Epoch 617/2048, Avg Training Loss: 0.29847023847857745, Avg Validation Loss: 12.996621690609661\n",
            "Epoch 618/2048, Avg Training Loss: 0.29817989138814366, Avg Validation Loss: 12.981579702842511\n",
            "Epoch 619/2048, Avg Training Loss: 0.2978904240861087, Avg Validation Loss: 12.966573683151568\n",
            "Epoch 620/2048, Avg Training Loss: 0.2976018407276078, Avg Validation Loss: 12.951603948121424\n",
            "Epoch 621/2048, Avg Training Loss: 0.29731414415468244, Avg Validation Loss: 12.936670435658192\n",
            "Epoch 622/2048, Avg Training Loss: 0.2970273330452053, Avg Validation Loss: 12.921773435324528\n",
            "Epoch 623/2048, Avg Training Loss: 0.2967414132153455, Avg Validation Loss: 12.906913289801789\n",
            "Epoch 624/2048, Avg Training Loss: 0.2964563894579356, Avg Validation Loss: 12.892090271143394\n",
            "Epoch 625/2048, Avg Training Loss: 0.29617226574891287, Avg Validation Loss: 12.877304612295742\n",
            "Epoch 626/2048, Avg Training Loss: 0.295889045603189, Avg Validation Loss: 12.862556523973916\n",
            "Epoch 627/2048, Avg Training Loss: 0.2956067322640606, Avg Validation Loss: 12.847846203699559\n",
            "Epoch 628/2048, Avg Training Loss: 0.295325328804056, Avg Validation Loss: 12.833173840643756\n",
            "Epoch 629/2048, Avg Training Loss: 0.29504483817866994, Avg Validation Loss: 12.8185396182247\n",
            "Epoch 630/2048, Avg Training Loss: 0.29476526325503155, Avg Validation Loss: 12.803943715503552\n",
            "Epoch 631/2048, Avg Training Loss: 0.29448660682724054, Avg Validation Loss: 12.789386307937006\n",
            "Epoch 632/2048, Avg Training Loss: 0.2942088716246121, Avg Validation Loss: 12.774867567785542\n",
            "Epoch 633/2048, Avg Training Loss: 0.29393206031614966, Avg Validation Loss: 12.760387664337404\n",
            "Epoch 634/2048, Avg Training Loss: 0.29365617551301426, Avg Validation Loss: 12.745946764033935\n",
            "Epoch 635/2048, Avg Training Loss: 0.29338121976992937, Avg Validation Loss: 12.731545030542149\n",
            "Epoch 636/2048, Avg Training Loss: 0.29310719558602, Avg Validation Loss: 12.717182624799085\n",
            "Epoch 637/2048, Avg Training Loss: 0.29283410540535315, Avg Validation Loss: 12.702859705041112\n",
            "Epoch 638/2048, Avg Training Loss: 0.2925619516173215, Avg Validation Loss: 12.68857642682515\n",
            "Epoch 639/2048, Avg Training Loss: 0.29229073655694443, Avg Validation Loss: 12.674332943045695\n",
            "Epoch 640/2048, Avg Training Loss: 0.2920204625051264, Avg Validation Loss: 12.660129403949538\n",
            "Epoch 641/2048, Avg Training Loss: 0.29175113168889505, Avg Validation Loss: 12.645965957149327\n",
            "Epoch 642/2048, Avg Training Loss: 0.29148274628162846, Avg Validation Loss: 12.631842747636519\n",
            "Epoch 643/2048, Avg Training Loss: 0.2912153084032799, Avg Validation Loss: 12.61775991765149\n",
            "Epoch 644/2048, Avg Training Loss: 0.2909488201195125, Avg Validation Loss: 12.603717606258705\n",
            "Epoch 645/2048, Avg Training Loss: 0.29068328327213927, Avg Validation Loss: 12.589715949793556\n",
            "Epoch 646/2048, Avg Training Loss: 0.2904186996099597, Avg Validation Loss: 12.57575508201331\n",
            "Epoch 647/2048, Avg Training Loss: 0.29015507096799437, Avg Validation Loss: 12.561835134591089\n",
            "Epoch 648/2048, Avg Training Loss: 0.28989239788739274, Avg Validation Loss: 12.547954662939183\n",
            "Epoch 649/2048, Avg Training Loss: 0.289630649223523, Avg Validation Loss: 12.534113382731945\n",
            "Epoch 650/2048, Avg Training Loss: 0.2893698374193524, Avg Validation Loss: 12.520312349504781\n",
            "Epoch 651/2048, Avg Training Loss: 0.2891097712751339, Avg Validation Loss: 12.506494046866473\n",
            "Epoch 652/2048, Avg Training Loss: 0.28885040819103236, Avg Validation Loss: 12.492686511809854\n",
            "Epoch 653/2048, Avg Training Loss: 0.288591874820553, Avg Validation Loss: 12.478904881734051\n",
            "Epoch 654/2048, Avg Training Loss: 0.2883342667329299, Avg Validation Loss: 12.465166071688671\n",
            "Epoch 655/2048, Avg Training Loss: 0.2880776268878505, Avg Validation Loss: 12.451470707989658\n",
            "Epoch 656/2048, Avg Training Loss: 0.28782196505644964, Avg Validation Loss: 12.437816574421099\n",
            "Epoch 657/2048, Avg Training Loss: 0.28756721699446264, Avg Validation Loss: 12.424201300866875\n",
            "Epoch 658/2048, Avg Training Loss: 0.2873133918167319, Avg Validation Loss: 12.410627150366379\n",
            "Epoch 659/2048, Avg Training Loss: 0.2870605166199622, Avg Validation Loss: 12.397095372475647\n",
            "Epoch 660/2048, Avg Training Loss: 0.2868086064151656, Avg Validation Loss: 12.383606673685934\n",
            "Epoch 661/2048, Avg Training Loss: 0.2865576659955987, Avg Validation Loss: 12.370156610000517\n",
            "Epoch 662/2048, Avg Training Loss: 0.28630760134907435, Avg Validation Loss: 12.356744175857413\n",
            "Epoch 663/2048, Avg Training Loss: 0.2860585299567026, Avg Validation Loss: 12.343457476833748\n",
            "Epoch 664/2048, Avg Training Loss: 0.28581107977161113, Avg Validation Loss: 12.330288443136073\n",
            "Epoch 665/2048, Avg Training Loss: 0.28556492069725725, Avg Validation Loss: 12.317201489081471\n",
            "Epoch 666/2048, Avg Training Loss: 0.28531990481848846, Avg Validation Loss: 12.304177521742542\n",
            "Epoch 667/2048, Avg Training Loss: 0.28507595406115205, Avg Validation Loss: 12.291206319687317\n",
            "Epoch 668/2048, Avg Training Loss: 0.28483302737422445, Avg Validation Loss: 12.278282434316854\n",
            "Epoch 669/2048, Avg Training Loss: 0.2845911033378996, Avg Validation Loss: 12.265402984728885\n",
            "Epoch 670/2048, Avg Training Loss: 0.28435017093613124, Avg Validation Loss: 12.252566471017136\n",
            "Epoch 671/2048, Avg Training Loss: 0.2841102246578835, Avg Validation Loss: 12.239772135533157\n",
            "Epoch 672/2048, Avg Training Loss: 0.28387126189560985, Avg Validation Loss: 12.227019619044196\n",
            "Epoch 673/2048, Avg Training Loss: 0.283633281563493, Avg Validation Loss: 12.21430877562092\n",
            "Epoch 674/2048, Avg Training Loss: 0.2833962820175073, Avg Validation Loss: 12.201637799837203\n",
            "Epoch 675/2048, Avg Training Loss: 0.2831602269961081, Avg Validation Loss: 12.189006258273388\n",
            "Epoch 676/2048, Avg Training Loss: 0.2829251464133422, Avg Validation Loss: 12.17637736741859\n",
            "Epoch 677/2048, Avg Training Loss: 0.28269065881774347, Avg Validation Loss: 12.163742269274154\n",
            "Epoch 678/2048, Avg Training Loss: 0.28245697435596845, Avg Validation Loss: 12.151124337907637\n",
            "Epoch 679/2048, Avg Training Loss: 0.2822241487466976, Avg Validation Loss: 12.138533829804144\n",
            "Epoch 680/2048, Avg Training Loss: 0.2819921552932851, Avg Validation Loss: 12.125978340834855\n",
            "Epoch 681/2048, Avg Training Loss: 0.28176102129976766, Avg Validation Loss: 12.113449537927162\n",
            "Epoch 682/2048, Avg Training Loss: 0.28153074124751387, Avg Validation Loss: 12.101041877097028\n",
            "Epoch 683/2048, Avg Training Loss: 0.28130221804097544, Avg Validation Loss: 12.088781509459665\n",
            "Epoch 684/2048, Avg Training Loss: 0.2810750417258211, Avg Validation Loss: 12.076618799743054\n",
            "Epoch 685/2048, Avg Training Loss: 0.2808490400086425, Avg Validation Loss: 12.064527059123101\n",
            "Epoch 686/2048, Avg Training Loss: 0.28062412147018606, Avg Validation Loss: 12.052491955888083\n",
            "Epoch 687/2048, Avg Training Loss: 0.28040031786508723, Avg Validation Loss: 12.040544692270185\n",
            "Epoch 688/2048, Avg Training Loss: 0.2801777003292031, Avg Validation Loss: 12.028668289296796\n",
            "Epoch 689/2048, Avg Training Loss: 0.2799561588842341, Avg Validation Loss: 12.016847955044094\n",
            "Epoch 690/2048, Avg Training Loss: 0.27973564598696, Avg Validation Loss: 12.005075769274601\n",
            "Epoch 691/2048, Avg Training Loss: 0.2795161362412097, Avg Validation Loss: 11.993347514081762\n",
            "Epoch 692/2048, Avg Training Loss: 0.279297616005129, Avg Validation Loss: 11.981660966040039\n",
            "Epoch 693/2048, Avg Training Loss: 0.27908007787177536, Avg Validation Loss: 11.97001497570251\n",
            "Epoch 694/2048, Avg Training Loss: 0.2788635177356885, Avg Validation Loss: 11.958408971390638\n",
            "Epoch 695/2048, Avg Training Loss: 0.2786479332331971, Avg Validation Loss: 11.946842692088355\n",
            "Epoch 696/2048, Avg Training Loss: 0.2784333229129351, Avg Validation Loss: 11.935316042938911\n",
            "Epoch 697/2048, Avg Training Loss: 0.27821968579463463, Avg Validation Loss: 11.92382901676619\n",
            "Epoch 698/2048, Avg Training Loss: 0.278006824702079, Avg Validation Loss: 11.912322140887271\n",
            "Epoch 699/2048, Avg Training Loss: 0.2777946566335635, Avg Validation Loss: 11.900870601216301\n",
            "Epoch 700/2048, Avg Training Loss: 0.2775839241727069, Avg Validation Loss: 11.889536184249854\n",
            "Epoch 701/2048, Avg Training Loss: 0.2773744580194774, Avg Validation Loss: 11.878282429004628\n",
            "Epoch 702/2048, Avg Training Loss: 0.2771661162592262, Avg Validation Loss: 11.86708973025847\n",
            "Epoch 703/2048, Avg Training Loss: 0.2769588231198558, Avg Validation Loss: 11.855947564746911\n",
            "Epoch 704/2048, Avg Training Loss: 0.27675253794264304, Avg Validation Loss: 11.84485030360216\n",
            "Epoch 705/2048, Avg Training Loss: 0.2765472026460299, Avg Validation Loss: 11.833788691039794\n",
            "Epoch 706/2048, Avg Training Loss: 0.2763427830484485, Avg Validation Loss: 11.8227451303305\n",
            "Epoch 707/2048, Avg Training Loss: 0.27613911749019227, Avg Validation Loss: 11.811715597110299\n",
            "Epoch 708/2048, Avg Training Loss: 0.2759363170024232, Avg Validation Loss: 11.80071201635963\n",
            "Epoch 709/2048, Avg Training Loss: 0.27573442732484, Avg Validation Loss: 11.789740848967554\n",
            "Epoch 710/2048, Avg Training Loss: 0.2755334724031884, Avg Validation Loss: 11.77880560927792\n",
            "Epoch 711/2048, Avg Training Loss: 0.27533346456891, Avg Validation Loss: 11.767908222135011\n",
            "Epoch 712/2048, Avg Training Loss: 0.2751344099595551, Avg Validation Loss: 11.757049754387062\n",
            "Epoch 713/2048, Avg Training Loss: 0.27493631140370717, Avg Validation Loss: 11.746230809158444\n",
            "Epoch 714/2048, Avg Training Loss: 0.27473916995605835, Avg Validation Loss: 11.735451738395435\n",
            "Epoch 715/2048, Avg Training Loss: 0.2745429857141263, Avg Validation Loss: 11.72471275745318\n",
            "Epoch 716/2048, Avg Training Loss: 0.2743477582527577, Avg Validation Loss: 11.71401400687956\n",
            "Epoch 717/2048, Avg Training Loss: 0.27415348685529717, Avg Validation Loss: 11.703355585734846\n",
            "Epoch 718/2048, Avg Training Loss: 0.2739601706365915, Avg Validation Loss: 11.692737569566342\n",
            "Epoch 719/2048, Avg Training Loss: 0.2737678126649989, Avg Validation Loss: 11.682152456728831\n",
            "Epoch 720/2048, Avg Training Loss: 0.27357633621942834, Avg Validation Loss: 11.671598379763283\n",
            "Epoch 721/2048, Avg Training Loss: 0.27338574766614954, Avg Validation Loss: 11.66114413682422\n",
            "Epoch 722/2048, Avg Training Loss: 0.2731967173540162, Avg Validation Loss: 11.650808420609113\n",
            "Epoch 723/2048, Avg Training Loss: 0.2730089091407467, Avg Validation Loss: 11.640553745035634\n",
            "Epoch 724/2048, Avg Training Loss: 0.2728222396808215, Avg Validation Loss: 11.630360961915787\n",
            "Epoch 725/2048, Avg Training Loss: 0.272636692937972, Avg Validation Loss: 11.620216544039124\n",
            "Epoch 726/2048, Avg Training Loss: 0.27245213245845074, Avg Validation Loss: 11.610178004505805\n",
            "Epoch 727/2048, Avg Training Loss: 0.2722691552389678, Avg Validation Loss: 11.600258523968973\n",
            "Epoch 728/2048, Avg Training Loss: 0.27208741157514427, Avg Validation Loss: 11.590418912451371\n",
            "Epoch 729/2048, Avg Training Loss: 0.2719067493405618, Avg Validation Loss: 11.580638070333917\n",
            "Epoch 730/2048, Avg Training Loss: 0.27172708671482076, Avg Validation Loss: 11.570904656300279\n",
            "Epoch 731/2048, Avg Training Loss: 0.2715483792929741, Avg Validation Loss: 11.561212593995242\n",
            "Epoch 732/2048, Avg Training Loss: 0.271370601617846, Avg Validation Loss: 11.551559887489791\n",
            "Epoch 733/2048, Avg Training Loss: 0.2711937549162454, Avg Validation Loss: 11.541946055859578\n",
            "Epoch 734/2048, Avg Training Loss: 0.271017824512184, Avg Validation Loss: 11.532369029119623\n",
            "Epoch 735/2048, Avg Training Loss: 0.27084280150410434, Avg Validation Loss: 11.522827731381883\n",
            "Epoch 736/2048, Avg Training Loss: 0.270668680337807, Avg Validation Loss: 11.513321622199017\n",
            "Epoch 737/2048, Avg Training Loss: 0.2704954572423983, Avg Validation Loss: 11.503850448994378\n",
            "Epoch 738/2048, Avg Training Loss: 0.27032312939550984, Avg Validation Loss: 11.494414113419936\n",
            "Epoch 739/2048, Avg Training Loss: 0.2701516944775651, Avg Validation Loss: 11.485012599210165\n",
            "Epoch 740/2048, Avg Training Loss: 0.26998115043364623, Avg Validation Loss: 11.475645933233038\n",
            "Epoch 741/2048, Avg Training Loss: 0.2698114953461547, Avg Validation Loss: 11.466314164463347\n",
            "Epoch 742/2048, Avg Training Loss: 0.26964272736661543, Avg Validation Loss: 11.457017352632587\n",
            "Epoch 743/2048, Avg Training Loss: 0.2694748446790587, Avg Validation Loss: 11.447755562103982\n",
            "Epoch 744/2048, Avg Training Loss: 0.2693078454802691, Avg Validation Loss: 11.438528858569185\n",
            "Epoch 745/2048, Avg Training Loss: 0.2691417279690483, Avg Validation Loss: 11.429337307269071\n",
            "Epoch 746/2048, Avg Training Loss: 0.26897649034030163, Avg Validation Loss: 11.42018097203786\n",
            "Epoch 747/2048, Avg Training Loss: 0.2688121307817094, Avg Validation Loss: 11.411059914792178\n",
            "Epoch 748/2048, Avg Training Loss: 0.26864864747178807, Avg Validation Loss: 11.4019741952607\n",
            "Epoch 749/2048, Avg Training Loss: 0.26848603857870457, Avg Validation Loss: 11.392923870843953\n",
            "Epoch 750/2048, Avg Training Loss: 0.2683243022595005, Avg Validation Loss: 11.383908996544728\n",
            "Epoch 751/2048, Avg Training Loss: 0.2681634366595462, Avg Validation Loss: 11.374929624958996\n",
            "Epoch 752/2048, Avg Training Loss: 0.26800343543879723, Avg Validation Loss: 11.36599422706698\n",
            "Epoch 753/2048, Avg Training Loss: 0.26784438341063654, Avg Validation Loss: 11.357104981083868\n",
            "Epoch 754/2048, Avg Training Loss: 0.2676862337932218, Avg Validation Loss: 11.348256844073505\n",
            "Epoch 755/2048, Avg Training Loss: 0.267528966352069, Avg Validation Loss: 11.339447113354309\n",
            "Epoch 756/2048, Avg Training Loss: 0.26737256937668963, Avg Validation Loss: 11.330674349095494\n",
            "Epoch 757/2048, Avg Training Loss: 0.2672170356992005, Avg Validation Loss: 11.321937792489903\n",
            "Epoch 758/2048, Avg Training Loss: 0.2670623605703975, Avg Validation Loss: 11.313237051517575\n",
            "Epoch 759/2048, Avg Training Loss: 0.26690854052634494, Avg Validation Loss: 11.304571931218673\n",
            "Epoch 760/2048, Avg Training Loss: 0.266755572783395, Avg Validation Loss: 11.29594234201575\n",
            "Epoch 761/2048, Avg Training Loss: 0.2666034549151929, Avg Validation Loss: 11.2873482501942\n",
            "Epoch 762/2048, Avg Training Loss: 0.26645218468018333, Avg Validation Loss: 11.278789651156128\n",
            "Epoch 763/2048, Avg Training Loss: 0.2663017599294565, Avg Validation Loss: 11.270266554977185\n",
            "Epoch 764/2048, Avg Training Loss: 0.26615217855748496, Avg Validation Loss: 11.261778978610348\n",
            "Epoch 765/2048, Avg Training Loss: 0.2660034384757648, Avg Validation Loss: 11.253326941681152\n",
            "Epoch 766/2048, Avg Training Loss: 0.26585553759868985, Avg Validation Loss: 11.244910464223633\n",
            "Epoch 767/2048, Avg Training Loss: 0.2657084738359638, Avg Validation Loss: 11.236529565465066\n",
            "Epoch 768/2048, Avg Training Loss: 0.26556224508850895, Avg Validation Loss: 11.228184263177628\n",
            "Epoch 769/2048, Avg Training Loss: 0.26541684924624825, Avg Validation Loss: 11.219874573336517\n",
            "Epoch 770/2048, Avg Training Loss: 0.2652722841868915, Avg Validation Loss: 11.211600509943802\n",
            "Epoch 771/2048, Avg Training Loss: 0.2651285477752654, Avg Validation Loss: 11.203362084941965\n",
            "Epoch 772/2048, Avg Training Loss: 0.2649856378629373, Avg Validation Loss: 11.195159308176041\n",
            "Epoch 773/2048, Avg Training Loss: 0.2648435522880016, Avg Validation Loss: 11.186992187382033\n",
            "Epoch 774/2048, Avg Training Loss: 0.2647022888749582, Avg Validation Loss: 11.178860728189726\n",
            "Epoch 775/2048, Avg Training Loss: 0.26456184543464395, Avg Validation Loss: 11.170764934133226\n",
            "Epoch 776/2048, Avg Training Loss: 0.26442224055908053, Avg Validation Loss: 11.162705645819056\n",
            "Epoch 777/2048, Avg Training Loss: 0.2642835376481462, Avg Validation Loss: 11.154682560317205\n",
            "Epoch 778/2048, Avg Training Loss: 0.26414569826983775, Avg Validation Loss: 11.146695413664096\n",
            "Epoch 779/2048, Avg Training Loss: 0.2640086943919119, Avg Validation Loss: 11.138744075921316\n",
            "Epoch 780/2048, Avg Training Loss: 0.26387251009731777, Avg Validation Loss: 11.130828481386784\n",
            "Epoch 781/2048, Avg Training Loss: 0.2637371239330832, Avg Validation Loss: 11.122948033599908\n",
            "Epoch 782/2048, Avg Training Loss: 0.26360246831976136, Avg Validation Loss: 11.115102790014742\n",
            "Epoch 783/2048, Avg Training Loss: 0.2634685574342231, Avg Validation Loss: 11.107292805207397\n",
            "Epoch 784/2048, Avg Training Loss: 0.26333538114116495, Avg Validation Loss: 11.099517718757946\n",
            "Epoch 785/2048, Avg Training Loss: 0.2632029509490222, Avg Validation Loss: 11.0917775498667\n",
            "Epoch 786/2048, Avg Training Loss: 0.2630712881821442, Avg Validation Loss: 11.084072597723493\n",
            "Epoch 787/2048, Avg Training Loss: 0.2629404040502391, Avg Validation Loss: 11.07640300519319\n",
            "Epoch 788/2048, Avg Training Loss: 0.26281030339170613, Avg Validation Loss: 11.068768834120323\n",
            "Epoch 789/2048, Avg Training Loss: 0.2626809876554421, Avg Validation Loss: 11.061170104024692\n",
            "Epoch 790/2048, Avg Training Loss: 0.2625524564843151, Avg Validation Loss: 11.053605288798282\n",
            "Epoch 791/2048, Avg Training Loss: 0.26242460988893623, Avg Validation Loss: 11.046044111275032\n",
            "Epoch 792/2048, Avg Training Loss: 0.26229716065825215, Avg Validation Loss: 11.038491755003443\n",
            "Epoch 793/2048, Avg Training Loss: 0.26217025093828805, Avg Validation Loss: 11.030961018409053\n",
            "Epoch 794/2048, Avg Training Loss: 0.26204399696758696, Avg Validation Loss: 11.02345872213058\n",
            "Epoch 795/2048, Avg Training Loss: 0.2619184593954691, Avg Validation Loss: 11.015988491162695\n",
            "Epoch 796/2048, Avg Training Loss: 0.2617937123180266, Avg Validation Loss: 11.008614859506027\n",
            "Epoch 797/2048, Avg Training Loss: 0.26167012290681596, Avg Validation Loss: 11.00133166722465\n",
            "Epoch 798/2048, Avg Training Loss: 0.26154746581521854, Avg Validation Loss: 10.994112466953824\n",
            "Epoch 799/2048, Avg Training Loss: 0.26142566190433253, Avg Validation Loss: 10.9869423779295\n",
            "Epoch 800/2048, Avg Training Loss: 0.26130465464904334, Avg Validation Loss: 10.979813097499768\n",
            "Epoch 801/2048, Avg Training Loss: 0.26118442195646696, Avg Validation Loss: 10.972720990973446\n",
            "Epoch 802/2048, Avg Training Loss: 0.261064954547366, Avg Validation Loss: 10.965664084118075\n",
            "Epoch 803/2048, Avg Training Loss: 0.26094624621217694, Avg Validation Loss: 10.958641302583745\n",
            "Epoch 804/2048, Avg Training Loss: 0.260828292388465, Avg Validation Loss: 10.951652058683624\n",
            "Epoch 805/2048, Avg Training Loss: 0.26071108939671306, Avg Validation Loss: 10.94469602752261\n",
            "Epoch 806/2048, Avg Training Loss: 0.26059463402954247, Avg Validation Loss: 10.937773025707793\n",
            "Epoch 807/2048, Avg Training Loss: 0.26047892333082723, Avg Validation Loss: 10.930882945637924\n",
            "Epoch 808/2048, Avg Training Loss: 0.2603639544768857, Avg Validation Loss: 10.924025719907211\n",
            "Epoch 809/2048, Avg Training Loss: 0.2602497247125617, Avg Validation Loss: 10.917201302026644\n",
            "Epoch 810/2048, Avg Training Loss: 0.2601362313168279, Avg Validation Loss: 10.910409655987245\n",
            "Epoch 811/2048, Avg Training Loss: 0.26002349031571637, Avg Validation Loss: 10.903650882607106\n",
            "Epoch 812/2048, Avg Training Loss: 0.2599115384198311, Avg Validation Loss: 10.896924885619589\n",
            "Epoch 813/2048, Avg Training Loss: 0.25980034842726857, Avg Validation Loss: 10.890231601536582\n",
            "Epoch 814/2048, Avg Training Loss: 0.25968990031449496, Avg Validation Loss: 10.883570995259785\n",
            "Epoch 815/2048, Avg Training Loss: 0.2595801822055966, Avg Validation Loss: 10.876943039415597\n",
            "Epoch 816/2048, Avg Training Loss: 0.25947118313010126, Avg Validation Loss: 10.870347772725399\n",
            "Epoch 817/2048, Avg Training Loss: 0.25936288655221623, Avg Validation Loss: 10.863785163479886\n",
            "Epoch 818/2048, Avg Training Loss: 0.2592552933404713, Avg Validation Loss: 10.857255148683736\n",
            "Epoch 819/2048, Avg Training Loss: 0.25914840499474984, Avg Validation Loss: 10.850757672094327\n",
            "Epoch 820/2048, Avg Training Loss: 0.25904222103752367, Avg Validation Loss: 10.844292681819233\n",
            "Epoch 821/2048, Avg Training Loss: 0.2589367399395897, Avg Validation Loss: 10.837860128582268\n",
            "Epoch 822/2048, Avg Training Loss: 0.2588319596111401, Avg Validation Loss: 10.831459964238311\n",
            "Epoch 823/2048, Avg Training Loss: 0.25872787766227046, Avg Validation Loss: 10.825092140765415\n",
            "Epoch 824/2048, Avg Training Loss: 0.2586244915411922, Avg Validation Loss: 10.818756609642346\n",
            "Epoch 825/2048, Avg Training Loss: 0.25852179860757984, Avg Validation Loss: 10.812453321464034\n",
            "Epoch 826/2048, Avg Training Loss: 0.2584197961715155, Avg Validation Loss: 10.806182225712778\n",
            "Epoch 827/2048, Avg Training Loss: 0.2583184815141902, Avg Validation Loss: 10.799943270628221\n",
            "Epoch 828/2048, Avg Training Loss: 0.2582178518989292, Avg Validation Loss: 10.79373640319456\n",
            "Epoch 829/2048, Avg Training Loss: 0.2581179045770894, Avg Validation Loss: 10.78756156977305\n",
            "Epoch 830/2048, Avg Training Loss: 0.25801863679123715, Avg Validation Loss: 10.781418714829496\n",
            "Epoch 831/2048, Avg Training Loss: 0.25792004577688776, Avg Validation Loss: 10.77530778122793\n",
            "Epoch 832/2048, Avg Training Loss: 0.2578221287634808, Avg Validation Loss: 10.769228710493959\n",
            "Epoch 833/2048, Avg Training Loss: 0.2577248829749545, Avg Validation Loss: 10.763181442830374\n",
            "Epoch 834/2048, Avg Training Loss: 0.257628305630107, Avg Validation Loss: 10.757165917135227\n",
            "Epoch 835/2048, Avg Training Loss: 0.2575323939428463, Avg Validation Loss: 10.751182071021386\n",
            "Epoch 836/2048, Avg Training Loss: 0.2574371451223832, Avg Validation Loss: 10.74522984083676\n",
            "Epoch 837/2048, Avg Training Loss: 0.25734255637339365, Avg Validation Loss: 10.73930916168499\n",
            "Epoch 838/2048, Avg Training Loss: 0.2572486248961679, Avg Validation Loss: 10.733419967446217\n",
            "Epoch 839/2048, Avg Training Loss: 0.2571553478867514, Avg Validation Loss: 10.727562190797947\n",
            "Epoch 840/2048, Avg Training Loss: 0.25706272253708423, Avg Validation Loss: 10.721735763235845\n",
            "Epoch 841/2048, Avg Training Loss: 0.25697074603513975, Avg Validation Loss: 10.715940615094434\n",
            "Epoch 842/2048, Avg Training Loss: 0.25687941556506266, Avg Validation Loss: 10.710176675567725\n",
            "Epoch 843/2048, Avg Training Loss: 0.25678872830731014, Avg Validation Loss: 10.704443872729687\n",
            "Epoch 844/2048, Avg Training Loss: 0.25669868143879226, Avg Validation Loss: 10.69874213355461\n",
            "Epoch 845/2048, Avg Training Loss: 0.2566092721330147, Avg Validation Loss: 10.693071383937303\n",
            "Epoch 846/2048, Avg Training Loss: 0.25652048407253136, Avg Validation Loss: 10.687425191332828\n",
            "Epoch 847/2048, Avg Training Loss: 0.2564320499918646, Avg Validation Loss: 10.681793581283584\n",
            "Epoch 848/2048, Avg Training Loss: 0.25634394395857824, Avg Validation Loss: 10.676183920149459\n",
            "Epoch 849/2048, Avg Training Loss: 0.25625630725980514, Avg Validation Loss: 10.670600501466836\n",
            "Epoch 850/2048, Avg Training Loss: 0.2561692161996106, Avg Validation Loss: 10.665045566205741\n",
            "Epoch 851/2048, Avg Training Loss: 0.2560827099976606, Avg Validation Loss: 10.659520264489052\n",
            "Epoch 852/2048, Avg Training Loss: 0.25599680813992304, Avg Validation Loss: 10.654025166560617\n",
            "Epoch 853/2048, Avg Training Loss: 0.2559115196116539, Avg Validation Loss: 10.648560534239826\n",
            "Epoch 854/2048, Avg Training Loss: 0.2558268478103391, Avg Validation Loss: 10.643126465127382\n",
            "Epoch 855/2048, Avg Training Loss: 0.2557427931598558, Avg Validation Loss: 10.637722969212588\n",
            "Epoch 856/2048, Avg Training Loss: 0.25565935450147254, Avg Validation Loss: 10.632350009572363\n",
            "Epoch 857/2048, Avg Training Loss: 0.25557652983403156, Avg Validation Loss: 10.627007523996628\n",
            "Epoch 858/2048, Avg Training Loss: 0.2554943167078503, Avg Validation Loss: 10.621695436482431\n",
            "Epoch 859/2048, Avg Training Loss: 0.2554127089928743, Avg Validation Loss: 10.616412798384115\n",
            "Epoch 860/2048, Avg Training Loss: 0.25533170356735935, Avg Validation Loss: 10.611159922541667\n",
            "Epoch 861/2048, Avg Training Loss: 0.2552513000606949, Avg Validation Loss: 10.605936945856353\n",
            "Epoch 862/2048, Avg Training Loss: 0.25517149609941053, Avg Validation Loss: 10.600743898645876\n",
            "Epoch 863/2048, Avg Training Loss: 0.2550922890390029, Avg Validation Loss: 10.595580753272916\n",
            "Epoch 864/2048, Avg Training Loss: 0.2550136760900539, Avg Validation Loss: 10.590447450451235\n",
            "Epoch 865/2048, Avg Training Loss: 0.254935654385765, Avg Validation Loss: 10.585343913480616\n",
            "Epoch 866/2048, Avg Training Loss: 0.25485822101815586, Avg Validation Loss: 10.580270055953637\n",
            "Epoch 867/2048, Avg Training Loss: 0.2547813730575021, Avg Validation Loss: 10.575225785931705\n",
            "Epoch 868/2048, Avg Training Loss: 0.25470510756280407, Avg Validation Loss: 10.57021100821134\n",
            "Epoch 869/2048, Avg Training Loss: 0.25462941758219104, Avg Validation Loss: 10.565225963615788\n",
            "Epoch 870/2048, Avg Training Loss: 0.25455426842987233, Avg Validation Loss: 10.560271378769325\n",
            "Epoch 871/2048, Avg Training Loss: 0.2544796237017212, Avg Validation Loss: 10.55534659711818\n",
            "Epoch 872/2048, Avg Training Loss: 0.25440551503029074, Avg Validation Loss: 10.55045258873695\n",
            "Epoch 873/2048, Avg Training Loss: 0.25433200404524553, Avg Validation Loss: 10.545604022877097\n",
            "Epoch 874/2048, Avg Training Loss: 0.25425910455172, Avg Validation Loss: 10.540794915180749\n",
            "Epoch 875/2048, Avg Training Loss: 0.2541867835737446, Avg Validation Loss: 10.536019740522937\n",
            "Epoch 876/2048, Avg Training Loss: 0.2541150285848303, Avg Validation Loss: 10.531275475109222\n",
            "Epoch 877/2048, Avg Training Loss: 0.25404383153373994, Avg Validation Loss: 10.526560447549231\n",
            "Epoch 878/2048, Avg Training Loss: 0.253973186770764, Avg Validation Loss: 10.521873717522315\n",
            "Epoch 879/2048, Avg Training Loss: 0.25390308993607125, Avg Validation Loss: 10.517214739742029\n",
            "Epoch 880/2048, Avg Training Loss: 0.2538335373633313, Avg Validation Loss: 10.512583182210895\n",
            "Epoch 881/2048, Avg Training Loss: 0.25376452575963415, Avg Validation Loss: 10.507978827921644\n",
            "Epoch 882/2048, Avg Training Loss: 0.25369605203366913, Avg Validation Loss: 10.503401521692677\n",
            "Epoch 883/2048, Avg Training Loss: 0.25362811320349343, Avg Validation Loss: 10.498851141416843\n",
            "Epoch 884/2048, Avg Training Loss: 0.2535607063470399, Avg Validation Loss: 10.494327582542377\n",
            "Epoch 885/2048, Avg Training Loss: 0.2534938285755809, Avg Validation Loss: 10.489830749760825\n",
            "Epoch 886/2048, Avg Training Loss: 0.25342747701952334, Avg Validation Loss: 10.485360552230794\n",
            "Epoch 887/2048, Avg Training Loss: 0.2533616488208304, Avg Validation Loss: 10.480916901235009\n",
            "Epoch 888/2048, Avg Training Loss: 0.2532963411290037, Avg Validation Loss: 10.47649970888787\n",
            "Epoch 889/2048, Avg Training Loss: 0.25323155109897866, Avg Validation Loss: 10.47210888743295\n",
            "Epoch 890/2048, Avg Training Loss: 0.2531672758900491, Avg Validation Loss: 10.4677443488687\n",
            "Epoch 891/2048, Avg Training Loss: 0.25310351266534437, Avg Validation Loss: 10.46340600474441\n",
            "Epoch 892/2048, Avg Training Loss: 0.25304025859160384, Avg Validation Loss: 10.45909376608679\n",
            "Epoch 893/2048, Avg Training Loss: 0.2529775108391109, Avg Validation Loss: 10.45480754334958\n",
            "Epoch 894/2048, Avg Training Loss: 0.2529152665817136, Avg Validation Loss: 10.450547246396425\n",
            "Epoch 895/2048, Avg Training Loss: 0.25285352299688996, Avg Validation Loss: 10.446312784500778\n",
            "Epoch 896/2048, Avg Training Loss: 0.252792277265839, Avg Validation Loss: 10.442104066353853\n",
            "Epoch 897/2048, Avg Training Loss: 0.25273152657358455, Avg Validation Loss: 10.437921000076868\n",
            "Epoch 898/2048, Avg Training Loss: 0.2526712681090855, Avg Validation Loss: 10.43376349323552\n",
            "Epoch 899/2048, Avg Training Loss: 0.25261149906534985, Avg Validation Loss: 10.429631452855567\n",
            "Epoch 900/2048, Avg Training Loss: 0.2525522166395503, Avg Validation Loss: 10.425524785438846\n",
            "Epoch 901/2048, Avg Training Loss: 0.2524934661894912, Avg Validation Loss: 10.42137496957958\n",
            "Epoch 902/2048, Avg Training Loss: 0.2524346823669065, Avg Validation Loss: 10.417164757207418\n",
            "Epoch 903/2048, Avg Training Loss: 0.2523762066389377, Avg Validation Loss: 10.412935669589018\n",
            "Epoch 904/2048, Avg Training Loss: 0.25231812671956017, Avg Validation Loss: 10.408710111602192\n",
            "Epoch 905/2048, Avg Training Loss: 0.2522604882435754, Avg Validation Loss: 10.404500129156267\n",
            "Epoch 906/2048, Avg Training Loss: 0.25220331420375985, Avg Validation Loss: 10.40031215705966\n",
            "Epoch 907/2048, Avg Training Loss: 0.2521466154453516, Avg Validation Loss: 10.396149590782777\n",
            "Epoch 908/2048, Avg Training Loss: 0.25209039630864255, Avg Validation Loss: 10.392014180549099\n",
            "Epoch 909/2048, Avg Training Loss: 0.25203465765606126, Avg Validation Loss: 10.387906786241416\n",
            "Epoch 910/2048, Avg Training Loss: 0.2519793984941691, Avg Validation Loss: 10.383827786247084\n",
            "Epoch 911/2048, Avg Training Loss: 0.25192461684221973, Avg Validation Loss: 10.379777298881864\n",
            "Epoch 912/2048, Avg Training Loss: 0.25187031019710343, Avg Validation Loss: 10.375755302304851\n",
            "Epoch 913/2048, Avg Training Loss: 0.251816468742045, Avg Validation Loss: 10.371764061273653\n",
            "Epoch 914/2048, Avg Training Loss: 0.251763042481306, Avg Validation Loss: 10.367802654520112\n",
            "Epoch 915/2048, Avg Training Loss: 0.25171004591751106, Avg Validation Loss: 10.36387012399964\n",
            "Epoch 916/2048, Avg Training Loss: 0.2516574947832614, Avg Validation Loss: 10.359965852480398\n",
            "Epoch 917/2048, Avg Training Loss: 0.2516053959934679, Avg Validation Loss: 10.356089416855548\n",
            "Epoch 918/2048, Avg Training Loss: 0.25155375178641776, Avg Validation Loss: 10.35224050451713\n",
            "Epoch 919/2048, Avg Training Loss: 0.25150256192296894, Avg Validation Loss: 10.348418865813551\n",
            "Epoch 920/2048, Avg Training Loss: 0.2514518248543177, Avg Validation Loss: 10.344624287090133\n",
            "Epoch 921/2048, Avg Training Loss: 0.2514015383420218, Avg Validation Loss: 10.34085657543642\n",
            "Epoch 922/2048, Avg Training Loss: 0.2513516997871771, Avg Validation Loss: 10.337115550073639\n",
            "Epoch 923/2048, Avg Training Loss: 0.2513023064051698, Avg Validation Loss: 10.333401037499037\n",
            "Epoch 924/2048, Avg Training Loss: 0.25125335531844195, Avg Validation Loss: 10.329712868751008\n",
            "Epoch 925/2048, Avg Training Loss: 0.2512048436057327, Avg Validation Loss: 10.326050877868996\n",
            "Epoch 926/2048, Avg Training Loss: 0.25115676832821415, Avg Validation Loss: 10.322414901025175\n",
            "Epoch 927/2048, Avg Training Loss: 0.2511091265433615, Avg Validation Loss: 10.318804776033321\n",
            "Epoch 928/2048, Avg Training Loss: 0.2510619153123149, Avg Validation Loss: 10.315220342069107\n",
            "Epoch 929/2048, Avg Training Loss: 0.2510151317037848, Avg Validation Loss: 10.311661439508834\n",
            "Epoch 930/2048, Avg Training Loss: 0.2509687727961253, Avg Validation Loss: 10.308127909834477\n",
            "Epoch 931/2048, Avg Training Loss: 0.2509228356784335, Avg Validation Loss: 10.304619595575769\n",
            "Epoch 932/2048, Avg Training Loss: 0.2508773174511349, Avg Validation Loss: 10.301136340273157\n",
            "Epoch 933/2048, Avg Training Loss: 0.2508322152262934, Avg Validation Loss: 10.297677988452406\n",
            "Epoch 934/2048, Avg Training Loss: 0.25078752612777866, Avg Validation Loss: 10.294244385605818\n",
            "Epoch 935/2048, Avg Training Loss: 0.2507432472913552, Avg Validation Loss: 10.29083537817728\n",
            "Epoch 936/2048, Avg Training Loss: 0.2506993758647328, Avg Validation Loss: 10.287450813549466\n",
            "Epoch 937/2048, Avg Training Loss: 0.2506559090075949, Avg Validation Loss: 10.284090540032414\n",
            "Epoch 938/2048, Avg Training Loss: 0.2506128438916162, Avg Validation Loss: 10.280754406852903\n",
            "Epoch 939/2048, Avg Training Loss: 0.2505701740038267, Avg Validation Loss: 10.277443559607025\n",
            "Epoch 940/2048, Avg Training Loss: 0.25052787414421057, Avg Validation Loss: 10.274157230233126\n",
            "Epoch 941/2048, Avg Training Loss: 0.25048595434750504, Avg Validation Loss: 10.270894924060913\n",
            "Epoch 942/2048, Avg Training Loss: 0.2504443951045283, Avg Validation Loss: 10.267595028233568\n",
            "Epoch 943/2048, Avg Training Loss: 0.25040294906642657, Avg Validation Loss: 10.264264479368151\n",
            "Epoch 944/2048, Avg Training Loss: 0.25036180823351545, Avg Validation Loss: 10.260929633112736\n",
            "Epoch 945/2048, Avg Training Loss: 0.25032103762316477, Avg Validation Loss: 10.257665768759766\n",
            "Epoch 946/2048, Avg Training Loss: 0.2502809027242291, Avg Validation Loss: 10.254473279758644\n",
            "Epoch 947/2048, Avg Training Loss: 0.25024119968415637, Avg Validation Loss: 10.251268664742929\n",
            "Epoch 948/2048, Avg Training Loss: 0.25020166973809044, Avg Validation Loss: 10.248107819715445\n",
            "Epoch 949/2048, Avg Training Loss: 0.2501627004101165, Avg Validation Loss: 10.24494241640738\n",
            "Epoch 950/2048, Avg Training Loss: 0.2501238903466989, Avg Validation Loss: 10.241763787505205\n",
            "Epoch 951/2048, Avg Training Loss: 0.25008540148274955, Avg Validation Loss: 10.23858971443788\n",
            "Epoch 952/2048, Avg Training Loss: 0.2500472806332143, Avg Validation Loss: 10.235490500395015\n",
            "Epoch 953/2048, Avg Training Loss: 0.25000977684548154, Avg Validation Loss: 10.232464076757346\n",
            "Epoch 954/2048, Avg Training Loss: 0.24997268645067092, Avg Validation Loss: 10.22942607146331\n",
            "Epoch 955/2048, Avg Training Loss: 0.24993573583989306, Avg Validation Loss: 10.226370719535534\n",
            "Epoch 956/2048, Avg Training Loss: 0.2498990898972114, Avg Validation Loss: 10.223317289562486\n",
            "Epoch 957/2048, Avg Training Loss: 0.24986277498143053, Avg Validation Loss: 10.220276127429285\n",
            "Epoch 958/2048, Avg Training Loss: 0.24982680389264353, Avg Validation Loss: 10.217252745711216\n",
            "Epoch 959/2048, Avg Training Loss: 0.24979120243582495, Avg Validation Loss: 10.214310469318983\n",
            "Epoch 960/2048, Avg Training Loss: 0.24975618162503405, Avg Validation Loss: 10.211383290546681\n",
            "Epoch 961/2048, Avg Training Loss: 0.24972132834175081, Avg Validation Loss: 10.208452818113\n",
            "Epoch 962/2048, Avg Training Loss: 0.24968678745530481, Avg Validation Loss: 10.205531374232415\n",
            "Epoch 963/2048, Avg Training Loss: 0.24965257396759982, Avg Validation Loss: 10.20262554663487\n",
            "Epoch 964/2048, Avg Training Loss: 0.24961869463333322, Avg Validation Loss: 10.199738817202508\n",
            "Epoch 965/2048, Avg Training Loss: 0.24958515179049479, Avg Validation Loss: 10.196872986221496\n",
            "Epoch 966/2048, Avg Training Loss: 0.24955194541859504, Avg Validation Loss: 10.194028943952171\n",
            "Epoch 967/2048, Avg Training Loss: 0.2495190742425057, Avg Validation Loss: 10.191207088582205\n",
            "Epoch 968/2048, Avg Training Loss: 0.24948653632383536, Avg Validation Loss: 10.188407552616079\n",
            "Epoch 969/2048, Avg Training Loss: 0.24945432937755935, Avg Validation Loss: 10.185630325494907\n",
            "Epoch 970/2048, Avg Training Loss: 0.24942244997017252, Avg Validation Loss: 10.18287639446056\n",
            "Epoch 971/2048, Avg Training Loss: 0.24939090402485176, Avg Validation Loss: 10.180146699356897\n",
            "Epoch 972/2048, Avg Training Loss: 0.24935968318359786, Avg Validation Loss: 10.177440078344295\n",
            "Epoch 973/2048, Avg Training Loss: 0.24932878388451624, Avg Validation Loss: 10.174755823709544\n",
            "Epoch 974/2048, Avg Training Loss: 0.24929820301575456, Avg Validation Loss: 10.172093474369374\n",
            "Epoch 975/2048, Avg Training Loss: 0.24926793771488073, Avg Validation Loss: 10.169452703557043\n",
            "Epoch 976/2048, Avg Training Loss: 0.24923798526121185, Avg Validation Loss: 10.166833257985006\n",
            "Epoch 977/2048, Avg Training Loss: 0.2492083430179824, Avg Validation Loss: 10.164234924897734\n",
            "Epoch 978/2048, Avg Training Loss: 0.24917900411129085, Avg Validation Loss: 10.161653803023452\n",
            "Epoch 979/2048, Avg Training Loss: 0.24915002123838934, Avg Validation Loss: 10.159091501309641\n",
            "Epoch 980/2048, Avg Training Loss: 0.2491213667720253, Avg Validation Loss: 10.15654884591018\n",
            "Epoch 981/2048, Avg Training Loss: 0.2490930248873622, Avg Validation Loss: 10.154026224349654\n",
            "Epoch 982/2048, Avg Training Loss: 0.2490649845909676, Avg Validation Loss: 10.151525040582614\n",
            "Epoch 983/2048, Avg Training Loss: 0.24903723638318584, Avg Validation Loss: 10.149044710493321\n",
            "Epoch 984/2048, Avg Training Loss: 0.24900977826120155, Avg Validation Loss: 10.146584842105375\n",
            "Epoch 985/2048, Avg Training Loss: 0.24898260803564018, Avg Validation Loss: 10.144145148786972\n",
            "Epoch 986/2048, Avg Training Loss: 0.24895572342794203, Avg Validation Loss: 10.141725401990286\n",
            "Epoch 987/2048, Avg Training Loss: 0.2489291221209712, Avg Validation Loss: 10.139325405514558\n",
            "Epoch 988/2048, Avg Training Loss: 0.2489028017852927, Avg Validation Loss: 10.136944981487034\n",
            "Epoch 989/2048, Avg Training Loss: 0.24887676009278012, Avg Validation Loss: 10.134583962722484\n",
            "Epoch 990/2048, Avg Training Loss: 0.2488509947236312, Avg Validation Loss: 10.13224218855475\n",
            "Epoch 991/2048, Avg Training Loss: 0.2488255033699541, Avg Validation Loss: 10.129919502557838\n",
            "Epoch 992/2048, Avg Training Loss: 0.24880028373757376, Avg Validation Loss: 10.127615751295409\n",
            "Epoch 993/2048, Avg Training Loss: 0.24877533354691467, Avg Validation Loss: 10.125330783629966\n",
            "Epoch 994/2048, Avg Training Loss: 0.24875065053340453, Avg Validation Loss: 10.123064450336734\n",
            "Epoch 995/2048, Avg Training Loss: 0.2487262324476322, Avg Validation Loss: 10.12081660388344\n",
            "Epoch 996/2048, Avg Training Loss: 0.24870207705537878, Avg Validation Loss: 10.118587098300614\n",
            "Epoch 997/2048, Avg Training Loss: 0.24867818213758497, Avg Validation Loss: 10.116375789101225\n",
            "Epoch 998/2048, Avg Training Loss: 0.24865454549028576, Avg Validation Loss: 10.114182533227453\n",
            "Epoch 999/2048, Avg Training Loss: 0.24863116492453086, Avg Validation Loss: 10.11200718901237\n",
            "Epoch 1000/2048, Avg Training Loss: 0.24860803826629724, Avg Validation Loss: 10.10984961614995\n",
            "Epoch 1001/2048, Avg Training Loss: 0.24858516335640077, Avg Validation Loss: 10.107709675669788\n",
            "Epoch 1002/2048, Avg Training Loss: 0.24856253805040657, Avg Validation Loss: 10.10558722991461\n",
            "Epoch 1003/2048, Avg Training Loss: 0.2485401602185411, Avg Validation Loss: 10.103482142519463\n",
            "Epoch 1004/2048, Avg Training Loss: 0.24851813178649287, Avg Validation Loss: 10.101339105322523\n",
            "Epoch 1005/2048, Avg Training Loss: 0.24849639873303367, Avg Validation Loss: 10.099159510453413\n",
            "Epoch 1006/2048, Avg Training Loss: 0.24847520200853176, Avg Validation Loss: 10.097050682645325\n",
            "Epoch 1007/2048, Avg Training Loss: 0.24845403991212717, Avg Validation Loss: 10.09498789013978\n",
            "Epoch 1008/2048, Avg Training Loss: 0.24843300591559547, Avg Validation Loss: 10.092957605413403\n",
            "Epoch 1009/2048, Avg Training Loss: 0.24841214880016932, Avg Validation Loss: 10.090952404286597\n",
            "Epoch 1010/2048, Avg Training Loss: 0.24839149354566534, Avg Validation Loss: 10.088968187633034\n",
            "Epoch 1011/2048, Avg Training Loss: 0.24837105248252417, Avg Validation Loss: 10.08700266795889\n",
            "Epoch 1012/2048, Avg Training Loss: 0.24835083122871976, Avg Validation Loss: 10.085054544977977\n",
            "Epoch 1013/2048, Avg Training Loss: 0.24833093636057968, Avg Validation Loss: 10.083068229926171\n",
            "Epoch 1014/2048, Avg Training Loss: 0.24831133982166775, Avg Validation Loss: 10.081044763105744\n",
            "Epoch 1015/2048, Avg Training Loss: 0.24829223843792916, Avg Validation Loss: 10.079109886136731\n",
            "Epoch 1016/2048, Avg Training Loss: 0.24827323501948115, Avg Validation Loss: 10.077243772768757\n",
            "Epoch 1017/2048, Avg Training Loss: 0.24825434391111842, Avg Validation Loss: 10.07542123032933\n",
            "Epoch 1018/2048, Avg Training Loss: 0.2482356106274218, Avg Validation Loss: 10.073628530770478\n",
            "Epoch 1019/2048, Avg Training Loss: 0.24821705847911277, Avg Validation Loss: 10.071858176300378\n",
            "Epoch 1020/2048, Avg Training Loss: 0.24819870629440757, Avg Validation Loss: 10.07009821135618\n",
            "Epoch 1021/2048, Avg Training Loss: 0.24818050656192678, Avg Validation Loss: 10.068344332105594\n",
            "Epoch 1022/2048, Avg Training Loss: 0.248162602814234, Avg Validation Loss: 10.066545438603066\n",
            "Epoch 1023/2048, Avg Training Loss: 0.24814499316462005, Avg Validation Loss: 10.064704932275509\n",
            "Epoch 1024/2048, Avg Training Loss: 0.24812787155206403, Avg Validation Loss: 10.062931185900174\n",
            "Epoch 1025/2048, Avg Training Loss: 0.2481107238794378, Avg Validation Loss: 10.061200021519301\n",
            "Epoch 1026/2048, Avg Training Loss: 0.2480936535487096, Avg Validation Loss: 10.059498051272268\n",
            "Epoch 1027/2048, Avg Training Loss: 0.24807671575572396, Avg Validation Loss: 10.057817938595395\n",
            "Epoch 1028/2048, Avg Training Loss: 0.24805993901367668, Avg Validation Loss: 10.05615564635206\n",
            "Epoch 1029/2048, Avg Training Loss: 0.24804333765995354, Avg Validation Loss: 10.054508936394722\n",
            "Epoch 1030/2048, Avg Training Loss: 0.24802691851062758, Avg Validation Loss: 10.052876551459763\n",
            "Epoch 1031/2048, Avg Training Loss: 0.24801078974758356, Avg Validation Loss: 10.051203510864381\n",
            "Epoch 1032/2048, Avg Training Loss: 0.24799496381686734, Avg Validation Loss: 10.049490862756695\n",
            "Epoch 1033/2048, Avg Training Loss: 0.2479796109659903, Avg Validation Loss: 10.047844042315461\n",
            "Epoch 1034/2048, Avg Training Loss: 0.24796420860722668, Avg Validation Loss: 10.04623869008699\n",
            "Epoch 1035/2048, Avg Training Loss: 0.24794886488182494, Avg Validation Loss: 10.044661492393125\n",
            "Epoch 1036/2048, Avg Training Loss: 0.2479336365370592, Avg Validation Loss: 10.043105157103822\n",
            "Epoch 1037/2048, Avg Training Loss: 0.24791855296346718, Avg Validation Loss: 10.041565675031096\n",
            "Epoch 1038/2048, Avg Training Loss: 0.2479036290169564, Avg Validation Loss: 10.04004082682833\n",
            "Epoch 1039/2048, Avg Training Loss: 0.24788897754808212, Avg Validation Loss: 10.038475341206292\n",
            "Epoch 1040/2048, Avg Training Loss: 0.24787463008166252, Avg Validation Loss: 10.036869975428708\n",
            "Epoch 1041/2048, Avg Training Loss: 0.24786073919050755, Avg Validation Loss: 10.035329572508608\n",
            "Epoch 1042/2048, Avg Training Loss: 0.2478467772049899, Avg Validation Loss: 10.033829801056049\n",
            "Epoch 1043/2048, Avg Training Loss: 0.2478328562982851, Avg Validation Loss: 10.032357363361308\n",
            "Epoch 1044/2048, Avg Training Loss: 0.24781903540682346, Avg Validation Loss: 10.030904977947548\n",
            "Epoch 1045/2048, Avg Training Loss: 0.2478053451298048, Avg Validation Loss: 10.02946864386957\n",
            "Epoch 1046/2048, Avg Training Loss: 0.2477919070193175, Avg Validation Loss: 10.027992314045411\n",
            "Epoch 1047/2048, Avg Training Loss: 0.2477787713446091, Avg Validation Loss: 10.026476211611232\n",
            "Epoch 1048/2048, Avg Training Loss: 0.2477660769625147, Avg Validation Loss: 10.025024525490094\n",
            "Epoch 1049/2048, Avg Training Loss: 0.24775329260723108, Avg Validation Loss: 10.023612861254389\n",
            "Epoch 1050/2048, Avg Training Loss: 0.24774053425273346, Avg Validation Loss: 10.022227887830084\n",
            "Epoch 1051/2048, Avg Training Loss: 0.24772786289544754, Avg Validation Loss: 10.020862307325666\n",
            "Epoch 1052/2048, Avg Training Loss: 0.24771531026803165, Avg Validation Loss: 10.019512112022332\n",
            "Epoch 1053/2048, Avg Training Loss: 0.24770289255430572, Avg Validation Loss: 10.018175088425577\n",
            "Epoch 1054/2048, Avg Training Loss: 0.2476907240395779, Avg Validation Loss: 10.016796368794004\n",
            "Epoch 1055/2048, Avg Training Loss: 0.2476788664419607, Avg Validation Loss: 10.015376702623678\n",
            "Epoch 1056/2048, Avg Training Loss: 0.2476674390383145, Avg Validation Loss: 10.014020184674353\n",
            "Epoch 1057/2048, Avg Training Loss: 0.24765590414647778, Avg Validation Loss: 10.012702676517184\n",
            "Epoch 1058/2048, Avg Training Loss: 0.24764438054534738, Avg Validation Loss: 10.011410987236195\n",
            "Epoch 1059/2048, Avg Training Loss: 0.24763293076360016, Avg Validation Loss: 10.010137896851608\n",
            "Epoch 1060/2048, Avg Training Loss: 0.2476215873901333, Avg Validation Loss: 10.008879442087007\n",
            "Epoch 1061/2048, Avg Training Loss: 0.24761047375093032, Avg Validation Loss: 10.007579973024196\n",
            "Epoch 1062/2048, Avg Training Loss: 0.24759967006860836, Avg Validation Loss: 10.006237922274714\n",
            "Epoch 1063/2048, Avg Training Loss: 0.2475892646942645, Avg Validation Loss: 10.004956330986078\n",
            "Epoch 1064/2048, Avg Training Loss: 0.24757872600044692, Avg Validation Loss: 10.00371207156082\n",
            "Epoch 1065/2048, Avg Training Loss: 0.24756818064940186, Avg Validation Loss: 10.002492496862917\n",
            "Epoch 1066/2048, Avg Training Loss: 0.24755769539612954, Avg Validation Loss: 10.001290680213831\n",
            "Epoch 1067/2048, Avg Training Loss: 0.24754730511059178, Avg Validation Loss: 10.000102817247331\n",
            "Epoch 1068/2048, Avg Training Loss: 0.24753713465366933, Avg Validation Loss: 9.998873506721953\n",
            "Epoch 1069/2048, Avg Training Loss: 0.24752727713546194, Avg Validation Loss: 9.997603015352306\n",
            "Epoch 1070/2048, Avg Training Loss: 0.247517825284589, Avg Validation Loss: 9.99639454800335\n",
            "Epoch 1071/2048, Avg Training Loss: 0.24750823403086178, Avg Validation Loss: 9.99522398441888\n",
            "Epoch 1072/2048, Avg Training Loss: 0.24749862865204297, Avg Validation Loss: 9.99407814433842\n",
            "Epoch 1073/2048, Avg Training Loss: 0.2474890378593593, Avg Validation Loss: 9.992986722985304\n",
            "Epoch 1074/2048, Avg Training Loss: 0.24747970268711675, Avg Validation Loss: 9.991955055563736\n",
            "Epoch 1075/2048, Avg Training Loss: 0.24747058470558883, Avg Validation Loss: 9.990905381464005\n",
            "Epoch 1076/2048, Avg Training Loss: 0.24746184082030298, Avg Validation Loss: 9.98976514023791\n",
            "Epoch 1077/2048, Avg Training Loss: 0.24745314916485028, Avg Validation Loss: 9.988577736575143\n",
            "Epoch 1078/2048, Avg Training Loss: 0.24744425175680593, Avg Validation Loss: 9.987365077438769\n",
            "Epoch 1079/2048, Avg Training Loss: 0.24743532407073124, Avg Validation Loss: 9.986144453564801\n",
            "Epoch 1080/2048, Avg Training Loss: 0.24742644052504234, Avg Validation Loss: 9.98492512810013\n",
            "Epoch 1081/2048, Avg Training Loss: 0.24741774763216456, Avg Validation Loss: 9.983659039084452\n",
            "Epoch 1082/2048, Avg Training Loss: 0.24740937446293457, Avg Validation Loss: 9.982350259371271\n",
            "Epoch 1083/2048, Avg Training Loss: 0.2474013931205898, Avg Validation Loss: 9.981103413940914\n",
            "Epoch 1084/2048, Avg Training Loss: 0.24739325245937568, Avg Validation Loss: 9.979895555749613\n",
            "Epoch 1085/2048, Avg Training Loss: 0.24738508410223417, Avg Validation Loss: 9.978714106652232\n",
            "Epoch 1086/2048, Avg Training Loss: 0.24737695753554478, Avg Validation Loss: 9.977552149601346\n",
            "Epoch 1087/2048, Avg Training Loss: 0.24736901693317764, Avg Validation Loss: 9.976353045451201\n",
            "Epoch 1088/2048, Avg Training Loss: 0.2473614045147306, Avg Validation Loss: 9.975116114008719\n",
            "Epoch 1089/2048, Avg Training Loss: 0.24735417510530772, Avg Validation Loss: 9.973943061313117\n",
            "Epoch 1090/2048, Avg Training Loss: 0.24734677206961725, Avg Validation Loss: 9.972809644200469\n",
            "Epoch 1091/2048, Avg Training Loss: 0.24733933006677378, Avg Validation Loss: 9.97170258771674\n",
            "Epoch 1092/2048, Avg Training Loss: 0.24733192024683603, Avg Validation Loss: 9.970614602894539\n",
            "Epoch 1093/2048, Avg Training Loss: 0.24732468801946464, Avg Validation Loss: 9.96948902578818\n",
            "Epoch 1094/2048, Avg Training Loss: 0.247317789939345, Avg Validation Loss: 9.96832506403423\n",
            "Epoch 1095/2048, Avg Training Loss: 0.24731126543793952, Avg Validation Loss: 9.967224046599\n",
            "Epoch 1096/2048, Avg Training Loss: 0.2473045532746168, Avg Validation Loss: 9.96616179019961\n",
            "Epoch 1097/2048, Avg Training Loss: 0.24729779129114146, Avg Validation Loss: 9.965125056816925\n",
            "Epoch 1098/2048, Avg Training Loss: 0.24729105236914167, Avg Validation Loss: 9.964106582389778\n",
            "Epoch 1099/2048, Avg Training Loss: 0.24728448317425727, Avg Validation Loss: 9.96304987939316\n",
            "Epoch 1100/2048, Avg Training Loss: 0.24727825388378707, Avg Validation Loss: 9.961954159795326\n",
            "Epoch 1101/2048, Avg Training Loss: 0.24727238936448184, Avg Validation Loss: 9.96092046259604\n",
            "Epoch 1102/2048, Avg Training Loss: 0.24726632402817467, Avg Validation Loss: 9.959924691043991\n",
            "Epoch 1103/2048, Avg Training Loss: 0.24726019872985716, Avg Validation Loss: 9.958953657921949\n",
            "Epoch 1104/2048, Avg Training Loss: 0.24725408798879678, Avg Validation Loss: 9.958000130999682\n",
            "Epoch 1105/2048, Avg Training Loss: 0.24724813966406747, Avg Validation Loss: 9.957007790373654\n",
            "Epoch 1106/2048, Avg Training Loss: 0.24724253686900766, Avg Validation Loss: 9.95597585442736\n",
            "Epoch 1107/2048, Avg Training Loss: 0.24723729063093944, Avg Validation Loss: 9.955005095186184\n",
            "Epoch 1108/2048, Avg Training Loss: 0.24723187710547648, Avg Validation Loss: 9.954025126336122\n",
            "Epoch 1109/2048, Avg Training Loss: 0.24722611648188206, Avg Validation Loss: 9.952977147727525\n",
            "Epoch 1110/2048, Avg Training Loss: 0.2472204389866391, Avg Validation Loss: 9.951846329835172\n",
            "Epoch 1111/2048, Avg Training Loss: 0.2472150811485252, Avg Validation Loss: 9.950654292994276\n",
            "Epoch 1112/2048, Avg Training Loss: 0.2472100638907697, Avg Validation Loss: 9.949513585393493\n",
            "Epoch 1113/2048, Avg Training Loss: 0.24720482398044583, Avg Validation Loss: 9.948406719274585\n",
            "Epoch 1114/2048, Avg Training Loss: 0.2471995129771578, Avg Validation Loss: 9.94732404700156\n",
            "Epoch 1115/2048, Avg Training Loss: 0.2471942110745835, Avg Validation Loss: 9.946260214370172\n",
            "Epoch 1116/2048, Avg Training Loss: 0.24718906975262409, Avg Validation Loss: 9.945160241622384\n",
            "Epoch 1117/2048, Avg Training Loss: 0.24718430330057117, Avg Validation Loss: 9.944023874597638\n",
            "Epoch 1118/2048, Avg Training Loss: 0.24717989172689003, Avg Validation Loss: 9.94295140232097\n",
            "Epoch 1119/2048, Avg Training Loss: 0.2471752542017102, Avg Validation Loss: 9.9419190346502\n",
            "Epoch 1120/2048, Avg Training Loss: 0.24717054020561574, Avg Validation Loss: 9.940913720263746\n",
            "Epoch 1121/2048, Avg Training Loss: 0.24716593857437946, Avg Validation Loss: 9.939876467648748\n",
            "Epoch 1122/2048, Avg Training Loss: 0.2471616974040728, Avg Validation Loss: 9.9388047370229\n",
            "Epoch 1123/2048, Avg Training Loss: 0.247157791332151, Avg Validation Loss: 9.937797253258735\n",
            "Epoch 1124/2048, Avg Training Loss: 0.24715364029216239, Avg Validation Loss: 9.936829646500072\n",
            "Epoch 1125/2048, Avg Training Loss: 0.24714939994299856, Avg Validation Loss: 9.935888555960855\n",
            "Epoch 1126/2048, Avg Training Loss: 0.24714526275916332, Avg Validation Loss: 9.93491498857247\n",
            "Epoch 1127/2048, Avg Training Loss: 0.2471414919102828, Avg Validation Loss: 9.933906310767739\n",
            "Epoch 1128/2048, Avg Training Loss: 0.24713804791076924, Avg Validation Loss: 9.932960902937735\n",
            "Epoch 1129/2048, Avg Training Loss: 0.24713434663705175, Avg Validation Loss: 9.932054458132518\n",
            "Epoch 1130/2048, Avg Training Loss: 0.24713054703752885, Avg Validation Loss: 9.931173655188477\n",
            "Epoch 1131/2048, Avg Training Loss: 0.24712684366350732, Avg Validation Loss: 9.930259676529415\n",
            "Epoch 1132/2048, Avg Training Loss: 0.24712351320073372, Avg Validation Loss: 9.929309898199188\n",
            "Epoch 1133/2048, Avg Training Loss: 0.24712050241556174, Avg Validation Loss: 9.928422434613946\n",
            "Epoch 1134/2048, Avg Training Loss: 0.24711722314341447, Avg Validation Loss: 9.927573065028428\n",
            "Epoch 1135/2048, Avg Training Loss: 0.24711383732104406, Avg Validation Loss: 9.926748519944347\n",
            "Epoch 1136/2048, Avg Training Loss: 0.247110541401632, Avg Validation Loss: 9.925890151964321\n",
            "Epoch 1137/2048, Avg Training Loss: 0.2471076248335967, Avg Validation Loss: 9.92499535029318\n",
            "Epoch 1138/2048, Avg Training Loss: 0.2471050213340802, Avg Validation Loss: 9.9241619856092\n",
            "Epoch 1139/2048, Avg Training Loss: 0.2471021389000097, Avg Validation Loss: 9.923365917733182\n",
            "Epoch 1140/2048, Avg Training Loss: 0.24709925299327898, Avg Validation Loss: 9.922542674992632\n",
            "Epoch 1141/2048, Avg Training Loss: 0.2470967044428923, Avg Validation Loss: 9.921686403903765\n",
            "Epoch 1142/2048, Avg Training Loss: 0.24709444264836808, Avg Validation Loss: 9.920894360381189\n",
            "Epoch 1143/2048, Avg Training Loss: 0.24709188399930118, Avg Validation Loss: 9.920140852739543\n",
            "Epoch 1144/2048, Avg Training Loss: 0.2470891999597458, Avg Validation Loss: 9.919411821071265\n",
            "Epoch 1145/2048, Avg Training Loss: 0.24708659259243498, Avg Validation Loss: 9.91864842237049\n",
            "Epoch 1146/2048, Avg Training Loss: 0.24708437428777022, Avg Validation Loss: 9.91784780859558\n",
            "Epoch 1147/2048, Avg Training Loss: 0.24708245724930672, Avg Validation Loss: 9.917107310388914\n",
            "Epoch 1148/2048, Avg Training Loss: 0.24708024336610088, Avg Validation Loss: 9.916402844632279\n",
            "Epoch 1149/2048, Avg Training Loss: 0.24707801351501255, Avg Validation Loss: 9.915670187999911\n",
            "Epoch 1150/2048, Avg Training Loss: 0.24707613054113572, Avg Validation Loss: 9.914903498796214\n",
            "Epoch 1151/2048, Avg Training Loss: 0.24707451936281158, Avg Validation Loss: 9.914198300399368\n",
            "Epoch 1152/2048, Avg Training Loss: 0.24707259085923022, Avg Validation Loss: 9.913529667722571\n",
            "Epoch 1153/2048, Avg Training Loss: 0.24707052295221643, Avg Validation Loss: 9.912883960660741\n",
            "Epoch 1154/2048, Avg Training Loss: 0.24706852162690132, Avg Validation Loss: 9.912202763600526\n",
            "Epoch 1155/2048, Avg Training Loss: 0.24706691958353202, Avg Validation Loss: 9.911483346945962\n",
            "Epoch 1156/2048, Avg Training Loss: 0.24706561067500635, Avg Validation Loss: 9.910821184475678\n",
            "Epoch 1157/2048, Avg Training Loss: 0.24706398259133436, Avg Validation Loss: 9.910191949322945\n",
            "Epoch 1158/2048, Avg Training Loss: 0.24706233034366862, Avg Validation Loss: 9.909533716363065\n",
            "Epoch 1159/2048, Avg Training Loss: 0.24706104687119806, Avg Validation Loss: 9.908842261964487\n",
            "Epoch 1160/2048, Avg Training Loss: 0.24706003131659537, Avg Validation Loss: 9.908212049245058\n",
            "Epoch 1161/2048, Avg Training Loss: 0.24705868614929744, Avg Validation Loss: 9.907617863309632\n",
            "Epoch 1162/2048, Avg Training Loss: 0.24705739451925945, Avg Validation Loss: 9.906912320972072\n",
            "Epoch 1163/2048, Avg Training Loss: 0.24705621775213538, Avg Validation Loss: 9.906068621403437\n",
            "Epoch 1164/2048, Avg Training Loss: 0.24705539427781245, Avg Validation Loss: 9.905238668864836\n",
            "Epoch 1165/2048, Avg Training Loss: 0.24705429020804034, Avg Validation Loss: 9.904421742589914\n",
            "Epoch 1166/2048, Avg Training Loss: 0.24705318234341728, Avg Validation Loss: 9.903566600958975\n",
            "Epoch 1167/2048, Avg Training Loss: 0.24705246909168413, Avg Validation Loss: 9.902674195406332\n",
            "Epoch 1168/2048, Avg Training Loss: 0.24705203235609885, Avg Validation Loss: 9.901842865485587\n",
            "Epoch 1169/2048, Avg Training Loss: 0.24705126753587597, Avg Validation Loss: 9.901049741468178\n",
            "Epoch 1170/2048, Avg Training Loss: 0.247050471655964, Avg Validation Loss: 9.90023174855773\n",
            "Epoch 1171/2048, Avg Training Loss: 0.24705006620844377, Avg Validation Loss: 9.899383360464276\n",
            "Epoch 1172/2048, Avg Training Loss: 0.24704992506436743, Avg Validation Loss: 9.898599161602588\n",
            "Epoch 1173/2048, Avg Training Loss: 0.2470494429101737, Avg Validation Loss: 9.897854069462559\n",
            "Epoch 1174/2048, Avg Training Loss: 0.2470489191806531, Avg Validation Loss: 9.897084066406578\n",
            "Epoch 1175/2048, Avg Training Loss: 0.24704879049727013, Avg Validation Loss: 9.896284031741713\n",
            "Epoch 1176/2048, Avg Training Loss: 0.24704892380149104, Avg Validation Loss: 9.895548332214126\n",
            "Epoch 1177/2048, Avg Training Loss: 0.24704870499780757, Avg Validation Loss: 9.894851852162228\n",
            "Epoch 1178/2048, Avg Training Loss: 0.24704843961689427, Avg Validation Loss: 9.894130562288234\n",
            "Epoch 1179/2048, Avg Training Loss: 0.24704857626322693, Avg Validation Loss: 9.893378276068244\n",
            "Epoch 1180/2048, Avg Training Loss: 0.24704896509786384, Avg Validation Loss: 9.892688773942636\n",
            "Epoch 1181/2048, Avg Training Loss: 0.2470489938248137, Avg Validation Loss: 9.892037326828923\n",
            "Epoch 1182/2048, Avg Training Loss: 0.24704897060647177, Avg Validation Loss: 9.89136023156331\n",
            "Epoch 1183/2048, Avg Training Loss: 0.24704935587374902, Avg Validation Loss: 9.890651414761681\n",
            "Epoch 1184/2048, Avg Training Loss: 0.247049988400703, Avg Validation Loss: 9.890004519350125\n",
            "Epoch 1185/2048, Avg Training Loss: 0.2470503659351694, Avg Validation Loss: 9.889344851883802\n",
            "Epoch 1186/2048, Avg Training Loss: 0.24705105053247114, Avg Validation Loss: 9.888660287935167\n",
            "Epoch 1187/2048, Avg Training Loss: 0.24705192216248623, Avg Validation Loss: 9.888041011014836\n",
            "Epoch 1188/2048, Avg Training Loss: 0.2470523892798191, Avg Validation Loss: 9.887460634526382\n",
            "Epoch 1189/2048, Avg Training Loss: 0.24705277902730213, Avg Validation Loss: 9.886854737971689\n",
            "Epoch 1190/2048, Avg Training Loss: 0.24705357951128626, Avg Validation Loss: 9.886216750683994\n",
            "Epoch 1191/2048, Avg Training Loss: 0.24705461453404615, Avg Validation Loss: 9.885639720468694\n",
            "Epoch 1192/2048, Avg Training Loss: 0.2470552653263822, Avg Validation Loss: 9.885098975052836\n",
            "Epoch 1193/2048, Avg Training Loss: 0.24705584877547235, Avg Validation Loss: 9.884531122424333\n",
            "Epoch 1194/2048, Avg Training Loss: 0.24705685669311345, Avg Validation Loss: 9.883930100576016\n",
            "Epoch 1195/2048, Avg Training Loss: 0.24705809922063157, Avg Validation Loss: 9.883389073791509\n",
            "Epoch 1196/2048, Avg Training Loss: 0.24705906675687664, Avg Validation Loss: 9.882833762365332\n",
            "Epoch 1197/2048, Avg Training Loss: 0.2470603542487462, Avg Validation Loss: 9.882252117066162\n",
            "Epoch 1198/2048, Avg Training Loss: 0.24706181527345525, Avg Validation Loss: 9.881733904213897\n",
            "Epoch 1199/2048, Avg Training Loss: 0.24706285132901892, Avg Validation Loss: 9.881252795423288\n",
            "Epoch 1200/2048, Avg Training Loss: 0.2470637960948729, Avg Validation Loss: 9.880744598140405\n",
            "Epoch 1201/2048, Avg Training Loss: 0.24706516759943706, Avg Validation Loss: 9.880202897050546\n",
            "Epoch 1202/2048, Avg Training Loss: 0.24706676275290576, Avg Validation Loss: 9.879720399396216\n",
            "Epoch 1203/2048, Avg Training Loss: 0.24706806956042773, Avg Validation Loss: 9.879222942324793\n",
            "Epoch 1204/2048, Avg Training Loss: 0.2470697031921826, Avg Validation Loss: 9.878698457972092\n",
            "Epoch 1205/2048, Avg Training Loss: 0.24707150224499988, Avg Validation Loss: 9.878236453001007\n",
            "Epoch 1206/2048, Avg Training Loss: 0.24707297882511461, Avg Validation Loss: 9.877761168443822\n",
            "Epoch 1207/2048, Avg Training Loss: 0.2470747699753994, Avg Validation Loss: 9.87725963129182\n",
            "Epoch 1208/2048, Avg Training Loss: 0.2470767150804542, Avg Validation Loss: 9.876820751621732\n",
            "Epoch 1209/2048, Avg Training Loss: 0.24707821395718518, Avg Validation Loss: 9.876418151755763\n",
            "Epoch 1210/2048, Avg Training Loss: 0.24707960968883208, Avg Validation Loss: 9.875987847606394\n",
            "Epoch 1211/2048, Avg Training Loss: 0.2470814444548285, Avg Validation Loss: 9.875523259831319\n",
            "Epoch 1212/2048, Avg Training Loss: 0.24708360817340674, Avg Validation Loss: 9.875067168268268\n",
            "Epoch 1213/2048, Avg Training Loss: 0.2470858933233173, Avg Validation Loss: 9.874596462244753\n",
            "Epoch 1214/2048, Avg Training Loss: 0.24708822025587862, Avg Validation Loss: 9.874194317748872\n",
            "Epoch 1215/2048, Avg Training Loss: 0.24709003522081838, Avg Validation Loss: 9.873831389667844\n",
            "Epoch 1216/2048, Avg Training Loss: 0.24709171123361776, Avg Validation Loss: 9.873442173380282\n",
            "Epoch 1217/2048, Avg Training Loss: 0.24709381836833064, Avg Validation Loss: 9.87301920156184\n",
            "Epoch 1218/2048, Avg Training Loss: 0.2470961264005776, Avg Validation Loss: 9.87265410912147\n",
            "Epoch 1219/2048, Avg Training Loss: 0.24709811880236904, Avg Validation Loss: 9.872272840528026\n",
            "Epoch 1220/2048, Avg Training Loss: 0.24710045332733174, Avg Validation Loss: 9.871863229891359\n",
            "Epoch 1221/2048, Avg Training Loss: 0.24710293710355055, Avg Validation Loss: 9.871514261712289\n",
            "Epoch 1222/2048, Avg Training Loss: 0.24710507516966893, Avg Validation Loss: 9.871150543643477\n",
            "Epoch 1223/2048, Avg Training Loss: 0.24710754459827453, Avg Validation Loss: 9.870759154871344\n",
            "Epoch 1224/2048, Avg Training Loss: 0.2471101533315965, Avg Validation Loss: 9.870428592715301\n",
            "Epoch 1225/2048, Avg Training Loss: 0.24711229310843455, Avg Validation Loss: 9.870132638473144\n",
            "Epoch 1226/2048, Avg Training Loss: 0.24711431600589698, Avg Validation Loss: 9.869807629433177\n",
            "Epoch 1227/2048, Avg Training Loss: 0.24711691440369205, Avg Validation Loss: 9.869397724892732\n",
            "Epoch 1228/2048, Avg Training Loss: 0.24712028355778706, Avg Validation Loss: 9.868996136542151\n",
            "Epoch 1229/2048, Avg Training Loss: 0.24712345629224222, Avg Validation Loss: 9.868674703961506\n",
            "Epoch 1230/2048, Avg Training Loss: 0.24712597673325368, Avg Validation Loss: 9.868398223103931\n",
            "Epoch 1231/2048, Avg Training Loss: 0.24712816624302322, Avg Validation Loss: 9.868147457930043\n",
            "Epoch 1232/2048, Avg Training Loss: 0.24713031210095254, Avg Validation Loss: 9.867862634232178\n",
            "Epoch 1233/2048, Avg Training Loss: 0.24713296714253133, Avg Validation Loss: 9.867539294524251\n",
            "Epoch 1234/2048, Avg Training Loss: 0.24713595892891538, Avg Validation Loss: 9.867221129037937\n",
            "Epoch 1235/2048, Avg Training Loss: 0.24713909797671454, Avg Validation Loss: 9.866885805212663\n",
            "Epoch 1236/2048, Avg Training Loss: 0.24714226255991695, Avg Validation Loss: 9.866616413678486\n",
            "Epoch 1237/2048, Avg Training Loss: 0.24714488775435342, Avg Validation Loss: 9.86638400643933\n",
            "Epoch 1238/2048, Avg Training Loss: 0.247147357662468, Avg Validation Loss: 9.86612356631436\n",
            "Epoch 1239/2048, Avg Training Loss: 0.24715040289280887, Avg Validation Loss: 9.865778559725026\n",
            "Epoch 1240/2048, Avg Training Loss: 0.2471542244048824, Avg Validation Loss: 9.86544150076465\n",
            "Epoch 1241/2048, Avg Training Loss: 0.24715782904219127, Avg Validation Loss: 9.865183819934646\n",
            "Epoch 1242/2048, Avg Training Loss: 0.24716076109165133, Avg Validation Loss: 9.864970289401365\n",
            "Epoch 1243/2048, Avg Training Loss: 0.24716335015333757, Avg Validation Loss: 9.864781664785342\n",
            "Epoch 1244/2048, Avg Training Loss: 0.24716588846280135, Avg Validation Loss: 9.86455828739917\n",
            "Epoch 1245/2048, Avg Training Loss: 0.24716906756724352, Avg Validation Loss: 9.864246570174796\n",
            "Epoch 1246/2048, Avg Training Loss: 0.24717306050772095, Avg Validation Loss: 9.863940508628689\n",
            "Epoch 1247/2048, Avg Training Loss: 0.2471768455675183, Avg Validation Loss: 9.863712332311216\n",
            "Epoch 1248/2048, Avg Training Loss: 0.2471799584990582, Avg Validation Loss: 9.863527330922414\n",
            "Epoch 1249/2048, Avg Training Loss: 0.2471828447506871, Avg Validation Loss: 9.863317443224362\n",
            "Epoch 1250/2048, Avg Training Loss: 0.24718616971693083, Avg Validation Loss: 9.863073585727333\n",
            "Epoch 1251/2048, Avg Training Loss: 0.247189657307586, Avg Validation Loss: 9.86288586228719\n",
            "Epoch 1252/2048, Avg Training Loss: 0.24719278426819002, Avg Validation Loss: 9.862680285497799\n",
            "Epoch 1253/2048, Avg Training Loss: 0.24719628283600917, Avg Validation Loss: 9.862444519317432\n",
            "Epoch 1254/2048, Avg Training Loss: 0.24720002177016137, Avg Validation Loss: 9.862217789641324\n",
            "Epoch 1255/2048, Avg Training Loss: 0.24720387507046745, Avg Validation Loss: 9.861975499089052\n",
            "Epoch 1256/2048, Avg Training Loss: 0.2472077125589471, Avg Validation Loss: 9.861799188901657\n",
            "Epoch 1257/2048, Avg Training Loss: 0.24721109103160366, Avg Validation Loss: 9.8616102979453\n",
            "Epoch 1258/2048, Avg Training Loss: 0.2472147964377851, Avg Validation Loss: 9.86139396876263\n",
            "Epoch 1259/2048, Avg Training Loss: 0.24721859582924643, Avg Validation Loss: 9.861237039490517\n",
            "Epoch 1260/2048, Avg Training Loss: 0.24722199284621438, Avg Validation Loss: 9.86106388957662\n",
            "Epoch 1261/2048, Avg Training Loss: 0.24722575139005498, Avg Validation Loss: 9.860861256311418\n",
            "Epoch 1262/2048, Avg Training Loss: 0.24722961911035946, Avg Validation Loss: 9.860716818933358\n",
            "Epoch 1263/2048, Avg Training Loss: 0.24723309076146674, Avg Validation Loss: 9.860555459842674\n",
            "Epoch 1264/2048, Avg Training Loss: 0.24723704872772578, Avg Validation Loss: 9.86031519221191\n",
            "Epoch 1265/2048, Avg Training Loss: 0.2472417469948353, Avg Validation Loss: 9.860085079075716\n",
            "Epoch 1266/2048, Avg Training Loss: 0.2472461651321213, Avg Validation Loss: 9.859934628803586\n",
            "Epoch 1267/2048, Avg Training Loss: 0.24724985965654922, Avg Validation Loss: 9.859827857042783\n",
            "Epoch 1268/2048, Avg Training Loss: 0.24725329956988068, Avg Validation Loss: 9.859696135081697\n",
            "Epoch 1269/2048, Avg Training Loss: 0.24725719178243596, Avg Validation Loss: 9.859529992582003\n",
            "Epoch 1270/2048, Avg Training Loss: 0.24726123275246895, Avg Validation Loss: 9.859419104711648\n",
            "Epoch 1271/2048, Avg Training Loss: 0.247264885375405, Avg Validation Loss: 9.85928115217489\n",
            "Epoch 1272/2048, Avg Training Loss: 0.24726905823829906, Avg Validation Loss: 9.859058838675603\n",
            "Epoch 1273/2048, Avg Training Loss: 0.2472739932782245, Avg Validation Loss: 9.85884366790531\n",
            "Epoch 1274/2048, Avg Training Loss: 0.2472786468005392, Avg Validation Loss: 9.858706455592012\n",
            "Epoch 1275/2048, Avg Training Loss: 0.24728257089836186, Avg Validation Loss: 9.858611985357472\n",
            "Epoch 1276/2048, Avg Training Loss: 0.2472862372003797, Avg Validation Loss: 9.858492085614253\n",
            "Epoch 1277/2048, Avg Training Loss: 0.24729036602792354, Avg Validation Loss: 9.858337509386624\n",
            "Epoch 1278/2048, Avg Training Loss: 0.24729475840427045, Avg Validation Loss: 9.858189050880254\n",
            "Epoch 1279/2048, Avg Training Loss: 0.24729929908605008, Avg Validation Loss: 9.858023103102092\n",
            "Epoch 1280/2048, Avg Training Loss: 0.2473038159713502, Avg Validation Loss: 9.85792149807205\n",
            "Epoch 1281/2048, Avg Training Loss: 0.2473078550721879, Avg Validation Loss: 9.857806178129179\n",
            "Epoch 1282/2048, Avg Training Loss: 0.24731224678965955, Avg Validation Loss: 9.857662478868741\n",
            "Epoch 1283/2048, Avg Training Loss: 0.24731683839117952, Avg Validation Loss: 9.857528209387445\n",
            "Epoch 1284/2048, Avg Training Loss: 0.24732154895700745, Avg Validation Loss: 9.857378157541282\n",
            "Epoch 1285/2048, Avg Training Loss: 0.24732621469388158, Avg Validation Loss: 9.857293234569925\n",
            "Epoch 1286/2048, Avg Training Loss: 0.24733038848193328, Avg Validation Loss: 9.8571949427148\n",
            "Epoch 1287/2048, Avg Training Loss: 0.24733491483467906, Avg Validation Loss: 9.857068362654545\n",
            "Epoch 1288/2048, Avg Training Loss: 0.24733962097243867, Avg Validation Loss: 9.856958001890893\n",
            "Epoch 1289/2048, Avg Training Loss: 0.24734422129868483, Avg Validation Loss: 9.856860161057295\n",
            "Epoch 1290/2048, Avg Training Loss: 0.24734829980443684, Avg Validation Loss: 9.856843734811587\n",
            "Epoch 1291/2048, Avg Training Loss: 0.2473516143451337, Avg Validation Loss: 9.856821420439584\n",
            "Epoch 1292/2048, Avg Training Loss: 0.24735514682298687, Avg Validation Loss: 9.856773475883308\n",
            "Epoch 1293/2048, Avg Training Loss: 0.24735880194275137, Avg Validation Loss: 9.856734950070894\n",
            "Epoch 1294/2048, Avg Training Loss: 0.24736254552554593, Avg Validation Loss: 9.856679165635795\n",
            "Epoch 1295/2048, Avg Training Loss: 0.2473662229481738, Avg Validation Loss: 9.856686266200946\n",
            "Epoch 1296/2048, Avg Training Loss: 0.24736939676263203, Avg Validation Loss: 9.856677388601195\n",
            "Epoch 1297/2048, Avg Training Loss: 0.24737304870176605, Avg Validation Loss: 9.856588529652711\n",
            "Epoch 1298/2048, Avg Training Loss: 0.24737745237705897, Avg Validation Loss: 9.856507405734847\n",
            "Epoch 1299/2048, Avg Training Loss: 0.2473815367531417, Avg Validation Loss: 9.856502846113049\n",
            "Epoch 1300/2048, Avg Training Loss: 0.24738486238066254, Avg Validation Loss: 9.856538690898264\n",
            "Epoch 1301/2048, Avg Training Loss: 0.2473879185106007, Avg Validation Loss: 9.856546319778563\n",
            "Epoch 1302/2048, Avg Training Loss: 0.24739158142779846, Avg Validation Loss: 9.856467324640231\n",
            "Epoch 1303/2048, Avg Training Loss: 0.2473960655244269, Avg Validation Loss: 9.856392506945022\n",
            "Epoch 1304/2048, Avg Training Loss: 0.2474002626199019, Avg Validation Loss: 9.856392385129443\n",
            "Epoch 1305/2048, Avg Training Loss: 0.24740383501976268, Avg Validation Loss: 9.856382797051992\n",
            "Epoch 1306/2048, Avg Training Loss: 0.24740770971058917, Avg Validation Loss: 9.856345736508608\n",
            "Epoch 1307/2048, Avg Training Loss: 0.24741162425922802, Avg Validation Loss: 9.856366133819678\n",
            "Epoch 1308/2048, Avg Training Loss: 0.24741508716497523, Avg Validation Loss: 9.856367683097064\n",
            "Epoch 1309/2048, Avg Training Loss: 0.24741906564863939, Avg Validation Loss: 9.856287767394\n",
            "Epoch 1310/2048, Avg Training Loss: 0.24742381860629964, Avg Validation Loss: 9.856214908528829\n",
            "Epoch 1311/2048, Avg Training Loss: 0.24742825273876443, Avg Validation Loss: 9.856218378829785\n",
            "Epoch 1312/2048, Avg Training Loss: 0.24743204251063838, Avg Validation Loss: 9.856213310937603\n",
            "Epoch 1313/2048, Avg Training Loss: 0.2474361301920747, Avg Validation Loss: 9.856181317108879\n",
            "Epoch 1314/2048, Avg Training Loss: 0.24744036966501526, Avg Validation Loss: 9.856158218375102\n",
            "Epoch 1315/2048, Avg Training Loss: 0.24744472337098652, Avg Validation Loss: 9.856117724913704\n",
            "Epoch 1316/2048, Avg Training Loss: 0.24744901063446667, Avg Validation Loss: 9.856140247181639\n",
            "Epoch 1317/2048, Avg Training Loss: 0.2474527860545926, Avg Validation Loss: 9.856146987229845\n",
            "Epoch 1318/2048, Avg Training Loss: 0.24745705366379728, Avg Validation Loss: 9.856073961892173\n",
            "Epoch 1319/2048, Avg Training Loss: 0.2474620859831773, Avg Validation Loss: 9.856008961394268\n",
            "Epoch 1320/2048, Avg Training Loss: 0.24746678447303927, Avg Validation Loss: 9.856020859699322\n",
            "Epoch 1321/2048, Avg Training Loss: 0.24747082693990854, Avg Validation Loss: 9.856024553398754\n",
            "Epoch 1322/2048, Avg Training Loss: 0.2474751698330847, Avg Validation Loss: 9.85600152766367\n",
            "Epoch 1323/2048, Avg Training Loss: 0.24747954025468705, Avg Validation Loss: 9.85603645818907\n",
            "Epoch 1324/2048, Avg Training Loss: 0.24748344603733555, Avg Validation Loss: 9.856052863656599\n",
            "Epoch 1325/2048, Avg Training Loss: 0.24748787621693932, Avg Validation Loss: 9.855988022846923\n",
            "Epoch 1326/2048, Avg Training Loss: 0.24749309041930284, Avg Validation Loss: 9.855930427168671\n",
            "Epoch 1327/2048, Avg Training Loss: 0.24749797330997086, Avg Validation Loss: 9.85594933502125\n",
            "Epoch 1328/2048, Avg Training Loss: 0.2475021986834407, Avg Validation Loss: 9.855959839741606\n",
            "Epoch 1329/2048, Avg Training Loss: 0.24750685017881968, Avg Validation Loss: 9.855894617412114\n",
            "Epoch 1330/2048, Avg Training Loss: 0.24751223492424843, Avg Validation Loss: 9.855839639477756\n",
            "Epoch 1331/2048, Avg Training Loss: 0.24751725633652308, Avg Validation Loss: 9.855862787190546\n",
            "Epoch 1332/2048, Avg Training Loss: 0.2475214816563206, Avg Validation Loss: 9.8559273271248\n",
            "Epoch 1333/2048, Avg Training Loss: 0.24752541655646862, Avg Validation Loss: 9.85596429434151\n",
            "Epoch 1334/2048, Avg Training Loss: 0.24752997860586953, Avg Validation Loss: 9.855915072842933\n",
            "Epoch 1335/2048, Avg Training Loss: 0.24753538238274006, Avg Validation Loss: 9.855870406804643\n",
            "Epoch 1336/2048, Avg Training Loss: 0.24754059470884002, Avg Validation Loss: 9.855851867431246\n",
            "Epoch 1337/2048, Avg Training Loss: 0.24754575237820153, Avg Validation Loss: 9.855825364321186\n",
            "Epoch 1338/2048, Avg Training Loss: 0.24755073514322373, Avg Validation Loss: 9.85587527276525\n",
            "Epoch 1339/2048, Avg Training Loss: 0.24755517868700547, Avg Validation Loss: 9.855920685562646\n",
            "Epoch 1340/2048, Avg Training Loss: 0.24756010725992808, Avg Validation Loss: 9.855892226136875\n",
            "Epoch 1341/2048, Avg Training Loss: 0.247565804239736, Avg Validation Loss: 9.855874775573884\n",
            "Epoch 1342/2048, Avg Training Loss: 0.24757114357205992, Avg Validation Loss: 9.855935647332377\n",
            "Epoch 1343/2048, Avg Training Loss: 0.24757580521693123, Avg Validation Loss: 9.855988867058612\n",
            "Epoch 1344/2048, Avg Training Loss: 0.24758077923819244, Avg Validation Loss: 9.85601544766174\n",
            "Epoch 1345/2048, Avg Training Loss: 0.2475858906731785, Avg Validation Loss: 9.856050907375113\n",
            "Epoch 1346/2048, Avg Training Loss: 0.2475912485598605, Avg Validation Loss: 9.856019837714497\n",
            "Epoch 1347/2048, Avg Training Loss: 0.24759725003537508, Avg Validation Loss: 9.856003819781973\n",
            "Epoch 1348/2048, Avg Training Loss: 0.24760282061416392, Avg Validation Loss: 9.856068357097685\n",
            "Epoch 1349/2048, Avg Training Loss: 0.247607551750208, Avg Validation Loss: 9.856175401171075\n",
            "Epoch 1350/2048, Avg Training Loss: 0.24761196965830803, Avg Validation Loss: 9.856255243347661\n",
            "Epoch 1351/2048, Avg Training Loss: 0.24761702138789518, Avg Validation Loss: 9.856248842471365\n",
            "Epoch 1352/2048, Avg Training Loss: 0.24762304445455044, Avg Validation Loss: 9.856197824133194\n",
            "Epoch 1353/2048, Avg Training Loss: 0.24762947497251345, Avg Validation Loss: 9.856173834685391\n",
            "Epoch 1354/2048, Avg Training Loss: 0.2476353420396648, Avg Validation Loss: 9.856236942832274\n",
            "Epoch 1355/2048, Avg Training Loss: 0.24764029626238274, Avg Validation Loss: 9.856346117380637\n",
            "Epoch 1356/2048, Avg Training Loss: 0.24764489808413087, Avg Validation Loss: 9.856430024344956\n",
            "Epoch 1357/2048, Avg Training Loss: 0.24765011979718557, Avg Validation Loss: 9.856428734074766\n",
            "Epoch 1358/2048, Avg Training Loss: 0.24765618737218953, Avg Validation Loss: 9.856432337222508\n",
            "Epoch 1359/2048, Avg Training Loss: 0.24766203959548297, Avg Validation Loss: 9.856462046946197\n",
            "Epoch 1360/2048, Avg Training Loss: 0.24766783988101088, Avg Validation Loss: 9.856483548634253\n",
            "Epoch 1361/2048, Avg Training Loss: 0.24767357189032693, Avg Validation Loss: 9.856523992014617\n",
            "Epoch 1362/2048, Avg Training Loss: 0.24767933181216176, Avg Validation Loss: 9.856552306193498\n",
            "Epoch 1363/2048, Avg Training Loss: 0.2476850643748966, Avg Validation Loss: 9.856597417486729\n",
            "Epoch 1364/2048, Avg Training Loss: 0.24769084810344705, Avg Validation Loss: 9.856629223753771\n",
            "Epoch 1365/2048, Avg Training Loss: 0.2476966152844759, Avg Validation Loss: 9.856677183083754\n",
            "Epoch 1366/2048, Avg Training Loss: 0.24770244084891138, Avg Validation Loss: 9.85671148162922\n",
            "Epoch 1367/2048, Avg Training Loss: 0.24770825214367936, Avg Validation Loss: 9.856761737085305\n",
            "Epoch 1368/2048, Avg Training Loss: 0.247714124501631, Avg Validation Loss: 9.856798220582778\n",
            "Epoch 1369/2048, Avg Training Loss: 0.24771998244765425, Avg Validation Loss: 9.856850598256262\n",
            "Epoch 1370/2048, Avg Training Loss: 0.24772590696708102, Avg Validation Loss: 9.856892208367638\n",
            "Epoch 1371/2048, Avg Training Loss: 0.2477318247351695, Avg Validation Loss: 9.856951716876173\n",
            "Epoch 1372/2048, Avg Training Loss: 0.24773780900696143, Avg Validation Loss: 9.856998428411707\n",
            "Epoch 1373/2048, Avg Training Loss: 0.24774377860828284, Avg Validation Loss: 9.857061491703993\n",
            "Epoch 1374/2048, Avg Training Loss: 0.24774981344835814, Avg Validation Loss: 9.8571109179702\n",
            "Epoch 1375/2048, Avg Training Loss: 0.24775583139599625, Avg Validation Loss: 9.857176241371349\n",
            "Epoch 1376/2048, Avg Training Loss: 0.2477620360701129, Avg Validation Loss: 9.85717871188988\n",
            "Epoch 1377/2048, Avg Training Loss: 0.24776886387409006, Avg Validation Loss: 9.857198169698634\n",
            "Epoch 1378/2048, Avg Training Loss: 0.24777521260708663, Avg Validation Loss: 9.857299187258551\n",
            "Epoch 1379/2048, Avg Training Loss: 0.24778080352250328, Avg Validation Loss: 9.857394179398115\n",
            "Epoch 1380/2048, Avg Training Loss: 0.2477868267896238, Avg Validation Loss: 9.85741432317989\n",
            "Epoch 1381/2048, Avg Training Loss: 0.24779360544049217, Avg Validation Loss: 9.857444901111204\n",
            "Epoch 1382/2048, Avg Training Loss: 0.24779997011071184, Avg Validation Loss: 9.857553466395995\n",
            "Epoch 1383/2048, Avg Training Loss: 0.24780560990643336, Avg Validation Loss: 9.85765404860655\n",
            "Epoch 1384/2048, Avg Training Loss: 0.24781170457402005, Avg Validation Loss: 9.857678703554122\n",
            "Epoch 1385/2048, Avg Training Loss: 0.24781856818539524, Avg Validation Loss: 9.857713206932349\n",
            "Epoch 1386/2048, Avg Training Loss: 0.2478251415170164, Avg Validation Loss: 9.857776404583449\n",
            "Epoch 1387/2048, Avg Training Loss: 0.2478316434445294, Avg Validation Loss: 9.857832658433459\n",
            "Epoch 1388/2048, Avg Training Loss: 0.24783804498055928, Avg Validation Loss: 9.857908427629633\n",
            "Epoch 1389/2048, Avg Training Loss: 0.2478444778647832, Avg Validation Loss: 9.857972233770791\n",
            "Epoch 1390/2048, Avg Training Loss: 0.2478508634014067, Avg Validation Loss: 9.858052813320015\n",
            "Epoch 1391/2048, Avg Training Loss: 0.24785743187135043, Avg Validation Loss: 9.858070945281137\n",
            "Epoch 1392/2048, Avg Training Loss: 0.24786462779261742, Avg Validation Loss: 9.858106254534537\n",
            "Epoch 1393/2048, Avg Training Loss: 0.24787132663833258, Avg Validation Loss: 9.858223206420563\n",
            "Epoch 1394/2048, Avg Training Loss: 0.24787725110509162, Avg Validation Loss: 9.858334114652278\n",
            "Epoch 1395/2048, Avg Training Loss: 0.24788360584570152, Avg Validation Loss: 9.85838102253345\n",
            "Epoch 1396/2048, Avg Training Loss: 0.2478907491405411, Avg Validation Loss: 9.858451955259822\n",
            "Epoch 1397/2048, Avg Training Loss: 0.24789756730144033, Avg Validation Loss: 9.858558838587827\n",
            "Epoch 1398/2048, Avg Training Loss: 0.24790430513890838, Avg Validation Loss: 9.85865961552806\n",
            "Epoch 1399/2048, Avg Training Loss: 0.24791090909995198, Avg Validation Loss: 9.858776097105626\n",
            "Epoch 1400/2048, Avg Training Loss: 0.24791753799590935, Avg Validation Loss: 9.858878236692224\n",
            "Epoch 1401/2048, Avg Training Loss: 0.24792410657134126, Avg Validation Loss: 9.858995572326641\n",
            "Epoch 1402/2048, Avg Training Loss: 0.24793085964533532, Avg Validation Loss: 9.859049270237458\n",
            "Epoch 1403/2048, Avg Training Loss: 0.24793824482546944, Avg Validation Loss: 9.859119233577813\n",
            "Epoch 1404/2048, Avg Training Loss: 0.24794512050166687, Avg Validation Loss: 9.859270107076307\n",
            "Epoch 1405/2048, Avg Training Loss: 0.24795121004511345, Avg Validation Loss: 9.859414231635002\n",
            "Epoch 1406/2048, Avg Training Loss: 0.24795774987393285, Avg Validation Loss: 9.859482714810802\n",
            "Epoch 1407/2048, Avg Training Loss: 0.2479651870719773, Avg Validation Loss: 9.859511902873237\n",
            "Epoch 1408/2048, Avg Training Loss: 0.24797298864407724, Avg Validation Loss: 9.859570658797978\n",
            "Epoch 1409/2048, Avg Training Loss: 0.24798013156514992, Avg Validation Loss: 9.859717592941564\n",
            "Epoch 1410/2048, Avg Training Loss: 0.24798640646441636, Avg Validation Loss: 9.859861714900378\n",
            "Epoch 1411/2048, Avg Training Loss: 0.24799309485842075, Avg Validation Loss: 9.85993230785476\n",
            "Epoch 1412/2048, Avg Training Loss: 0.24800054089680199, Avg Validation Loss: 9.860013780459267\n",
            "Epoch 1413/2048, Avg Training Loss: 0.24800765489167625, Avg Validation Loss: 9.860124214663681\n",
            "Epoch 1414/2048, Avg Training Loss: 0.2480146960856421, Avg Validation Loss: 9.860227497463008\n",
            "Epoch 1415/2048, Avg Training Loss: 0.24802161337017814, Avg Validation Loss: 9.86034989622075\n",
            "Epoch 1416/2048, Avg Training Loss: 0.24802869305527478, Avg Validation Loss: 9.860410718098455\n",
            "Epoch 1417/2048, Avg Training Loss: 0.24803639908759223, Avg Validation Loss: 9.860488948295837\n",
            "Epoch 1418/2048, Avg Training Loss: 0.2480436964482158, Avg Validation Loss: 9.860599694344776\n",
            "Epoch 1419/2048, Avg Training Loss: 0.24805088440666753, Avg Validation Loss: 9.8607052054798\n",
            "Epoch 1420/2048, Avg Training Loss: 0.24805792456111433, Avg Validation Loss: 9.860830866683914\n",
            "Epoch 1421/2048, Avg Training Loss: 0.24806511888373914, Avg Validation Loss: 9.860895478685581\n",
            "Epoch 1422/2048, Avg Training Loss: 0.24807293747073375, Avg Validation Loss: 9.860977781118208\n",
            "Epoch 1423/2048, Avg Training Loss: 0.24808021610739056, Avg Validation Loss: 9.86114180697266\n",
            "Epoch 1424/2048, Avg Training Loss: 0.24808668346891702, Avg Validation Loss: 9.861299438802753\n",
            "Epoch 1425/2048, Avg Training Loss: 0.24809361353376153, Avg Validation Loss: 9.861381513130265\n",
            "Epoch 1426/2048, Avg Training Loss: 0.24810145672388112, Avg Validation Loss: 9.861424300084812\n",
            "Epoch 1427/2048, Avg Training Loss: 0.24810967167163656, Avg Validation Loss: 9.861496672157859\n",
            "Epoch 1428/2048, Avg Training Loss: 0.24811720529735382, Avg Validation Loss: 9.8616572669035\n",
            "Epoch 1429/2048, Avg Training Loss: 0.24812384983127012, Avg Validation Loss: 9.861814987435611\n",
            "Epoch 1430/2048, Avg Training Loss: 0.24813092272014847, Avg Validation Loss: 9.86189903845022\n",
            "Epoch 1431/2048, Avg Training Loss: 0.24813889302400083, Avg Validation Loss: 9.86194481517827\n",
            "Epoch 1432/2048, Avg Training Loss: 0.2481472264581294, Avg Validation Loss: 9.862020724106678\n",
            "Epoch 1433/2048, Avg Training Loss: 0.2481549901511318, Avg Validation Loss: 9.862136065384824\n",
            "Epoch 1434/2048, Avg Training Loss: 0.24816257157096158, Avg Validation Loss: 9.862249878871015\n",
            "Epoch 1435/2048, Avg Training Loss: 0.24816995287202534, Avg Validation Loss: 9.862385831341603\n",
            "Epoch 1436/2048, Avg Training Loss: 0.24817747420661013, Avg Validation Loss: 9.862461715659247\n",
            "Epoch 1437/2048, Avg Training Loss: 0.2481856192841074, Avg Validation Loss: 9.862555808819272\n",
            "Epoch 1438/2048, Avg Training Loss: 0.24819332575679498, Avg Validation Loss: 9.862682809028605\n",
            "Epoch 1439/2048, Avg Training Loss: 0.24820092432445082, Avg Validation Loss: 9.86280469509733\n",
            "Epoch 1440/2048, Avg Training Loss: 0.24820835775316527, Avg Validation Loss: 9.862946751424344\n",
            "Epoch 1441/2048, Avg Training Loss: 0.24821595448714553, Avg Validation Loss: 9.863027632616575\n",
            "Epoch 1442/2048, Avg Training Loss: 0.24822418969061955, Avg Validation Loss: 9.863126114741775\n",
            "Epoch 1443/2048, Avg Training Loss: 0.24823198686967504, Avg Validation Loss: 9.86325716337684\n",
            "Epoch 1444/2048, Avg Training Loss: 0.2482398049338595, Avg Validation Loss: 9.863333776590272\n",
            "Epoch 1445/2048, Avg Training Loss: 0.2482481872431569, Avg Validation Loss: 9.863431656425544\n",
            "Epoch 1446/2048, Avg Training Loss: 0.24825608730341697, Avg Validation Loss: 9.863564083991454\n",
            "Epoch 1447/2048, Avg Training Loss: 0.24826399476798178, Avg Validation Loss: 9.863647139266599\n",
            "Epoch 1448/2048, Avg Training Loss: 0.2482724666983223, Avg Validation Loss: 9.863754684787024\n",
            "Epoch 1449/2048, Avg Training Loss: 0.24828032273108902, Avg Validation Loss: 9.863947536614662\n",
            "Epoch 1450/2048, Avg Training Loss: 0.24828731370422197, Avg Validation Loss: 9.864135678762091\n",
            "Epoch 1451/2048, Avg Training Loss: 0.2482947772912987, Avg Validation Loss: 9.864248885954806\n",
            "Epoch 1452/2048, Avg Training Loss: 0.2483031666168178, Avg Validation Loss: 9.864333278007209\n",
            "Epoch 1453/2048, Avg Training Loss: 0.2483121432466993, Avg Validation Loss: 9.864418621353218\n",
            "Epoch 1454/2048, Avg Training Loss: 0.24832130270530403, Avg Validation Loss: 9.864554858565581\n",
            "Epoch 1455/2048, Avg Training Loss: 0.2483296389495259, Avg Validation Loss: 9.864790392199746\n",
            "Epoch 1456/2048, Avg Training Loss: 0.248336997041085, Avg Validation Loss: 9.865028393409418\n",
            "Epoch 1457/2048, Avg Training Loss: 0.24834477639241584, Avg Validation Loss: 9.865194906205222\n",
            "Epoch 1458/2048, Avg Training Loss: 0.24835346317749288, Avg Validation Loss: 9.865323721785456\n",
            "Epoch 1459/2048, Avg Training Loss: 0.24836251643488097, Avg Validation Loss: 9.865482480634016\n",
            "Epoch 1460/2048, Avg Training Loss: 0.2483709627952407, Avg Validation Loss: 9.865680047767999\n",
            "Epoch 1461/2048, Avg Training Loss: 0.24837935578788553, Avg Validation Loss: 9.865825954919234\n",
            "Epoch 1462/2048, Avg Training Loss: 0.2483882810410711, Avg Validation Loss: 9.865994194322885\n",
            "Epoch 1463/2048, Avg Training Loss: 0.2483966952718259, Avg Validation Loss: 9.866185975503193\n",
            "Epoch 1464/2048, Avg Training Loss: 0.24840507962898778, Avg Validation Loss: 9.8663017488784\n",
            "Epoch 1465/2048, Avg Training Loss: 0.2484140650878695, Avg Validation Loss: 9.86642725990006\n",
            "Epoch 1466/2048, Avg Training Loss: 0.24842256030008852, Avg Validation Loss: 9.866581185126181\n",
            "Epoch 1467/2048, Avg Training Loss: 0.24843107014036261, Avg Validation Loss: 9.86667833668562\n",
            "Epoch 1468/2048, Avg Training Loss: 0.2484401518628775, Avg Validation Loss: 9.866795627916735\n",
            "Epoch 1469/2048, Avg Training Loss: 0.24844872284827893, Avg Validation Loss: 9.866946943194836\n",
            "Epoch 1470/2048, Avg Training Loss: 0.2484573006965607, Avg Validation Loss: 9.867044491103627\n",
            "Epoch 1471/2048, Avg Training Loss: 0.2484664479093396, Avg Validation Loss: 9.867163784800372\n",
            "Epoch 1472/2048, Avg Training Loss: 0.2484750780036811, Avg Validation Loss: 9.867317948476257\n",
            "Epoch 1473/2048, Avg Training Loss: 0.24848371476685632, Avg Validation Loss: 9.86741877159398\n",
            "Epoch 1474/2048, Avg Training Loss: 0.2484929224022216, Avg Validation Loss: 9.867541549997746\n",
            "Epoch 1475/2048, Avg Training Loss: 0.2485016086785029, Avg Validation Loss: 9.86769928911341\n",
            "Epoch 1476/2048, Avg Training Loss: 0.2485103025739907, Avg Validation Loss: 9.86780370642763\n",
            "Epoch 1477/2048, Avg Training Loss: 0.2485196950452421, Avg Validation Loss: 9.86788084483419\n",
            "Epoch 1478/2048, Avg Training Loss: 0.24852934247055883, Avg Validation Loss: 9.867994138628426\n",
            "Epoch 1479/2048, Avg Training Loss: 0.2485381675274175, Avg Validation Loss: 9.868199317783574\n",
            "Epoch 1480/2048, Avg Training Loss: 0.24854599826555934, Avg Validation Loss: 9.868406201443028\n",
            "Epoch 1481/2048, Avg Training Loss: 0.24855413082055702, Avg Validation Loss: 9.868562430870762\n",
            "Epoch 1482/2048, Avg Training Loss: 0.24856275728727584, Avg Validation Loss: 9.86869563902366\n",
            "Epoch 1483/2048, Avg Training Loss: 0.24857167945094494, Avg Validation Loss: 9.868876519770906\n",
            "Epoch 1484/2048, Avg Training Loss: 0.24858054507292107, Avg Validation Loss: 9.869140460218349\n",
            "Epoch 1485/2048, Avg Training Loss: 0.2485885097862123, Avg Validation Loss: 9.869470045706539\n",
            "Epoch 1486/2048, Avg Training Loss: 0.24859629221620044, Avg Validation Loss: 9.869760383133634\n",
            "Epoch 1487/2048, Avg Training Loss: 0.24860461128858047, Avg Validation Loss: 9.87012516809275\n",
            "Epoch 1488/2048, Avg Training Loss: 0.24861228494679366, Avg Validation Loss: 9.87054998375434\n",
            "Epoch 1489/2048, Avg Training Loss: 0.24861991324871532, Avg Validation Loss: 9.870931512536366\n",
            "Epoch 1490/2048, Avg Training Loss: 0.2486282175266452, Avg Validation Loss: 9.871287832664992\n",
            "Epoch 1491/2048, Avg Training Loss: 0.24863676424491354, Avg Validation Loss: 9.871678317000539\n",
            "Epoch 1492/2048, Avg Training Loss: 0.24864460065413788, Avg Validation Loss: 9.872107327523794\n",
            "Epoch 1493/2048, Avg Training Loss: 0.24865235673038794, Avg Validation Loss: 9.872481508088079\n",
            "Epoch 1494/2048, Avg Training Loss: 0.2486606433875764, Avg Validation Loss: 9.872873922669395\n",
            "Epoch 1495/2048, Avg Training Loss: 0.24866837260387095, Avg Validation Loss: 9.873296369356623\n",
            "Epoch 1496/2048, Avg Training Loss: 0.24867610241800875, Avg Validation Loss: 9.873659425789857\n",
            "Epoch 1497/2048, Avg Training Loss: 0.24868453133057297, Avg Validation Loss: 9.873988927076674\n",
            "Epoch 1498/2048, Avg Training Loss: 0.24869321419204418, Avg Validation Loss: 9.874348590268268\n",
            "Epoch 1499/2048, Avg Training Loss: 0.2487011918550889, Avg Validation Loss: 9.874745032216714\n",
            "Epoch 1500/2048, Avg Training Loss: 0.24870911323409997, Avg Validation Loss: 9.875100053163454\n",
            "Epoch 1501/2048, Avg Training Loss: 0.2487175500878029, Avg Validation Loss: 9.875480758379668\n",
            "Epoch 1502/2048, Avg Training Loss: 0.24872542026122532, Avg Validation Loss: 9.875895630029488\n",
            "Epoch 1503/2048, Avg Training Loss: 0.24873328636609826, Avg Validation Loss: 9.876253176065315\n",
            "Epoch 1504/2048, Avg Training Loss: 0.24874184859631512, Avg Validation Loss: 9.876578323265326\n",
            "Epoch 1505/2048, Avg Training Loss: 0.24875066260581893, Avg Validation Loss: 9.876934433319686\n",
            "Epoch 1506/2048, Avg Training Loss: 0.24875882583363823, Avg Validation Loss: 9.877366002191712\n",
            "Epoch 1507/2048, Avg Training Loss: 0.24876687965951638, Avg Validation Loss: 9.877761881490967\n",
            "Epoch 1508/2048, Avg Training Loss: 0.24877557367435976, Avg Validation Loss: 9.878136467103127\n",
            "Epoch 1509/2048, Avg Training Loss: 0.24878448933277364, Avg Validation Loss: 9.878547514258404\n",
            "Epoch 1510/2048, Avg Training Loss: 0.24879268070661884, Avg Validation Loss: 9.878998302195226\n",
            "Epoch 1511/2048, Avg Training Loss: 0.2488007846088315, Avg Validation Loss: 9.87939415825197\n",
            "Epoch 1512/2048, Avg Training Loss: 0.2488095396020424, Avg Validation Loss: 9.879758528914794\n",
            "Epoch 1513/2048, Avg Training Loss: 0.24881852153791806, Avg Validation Loss: 9.880154155057678\n",
            "Epoch 1514/2048, Avg Training Loss: 0.2488267813746901, Avg Validation Loss: 9.880586955193609\n",
            "Epoch 1515/2048, Avg Training Loss: 0.24883495481184045, Avg Validation Loss: 9.880963511695873\n",
            "Epoch 1516/2048, Avg Training Loss: 0.24884377958907045, Avg Validation Loss: 9.881308081446186\n",
            "Epoch 1517/2048, Avg Training Loss: 0.24885283115751924, Avg Validation Loss: 9.881683922115574\n",
            "Epoch 1518/2048, Avg Training Loss: 0.24886115955554744, Avg Validation Loss: 9.88209717572323\n",
            "Epoch 1519/2048, Avg Training Loss: 0.24886940127459226, Avg Validation Loss: 9.88245438608514\n",
            "Epoch 1520/2048, Avg Training Loss: 0.24887829413419074, Avg Validation Loss: 9.882779910391545\n",
            "Epoch 1521/2048, Avg Training Loss: 0.24888741344637888, Avg Validation Loss: 9.883137132258982\n",
            "Epoch 1522/2048, Avg Training Loss: 0.2488958079951867, Avg Validation Loss: 9.883532210014309\n",
            "Epoch 1523/2048, Avg Training Loss: 0.24890411566123002, Avg Validation Loss: 9.883871549658213\n",
            "Epoch 1524/2048, Avg Training Loss: 0.24891307453617936, Avg Validation Loss: 9.884179545210179\n",
            "Epoch 1525/2048, Avg Training Loss: 0.2489222596957988, Avg Validation Loss: 9.884519665993412\n",
            "Epoch 1526/2048, Avg Training Loss: 0.24893071818700338, Avg Validation Loss: 9.884898069045498\n",
            "Epoch 1527/2048, Avg Training Loss: 0.24893908974666662, Avg Validation Loss: 9.885221025024292\n",
            "Epoch 1528/2048, Avg Training Loss: 0.24894811286900773, Avg Validation Loss: 9.885512956993265\n",
            "Epoch 1529/2048, Avg Training Loss: 0.24895736227892962, Avg Validation Loss: 9.88583741029931\n",
            "Epoch 1530/2048, Avg Training Loss: 0.24896588242066023, Avg Validation Loss: 9.886199529207424\n",
            "Epoch 1531/2048, Avg Training Loss: 0.24897431067770007, Avg Validation Loss: 9.886505568689374\n",
            "Epoch 1532/2048, Avg Training Loss: 0.24898338937602071, Avg Validation Loss: 9.886780407603355\n",
            "Epoch 1533/2048, Avg Training Loss: 0.24899283451340243, Avg Validation Loss: 9.887042451831656\n",
            "Epoch 1534/2048, Avg Training Loss: 0.24900235257416187, Avg Validation Loss: 9.887347111620944\n",
            "Epoch 1535/2048, Avg Training Loss: 0.24901105641915222, Avg Validation Loss: 9.887696054863575\n",
            "Epoch 1536/2048, Avg Training Loss: 0.2490196301425996, Avg Validation Loss: 9.88799283548233\n",
            "Epoch 1537/2048, Avg Training Loss: 0.24902883410217816, Avg Validation Loss: 9.88826060307128\n",
            "Epoch 1538/2048, Avg Training Loss: 0.24903825244543415, Avg Validation Loss: 9.888562333575907\n",
            "Epoch 1539/2048, Avg Training Loss: 0.2490469287441404, Avg Validation Loss: 9.888903785756998\n",
            "Epoch 1540/2048, Avg Training Loss: 0.24905551511563984, Avg Validation Loss: 9.889190637584875\n",
            "Epoch 1541/2048, Avg Training Loss: 0.24906475393413546, Avg Validation Loss: 9.889447276707115\n",
            "Epoch 1542/2048, Avg Training Loss: 0.24907434598791295, Avg Validation Loss: 9.88968705346108\n",
            "Epoch 1543/2048, Avg Training Loss: 0.24908400516069715, Avg Validation Loss: 9.889967640925596\n",
            "Epoch 1544/2048, Avg Training Loss: 0.24909283954702163, Avg Validation Loss: 9.890291849951131\n",
            "Epoch 1545/2048, Avg Training Loss: 0.24910154278960064, Avg Validation Loss: 9.890563619325297\n",
            "Epoch 1546/2048, Avg Training Loss: 0.24911087813007673, Avg Validation Loss: 9.89080358996828\n",
            "Epoch 1547/2048, Avg Training Loss: 0.2491205756515216, Avg Validation Loss: 9.890966790658181\n",
            "Epoch 1548/2048, Avg Training Loss: 0.2491301953713702, Avg Validation Loss: 9.891120530075373\n",
            "Epoch 1549/2048, Avg Training Loss: 0.24913888207506568, Avg Validation Loss: 9.89129213661421\n",
            "Epoch 1550/2048, Avg Training Loss: 0.24914738452881594, Avg Validation Loss: 9.891398685868953\n",
            "Epoch 1551/2048, Avg Training Loss: 0.24915649432293024, Avg Validation Loss: 9.89147087558779\n",
            "Epoch 1552/2048, Avg Training Loss: 0.24916593506679172, Avg Validation Loss: 9.891525432522709\n",
            "Epoch 1553/2048, Avg Training Loss: 0.24917543233641862, Avg Validation Loss: 9.891621908355969\n",
            "Epoch 1554/2048, Avg Training Loss: 0.24918408882369733, Avg Validation Loss: 9.89176403268567\n",
            "Epoch 1555/2048, Avg Training Loss: 0.249192614524198, Avg Validation Loss: 9.891856072598598\n",
            "Epoch 1556/2048, Avg Training Loss: 0.24920177843916508, Avg Validation Loss: 9.891921846883129\n",
            "Epoch 1557/2048, Avg Training Loss: 0.24921128963364758, Avg Validation Loss: 9.891974362779711\n",
            "Epoch 1558/2048, Avg Training Loss: 0.24922086566833357, Avg Validation Loss: 9.892071185459331\n",
            "Epoch 1559/2048, Avg Training Loss: 0.24922959822339213, Avg Validation Loss: 9.892214954982775\n",
            "Epoch 1560/2048, Avg Training Loss: 0.24923820288770332, Avg Validation Loss: 9.892309303841818\n",
            "Epoch 1561/2048, Avg Training Loss: 0.24924744965205747, Avg Validation Loss: 9.892377744021667\n",
            "Epoch 1562/2048, Avg Training Loss: 0.24925704556796047, Avg Validation Loss: 9.892433116325268\n",
            "Epoch 1563/2048, Avg Training Loss: 0.24926683504532793, Avg Validation Loss: 9.892482422526427\n",
            "Epoch 1564/2048, Avg Training Loss: 0.24927660715358158, Avg Validation Loss: 9.892579972890166\n",
            "Epoch 1565/2048, Avg Training Loss: 0.24928548276176443, Avg Validation Loss: 9.892726610471136\n",
            "Epoch 1566/2048, Avg Training Loss: 0.24929420791638718, Avg Validation Loss: 9.892824933489832\n",
            "Epoch 1567/2048, Avg Training Loss: 0.24930356632348888, Avg Validation Loss: 9.892897931670179\n",
            "Epoch 1568/2048, Avg Training Loss: 0.24931326894203185, Avg Validation Loss: 9.892958160203039\n",
            "Epoch 1569/2048, Avg Training Loss: 0.2493231622493282, Avg Validation Loss: 9.8930124647613\n",
            "Epoch 1570/2048, Avg Training Loss: 0.24933303611623103, Avg Validation Loss: 9.893115094518873\n",
            "Epoch 1571/2048, Avg Training Loss: 0.24934200278185814, Avg Validation Loss: 9.893266844217464\n",
            "Epoch 1572/2048, Avg Training Loss: 0.24935081916201732, Avg Validation Loss: 9.893370240770556\n",
            "Epoch 1573/2048, Avg Training Loss: 0.24936027211622386, Avg Validation Loss: 9.893448271071179\n",
            "Epoch 1574/2048, Avg Training Loss: 0.249370070808005, Avg Validation Loss: 9.893513488578416\n",
            "Epoch 1575/2048, Avg Training Loss: 0.24938005692740278, Avg Validation Loss: 9.893566999520395\n",
            "Epoch 1576/2048, Avg Training Loss: 0.24938999645410087, Avg Validation Loss: 9.893663743879474\n",
            "Epoch 1577/2048, Avg Training Loss: 0.2493990131172604, Avg Validation Loss: 9.893806990899526\n",
            "Epoch 1578/2048, Avg Training Loss: 0.2494078774122066, Avg Validation Loss: 9.89390057008046\n",
            "Epoch 1579/2048, Avg Training Loss: 0.24941738078111772, Avg Validation Loss: 9.893968199552974\n",
            "Epoch 1580/2048, Avg Training Loss: 0.24942723127603111, Avg Validation Loss: 9.894022825109857\n",
            "Epoch 1581/2048, Avg Training Loss: 0.24943727381934536, Avg Validation Loss: 9.894071500583982\n",
            "Epoch 1582/2048, Avg Training Loss: 0.2494474259566356, Avg Validation Loss: 9.894118059089882\n",
            "Epoch 1583/2048, Avg Training Loss: 0.2494575146287845, Avg Validation Loss: 9.894215132649418\n",
            "Epoch 1584/2048, Avg Training Loss: 0.24946665059882603, Avg Validation Loss: 9.894362604725853\n",
            "Epoch 1585/2048, Avg Training Loss: 0.24947562598788237, Avg Validation Loss: 9.894462445711701\n",
            "Epoch 1586/2048, Avg Training Loss: 0.24948524036218617, Avg Validation Loss: 9.894537386177351\n",
            "Epoch 1587/2048, Avg Training Loss: 0.24949520151501095, Avg Validation Loss: 9.894599836708656\n",
            "Epoch 1588/2048, Avg Training Loss: 0.24950535421871115, Avg Validation Loss: 9.894656561500073\n",
            "Epoch 1589/2048, Avg Training Loss: 0.24951561593499927, Avg Validation Loss: 9.894711237269384\n",
            "Epoch 1590/2048, Avg Training Loss: 0.249525942733598, Avg Validation Loss: 9.894765853345687\n",
            "Epoch 1591/2048, Avg Training Loss: 0.24953618158327912, Avg Validation Loss: 9.89487204941682\n",
            "Epoch 1592/2048, Avg Training Loss: 0.2495454552243536, Avg Validation Loss: 9.895034010830631\n",
            "Epoch 1593/2048, Avg Training Loss: 0.24955456394717956, Avg Validation Loss: 9.895151057042526\n",
            "Epoch 1594/2048, Avg Training Loss: 0.24956431424682837, Avg Validation Loss: 9.895244503545424\n",
            "Epoch 1595/2048, Avg Training Loss: 0.24957441212055656, Avg Validation Loss: 9.895325996957501\n",
            "Epoch 1596/2048, Avg Training Loss: 0.24958470138709143, Avg Validation Loss: 9.895401890574831\n",
            "Epoch 1597/2048, Avg Training Loss: 0.2495950990086853, Avg Validation Loss: 9.89547564153829\n",
            "Epoch 1598/2048, Avg Training Loss: 0.24960556079739782, Avg Validation Loss: 9.89554912321431\n",
            "Epoch 1599/2048, Avg Training Loss: 0.249616063209403, Avg Validation Loss: 9.8956233433115\n",
            "Epoch 1600/2048, Avg Training Loss: 0.24962646355774115, Avg Validation Loss: 9.89574942759142\n",
            "Epoch 1601/2048, Avg Training Loss: 0.2496358590474296, Avg Validation Loss: 9.895926424404133\n",
            "Epoch 1602/2048, Avg Training Loss: 0.2496450855203697, Avg Validation Loss: 9.89605579032144\n",
            "Epoch 1603/2048, Avg Training Loss: 0.24965495702712537, Avg Validation Loss: 9.896160025534359\n",
            "Epoch 1604/2048, Avg Training Loss: 0.24966517755996526, Avg Validation Loss: 9.89625141886291\n",
            "Epoch 1605/2048, Avg Training Loss: 0.24967558987815622, Avg Validation Loss: 9.89633667112518\n",
            "Epoch 1606/2048, Avg Training Loss: 0.24968611038097374, Avg Validation Loss: 9.89641941957158\n",
            "Epoch 1607/2048, Avg Training Loss: 0.24969669458081512, Avg Validation Loss: 9.896501638087384\n",
            "Epoch 1608/2048, Avg Training Loss: 0.24970731877460967, Avg Validation Loss: 9.896584390843737\n",
            "Epoch 1609/2048, Avg Training Loss: 0.24971797030709084, Avg Validation Loss: 9.89666824390251\n",
            "Epoch 1610/2048, Avg Training Loss: 0.24972864239632558, Avg Validation Loss: 9.896753491539885\n",
            "Epoch 1611/2048, Avg Training Loss: 0.24973920063106889, Avg Validation Loss: 9.896890891626356\n",
            "Epoch 1612/2048, Avg Training Loss: 0.24974872671957657, Avg Validation Loss: 9.897079279130407\n",
            "Epoch 1613/2048, Avg Training Loss: 0.24975808231623478, Avg Validation Loss: 9.897219957107826\n",
            "Epoch 1614/2048, Avg Training Loss: 0.24976808937861203, Avg Validation Loss: 9.89733537380885\n",
            "Epoch 1615/2048, Avg Training Loss: 0.24977844841998648, Avg Validation Loss: 9.897437790786897\n",
            "Epoch 1616/2048, Avg Training Loss: 0.2497890003468795, Avg Validation Loss: 9.897533895858523\n",
            "Epoch 1617/2048, Avg Training Loss: 0.24979966109884863, Avg Validation Loss: 9.897626137303323\n",
            "Epoch 1618/2048, Avg Training Loss: 0.24981037594271965, Avg Validation Loss: 9.897716194386962\n",
            "Epoch 1619/2048, Avg Training Loss: 0.24982112695527664, Avg Validation Loss: 9.8978058578325\n",
            "Epoch 1620/2048, Avg Training Loss: 0.2498319028655376, Avg Validation Loss: 9.897896086800094\n",
            "Epoch 1621/2048, Avg Training Loss: 0.24984269762880226, Avg Validation Loss: 9.897987388337405\n",
            "Epoch 1622/2048, Avg Training Loss: 0.24985350797865735, Avg Validation Loss: 9.898080023829044\n",
            "Epoch 1623/2048, Avg Training Loss: 0.24986433212590975, Avg Validation Loss: 9.898174121300544\n",
            "Epoch 1624/2048, Avg Training Loss: 0.2498751690670135, Avg Validation Loss: 9.898269736498698\n",
            "Epoch 1625/2048, Avg Training Loss: 0.24988601821644685, Avg Validation Loss: 9.898366886105784\n",
            "Epoch 1626/2048, Avg Training Loss: 0.24989687921128378, Avg Validation Loss: 9.89846556579663\n",
            "Epoch 1627/2048, Avg Training Loss: 0.2499076200669459, Avg Validation Loss: 9.898616396490478\n",
            "Epoch 1628/2048, Avg Training Loss: 0.24991729309155833, Avg Validation Loss: 9.898818122856069\n",
            "Epoch 1629/2048, Avg Training Loss: 0.24992679684463745, Avg Validation Loss: 9.89897195568262\n",
            "Epoch 1630/2048, Avg Training Loss: 0.2499369639767479, Avg Validation Loss: 9.899100330657586\n",
            "Epoch 1631/2048, Avg Training Loss: 0.24994748878537745, Avg Validation Loss: 9.899215503660981\n",
            "Epoch 1632/2048, Avg Training Loss: 0.24995820886602113, Avg Validation Loss: 9.899324160610778\n",
            "Epoch 1633/2048, Avg Training Loss: 0.24996903786854202, Avg Validation Loss: 9.89942994156976\n",
            "Epoch 1634/2048, Avg Training Loss: 0.24997992984285622, Avg Validation Loss: 9.899534821828455\n",
            "Epoch 1635/2048, Avg Training Loss: 0.24999086030949821, Avg Validation Loss: 9.8996398671826\n",
            "Epoch 1636/2048, Avg Training Loss: 0.2500018162025868, Avg Validation Loss: 9.899745646804746\n",
            "Epoch 1637/2048, Avg Training Loss: 0.2500127905245709, Avg Validation Loss: 9.89985245886115\n",
            "Epoch 1638/2048, Avg Training Loss: 0.2500237795046726, Avg Validation Loss: 9.899960453762766\n",
            "Epoch 1639/2048, Avg Training Loss: 0.2500347810881365, Avg Validation Loss: 9.900069701474981\n",
            "Epoch 1640/2048, Avg Training Loss: 0.25004579413297134, Avg Validation Loss: 9.900180228265416\n",
            "Epoch 1641/2048, Avg Training Loss: 0.2500568179828455, Avg Validation Loss: 9.900292036759891\n",
            "Epoch 1642/2048, Avg Training Loss: 0.25006786073403864, Avg Validation Loss: 9.900412246014804\n",
            "Epoch 1643/2048, Avg Training Loss: 0.25007891048762915, Avg Validation Loss: 9.900538445244132\n",
            "Epoch 1644/2048, Avg Training Loss: 0.2500899645781727, Avg Validation Loss: 9.900668271188584\n",
            "Epoch 1645/2048, Avg Training Loss: 0.2501010252946228, Avg Validation Loss: 9.90080043683519\n",
            "Epoch 1646/2048, Avg Training Loss: 0.2501120938041191, Avg Validation Loss: 9.900934237912429\n",
            "Epoch 1647/2048, Avg Training Loss: 0.2501231706809067, Avg Validation Loss: 9.901069285733222\n",
            "Epoch 1648/2048, Avg Training Loss: 0.25013425618610413, Avg Validation Loss: 9.901205362566367\n",
            "Epoch 1649/2048, Avg Training Loss: 0.2501453504155854, Avg Validation Loss: 9.901342343338813\n",
            "Epoch 1650/2048, Avg Training Loss: 0.2501564533781152, Avg Validation Loss: 9.901480153246865\n",
            "Epoch 1651/2048, Avg Training Loss: 0.2501675650366215, Avg Validation Loss: 9.901618744807054\n",
            "Epoch 1652/2048, Avg Training Loss: 0.2501786853299894, Avg Validation Loss: 9.901758085430828\n",
            "Epoch 1653/2048, Avg Training Loss: 0.2501898141845667, Avg Validation Loss: 9.901898150696452\n",
            "Epoch 1654/2048, Avg Training Loss: 0.25020095152023747, Avg Validation Loss: 9.902038920705031\n",
            "Epoch 1655/2048, Avg Training Loss: 0.25021209725362425, Avg Validation Loss: 9.902180378106134\n",
            "Epoch 1656/2048, Avg Training Loss: 0.25022325129977785, Avg Validation Loss: 9.902322507027343\n",
            "Epoch 1657/2048, Avg Training Loss: 0.2502344135730658, Avg Validation Loss: 9.902465292493012\n",
            "Epoch 1658/2048, Avg Training Loss: 0.2502455839876402, Avg Validation Loss: 9.902608720108026\n",
            "Epoch 1659/2048, Avg Training Loss: 0.25025676245768175, Avg Validation Loss: 9.90275277588492\n",
            "Epoch 1660/2048, Avg Training Loss: 0.25026794889752746, Avg Validation Loss: 9.902897446148714\n",
            "Epoch 1661/2048, Avg Training Loss: 0.25027914322173606, Avg Validation Loss: 9.903042717483778\n",
            "Epoch 1662/2048, Avg Training Loss: 0.2502903453451202, Avg Validation Loss: 9.903188576703561\n",
            "Epoch 1663/2048, Avg Training Loss: 0.2503015551827624, Avg Validation Loss: 9.903335010832615\n",
            "Epoch 1664/2048, Avg Training Loss: 0.2503127726500222, Avg Validation Loss: 9.903482007095418\n",
            "Epoch 1665/2048, Avg Training Loss: 0.2503239976625368, Avg Validation Loss: 9.903629552908807\n",
            "Epoch 1666/2048, Avg Training Loss: 0.250335230136222, Avg Validation Loss: 9.90377763592567\n",
            "Epoch 1667/2048, Avg Training Loss: 0.2503464699872695, Avg Validation Loss: 9.903926244964502\n",
            "Epoch 1668/2048, Avg Training Loss: 0.25035771713214505, Avg Validation Loss: 9.90407536840161\n",
            "Epoch 1669/2048, Avg Training Loss: 0.2503689714875859, Avg Validation Loss: 9.904224994374953\n",
            "Epoch 1670/2048, Avg Training Loss: 0.25038023297059786, Avg Validation Loss: 9.904375111194517\n",
            "Epoch 1671/2048, Avg Training Loss: 0.2503915014984529, Avg Validation Loss: 9.90452570733929\n",
            "Epoch 1672/2048, Avg Training Loss: 0.2504027769886864, Avg Validation Loss: 9.904676771454316\n",
            "Epoch 1673/2048, Avg Training Loss: 0.25041405935909444, Avg Validation Loss: 9.904828292347831\n",
            "Epoch 1674/2048, Avg Training Loss: 0.25042534852773146, Avg Validation Loss: 9.904980258988497\n",
            "Epoch 1675/2048, Avg Training Loss: 0.2504366444129077, Avg Validation Loss: 9.905132660502636\n",
            "Epoch 1676/2048, Avg Training Loss: 0.25044794693318717, Avg Validation Loss: 9.905285486171575\n",
            "Epoch 1677/2048, Avg Training Loss: 0.2504592560073848, Avg Validation Loss: 9.905438725429024\n",
            "Epoch 1678/2048, Avg Training Loss: 0.25047057155456465, Avg Validation Loss: 9.905592367858489\n",
            "Epoch 1679/2048, Avg Training Loss: 0.2504818934940377, Avg Validation Loss: 9.905746403190749\n",
            "Epoch 1680/2048, Avg Training Loss: 0.2504932217453596, Avg Validation Loss: 9.905900821301381\n",
            "Epoch 1681/2048, Avg Training Loss: 0.250504556228329, Avg Validation Loss: 9.906055612208323\n",
            "Epoch 1682/2048, Avg Training Loss: 0.2505158968629852, Avg Validation Loss: 9.906210766069474\n",
            "Epoch 1683/2048, Avg Training Loss: 0.2505272435696066, Avg Validation Loss: 9.906366273180371\n",
            "Epoch 1684/2048, Avg Training Loss: 0.2505385962687088, Avg Validation Loss: 9.90652212397187\n",
            "Epoch 1685/2048, Avg Training Loss: 0.25054995488104254, Avg Validation Loss: 9.906678309007894\n",
            "Epoch 1686/2048, Avg Training Loss: 0.25056130847868135, Avg Validation Loss: 9.906782498514877\n",
            "Epoch 1687/2048, Avg Training Loss: 0.2505725949986814, Avg Validation Loss: 9.906840820030755\n",
            "Epoch 1688/2048, Avg Training Loss: 0.25058391448876377, Avg Validation Loss: 9.906875905358467\n",
            "Epoch 1689/2048, Avg Training Loss: 0.25059525642942104, Avg Validation Loss: 9.906899949348576\n",
            "Epoch 1690/2048, Avg Training Loss: 0.2506066151830021, Avg Validation Loss: 9.906919508651646\n",
            "Epoch 1691/2048, Avg Training Loss: 0.2506179876712096, Avg Validation Loss: 9.90693809482199\n",
            "Epoch 1692/2048, Avg Training Loss: 0.2506293721668666, Avg Validation Loss: 9.906957575182105\n",
            "Epoch 1693/2048, Avg Training Loss: 0.25064076765938026, Avg Validation Loss: 9.906978929506263\n",
            "Epoch 1694/2048, Avg Training Loss: 0.2506521735197018, Avg Validation Loss: 9.907002658719358\n",
            "Epoch 1695/2048, Avg Training Loss: 0.2506635893203518, Avg Validation Loss: 9.907029004263261\n",
            "Epoch 1696/2048, Avg Training Loss: 0.25067501471087567, Avg Validation Loss: 9.907058065023755\n",
            "Epoch 1697/2048, Avg Training Loss: 0.2506864494492238, Avg Validation Loss: 9.907089864379337\n",
            "Epoch 1698/2048, Avg Training Loss: 0.2506978933462336, Avg Validation Loss: 9.907124388819513\n",
            "Epoch 1699/2048, Avg Training Loss: 0.2507093462172985, Avg Validation Loss: 9.907161605404307\n",
            "Epoch 1700/2048, Avg Training Loss: 0.2507208078815889, Avg Validation Loss: 9.907201471182308\n",
            "Epoch 1701/2048, Avg Training Loss: 0.25073227816161, Avg Validation Loss: 9.907243938266838\n",
            "Epoch 1702/2048, Avg Training Loss: 0.250743756882934, Avg Validation Loss: 9.907288956567808\n",
            "Epoch 1703/2048, Avg Training Loss: 0.2507552438740254, Avg Validation Loss: 9.907336475257823\n",
            "Epoch 1704/2048, Avg Training Loss: 0.2507667389661156, Avg Validation Loss: 9.907386443554959\n",
            "Epoch 1705/2048, Avg Training Loss: 0.2507782419931038, Avg Validation Loss: 9.907438811136783\n",
            "Epoch 1706/2048, Avg Training Loss: 0.2507897527914738, Avg Validation Loss: 9.907493528355312\n",
            "Epoch 1707/2048, Avg Training Loss: 0.2508012712002192, Avg Validation Loss: 9.907550546344721\n",
            "Epoch 1708/2048, Avg Training Loss: 0.25081279706077364, Avg Validation Loss: 9.907609817071283\n",
            "Epoch 1709/2048, Avg Training Loss: 0.2508243302169457, Avg Validation Loss: 9.907671293352276\n",
            "Epoch 1710/2048, Avg Training Loss: 0.2508358705148565, Avg Validation Loss: 9.907734928858307\n",
            "Epoch 1711/2048, Avg Training Loss: 0.2508474178028799, Avg Validation Loss: 9.907800678106831\n",
            "Epoch 1712/2048, Avg Training Loss: 0.25085897193158374, Avg Validation Loss: 9.907868496451108\n",
            "Epoch 1713/2048, Avg Training Loss: 0.2508705327536747, Avg Validation Loss: 9.907938340066803\n",
            "Epoch 1714/2048, Avg Training Loss: 0.25088210012394374, Avg Validation Loss: 9.908010165937512\n",
            "Epoch 1715/2048, Avg Training Loss: 0.25089367389921363, Avg Validation Loss: 9.908083931839847\n",
            "Epoch 1716/2048, Avg Training Loss: 0.25090525393828816, Avg Validation Loss: 9.908159596328405\n",
            "Epoch 1717/2048, Avg Training Loss: 0.2509168401019029, Avg Validation Loss: 9.908237118720859\n",
            "Epoch 1718/2048, Avg Training Loss: 0.2509284322526774, Avg Validation Loss: 9.90831645908322\n",
            "Epoch 1719/2048, Avg Training Loss: 0.2509400477151812, Avg Validation Loss: 9.908418096994833\n",
            "Epoch 1720/2048, Avg Training Loss: 0.25095163598100867, Avg Validation Loss: 9.908535083532119\n",
            "Epoch 1721/2048, Avg Training Loss: 0.2509632004393599, Avg Validation Loss: 9.908660625539945\n",
            "Epoch 1722/2048, Avg Training Loss: 0.2509747542779107, Avg Validation Loss: 9.908791039125672\n",
            "Epoch 1723/2048, Avg Training Loss: 0.2509862963126663, Avg Validation Loss: 9.908877044537999\n",
            "Epoch 1724/2048, Avg Training Loss: 0.25099781722358616, Avg Validation Loss: 9.908923145658346\n",
            "Epoch 1725/2048, Avg Training Loss: 0.25100939757033575, Avg Validation Loss: 9.90894925896739\n",
            "Epoch 1726/2048, Avg Training Loss: 0.25102101325914644, Avg Validation Loss: 9.908966123214977\n",
            "Epoch 1727/2048, Avg Training Loss: 0.25103265138881165, Avg Validation Loss: 9.908979513216375\n",
            "Epoch 1728/2048, Avg Training Loss: 0.25104430499422936, Avg Validation Loss: 9.90899251973234\n",
            "Epoch 1729/2048, Avg Training Loss: 0.2510559702676136, Avg Validation Loss: 9.909006782638945\n",
            "Epoch 1730/2048, Avg Training Loss: 0.2510676450838016, Avg Validation Loss: 9.909023157834502\n",
            "Epoch 1731/2048, Avg Training Loss: 0.2510793282159305, Avg Validation Loss: 9.909042077874977\n",
            "Epoch 1732/2048, Avg Training Loss: 0.251091018917764, Avg Validation Loss: 9.909063746978315\n",
            "Epoch 1733/2048, Avg Training Loss: 0.2511027167010929, Avg Validation Loss: 9.909088246461312\n",
            "Epoch 1734/2048, Avg Training Loss: 0.25111442121703226, Avg Validation Loss: 9.909115591742827\n",
            "Epoch 1735/2048, Avg Training Loss: 0.2511261321926834, Avg Validation Loss: 9.909145763157024\n",
            "Epoch 1736/2048, Avg Training Loss: 0.2511378493973116, Avg Validation Loss: 9.909178722604588\n",
            "Epoch 1737/2048, Avg Training Loss: 0.2511495726242621, Avg Validation Loss: 9.909214422546134\n",
            "Epoch 1738/2048, Avg Training Loss: 0.25116130168127004, Avg Validation Loss: 9.909252810854856\n",
            "Epoch 1739/2048, Avg Training Loss: 0.2511730309847555, Avg Validation Loss: 9.909299652852937\n",
            "Epoch 1740/2048, Avg Training Loss: 0.25118477384447657, Avg Validation Loss: 9.90935633903477\n",
            "Epoch 1741/2048, Avg Training Loss: 0.25119651004476945, Avg Validation Loss: 9.909419265760466\n",
            "Epoch 1742/2048, Avg Training Loss: 0.251208244608553, Avg Validation Loss: 9.909486465274075\n",
            "Epoch 1743/2048, Avg Training Loss: 0.2512199801487474, Avg Validation Loss: 9.9095568537329\n",
            "Epoch 1744/2048, Avg Training Loss: 0.2512317179945466, Avg Validation Loss: 9.909629825808933\n",
            "Epoch 1745/2048, Avg Training Loss: 0.25124345874567744, Avg Validation Loss: 9.90970503194601\n",
            "Epoch 1746/2048, Avg Training Loss: 0.251255202668927, Avg Validation Loss: 9.909782262053987\n",
            "Epoch 1747/2048, Avg Training Loss: 0.2512669499028002, Avg Validation Loss: 9.909861384202124\n",
            "Epoch 1748/2048, Avg Training Loss: 0.2512787004622175, Avg Validation Loss: 9.909942309057744\n",
            "Epoch 1749/2048, Avg Training Loss: 0.2512904542971207, Avg Validation Loss: 9.910024970655805\n",
            "Epoch 1750/2048, Avg Training Loss: 0.2513022113236755, Avg Validation Loss: 9.910109315996477\n",
            "Epoch 1751/2048, Avg Training Loss: 0.2513139714408734, Avg Validation Loss: 9.910195299415134\n",
            "Epoch 1752/2048, Avg Training Loss: 0.25132573453935797, Avg Validation Loss: 9.910282879492504\n",
            "Epoch 1753/2048, Avg Training Loss: 0.2513375005061057, Avg Validation Loss: 9.910372017296874\n",
            "Epoch 1754/2048, Avg Training Loss: 0.2513492692269026, Avg Validation Loss: 9.91046267577068\n",
            "Epoch 1755/2048, Avg Training Loss: 0.25136104058764347, Avg Validation Loss: 9.910554819112454\n",
            "Epoch 1756/2048, Avg Training Loss: 0.25137281447500703, Avg Validation Loss: 9.910648412484223\n",
            "Epoch 1757/2048, Avg Training Loss: 0.25138459077679753, Avg Validation Loss: 9.910743421858442\n",
            "Epoch 1758/2048, Avg Training Loss: 0.2513963693821094, Avg Validation Loss: 9.910839813929597\n",
            "Epoch 1759/2048, Avg Training Loss: 0.2514081501813991, Avg Validation Loss: 9.910937556060889\n",
            "Epoch 1760/2048, Avg Training Loss: 0.25141993306650623, Avg Validation Loss: 9.911036616250009\n",
            "Epoch 1761/2048, Avg Training Loss: 0.25143171793065, Avg Validation Loss: 9.9111369631053\n",
            "Epoch 1762/2048, Avg Training Loss: 0.2514435046684115, Avg Validation Loss: 9.911238565827704\n",
            "Epoch 1763/2048, Avg Training Loss: 0.25145529317570975, Avg Validation Loss: 9.911341394195903\n",
            "Epoch 1764/2048, Avg Training Loss: 0.25146708334977486, Avg Validation Loss: 9.911445418553294\n",
            "Epoch 1765/2048, Avg Training Loss: 0.251478875089119, Avg Validation Loss: 9.911550609796077\n",
            "Epoch 1766/2048, Avg Training Loss: 0.2514906682935082, Avg Validation Loss: 9.911656939362008\n",
            "Epoch 1767/2048, Avg Training Loss: 0.25150246286393396, Avg Validation Loss: 9.911764379219655\n",
            "Epoch 1768/2048, Avg Training Loss: 0.2515142587025853, Avg Validation Loss: 9.911872901857988\n",
            "Epoch 1769/2048, Avg Training Loss: 0.2515260557128216, Avg Validation Loss: 9.911982480276244\n",
            "Epoch 1770/2048, Avg Training Loss: 0.25153785379914584, Avg Validation Loss: 9.912093087974043\n",
            "Epoch 1771/2048, Avg Training Loss: 0.2515496528671791, Avg Validation Loss: 9.912204698941727\n",
            "Epoch 1772/2048, Avg Training Loss: 0.25156145282363557, Avg Validation Loss: 9.912317287650904\n",
            "Epoch 1773/2048, Avg Training Loss: 0.25157325357629773, Avg Validation Loss: 9.912430829045178\n",
            "Epoch 1774/2048, Avg Training Loss: 0.2515850550339932, Avg Validation Loss: 9.912545298531095\n",
            "Epoch 1775/2048, Avg Training Loss: 0.2515968571065712, Avg Validation Loss: 9.912660671969226\n",
            "Epoch 1776/2048, Avg Training Loss: 0.25160865970488083, Avg Validation Loss: 9.91277692566549\n",
            "Epoch 1777/2048, Avg Training Loss: 0.25162046274074884, Avg Validation Loss: 9.9128940363626\n",
            "Epoch 1778/2048, Avg Training Loss: 0.25163226612695866, Avg Validation Loss: 9.913011981231714\n",
            "Epoch 1779/2048, Avg Training Loss: 0.2516440697772301, Avg Validation Loss: 9.913130737864229\n",
            "Epoch 1780/2048, Avg Training Loss: 0.2516558736061991, Avg Validation Loss: 9.913250284263762\n",
            "Epoch 1781/2048, Avg Training Loss: 0.25166767752939867, Avg Validation Loss: 9.913370598838277\n",
            "Epoch 1782/2048, Avg Training Loss: 0.25167948146324004, Avg Validation Loss: 9.913491660392358\n",
            "Epoch 1783/2048, Avg Training Loss: 0.25169128532499385, Avg Validation Loss: 9.913613448119673\n",
            "Epoch 1784/2048, Avg Training Loss: 0.25170308903277305, Avg Validation Loss: 9.913735941595554\n",
            "Epoch 1785/2048, Avg Training Loss: 0.2517148925055153, Avg Validation Loss: 9.913859120769729\n",
            "Epoch 1786/2048, Avg Training Loss: 0.251726695662966, Avg Validation Loss: 9.913982965959223\n",
            "Epoch 1787/2048, Avg Training Loss: 0.2517384984256623, Avg Validation Loss: 9.914107457841357\n",
            "Epoch 1788/2048, Avg Training Loss: 0.25175030071491666, Avg Validation Loss: 9.91423257744693\n",
            "Epoch 1789/2048, Avg Training Loss: 0.251762102452802, Avg Validation Loss: 9.914358306153527\n",
            "Epoch 1790/2048, Avg Training Loss: 0.251773903562136, Avg Validation Loss: 9.914484625678913\n",
            "Epoch 1791/2048, Avg Training Loss: 0.251785703966467, Avg Validation Loss: 9.914611518074633\n",
            "Epoch 1792/2048, Avg Training Loss: 0.2517975035900596, Avg Validation Loss: 9.914738965719678\n",
            "Epoch 1793/2048, Avg Training Loss: 0.2518093023578804, Avg Validation Loss: 9.914866951314302\n",
            "Epoch 1794/2048, Avg Training Loss: 0.2518211001955853, Avg Validation Loss: 9.914995457873957\n",
            "Epoch 1795/2048, Avg Training Loss: 0.25183289702950545, Avg Validation Loss: 9.915124468723358\n",
            "Epoch 1796/2048, Avg Training Loss: 0.2518446927866353, Avg Validation Loss: 9.915253967490631\n",
            "Epoch 1797/2048, Avg Training Loss: 0.2518564873946196, Avg Validation Loss: 9.91538393810163\n",
            "Epoch 1798/2048, Avg Training Loss: 0.25186828078174145, Avg Validation Loss: 9.915514364774312\n",
            "Epoch 1799/2048, Avg Training Loss: 0.25188007287691083, Avg Validation Loss: 9.915645232013253\n",
            "Epoch 1800/2048, Avg Training Loss: 0.2518918636096525, Avg Validation Loss: 9.915776524604267\n",
            "Epoch 1801/2048, Avg Training Loss: 0.2519036529100954, Avg Validation Loss: 9.915908227609139\n",
            "Epoch 1802/2048, Avg Training Loss: 0.25191544070896155, Avg Validation Loss: 9.916040326360422\n",
            "Epoch 1803/2048, Avg Training Loss: 0.2519272269375555, Avg Validation Loss: 9.91617280645639\n",
            "Epoch 1804/2048, Avg Training Loss: 0.2519390115277538, Avg Validation Loss: 9.916305647358323\n",
            "Epoch 1805/2048, Avg Training Loss: 0.25195079441199525, Avg Validation Loss: 9.916438821258813\n",
            "Epoch 1806/2048, Avg Training Loss: 0.25196257552327095, Avg Validation Loss: 9.916572314011393\n",
            "Epoch 1807/2048, Avg Training Loss: 0.2519743547951149, Avg Validation Loss: 9.916706112198717\n",
            "Epoch 1808/2048, Avg Training Loss: 0.2519861321615944, Avg Validation Loss: 9.916840202645966\n",
            "Epoch 1809/2048, Avg Training Loss: 0.2519978966254361, Avg Validation Loss: 9.916961757425904\n",
            "Epoch 1810/2048, Avg Training Loss: 0.25200959870729156, Avg Validation Loss: 9.917072238793914\n",
            "Epoch 1811/2048, Avg Training Loss: 0.252021279639376, Avg Validation Loss: 9.917177154592734\n",
            "Epoch 1812/2048, Avg Training Loss: 0.25203294826724754, Avg Validation Loss: 9.91727947104117\n",
            "Epoch 1813/2048, Avg Training Loss: 0.252044609244732, Avg Validation Loss: 9.91738078041482\n",
            "Epoch 1814/2048, Avg Training Loss: 0.25205626500214756, Avg Validation Loss: 9.917481932413798\n",
            "Epoch 1815/2048, Avg Training Loss: 0.2520679167911443, Avg Validation Loss: 9.9175833755033\n",
            "Epoch 1816/2048, Avg Training Loss: 0.2520795652389556, Avg Validation Loss: 9.917685341443095\n",
            "Epoch 1817/2048, Avg Training Loss: 0.25209121064227985, Avg Validation Loss: 9.91778794504082\n",
            "Epoch 1818/2048, Avg Training Loss: 0.25210285312306063, Avg Validation Loss: 9.917891236743937\n",
            "Epoch 1819/2048, Avg Training Loss: 0.2521144927110428, Avg Validation Loss: 9.917995225723587\n",
            "Epoch 1820/2048, Avg Training Loss: 0.25212612938751505, Avg Validation Loss: 9.918099910148435\n",
            "Epoch 1821/2048, Avg Training Loss: 0.2521377631084792, Avg Validation Loss: 9.918205280154131\n",
            "Epoch 1822/2048, Avg Training Loss: 0.25214939381691864, Avg Validation Loss: 9.918311320860477\n",
            "Epoch 1823/2048, Avg Training Loss: 0.25216102144929065, Avg Validation Loss: 9.918418014854261\n",
            "Epoch 1824/2048, Avg Training Loss: 0.2521726459389566, Avg Validation Loss: 9.918525343528213\n",
            "Epoch 1825/2048, Avg Training Loss: 0.2521842672179932, Avg Validation Loss: 9.918633287801589\n",
            "Epoch 1826/2048, Avg Training Loss: 0.2521958852181434, Avg Validation Loss: 9.918741828506503\n",
            "Epoch 1827/2048, Avg Training Loss: 0.25220742092427473, Avg Validation Loss: 9.918830646175952\n",
            "Epoch 1828/2048, Avg Training Loss: 0.25221875359919627, Avg Validation Loss: 9.918897752365384\n",
            "Epoch 1829/2048, Avg Training Loss: 0.2522300123218991, Avg Validation Loss: 9.918954015738475\n",
            "Epoch 1830/2048, Avg Training Loss: 0.25224123120839714, Avg Validation Loss: 9.919005311113098\n",
            "Epoch 1831/2048, Avg Training Loss: 0.2522524282807135, Avg Validation Loss: 9.919054803803604\n",
            "Epoch 1832/2048, Avg Training Loss: 0.2522636130266077, Avg Validation Loss: 9.919104189582766\n",
            "Epoch 1833/2048, Avg Training Loss: 0.25227479041329925, Avg Validation Loss: 9.919154367500722\n",
            "Epoch 1834/2048, Avg Training Loss: 0.2522859630153574, Avg Validation Loss: 9.919205804883372\n",
            "Epoch 1835/2048, Avg Training Loss: 0.2522971321418621, Avg Validation Loss: 9.919258735310182\n",
            "Epoch 1836/2048, Avg Training Loss: 0.25230828963198226, Avg Validation Loss: 9.91930501955845\n",
            "Epoch 1837/2048, Avg Training Loss: 0.2523194612596251, Avg Validation Loss: 9.919348653232086\n",
            "Epoch 1838/2048, Avg Training Loss: 0.25233063946580947, Avg Validation Loss: 9.919391773126561\n",
            "Epoch 1839/2048, Avg Training Loss: 0.25234182015935724, Avg Validation Loss: 9.91943551216518\n",
            "Epoch 1840/2048, Avg Training Loss: 0.2523530011011494, Avg Validation Loss: 9.919480461313062\n",
            "Epoch 1841/2048, Avg Training Loss: 0.25236418104209873, Avg Validation Loss: 9.919526919133553\n",
            "Epoch 1842/2048, Avg Training Loss: 0.25237535926293336, Avg Validation Loss: 9.91957502661068\n",
            "Epoch 1843/2048, Avg Training Loss: 0.25238653532840616, Avg Validation Loss: 9.919624839983706\n",
            "Epoch 1844/2048, Avg Training Loss: 0.2523977089559848, Avg Validation Loss: 9.919676370091238\n",
            "Epoch 1845/2048, Avg Training Loss: 0.2524088799456789, Avg Validation Loss: 9.919729603621306\n",
            "Epoch 1846/2048, Avg Training Loss: 0.25242004814252456, Avg Validation Loss: 9.91978451458551\n",
            "Epoch 1847/2048, Avg Training Loss: 0.25243121341651453, Avg Validation Loss: 9.919841070511465\n",
            "Epoch 1848/2048, Avg Training Loss: 0.25244237565184996, Avg Validation Loss: 9.919899235781557\n",
            "Epoch 1849/2048, Avg Training Loss: 0.2524535347411732, Avg Validation Loss: 9.919958973430017\n",
            "Epoch 1850/2048, Avg Training Loss: 0.2524646905824636, Avg Validation Loss: 9.920020246107041\n",
            "Epoch 1851/2048, Avg Training Loss: 0.2524758430773562, Avg Validation Loss: 9.920083016593058\n",
            "Epoch 1852/2048, Avg Training Loss: 0.2524869921302218, Avg Validation Loss: 9.920147248069968\n",
            "Epoch 1853/2048, Avg Training Loss: 0.252498137647655, Avg Validation Loss: 9.920212904261257\n",
            "Epoch 1854/2048, Avg Training Loss: 0.2525092795381806, Avg Validation Loss: 9.92027994950134\n",
            "Epoch 1855/2048, Avg Training Loss: 0.2525204177120769, Avg Validation Loss: 9.920348348309645\n",
            "Epoch 1856/2048, Avg Training Loss: 0.25253155208126316, Avg Validation Loss: 9.920418063402598\n",
            "Epoch 1857/2048, Avg Training Loss: 0.252542682559221, Avg Validation Loss: 9.920489060528727\n",
            "Epoch 1858/2048, Avg Training Loss: 0.2525538090609342, Avg Validation Loss: 9.920561306594614\n",
            "Epoch 1859/2048, Avg Training Loss: 0.25256493150284093, Avg Validation Loss: 9.920634769144874\n",
            "Epoch 1860/2048, Avg Training Loss: 0.2525760498027898, Avg Validation Loss: 9.92070941635172\n",
            "Epoch 1861/2048, Avg Training Loss: 0.25258716388000213, Avg Validation Loss: 9.920785217003784\n",
            "Epoch 1862/2048, Avg Training Loss: 0.25259827365503473, Avg Validation Loss: 9.920862140494753\n",
            "Epoch 1863/2048, Avg Training Loss: 0.252609379049746, Avg Validation Loss: 9.920940156811861\n",
            "Epoch 1864/2048, Avg Training Loss: 0.2526204799872624, Avg Validation Loss: 9.921019236524556\n",
            "Epoch 1865/2048, Avg Training Loss: 0.2526315763919468, Avg Validation Loss: 9.92109935077328\n",
            "Epoch 1866/2048, Avg Training Loss: 0.25264266818936737, Avg Validation Loss: 9.921180471258428\n",
            "Epoch 1867/2048, Avg Training Loss: 0.2526537553062684, Avg Validation Loss: 9.921262570229523\n",
            "Epoch 1868/2048, Avg Training Loss: 0.2526648376705403, Avg Validation Loss: 9.921345620474595\n",
            "Epoch 1869/2048, Avg Training Loss: 0.25267591521119254, Avg Validation Loss: 9.921429595309746\n",
            "Epoch 1870/2048, Avg Training Loss: 0.2526869878583257, Avg Validation Loss: 9.92151446856895\n",
            "Epoch 1871/2048, Avg Training Loss: 0.2526980555431057, Avg Validation Loss: 9.921600214594033\n",
            "Epoch 1872/2048, Avg Training Loss: 0.25270911425504233, Avg Validation Loss: 9.92169501019225\n",
            "Epoch 1873/2048, Avg Training Loss: 0.2527202263719066, Avg Validation Loss: 9.921800880673214\n",
            "Epoch 1874/2048, Avg Training Loss: 0.2527313503506461, Avg Validation Loss: 9.921912831029513\n",
            "Epoch 1875/2048, Avg Training Loss: 0.2527424780528014, Avg Validation Loss: 9.922028155743906\n",
            "Epoch 1876/2048, Avg Training Loss: 0.25275360515199324, Avg Validation Loss: 9.922145386857014\n",
            "Epoch 1877/2048, Avg Training Loss: 0.25276472933708455, Avg Validation Loss: 9.922263725465541\n",
            "Epoch 1878/2048, Avg Training Loss: 0.25277584936269165, Avg Validation Loss: 9.922382734441548\n",
            "Epoch 1879/2048, Avg Training Loss: 0.2527869645472132, Avg Validation Loss: 9.922502172330438\n",
            "Epoch 1880/2048, Avg Training Loss: 0.25279807450739267, Avg Validation Loss: 9.92262190355848\n",
            "Epoch 1881/2048, Avg Training Loss: 0.2528091790179478, Avg Validation Loss: 9.922741849889855\n",
            "Epoch 1882/2048, Avg Training Loss: 0.2528202779373392, Avg Validation Loss: 9.922861964182214\n",
            "Epoch 1883/2048, Avg Training Loss: 0.25283137116851473, Avg Validation Loss: 9.922982216196736\n",
            "Epoch 1884/2048, Avg Training Loss: 0.2528424586381523, Avg Validation Loss: 9.92310258492492\n",
            "Epoch 1885/2048, Avg Training Loss: 0.25285354028568174, Avg Validation Loss: 9.923223054438466\n",
            "Epoch 1886/2048, Avg Training Loss: 0.25286461605747895, Avg Validation Loss: 9.923343611643892\n",
            "Epoch 1887/2048, Avg Training Loss: 0.25287568590379406, Avg Validation Loss: 9.92346424506688\n",
            "Epoch 1888/2048, Avg Training Loss: 0.2528867497771242, Avg Validation Loss: 9.923584944193424\n",
            "Epoch 1889/2048, Avg Training Loss: 0.25289780666295797, Avg Validation Loss: 9.923707385906269\n",
            "Epoch 1890/2048, Avg Training Loss: 0.25290887492168884, Avg Validation Loss: 9.923833216919698\n",
            "Epoch 1891/2048, Avg Training Loss: 0.2529199377742567, Avg Validation Loss: 9.923960808749044\n",
            "Epoch 1892/2048, Avg Training Loss: 0.2529309947425867, Avg Validation Loss: 9.924089278233968\n",
            "Epoch 1893/2048, Avg Training Loss: 0.2529420455598114, Avg Validation Loss: 9.92421814537904\n",
            "Epoch 1894/2048, Avg Training Loss: 0.2529530900689512, Avg Validation Loss: 9.924347148289801\n",
            "Epoch 1895/2048, Avg Training Loss: 0.2529641281702612, Avg Validation Loss: 9.924476143077735\n",
            "Epoch 1896/2048, Avg Training Loss: 0.25297515979385515, Avg Validation Loss: 9.924605049722173\n",
            "Epoch 1897/2048, Avg Training Loss: 0.2529861848854724, Avg Validation Loss: 9.924733822788532\n",
            "Epoch 1898/2048, Avg Training Loss: 0.2529972033990819, Avg Validation Loss: 9.924862435590432\n",
            "Epoch 1899/2048, Avg Training Loss: 0.25300821529304246, Avg Validation Loss: 9.924990871622956\n",
            "Epoch 1900/2048, Avg Training Loss: 0.25301922052811054, Avg Validation Loss: 9.925119119928596\n",
            "Epoch 1901/2048, Avg Training Loss: 0.2530302190664069, Avg Validation Loss: 9.925247172590213\n",
            "Epoch 1902/2048, Avg Training Loss: 0.2530412108708819, Avg Validation Loss: 9.925375023374365\n",
            "Epoch 1903/2048, Avg Training Loss: 0.25305219590503675, Avg Validation Loss: 9.925502666996904\n",
            "Epoch 1904/2048, Avg Training Loss: 0.25306317413277934, Avg Validation Loss: 9.925630098725042\n",
            "Epoch 1905/2048, Avg Training Loss: 0.25307414551834867, Avg Validation Loss: 9.92575731416149\n",
            "Epoch 1906/2048, Avg Training Loss: 0.2530851100262741, Avg Validation Loss: 9.925884309127037\n",
            "Epoch 1907/2048, Avg Training Loss: 0.25309606305354687, Avg Validation Loss: 9.926006573686141\n",
            "Epoch 1908/2048, Avg Training Loss: 0.25310701747571773, Avg Validation Loss: 9.92612560891392\n",
            "Epoch 1909/2048, Avg Training Loss: 0.2531179717404776, Avg Validation Loss: 9.926242885844276\n",
            "Epoch 1910/2048, Avg Training Loss: 0.2531289226515342, Avg Validation Loss: 9.926359199263505\n",
            "Epoch 1911/2048, Avg Training Loss: 0.25313986848318704, Avg Validation Loss: 9.926474974988258\n",
            "Epoch 1912/2048, Avg Training Loss: 0.253150808295769, Avg Validation Loss: 9.926590440657156\n",
            "Epoch 1913/2048, Avg Training Loss: 0.25316174156989363, Avg Validation Loss: 9.926705717038873\n",
            "Epoch 1914/2048, Avg Training Loss: 0.2531726680109883, Avg Validation Loss: 9.926820865535435\n",
            "Epoch 1915/2048, Avg Training Loss: 0.25318358793301166, Avg Validation Loss: 9.926943130832123\n",
            "Epoch 1916/2048, Avg Training Loss: 0.25319449223601986, Avg Validation Loss: 9.927071667945908\n",
            "Epoch 1917/2048, Avg Training Loss: 0.2532053746826687, Avg Validation Loss: 9.927203350110918\n",
            "Epoch 1918/2048, Avg Training Loss: 0.253216242066143, Avg Validation Loss: 9.927336489356883\n",
            "Epoch 1919/2048, Avg Training Loss: 0.25322709800228216, Avg Validation Loss: 9.92747017490579\n",
            "Epoch 1920/2048, Avg Training Loss: 0.25323794440917957, Avg Validation Loss: 9.927603915807198\n",
            "Epoch 1921/2048, Avg Training Loss: 0.2532487822979326, Avg Validation Loss: 9.927737447895465\n",
            "Epoch 1922/2048, Avg Training Loss: 0.25325961219514115, Avg Validation Loss: 9.927870629506582\n",
            "Epoch 1923/2048, Avg Training Loss: 0.25327043436862445, Avg Validation Loss: 9.928003385141704\n",
            "Epoch 1924/2048, Avg Training Loss: 0.2532812489480044, Avg Validation Loss: 9.928135675032035\n",
            "Epoch 1925/2048, Avg Training Loss: 0.25329205598912546, Avg Validation Loss: 9.928267478696364\n",
            "Epoch 1926/2048, Avg Training Loss: 0.2533028555084709, Avg Validation Loss: 9.92839878605799\n",
            "Epoch 1927/2048, Avg Training Loss: 0.2533136475015499, Avg Validation Loss: 9.928529592645706\n",
            "Epoch 1928/2048, Avg Training Loss: 0.253324431952723, Avg Validation Loss: 9.928659897001253\n",
            "Epoch 1929/2048, Avg Training Loss: 0.2533352088404509, Avg Validation Loss: 9.928789699278909\n",
            "Epoch 1930/2048, Avg Training Loss: 0.25334597814010046, Avg Validation Loss: 9.928919000489149\n",
            "Epoch 1931/2048, Avg Training Loss: 0.25335673982544327, Avg Validation Loss: 9.92904780209034\n",
            "Epoch 1932/2048, Avg Training Loss: 0.25336749523107854, Avg Validation Loss: 9.929170068539042\n",
            "Epoch 1933/2048, Avg Training Loss: 0.2533780944031365, Avg Validation Loss: 9.929244098063531\n",
            "Epoch 1934/2048, Avg Training Loss: 0.25338852200023926, Avg Validation Loss: 9.929284677327086\n",
            "Epoch 1935/2048, Avg Training Loss: 0.2533988680839396, Avg Validation Loss: 9.929307786334983\n",
            "Epoch 1936/2048, Avg Training Loss: 0.25340916786849244, Avg Validation Loss: 9.929322000120166\n",
            "Epoch 1937/2048, Avg Training Loss: 0.253419439981435, Avg Validation Loss: 9.929331914359961\n",
            "Epoch 1938/2048, Avg Training Loss: 0.253429694258082, Avg Validation Loss: 9.929339986389845\n",
            "Epoch 1939/2048, Avg Training Loss: 0.2534399358768446, Avg Validation Loss: 9.929347524562887\n",
            "Epoch 1940/2048, Avg Training Loss: 0.2534501675508544, Avg Validation Loss: 9.929355219934868\n",
            "Epoch 1941/2048, Avg Training Loss: 0.25346039068875204, Avg Validation Loss: 9.929363432003687\n",
            "Epoch 1942/2048, Avg Training Loss: 0.25347060600926213, Avg Validation Loss: 9.929372342279137\n",
            "Epoch 1943/2048, Avg Training Loss: 0.2534808138664939, Avg Validation Loss: 9.929382036821984\n",
            "Epoch 1944/2048, Avg Training Loss: 0.25349101442209143, Avg Validation Loss: 9.929392550607329\n",
            "Epoch 1945/2048, Avg Training Loss: 0.25350120773631873, Avg Validation Loss: 9.929403891368976\n",
            "Epoch 1946/2048, Avg Training Loss: 0.2535113938162427, Avg Validation Loss: 9.929416052414194\n",
            "Epoch 1947/2048, Avg Training Loss: 0.25352157264121455, Avg Validation Loss: 9.929429019509389\n",
            "Epoch 1948/2048, Avg Training Loss: 0.2535317441763385, Avg Validation Loss: 9.929442774578305\n",
            "Epoch 1949/2048, Avg Training Loss: 0.25354190837958673, Avg Validation Loss: 9.929457297686524\n",
            "Epoch 1950/2048, Avg Training Loss: 0.25355206520555246, Avg Validation Loss: 9.92947256810462\n",
            "Epoch 1951/2048, Avg Training Loss: 0.25356221460742545, Avg Validation Loss: 9.929488564875978\n",
            "Epoch 1952/2048, Avg Training Loss: 0.253572356538028, Avg Validation Loss: 9.929505267118236\n",
            "Epoch 1953/2048, Avg Training Loss: 0.2535824909503534, Avg Validation Loss: 9.929522654181643\n",
            "Epoch 1954/2048, Avg Training Loss: 0.25359261779784237, Avg Validation Loss: 9.929540705730469\n",
            "Epoch 1955/2048, Avg Training Loss: 0.2536027370345206, Avg Validation Loss: 9.929559401783106\n",
            "Epoch 1956/2048, Avg Training Loss: 0.2536128486150637, Avg Validation Loss: 9.929578722730067\n",
            "Epoch 1957/2048, Avg Training Loss: 0.253622952494824, Avg Validation Loss: 9.929598649340095\n",
            "Epoch 1958/2048, Avg Training Loss: 0.253633048629836, Avg Validation Loss: 9.929619162760028\n",
            "Epoch 1959/2048, Avg Training Loss: 0.2536431369768133, Avg Validation Loss: 9.929640244511303\n",
            "Epoch 1960/2048, Avg Training Loss: 0.25365321749313857, Avg Validation Loss: 9.929661876484762\n",
            "Epoch 1961/2048, Avg Training Loss: 0.2536632901368523, Avg Validation Loss: 9.929684040934598\n",
            "Epoch 1962/2048, Avg Training Loss: 0.2536733548666392, Avg Validation Loss: 9.929706720471897\n",
            "Epoch 1963/2048, Avg Training Loss: 0.25368341164181496, Avg Validation Loss: 9.92972989805805\n",
            "Epoch 1964/2048, Avg Training Loss: 0.2536934604223129, Avg Validation Loss: 9.929753556998126\n",
            "Epoch 1965/2048, Avg Training Loss: 0.25370350116867013, Avg Validation Loss: 9.929777680934311\n",
            "Epoch 1966/2048, Avg Training Loss: 0.2537135338420151, Avg Validation Loss: 9.92980225383942\n",
            "Epoch 1967/2048, Avg Training Loss: 0.25372355840405425, Avg Validation Loss: 9.929827260010546\n",
            "Epoch 1968/2048, Avg Training Loss: 0.25373357481705966, Avg Validation Loss: 9.929852684062784\n",
            "Epoch 1969/2048, Avg Training Loss: 0.2537435830438571, Avg Validation Loss: 9.929878510923098\n",
            "Epoch 1970/2048, Avg Training Loss: 0.2537535830478139, Avg Validation Loss: 9.929904725824313\n",
            "Epoch 1971/2048, Avg Training Loss: 0.25376357479282785, Avg Validation Loss: 9.929931314299202\n",
            "Epoch 1972/2048, Avg Training Loss: 0.25377355824331554, Avg Validation Loss: 9.92995826217471\n",
            "Epoch 1973/2048, Avg Training Loss: 0.25378353336420173, Avg Validation Loss: 9.92998555556627\n",
            "Epoch 1974/2048, Avg Training Loss: 0.25379350012090884, Avg Validation Loss: 9.93001318087228\n",
            "Epoch 1975/2048, Avg Training Loss: 0.2538034584793464, Avg Validation Loss: 9.930041124768627\n",
            "Epoch 1976/2048, Avg Training Loss: 0.25381340840590116, Avg Validation Loss: 9.930069374203354\n",
            "Epoch 1977/2048, Avg Training Loss: 0.25382334986742733, Avg Validation Loss: 9.93009791639144\n",
            "Epoch 1978/2048, Avg Training Loss: 0.25383328283123713, Avg Validation Loss: 9.93012673880966\n",
            "Epoch 1979/2048, Avg Training Loss: 0.2538432319855744, Avg Validation Loss: 9.930127441863112\n",
            "Epoch 1980/2048, Avg Training Loss: 0.2538531282977293, Avg Validation Loss: 9.930093010434707\n",
            "Epoch 1981/2048, Avg Training Loss: 0.25386308641225946, Avg Validation Loss: 9.930040794478819\n",
            "Epoch 1982/2048, Avg Training Loss: 0.2538730748872312, Avg Validation Loss: 9.929980142532699\n",
            "Epoch 1983/2048, Avg Training Loss: 0.25388307681015926, Avg Validation Loss: 9.929916079563387\n",
            "Epoch 1984/2048, Avg Training Loss: 0.25389308305247094, Avg Validation Loss: 9.92985129487264\n",
            "Epoch 1985/2048, Avg Training Loss: 0.25390308866296685, Avg Validation Loss: 9.92978721631268\n",
            "Epoch 1986/2048, Avg Training Loss: 0.25391309093484743, Avg Validation Loss: 9.929724590695166\n",
            "Epoch 1987/2048, Avg Training Loss: 0.2539230883683043, Avg Validation Loss: 9.929663797372015\n",
            "Epoch 1988/2048, Avg Training Loss: 0.2539330801133202, Avg Validation Loss: 9.92960501764872\n",
            "Epoch 1989/2048, Avg Training Loss: 0.2539430656702523, Avg Validation Loss: 9.9295483263067\n",
            "Epoch 1990/2048, Avg Training Loss: 0.25395304448818395, Avg Validation Loss: 9.929494664276678\n",
            "Epoch 1991/2048, Avg Training Loss: 0.2539630231120202, Avg Validation Loss: 9.92944424433963\n",
            "Epoch 1992/2048, Avg Training Loss: 0.2539729949226667, Avg Validation Loss: 9.929396473257395\n",
            "Epoch 1993/2048, Avg Training Loss: 0.2539829597224701, Avg Validation Loss: 9.929351009886535\n",
            "Epoch 1994/2048, Avg Training Loss: 0.25399291737421836, Avg Validation Loss: 9.929307649713527\n",
            "Epoch 1995/2048, Avg Training Loss: 0.25400286777163206, Avg Validation Loss: 9.929266262459828\n",
            "Epoch 1996/2048, Avg Training Loss: 0.2540128108246144, Avg Validation Loss: 9.929226758360747\n",
            "Epoch 1997/2048, Avg Training Loss: 0.25402274645190076, Avg Validation Loss: 9.9291890699372\n",
            "Epoch 1998/2048, Avg Training Loss: 0.2540326745774051, Avg Validation Loss: 9.92915314213856\n",
            "Epoch 1999/2048, Avg Training Loss: 0.25404259512840355, Avg Validation Loss: 9.929118927008714\n",
            "Epoch 2000/2048, Avg Training Loss: 0.25405250803462726, Avg Validation Loss: 9.92908638079624\n",
            "Epoch 2001/2048, Avg Training Loss: 0.2540624132278021, Avg Validation Loss: 9.929055462385392\n",
            "Epoch 2002/2048, Avg Training Loss: 0.25407231064140556, Avg Validation Loss: 9.929026132440907\n",
            "Epoch 2003/2048, Avg Training Loss: 0.2540822002105295, Avg Validation Loss: 9.928998352938647\n",
            "Epoch 2004/2048, Avg Training Loss: 0.25409208187179355, Avg Validation Loss: 9.92897208690492\n",
            "Epoch 2005/2048, Avg Training Loss: 0.25410195556328385, Avg Validation Loss: 9.928947298268668\n",
            "Epoch 2006/2048, Avg Training Loss: 0.2541118212245022, Avg Validation Loss: 9.928923951774813\n",
            "Epoch 2007/2048, Avg Training Loss: 0.2541216787963225, Avg Validation Loss: 9.928902012930768\n",
            "Epoch 2008/2048, Avg Training Loss: 0.25413152822095025, Avg Validation Loss: 9.928881447971062\n",
            "Epoch 2009/2048, Avg Training Loss: 0.2541413694418829, Avg Validation Loss: 9.928862223831823\n",
            "Epoch 2010/2048, Avg Training Loss: 0.25415120240387384, Avg Validation Loss: 9.928844308130762\n",
            "Epoch 2011/2048, Avg Training Loss: 0.2541610270528952, Avg Validation Loss: 9.928827669150262\n",
            "Epoch 2012/2048, Avg Training Loss: 0.25417084333610407, Avg Validation Loss: 9.928812275822258\n",
            "Epoch 2013/2048, Avg Training Loss: 0.2541806512018081, Avg Validation Loss: 9.928798097714203\n",
            "Epoch 2014/2048, Avg Training Loss: 0.2541904505994329, Avg Validation Loss: 9.928785105015782\n",
            "Epoch 2015/2048, Avg Training Loss: 0.25420024147949044, Avg Validation Loss: 9.9287732685261\n",
            "Epoch 2016/2048, Avg Training Loss: 0.2542100237935484, Avg Validation Loss: 9.928762559641262\n",
            "Epoch 2017/2048, Avg Training Loss: 0.2542197974942004, Avg Validation Loss: 9.928752950342327\n",
            "Epoch 2018/2048, Avg Training Loss: 0.25422956253503676, Avg Validation Loss: 9.928744413183477\n",
            "Epoch 2019/2048, Avg Training Loss: 0.25423931887061757, Avg Validation Loss: 9.928736921280514\n",
            "Epoch 2020/2048, Avg Training Loss: 0.25424906645644485, Avg Validation Loss: 9.928730448299557\n",
            "Epoch 2021/2048, Avg Training Loss: 0.2542588052489366, Avg Validation Loss: 9.92872496844602\n",
            "Epoch 2022/2048, Avg Training Loss: 0.2542685352054016, Avg Validation Loss: 9.928720456453805\n",
            "Epoch 2023/2048, Avg Training Loss: 0.2542782562840146, Avg Validation Loss: 9.928716887574714\n",
            "Epoch 2024/2048, Avg Training Loss: 0.2542879684437929, Avg Validation Loss: 9.92871423756809\n",
            "Epoch 2025/2048, Avg Training Loss: 0.25429767164457295, Avg Validation Loss: 9.928712482690681\n",
            "Epoch 2026/2048, Avg Training Loss: 0.2543073658469882, Avg Validation Loss: 9.928711599686695\n",
            "Epoch 2027/2048, Avg Training Loss: 0.25431705101244745, Avg Validation Loss: 9.928711565778068\n",
            "Epoch 2028/2048, Avg Training Loss: 0.2543267271031138, Avg Validation Loss: 9.92871235865494\n",
            "Epoch 2029/2048, Avg Training Loss: 0.2543363940818846, Avg Validation Loss: 9.928713956466323\n",
            "Epoch 2030/2048, Avg Training Loss: 0.25434605191237136, Avg Validation Loss: 9.928716337810966\n",
            "Epoch 2031/2048, Avg Training Loss: 0.254355700558881, Avg Validation Loss: 9.928719481728388\n",
            "Epoch 2032/2048, Avg Training Loss: 0.2543653399863973, Avg Validation Loss: 9.928723367690113\n",
            "Epoch 2033/2048, Avg Training Loss: 0.25437497016056276, Avg Validation Loss: 9.928727975591094\n",
            "Epoch 2034/2048, Avg Training Loss: 0.25438459104766165, Avg Validation Loss: 9.928733285741291\n",
            "Epoch 2035/2048, Avg Training Loss: 0.25439420261460266, Avg Validation Loss: 9.92873927885744\n",
            "Epoch 2036/2048, Avg Training Loss: 0.25440381568293513, Avg Validation Loss: 9.928734462448118\n",
            "Epoch 2037/2048, Avg Training Loss: 0.25441340766393583, Avg Validation Loss: 9.92870755802337\n",
            "Epoch 2038/2048, Avg Training Loss: 0.25442305998049963, Avg Validation Loss: 9.928669604861435\n",
            "Epoch 2039/2048, Avg Training Loss: 0.25443274005342575, Avg Validation Loss: 9.928626548516245\n",
            "Epoch 2040/2048, Avg Training Loss: 0.2544424304028302, Avg Validation Loss: 9.928581582140177\n",
            "Epoch 2041/2048, Avg Training Loss: 0.2544521216351717, Avg Validation Loss: 9.928536411915273\n",
            "Epoch 2042/2048, Avg Training Loss: 0.2544618086907011, Avg Validation Loss: 9.928491940895297\n",
            "Epoch 2043/2048, Avg Training Loss: 0.2544714888335204, Avg Validation Loss: 9.928448638530677\n",
            "Epoch 2044/2048, Avg Training Loss: 0.25448116057440917, Avg Validation Loss: 9.928406740340137\n",
            "Epoch 2045/2048, Avg Training Loss: 0.25449081407604196, Avg Validation Loss: 9.928380307605652\n",
            "Epoch 2046/2048, Avg Training Loss: 0.25450062490713155, Avg Validation Loss: 9.928383186066188\n",
            "Epoch 2047/2048, Avg Training Loss: 0.2545104802070072, Avg Validation Loss: 9.928402062494133\n",
            "Epoch 2048/2048, Avg Training Loss: 0.2545203544984955, Avg Validation Loss: 9.92842974481445\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x_tot = np.random.rand(256, 15)\n",
        "target = np.random.rand(256, 3)\n",
        "\n",
        "test_split = 0.2\n",
        "train_split = 0.75\n",
        "\n",
        "layer_one = Layer(15, 8, ELU, d_ELU)\n",
        "layer_two = Layer(8, 4, ELU, d_ELU)\n",
        "layer_out = Layer(4, 3, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "# Split data\n",
        "x_train_val, target_train_val, x_test, target_test = NN.data_split(x_tot, target, test_split)\n",
        "\n",
        "NN.add_layer(layer_one)\n",
        "NN.add_layer(layer_two)\n",
        "NN.add_layer(layer_out)\n",
        "\n",
        "# Parametri di training\n",
        "K = 5\n",
        "epochs = 2048\n",
        "learning_rate_w = 0.0001 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
        "                         # learning_rate of minibatch for number of the examples in the minibatch\n",
        "learning_rate_b = 0.0001\n",
        "batch_size = 32\n",
        "Lambda_t = 0.1\n",
        "Lambda_l = 0.1\n",
        "momentum = 0.9 # tipically from 0.8 and 0.99\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# Cross-validation\n",
        "train_error, val_error = NN.train_val(x_train_val, target_train_val, train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, mean_squared_error, d_mean_squared_error, 'elastic', 'adam')\n",
        "\n",
        "# Plot degli errori\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_error, label='Training Error')\n",
        "plt.plot(val_error, label='Validation Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Error')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
