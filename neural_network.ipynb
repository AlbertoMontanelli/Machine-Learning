{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zBZ8ZOoCBL"
      },
      "source": [
        "# Cose da fare\n",
        "* inserire documentazione per le regolarizzazioni, adam, momento e ricontrollare tutto (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
        "* inserire loss function per problemi di classificazione: BCE o altro\n",
        "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
        "* analisi training error vs validation error vs test error\n",
        "### Novelties\n",
        "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
        "* early stopping\n",
        "* learning rate variabile (?)\n",
        "* standarditation e normalization (FACOLTATIVO FORSE)\n",
        "# Cose da fare secondo le (!) Micheli\n",
        "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qv1wwlgWsFM8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n",
        "\n",
        "# np.vectorize returns an object that acts like pyfunc, but takes arrays as input\n",
        "d_ReLU = np.vectorize(d_ReLU)\n",
        "d_leaky_relu = np.vectorize(d_leaky_relu)\n",
        "ELU = np.vectorize(ELU)\n",
        "d_ELU = np.vectorize(d_ELU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return - 2 * (y_true - y_pred)\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return - (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return - (y_true - y_pred) if(np.abs(y_true-y_pred)<=delta) else - delta * np.sign(y_true-y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters:**\n",
        " * input_size: $k_{i-1}$;\n",
        " * output_size: $k_i$;\n",
        " * activation_function;\n",
        " * activation_derivative.\n",
        "\n",
        "**Constructor attributes:**\n",
        "* self.input_size = input_size;\n",
        "* self.output_size = output_size;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative;\n",
        "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created;\n",
        "* self.t: number of iterations for Adam.\n",
        "\n",
        "**Methods :**\n",
        "\n",
        "* **initialize_weights**: initialize weights and biases\n",
        "  * attributes:\n",
        "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
        "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
        "    * self.velocity_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Nesterov optimization;\n",
        "    * self.velocity_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Nesterov optimization;\n",
        "    * self.m_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.v_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.m_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.v_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    \n",
        "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
        "  * parameter:\n",
        "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
        "  * attributes:\n",
        "    * self.input: input_array;\n",
        "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
        "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
        "\n",
        "* **Regularization_func**: computes the regularization term using Tikhonov, Lasso or Elastic rule\n",
        "  * parameter:\n",
        "    * Lambda_t: constant used in tikhonov regularization;\n",
        "    * Lambda_l: constant used in Lasso regularization;\n",
        "    * ww: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. It is a placeholder for the kind of weights that are used in the computation of regularization: in case of NAG optimization, we use the predictions of the weights; in case of adam optimization, we use self.weights. \n",
        "    * reg_type: one of the possible string: \"tikhonov\", \"lasso\" and \"elastic\".\n",
        "  \n",
        "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        "  * parameters:\n",
        "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "    * learning_rate.\n",
        "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "H8Ap9rLxLlNU",
        "outputId": "da307f31-1bbc-4d71-826e-718b36ac131a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "        self.initialize_weights()\n",
        "        self.t = 1 \n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialization of the parameters of the network\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
        "        self.biases = np.zeros((1, self.output_size))\n",
        "\n",
        "        # Initialization of the parameters for Nesterov optimization\n",
        "        self.velocity_weights = np.zeros_like(self.weights)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        # Initialization of the parameters for Adam optimization\n",
        "        self.m_weights = np.zeros_like(self.weights)\n",
        "        self.v_weights = np.zeros_like(self.weights)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "    def regularization_func(self, Lambda_t, Lambda_l, ww, reg_type):\n",
        "        if reg_type == 'tikhonov':\n",
        "            reg_term = 2 * Lambda_t * ww # learning rule of tikhonov regularization\n",
        "        elif reg_type == 'lasso':\n",
        "            reg_term = Lambda_l * np.sign(ww) # learning rule of lasso regularization\n",
        "        elif reg_type == 'elastic':\n",
        "            reg_term = (2 * Lambda_t * ww + Lambda_l * np.sign(ww)) # lasso + tikhonov regularization\n",
        "        return reg_term\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        self.delta = - d_Ep * self.activation_derivative(self.net)\n",
        "        if opt_type == 'NAG':\n",
        "            weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights in order to use the Nesterov momentum,\n",
        "                                                                            # used to evaluate the gradient after the momentum is applied\n",
        "            bias_pred = self.biases + momentum * self.velocity_biases # same thing for the biases\n",
        "            net_pred = np.dot(self.input, weights_pred) + bias_pred  #  Net has been computed with respect to the predicted weights and the predicted biases\n",
        "            delta_pred = - d_Ep * self.activation_derivative(net_pred)  # Loss gradient with respect to net, minus sign due to the definition\n",
        "            grad_weights = learning_rate_w * np.dot(self.input.T, delta_pred)  # Loss gradient multiplied by the learning rate.\n",
        "                                                                            # The gradient has been computed with respect to the predicted weights and biases\n",
        "            \n",
        "            reg_term = self.regularization_func(Lambda_t, Lambda_l, weights_pred, reg_type)\n",
        "            self.velocity_weights = momentum * self.velocity_weights + grad_weights - reg_term  # Delta w new \n",
        "                                                                                                # the minus sign before reg_term is due to the application of gradient descent algorithm.\n",
        "            self.weights += self.velocity_weights  # Updating the weights\n",
        "            self.velocity_biases = momentum * self.velocity_biases + learning_rate_b * np.sum(delta_pred, axis=0, keepdims=True)\n",
        "            self.biases += self.velocity_biases # Updating the biases\n",
        "\n",
        "        elif opt_type == 'adam':\n",
        "            reg_term = self.regularization_func(Lambda_t, Lambda_l, self.weights, reg_type)\n",
        "            self.m_weights = beta_1 * self.m_weights + (1 - beta_1) * (- np.dot(self.input.T, self.delta) - reg_term) # np.dot(self.input.T, delta) is dLoss/dw,\n",
        "                                                                                                                        # since self.delta is defined with a minus sign\n",
        "                                                                                                                        # and the formula is with a plus sign, we put a minus sign\n",
        "                                                                                                                        # in front of np.dot(xxx)\n",
        "            self.v_weights = beta_2* self.v_weights + (1 - beta_2) * ((- np.dot(self.input.T, self.delta) - reg_term)**2) # here we have a plus sign in front of (1 - beta_2) since\n",
        "                                                                                                                        # self.delta is squared\n",
        "            m_weights_hat = self.m_weights / (1 - beta_1**self.t)\n",
        "            v_weights_hat = self.v_weights / (1 - beta_2**self.t)\n",
        "\n",
        "            self.m_biases = beta_1 * self.m_biases - (1 - beta_1) * np.sum(self.delta, axis=0, keepdims=True)\n",
        "            self.v_biases = beta_2* self.v_biases + (1 - beta_2) * np.sum(self.delta**2, axis=0, keepdims=True)\n",
        "            m_biases_hat = self.m_biases / (1 - beta_1**self.t)\n",
        "            v_biases_hat = self.v_biases / (1 - beta_2**self.t)\n",
        "\n",
        "            self.weights -= learning_rate_w * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "            self.biases -= learning_rate_b * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)\n",
        "\n",
        "        sum_delta_weights = np.dot(self.delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
        "   * parameters:\n",
        "     * x_tot: total data given as input;\n",
        "     * target: total data labels given as input;\n",
        "     * test_split: percentile of test set with respect to the total data.\n",
        "    * return->:\n",
        "      * x_train_val: training & validation set extracted from input data;\n",
        "      * target_train_val: training & validation set labels;\n",
        "      * x_test_val: test set extracted from input data;\n",
        "      * target_test_val: test set for input data labels.\n",
        "\n",
        "     \n",
        "* **add_layer**: appends a layer to the empty list self.layers\n",
        "   * parameter:\n",
        "     * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        "  * parameter:\n",
        "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        "  * return -> input = $o_i$ for layer $L_i$.\n",
        "  \n",
        "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        "  * parameters:\n",
        "    * d_Ep;\n",
        "    * learning_rate.\n",
        "\n",
        "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
        "\n",
        "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
        "  * parameters:\n",
        "    * x_train: set of the original dataset used for training;\n",
        "    * target_train: labels corresponding to the training set;\n",
        "    * x_val: set of the original dataset used for validation;\n",
        "    * target_val: labels corresponding to the validation set;\n",
        "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
        "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
        "    * loss_function: hyperparameter;\n",
        "    * loss_function_derivative: hyperparameter;\n",
        "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
        "\n",
        "* **train_val**: actual training and validation process.\n",
        "  * parameters:\n",
        "    * x_train_val;\n",
        "    * target_train_val;\n",
        "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
        "    * epochs;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def data_split(self, x_tot, target, test_split):\n",
        "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
        "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
        "\n",
        "        x_test = x_tot[:test_size]\n",
        "        target_test = target[:test_size]\n",
        "        x_train_val = x_tot[test_size:]\n",
        "        target_train_val = target[test_size:]\n",
        "\n",
        "        return x_train_val, target_train_val, x_test, target_test\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "\n",
        "    def reinitialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.initialize_weights() # does it layer-by-layer\n",
        "\n",
        "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        train_error_epoch = np.zeros(epochs)\n",
        "        val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
        "            epoch_val_loss = 0\n",
        "\n",
        "            '''\n",
        "            # shuffling training data before splitting it into batches.\n",
        "            # done in order to avoid reinforcing neurons in the same way\n",
        "            # in different epochs due to invisible patterns in the data\n",
        "            train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(train_indices)\n",
        "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
        "            target_train = target_train[train_indices]\n",
        "            '''\n",
        "\n",
        "            # if batch_size=1 we get the online version,\n",
        "            # else we get mini-batch version with batches of size batch_size\n",
        "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
        "                                                             # Even if the number of examples is not divisible\n",
        "                                                             # for batch_size the last, smaller batch is processed anyway\n",
        "\n",
        "                x_batch = x_train[i:i+batch_size]\n",
        "                target_batch = target_train[i:i+batch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
        "                # computing loss and gradient\n",
        "                loss = loss_function(target_batch, predictions)\n",
        "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "                epoch_tr_loss += np.sum(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(loss_gradient, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "                Layer.t += 1 # number of iterations for adam\n",
        "\n",
        "            # validation\n",
        "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
        "            val_loss = loss_function(target_val, val_predictions)\n",
        "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished.\n",
        "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
        "            # Store average errors for the epoch\n",
        "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
        "            val_error_epoch[epoch] = epoch_val_loss\n",
        "\n",
        "        return train_error_epoch, val_error_epoch\n",
        "\n",
        "\n",
        "    def train_val(self, x_train_val, target_train_val, train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        num_samples = x_train_val.shape[0]\n",
        "        fold_size = num_samples // K\n",
        "\n",
        "        # error storage for averaging\n",
        "        avg_train_error_epoch = np.zeros(epochs)\n",
        "        avg_val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        if K==1: # hold-out validation\n",
        "            train_indices = np.arange(0, int(train_split*num_samples)) # training set is 75% of the training & validation set\n",
        "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            ) # computation of errors via train_val_setup method\n",
        "            return train_error_epoch, val_error_epoch\n",
        "\n",
        "        for k in range(K):\n",
        "            # creating fold indices\n",
        "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
        "                                                                        # It contains the indices of the examples used in validation set for\n",
        "                                                                        # this fold.\n",
        "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples -\n",
        "                                                                              # len(val_indices). It contains the indices of all the examples\n",
        "                                                                              # but the ones used in the validation set for this fold.\n",
        "                                                                              # It corresponds to the training set for the current fold.\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
        "\n",
        "            # shuffle \n",
        "            new_train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(new_train_indices)\n",
        "            x_train = x_train[new_train_indices]\n",
        "            target_train = target_train[new_train_indices]\n",
        "\n",
        "            '''\n",
        "            new_val_indices = np.arange(x_val.shape[0])\n",
        "            np.random.shuffle(new_val_indices)\n",
        "            x_val = x_val[new_val_indices]\n",
        "            target_val = target_val[new_val_indices]\n",
        "            print(f\"x val \\n {x_val}\")\n",
        "            '''\n",
        "            \n",
        "            # re-initializing weights for each fold\n",
        "            self.reinitialize_weights()\n",
        "            Layer.t = 1\n",
        "            # training on the current fold\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            )\n",
        "\n",
        "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
        "            avg_train_error_epoch += train_error_epoch\n",
        "            avg_val_error_epoch += val_error_epoch\n",
        "            print(f\"Fold {k+1} completed, t = {Layer.t}\")\n",
        "\n",
        "        # averaging errors across all folds\n",
        "        avg_train_error_epoch /= K\n",
        "        avg_val_error_epoch /= K\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
        "\n",
        "        return avg_train_error_epoch, avg_val_error_epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "# Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "5be15b38-62a4-439a-dd86-57790daaffe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 completed, t = 12289\n",
            "Fold 2 completed, t = 12289\n",
            "Fold 3 completed, t = 12289\n",
            "Fold 4 completed, t = 12289\n",
            "Fold 5 completed, t = 12289\n",
            "Epoch 1/2048, Avg Training Loss: 0.9198747713316746, Avg Validation Loss: 38.76178825383559\n",
            "Epoch 2/2048, Avg Training Loss: 0.9054487886202647, Avg Validation Loss: 38.12900004402255\n",
            "Epoch 3/2048, Avg Training Loss: 0.8896618700146333, Avg Validation Loss: 37.51085225791679\n",
            "Epoch 4/2048, Avg Training Loss: 0.8745971790134119, Avg Validation Loss: 36.93250877485148\n",
            "Epoch 5/2048, Avg Training Loss: 0.8605646128544072, Avg Validation Loss: 36.39526125879878\n",
            "Epoch 6/2048, Avg Training Loss: 0.8475714042273071, Avg Validation Loss: 35.89812016594529\n",
            "Epoch 7/2048, Avg Training Loss: 0.8355280961248622, Avg Validation Loss: 35.43491641861492\n",
            "Epoch 8/2048, Avg Training Loss: 0.8242905349858564, Avg Validation Loss: 35.000975932851496\n",
            "Epoch 9/2048, Avg Training Loss: 0.8137703542667175, Avg Validation Loss: 34.59492915836741\n",
            "Epoch 10/2048, Avg Training Loss: 0.8038941560138233, Avg Validation Loss: 34.21086473221339\n",
            "Epoch 11/2048, Avg Training Loss: 0.7945059153144288, Avg Validation Loss: 33.84687132955501\n",
            "Epoch 12/2048, Avg Training Loss: 0.7855859521371686, Avg Validation Loss: 33.50209065672921\n",
            "Epoch 13/2048, Avg Training Loss: 0.7771212785446561, Avg Validation Loss: 33.176334235433345\n",
            "Epoch 14/2048, Avg Training Loss: 0.769089401045602, Avg Validation Loss: 32.86777922532362\n",
            "Epoch 15/2048, Avg Training Loss: 0.7614641040746366, Avg Validation Loss: 32.57568681221244\n",
            "Epoch 16/2048, Avg Training Loss: 0.7542272198720922, Avg Validation Loss: 32.29853480282978\n",
            "Epoch 17/2048, Avg Training Loss: 0.7473229350728123, Avg Validation Loss: 32.033761962837296\n",
            "Epoch 18/2048, Avg Training Loss: 0.7407103715835821, Avg Validation Loss: 31.78091103890519\n",
            "Epoch 19/2048, Avg Training Loss: 0.7343777558758967, Avg Validation Loss: 31.538903709957303\n",
            "Epoch 20/2048, Avg Training Loss: 0.7283003004866295, Avg Validation Loss: 31.30695699401608\n",
            "Epoch 21/2048, Avg Training Loss: 0.7224672706578678, Avg Validation Loss: 31.08455877559032\n",
            "Epoch 22/2048, Avg Training Loss: 0.7168604507272657, Avg Validation Loss: 30.871322804401522\n",
            "Epoch 23/2048, Avg Training Loss: 0.7114716386758773, Avg Validation Loss: 30.666552800565803\n",
            "Epoch 24/2048, Avg Training Loss: 0.7062820032972256, Avg Validation Loss: 30.4695273323747\n",
            "Epoch 25/2048, Avg Training Loss: 0.7012724320426285, Avg Validation Loss: 30.27917079861835\n",
            "Epoch 26/2048, Avg Training Loss: 0.6964191658687934, Avg Validation Loss: 30.09527672159967\n",
            "Epoch 27/2048, Avg Training Loss: 0.6917251352698814, Avg Validation Loss: 29.917887520808886\n",
            "Epoch 28/2048, Avg Training Loss: 0.687186249303929, Avg Validation Loss: 29.746373769545652\n",
            "Epoch 29/2048, Avg Training Loss: 0.6827936491259075, Avg Validation Loss: 29.5805505285139\n",
            "Epoch 30/2048, Avg Training Loss: 0.6785441876457359, Avg Validation Loss: 29.41998419416804\n",
            "Epoch 31/2048, Avg Training Loss: 0.6744235061124334, Avg Validation Loss: 29.264521750495177\n",
            "Epoch 32/2048, Avg Training Loss: 0.6704314393005293, Avg Validation Loss: 29.114175975388704\n",
            "Epoch 33/2048, Avg Training Loss: 0.6665658960630859, Avg Validation Loss: 28.96868904082042\n",
            "Epoch 34/2048, Avg Training Loss: 0.6628186710292416, Avg Validation Loss: 28.827675846848695\n",
            "Epoch 35/2048, Avg Training Loss: 0.6591815771334444, Avg Validation Loss: 28.690822569129015\n",
            "Epoch 36/2048, Avg Training Loss: 0.6556438966308304, Avg Validation Loss: 28.557506624955433\n",
            "Epoch 37/2048, Avg Training Loss: 0.6521963337400383, Avg Validation Loss: 28.427676134419407\n",
            "Epoch 38/2048, Avg Training Loss: 0.6488383510139505, Avg Validation Loss: 28.3014419171729\n",
            "Epoch 39/2048, Avg Training Loss: 0.645571565468653, Avg Validation Loss: 28.17880345804694\n",
            "Epoch 40/2048, Avg Training Loss: 0.6423936249776837, Avg Validation Loss: 28.05964946974745\n",
            "Epoch 41/2048, Avg Training Loss: 0.639303792100432, Avg Validation Loss: 27.94414939639915\n",
            "Epoch 42/2048, Avg Training Loss: 0.6363075321995988, Avg Validation Loss: 27.833399349832877\n",
            "Epoch 43/2048, Avg Training Loss: 0.6334250958416036, Avg Validation Loss: 27.726688875788852\n",
            "Epoch 44/2048, Avg Training Loss: 0.6306380943866479, Avg Validation Loss: 27.623343524605716\n",
            "Epoch 45/2048, Avg Training Loss: 0.627934503322041, Avg Validation Loss: 27.523185596729263\n",
            "Epoch 46/2048, Avg Training Loss: 0.6253079658524227, Avg Validation Loss: 27.425969356731557\n",
            "Epoch 47/2048, Avg Training Loss: 0.6227486085215279, Avg Validation Loss: 27.3313056298972\n",
            "Epoch 48/2048, Avg Training Loss: 0.6202527909715421, Avg Validation Loss: 27.239084589615032\n",
            "Epoch 49/2048, Avg Training Loss: 0.6178158113012832, Avg Validation Loss: 27.149155221120477\n",
            "Epoch 50/2048, Avg Training Loss: 0.6154349816577505, Avg Validation Loss: 27.06130283304675\n",
            "Epoch 51/2048, Avg Training Loss: 0.6131060549269142, Avg Validation Loss: 26.97553015035326\n",
            "Epoch 52/2048, Avg Training Loss: 0.6108299330342256, Avg Validation Loss: 26.8917727461199\n",
            "Epoch 53/2048, Avg Training Loss: 0.6085998826130213, Avg Validation Loss: 26.809690455552584\n",
            "Epoch 54/2048, Avg Training Loss: 0.6064124869887955, Avg Validation Loss: 26.729328399263146\n",
            "Epoch 55/2048, Avg Training Loss: 0.6042689278548329, Avg Validation Loss: 26.650594673949293\n",
            "Epoch 56/2048, Avg Training Loss: 0.6021680354031478, Avg Validation Loss: 26.573503926197997\n",
            "Epoch 57/2048, Avg Training Loss: 0.6001095976532768, Avg Validation Loss: 26.49804481816729\n",
            "Epoch 58/2048, Avg Training Loss: 0.5980941603268024, Avg Validation Loss: 26.424194843392787\n",
            "Epoch 59/2048, Avg Training Loss: 0.5961213760261385, Avg Validation Loss: 26.351898878236803\n",
            "Epoch 60/2048, Avg Training Loss: 0.5941899876396477, Avg Validation Loss: 26.28114517677708\n",
            "Epoch 61/2048, Avg Training Loss: 0.5922981400395521, Avg Validation Loss: 26.21183371783732\n",
            "Epoch 62/2048, Avg Training Loss: 0.5904440840007278, Avg Validation Loss: 26.143965421439\n",
            "Epoch 63/2048, Avg Training Loss: 0.5886281742364515, Avg Validation Loss: 26.077506302616044\n",
            "Epoch 64/2048, Avg Training Loss: 0.5868483533963964, Avg Validation Loss: 26.012402334366715\n",
            "Epoch 65/2048, Avg Training Loss: 0.5851039809290776, Avg Validation Loss: 25.94862809985578\n",
            "Epoch 66/2048, Avg Training Loss: 0.5833940846250996, Avg Validation Loss: 25.886111130372793\n",
            "Epoch 67/2048, Avg Training Loss: 0.5817174961160911, Avg Validation Loss: 25.8247816454098\n",
            "Epoch 68/2048, Avg Training Loss: 0.5800709150263706, Avg Validation Loss: 25.764271889122252\n",
            "Epoch 69/2048, Avg Training Loss: 0.5784503602339262, Avg Validation Loss: 25.70460481850428\n",
            "Epoch 70/2048, Avg Training Loss: 0.576855399653752, Avg Validation Loss: 25.645798567643933\n",
            "Epoch 71/2048, Avg Training Loss: 0.5752858242052931, Avg Validation Loss: 25.587880290956186\n",
            "Epoch 72/2048, Avg Training Loss: 0.573740965598048, Avg Validation Loss: 25.53091852792272\n",
            "Epoch 73/2048, Avg Training Loss: 0.5722231896899321, Avg Validation Loss: 25.47501512952987\n",
            "Epoch 74/2048, Avg Training Loss: 0.5707331318305217, Avg Validation Loss: 25.42008126852241\n",
            "Epoch 75/2048, Avg Training Loss: 0.5692688086685751, Avg Validation Loss: 25.366025974871683\n",
            "Epoch 76/2048, Avg Training Loss: 0.5678265583847351, Avg Validation Loss: 25.31263166313638\n",
            "Epoch 77/2048, Avg Training Loss: 0.5664037103398041, Avg Validation Loss: 25.259962557152896\n",
            "Epoch 78/2048, Avg Training Loss: 0.5650014824451665, Avg Validation Loss: 25.20804850170489\n",
            "Epoch 79/2048, Avg Training Loss: 0.5636205928130399, Avg Validation Loss: 25.156986511745433\n",
            "Epoch 80/2048, Avg Training Loss: 0.562260251110316, Avg Validation Loss: 25.106431025439623\n",
            "Epoch 81/2048, Avg Training Loss: 0.5609168321713266, Avg Validation Loss: 25.056388627203674\n",
            "Epoch 82/2048, Avg Training Loss: 0.5595907801219827, Avg Validation Loss: 25.007044868381616\n",
            "Epoch 83/2048, Avg Training Loss: 0.5582851916389808, Avg Validation Loss: 24.958447170255802\n",
            "Epoch 84/2048, Avg Training Loss: 0.556999945297055, Avg Validation Loss: 24.91056181067739\n",
            "Epoch 85/2048, Avg Training Loss: 0.5557338996193687, Avg Validation Loss: 24.863342272298635\n",
            "Epoch 86/2048, Avg Training Loss: 0.5544860880898776, Avg Validation Loss: 24.816756691847768\n",
            "Epoch 87/2048, Avg Training Loss: 0.5532559571236577, Avg Validation Loss: 24.770824023188524\n",
            "Epoch 88/2048, Avg Training Loss: 0.5520441843100918, Avg Validation Loss: 24.725530786554984\n",
            "Epoch 89/2048, Avg Training Loss: 0.5508497697357491, Avg Validation Loss: 24.68083487142149\n",
            "Epoch 90/2048, Avg Training Loss: 0.5496721339720425, Avg Validation Loss: 24.636751169469797\n",
            "Epoch 91/2048, Avg Training Loss: 0.5485109637278707, Avg Validation Loss: 24.59323574112044\n",
            "Epoch 92/2048, Avg Training Loss: 0.5473653846394344, Avg Validation Loss: 24.55025950325762\n",
            "Epoch 93/2048, Avg Training Loss: 0.5462341800931653, Avg Validation Loss: 24.507421578226484\n",
            "Epoch 94/2048, Avg Training Loss: 0.5451069617015963, Avg Validation Loss: 24.464521806050616\n",
            "Epoch 95/2048, Avg Training Loss: 0.5439856553386405, Avg Validation Loss: 24.421811819049687\n",
            "Epoch 96/2048, Avg Training Loss: 0.5428737489072107, Avg Validation Loss: 24.37926727500125\n",
            "Epoch 97/2048, Avg Training Loss: 0.5417680578287444, Avg Validation Loss: 24.33682710776011\n",
            "Epoch 98/2048, Avg Training Loss: 0.5406702689325644, Avg Validation Loss: 24.29452554786643\n",
            "Epoch 99/2048, Avg Training Loss: 0.5395803683018998, Avg Validation Loss: 24.252534104856444\n",
            "Epoch 100/2048, Avg Training Loss: 0.5384990608593928, Avg Validation Loss: 24.210537341000062\n",
            "Epoch 101/2048, Avg Training Loss: 0.537420064218375, Avg Validation Loss: 24.168182459971984\n",
            "Epoch 102/2048, Avg Training Loss: 0.5363409835488621, Avg Validation Loss: 24.125768077089297\n",
            "Epoch 103/2048, Avg Training Loss: 0.5352669228217545, Avg Validation Loss: 24.083519377348953\n",
            "Epoch 104/2048, Avg Training Loss: 0.5342008708755934, Avg Validation Loss: 24.041549137954597\n",
            "Epoch 105/2048, Avg Training Loss: 0.5331471074457367, Avg Validation Loss: 24.00028335285475\n",
            "Epoch 106/2048, Avg Training Loss: 0.53210824862984, Avg Validation Loss: 23.959476113340763\n",
            "Epoch 107/2048, Avg Training Loss: 0.531080749621067, Avg Validation Loss: 23.91890824903318\n",
            "Epoch 108/2048, Avg Training Loss: 0.5300609011252424, Avg Validation Loss: 23.8784957008845\n",
            "Epoch 109/2048, Avg Training Loss: 0.5290463488703735, Avg Validation Loss: 23.838152882786407\n",
            "Epoch 110/2048, Avg Training Loss: 0.5280380782246309, Avg Validation Loss: 23.798017795546002\n",
            "Epoch 111/2048, Avg Training Loss: 0.5270372917468867, Avg Validation Loss: 23.757979950027526\n",
            "Epoch 112/2048, Avg Training Loss: 0.5260416932836748, Avg Validation Loss: 23.718139976134424\n",
            "Epoch 113/2048, Avg Training Loss: 0.5250526296193355, Avg Validation Loss: 23.678345718941305\n",
            "Epoch 114/2048, Avg Training Loss: 0.5240691870017983, Avg Validation Loss: 23.638740000299112\n",
            "Epoch 115/2048, Avg Training Loss: 0.5230931186904767, Avg Validation Loss: 23.599395702598414\n",
            "Epoch 116/2048, Avg Training Loss: 0.5221247596889667, Avg Validation Loss: 23.56019868515567\n",
            "Epoch 117/2048, Avg Training Loss: 0.5211618256077291, Avg Validation Loss: 23.521184158604335\n",
            "Epoch 118/2048, Avg Training Loss: 0.5202057790003409, Avg Validation Loss: 23.48242315341043\n",
            "Epoch 119/2048, Avg Training Loss: 0.5192577635104726, Avg Validation Loss: 23.443984296980734\n",
            "Epoch 120/2048, Avg Training Loss: 0.5183185963983907, Avg Validation Loss: 23.405880401369437\n",
            "Epoch 121/2048, Avg Training Loss: 0.5173882092968993, Avg Validation Loss: 23.367954764940276\n",
            "Epoch 122/2048, Avg Training Loss: 0.5164626368010653, Avg Validation Loss: 23.330158634574545\n",
            "Epoch 123/2048, Avg Training Loss: 0.5155429049813371, Avg Validation Loss: 23.29257746679486\n",
            "Epoch 124/2048, Avg Training Loss: 0.5146301402792129, Avg Validation Loss: 23.255254349188995\n",
            "Epoch 125/2048, Avg Training Loss: 0.5137248730136937, Avg Validation Loss: 23.21820944658367\n",
            "Epoch 126/2048, Avg Training Loss: 0.5128273214776835, Avg Validation Loss: 23.18145094137643\n",
            "Epoch 127/2048, Avg Training Loss: 0.5119375783630662, Avg Validation Loss: 23.145003602912645\n",
            "Epoch 128/2048, Avg Training Loss: 0.5110580903363023, Avg Validation Loss: 23.109170936113774\n",
            "Epoch 129/2048, Avg Training Loss: 0.5101906868949092, Avg Validation Loss: 23.073807608803133\n",
            "Epoch 130/2048, Avg Training Loss: 0.5093332345974457, Avg Validation Loss: 23.038516445086373\n",
            "Epoch 131/2048, Avg Training Loss: 0.5084768397488776, Avg Validation Loss: 23.00315724738827\n",
            "Epoch 132/2048, Avg Training Loss: 0.5076228599613414, Avg Validation Loss: 22.967804335854236\n",
            "Epoch 133/2048, Avg Training Loss: 0.5067715493733144, Avg Validation Loss: 22.932286313130113\n",
            "Epoch 134/2048, Avg Training Loss: 0.5059184779557998, Avg Validation Loss: 22.89666442125648\n",
            "Epoch 135/2048, Avg Training Loss: 0.5050679023914788, Avg Validation Loss: 22.861155582938466\n",
            "Epoch 136/2048, Avg Training Loss: 0.5042225073463539, Avg Validation Loss: 22.826014538988122\n",
            "Epoch 137/2048, Avg Training Loss: 0.5033858581661467, Avg Validation Loss: 22.79111100087388\n",
            "Epoch 138/2048, Avg Training Loss: 0.5025561715716999, Avg Validation Loss: 22.756494885567037\n",
            "Epoch 139/2048, Avg Training Loss: 0.5017332282482747, Avg Validation Loss: 22.722114708237015\n",
            "Epoch 140/2048, Avg Training Loss: 0.5009163876487139, Avg Validation Loss: 22.687779557385635\n",
            "Epoch 141/2048, Avg Training Loss: 0.5000954882243156, Avg Validation Loss: 22.652709159577302\n",
            "Epoch 142/2048, Avg Training Loss: 0.4992672902551513, Avg Validation Loss: 22.61739598229819\n",
            "Epoch 143/2048, Avg Training Loss: 0.4984387620625433, Avg Validation Loss: 22.58209841929068\n",
            "Epoch 144/2048, Avg Training Loss: 0.49761344657712125, Avg Validation Loss: 22.546878643295685\n",
            "Epoch 145/2048, Avg Training Loss: 0.4967915882815361, Avg Validation Loss: 22.511788706079535\n",
            "Epoch 146/2048, Avg Training Loss: 0.49597470585298126, Avg Validation Loss: 22.47690714158337\n",
            "Epoch 147/2048, Avg Training Loss: 0.4951641829868928, Avg Validation Loss: 22.44232085972124\n",
            "Epoch 148/2048, Avg Training Loss: 0.4943608833580263, Avg Validation Loss: 22.408024694107198\n",
            "Epoch 149/2048, Avg Training Loss: 0.4935646331650319, Avg Validation Loss: 22.374013405349224\n",
            "Epoch 150/2048, Avg Training Loss: 0.49277528718935976, Avg Validation Loss: 22.340281841441783\n",
            "Epoch 151/2048, Avg Training Loss: 0.4919927174737169, Avg Validation Loss: 22.30682497341459\n",
            "Epoch 152/2048, Avg Training Loss: 0.49121670492302816, Avg Validation Loss: 22.273624703223884\n",
            "Epoch 153/2048, Avg Training Loss: 0.4904470359927576, Avg Validation Loss: 22.240682683097475\n",
            "Epoch 154/2048, Avg Training Loss: 0.4896837111785094, Avg Validation Loss: 22.20799784079109\n",
            "Epoch 155/2048, Avg Training Loss: 0.4889266866653752, Avg Validation Loss: 22.175567677331077\n",
            "Epoch 156/2048, Avg Training Loss: 0.48817593388730085, Avg Validation Loss: 22.143143697389345\n",
            "Epoch 157/2048, Avg Training Loss: 0.4874236047538971, Avg Validation Loss: 22.1104867656076\n",
            "Epoch 158/2048, Avg Training Loss: 0.4866708719073693, Avg Validation Loss: 22.077832924208515\n",
            "Epoch 159/2048, Avg Training Loss: 0.4859210136828909, Avg Validation Loss: 22.045372303154327\n",
            "Epoch 160/2048, Avg Training Loss: 0.485175952848641, Avg Validation Loss: 22.012864539761466\n",
            "Epoch 161/2048, Avg Training Loss: 0.48443089698577274, Avg Validation Loss: 21.980348846516268\n",
            "Epoch 162/2048, Avg Training Loss: 0.4836879818754387, Avg Validation Loss: 21.947570340529502\n",
            "Epoch 163/2048, Avg Training Loss: 0.48294006965289504, Avg Validation Loss: 21.9145048245597\n",
            "Epoch 164/2048, Avg Training Loss: 0.4821912760462919, Avg Validation Loss: 21.881428861939956\n",
            "Epoch 165/2048, Avg Training Loss: 0.48144521747368074, Avg Validation Loss: 21.84848677756939\n",
            "Epoch 166/2048, Avg Training Loss: 0.48070387355534283, Avg Validation Loss: 21.815448985210892\n",
            "Epoch 167/2048, Avg Training Loss: 0.4799586017539659, Avg Validation Loss: 21.782081650248884\n",
            "Epoch 168/2048, Avg Training Loss: 0.47921177313647784, Avg Validation Loss: 21.74870730926034\n",
            "Epoch 169/2048, Avg Training Loss: 0.4784674466421363, Avg Validation Loss: 21.715477402177257\n",
            "Epoch 170/2048, Avg Training Loss: 0.4777254652042268, Avg Validation Loss: 21.681874205611575\n",
            "Epoch 171/2048, Avg Training Loss: 0.47697657185713604, Avg Validation Loss: 21.647923474831206\n",
            "Epoch 172/2048, Avg Training Loss: 0.476225734663166, Avg Validation Loss: 21.61397481900707\n",
            "Epoch 173/2048, Avg Training Loss: 0.47547902208127846, Avg Validation Loss: 21.580384888543648\n",
            "Epoch 174/2048, Avg Training Loss: 0.47473709027846694, Avg Validation Loss: 21.54675823332841\n",
            "Epoch 175/2048, Avg Training Loss: 0.47399693511349694, Avg Validation Loss: 21.51315754650303\n",
            "Epoch 176/2048, Avg Training Loss: 0.47325755418732923, Avg Validation Loss: 21.479446247648998\n",
            "Epoch 177/2048, Avg Training Loss: 0.4725196657595778, Avg Validation Loss: 21.445706320118056\n",
            "Epoch 178/2048, Avg Training Loss: 0.4717819098979327, Avg Validation Loss: 21.411948137629302\n",
            "Epoch 179/2048, Avg Training Loss: 0.471047037724155, Avg Validation Loss: 21.37836504342459\n",
            "Epoch 180/2048, Avg Training Loss: 0.470317347223068, Avg Validation Loss: 21.345024936388334\n",
            "Epoch 181/2048, Avg Training Loss: 0.469593618445324, Avg Validation Loss: 21.311966498755837\n",
            "Epoch 182/2048, Avg Training Loss: 0.4688761645042006, Avg Validation Loss: 21.279179755316346\n",
            "Epoch 183/2048, Avg Training Loss: 0.46816495630445276, Avg Validation Loss: 21.246683507820112\n",
            "Epoch 184/2048, Avg Training Loss: 0.46745776981670367, Avg Validation Loss: 21.214063063500877\n",
            "Epoch 185/2048, Avg Training Loss: 0.46674937754348383, Avg Validation Loss: 21.1811310906159\n",
            "Epoch 186/2048, Avg Training Loss: 0.4660377689426821, Avg Validation Loss: 21.148127713223808\n",
            "Epoch 187/2048, Avg Training Loss: 0.46532812317588634, Avg Validation Loss: 21.115279633023523\n",
            "Epoch 188/2048, Avg Training Loss: 0.464623047512679, Avg Validation Loss: 21.082469555153473\n",
            "Epoch 189/2048, Avg Training Loss: 0.46391855981643787, Avg Validation Loss: 21.049653871429943\n",
            "Epoch 190/2048, Avg Training Loss: 0.46321731181701103, Avg Validation Loss: 21.01712389878866\n",
            "Epoch 191/2048, Avg Training Loss: 0.46252285476487753, Avg Validation Loss: 20.984928591965435\n",
            "Epoch 192/2048, Avg Training Loss: 0.46183535245810714, Avg Validation Loss: 20.95299491438024\n",
            "Epoch 193/2048, Avg Training Loss: 0.46115185536258324, Avg Validation Loss: 20.921097896347053\n",
            "Epoch 194/2048, Avg Training Loss: 0.4604704411562538, Avg Validation Loss: 20.889279847657004\n",
            "Epoch 195/2048, Avg Training Loss: 0.45979267031977233, Avg Validation Loss: 20.857667136575575\n",
            "Epoch 196/2048, Avg Training Loss: 0.45912016487914203, Avg Validation Loss: 20.826322721171874\n",
            "Epoch 197/2048, Avg Training Loss: 0.45845339078676345, Avg Validation Loss: 20.79512040708216\n",
            "Epoch 198/2048, Avg Training Loss: 0.45779165039206526, Avg Validation Loss: 20.7643475921881\n",
            "Epoch 199/2048, Avg Training Loss: 0.457137942681848, Avg Validation Loss: 20.73395246227844\n",
            "Epoch 200/2048, Avg Training Loss: 0.45649199576886473, Avg Validation Loss: 20.7039160948909\n",
            "Epoch 201/2048, Avg Training Loss: 0.45585378606885674, Avg Validation Loss: 20.674215533684084\n",
            "Epoch 202/2048, Avg Training Loss: 0.45522263082595926, Avg Validation Loss: 20.64483338708056\n",
            "Epoch 203/2048, Avg Training Loss: 0.4545980769496199, Avg Validation Loss: 20.61575746360956\n",
            "Epoch 204/2048, Avg Training Loss: 0.4539794852331178, Avg Validation Loss: 20.586908974147565\n",
            "Epoch 205/2048, Avg Training Loss: 0.45336543029402915, Avg Validation Loss: 20.558145208444763\n",
            "Epoch 206/2048, Avg Training Loss: 0.4527533689474237, Avg Validation Loss: 20.529456777051042\n",
            "Epoch 207/2048, Avg Training Loss: 0.4521436031680546, Avg Validation Loss: 20.500807206600562\n",
            "Epoch 208/2048, Avg Training Loss: 0.4515359892241022, Avg Validation Loss: 20.472170195677386\n",
            "Epoch 209/2048, Avg Training Loss: 0.45092886182145947, Avg Validation Loss: 20.443474320467597\n",
            "Epoch 210/2048, Avg Training Loss: 0.45032305754502, Avg Validation Loss: 20.41489096932906\n",
            "Epoch 211/2048, Avg Training Loss: 0.4497208967848308, Avg Validation Loss: 20.386507387402773\n",
            "Epoch 212/2048, Avg Training Loss: 0.4491235135353522, Avg Validation Loss: 20.358363900078956\n",
            "Epoch 213/2048, Avg Training Loss: 0.4485313249834852, Avg Validation Loss: 20.330479254767287\n",
            "Epoch 214/2048, Avg Training Loss: 0.44794416990332325, Avg Validation Loss: 20.30272217066756\n",
            "Epoch 215/2048, Avg Training Loss: 0.4473590879603647, Avg Validation Loss: 20.275150610067314\n",
            "Epoch 216/2048, Avg Training Loss: 0.44677774289438565, Avg Validation Loss: 20.24759256756443\n",
            "Epoch 217/2048, Avg Training Loss: 0.446196021192707, Avg Validation Loss: 20.219925859518668\n",
            "Epoch 218/2048, Avg Training Loss: 0.44561276734422867, Avg Validation Loss: 20.191970756066862\n",
            "Epoch 219/2048, Avg Training Loss: 0.4450260803134758, Avg Validation Loss: 20.16375036530117\n",
            "Epoch 220/2048, Avg Training Loss: 0.4444365711573786, Avg Validation Loss: 20.13543252364568\n",
            "Epoch 221/2048, Avg Training Loss: 0.44384803766393044, Avg Validation Loss: 20.107231479972313\n",
            "Epoch 222/2048, Avg Training Loss: 0.4432620659330057, Avg Validation Loss: 20.07906163513568\n",
            "Epoch 223/2048, Avg Training Loss: 0.44267861419288884, Avg Validation Loss: 20.050962842431716\n",
            "Epoch 224/2048, Avg Training Loss: 0.44209723271440965, Avg Validation Loss: 20.022983640793793\n",
            "Epoch 225/2048, Avg Training Loss: 0.4415194663609557, Avg Validation Loss: 19.99521080488534\n",
            "Epoch 226/2048, Avg Training Loss: 0.44094641208203444, Avg Validation Loss: 19.967691258909493\n",
            "Epoch 227/2048, Avg Training Loss: 0.4403785983628408, Avg Validation Loss: 19.940444897252824\n",
            "Epoch 228/2048, Avg Training Loss: 0.43981633201201475, Avg Validation Loss: 19.913477803399996\n",
            "Epoch 229/2048, Avg Training Loss: 0.43925805901195353, Avg Validation Loss: 19.886513658434843\n",
            "Epoch 230/2048, Avg Training Loss: 0.4387015254531877, Avg Validation Loss: 19.859653669979338\n",
            "Epoch 231/2048, Avg Training Loss: 0.4381474384321778, Avg Validation Loss: 19.83279679689929\n",
            "Epoch 232/2048, Avg Training Loss: 0.4375948950613274, Avg Validation Loss: 19.806053714387957\n",
            "Epoch 233/2048, Avg Training Loss: 0.4370457177578108, Avg Validation Loss: 19.779508427418115\n",
            "Epoch 234/2048, Avg Training Loss: 0.43650034661080905, Avg Validation Loss: 19.753116932304025\n",
            "Epoch 235/2048, Avg Training Loss: 0.43595882941963626, Avg Validation Loss: 19.726940027193084\n",
            "Epoch 236/2048, Avg Training Loss: 0.4354218229558383, Avg Validation Loss: 19.70100693317045\n",
            "Epoch 237/2048, Avg Training Loss: 0.4348896177281542, Avg Validation Loss: 19.67533023121961\n",
            "Epoch 238/2048, Avg Training Loss: 0.4343623226777273, Avg Validation Loss: 19.64986934531728\n",
            "Epoch 239/2048, Avg Training Loss: 0.43383866142793054, Avg Validation Loss: 19.62458129394343\n",
            "Epoch 240/2048, Avg Training Loss: 0.4333188175364372, Avg Validation Loss: 19.5995055195519\n",
            "Epoch 241/2048, Avg Training Loss: 0.432803101643995, Avg Validation Loss: 19.57462522055747\n",
            "Epoch 242/2048, Avg Training Loss: 0.43229099629422607, Avg Validation Loss: 19.549837631018516\n",
            "Epoch 243/2048, Avg Training Loss: 0.43178091618262576, Avg Validation Loss: 19.525172599198534\n",
            "Epoch 244/2048, Avg Training Loss: 0.4312739437545221, Avg Validation Loss: 19.500688048712387\n",
            "Epoch 245/2048, Avg Training Loss: 0.4307705924850161, Avg Validation Loss: 19.476411988136586\n",
            "Epoch 246/2048, Avg Training Loss: 0.4302709767890617, Avg Validation Loss: 19.45212695092385\n",
            "Epoch 247/2048, Avg Training Loss: 0.4297720828788542, Avg Validation Loss: 19.42803420086013\n",
            "Epoch 248/2048, Avg Training Loss: 0.4292757073415299, Avg Validation Loss: 19.403713538708868\n",
            "Epoch 249/2048, Avg Training Loss: 0.4287756442140709, Avg Validation Loss: 19.379218959065735\n",
            "Epoch 250/2048, Avg Training Loss: 0.4282752467481189, Avg Validation Loss: 19.354780737992076\n",
            "Epoch 251/2048, Avg Training Loss: 0.42777701457411615, Avg Validation Loss: 19.33050126307058\n",
            "Epoch 252/2048, Avg Training Loss: 0.4272820707111541, Avg Validation Loss: 19.306422416847568\n",
            "Epoch 253/2048, Avg Training Loss: 0.42679098534580706, Avg Validation Loss: 19.28256353654801\n",
            "Epoch 254/2048, Avg Training Loss: 0.4263040108274078, Avg Validation Loss: 19.25893212430935\n",
            "Epoch 255/2048, Avg Training Loss: 0.42581946714511, Avg Validation Loss: 19.23521462882726\n",
            "Epoch 256/2048, Avg Training Loss: 0.42533480853376515, Avg Validation Loss: 19.211410946996782\n",
            "Epoch 257/2048, Avg Training Loss: 0.4248492111704834, Avg Validation Loss: 19.18758173938426\n",
            "Epoch 258/2048, Avg Training Loss: 0.4243648384691494, Avg Validation Loss: 19.163811791104685\n",
            "Epoch 259/2048, Avg Training Loss: 0.4238820125878218, Avg Validation Loss: 19.140127887025496\n",
            "Epoch 260/2048, Avg Training Loss: 0.42340169138928796, Avg Validation Loss: 19.116608856400244\n",
            "Epoch 261/2048, Avg Training Loss: 0.42292473272696923, Avg Validation Loss: 19.09329083627636\n",
            "Epoch 262/2048, Avg Training Loss: 0.42245071877026746, Avg Validation Loss: 19.0698470505858\n",
            "Epoch 263/2048, Avg Training Loss: 0.4219743821544245, Avg Validation Loss: 19.046284062215285\n",
            "Epoch 264/2048, Avg Training Loss: 0.42149823548134546, Avg Validation Loss: 19.02277822751264\n",
            "Epoch 265/2048, Avg Training Loss: 0.4210242988336316, Avg Validation Loss: 18.999420040733856\n",
            "Epoch 266/2048, Avg Training Loss: 0.42055318347663195, Avg Validation Loss: 18.976130561190075\n",
            "Epoch 267/2048, Avg Training Loss: 0.4200837444006767, Avg Validation Loss: 18.95294998453766\n",
            "Epoch 268/2048, Avg Training Loss: 0.4196170678284601, Avg Validation Loss: 18.929940629750824\n",
            "Epoch 269/2048, Avg Training Loss: 0.41915383357129404, Avg Validation Loss: 18.907132867041522\n",
            "Epoch 270/2048, Avg Training Loss: 0.4186944680441564, Avg Validation Loss: 18.884543723133387\n",
            "Epoch 271/2048, Avg Training Loss: 0.41823915901006914, Avg Validation Loss: 18.86217429212654\n",
            "Epoch 272/2048, Avg Training Loss: 0.4177876003542466, Avg Validation Loss: 18.839951928801668\n",
            "Epoch 273/2048, Avg Training Loss: 0.41733868706562594, Avg Validation Loss: 18.817883538852243\n",
            "Epoch 274/2048, Avg Training Loss: 0.41689289247930067, Avg Validation Loss: 18.795996811108015\n",
            "Epoch 275/2048, Avg Training Loss: 0.41645041416556844, Avg Validation Loss: 18.77419660676136\n",
            "Epoch 276/2048, Avg Training Loss: 0.4160083819822307, Avg Validation Loss: 18.75227845425595\n",
            "Epoch 277/2048, Avg Training Loss: 0.41556596704716237, Avg Validation Loss: 18.73037397147575\n",
            "Epoch 278/2048, Avg Training Loss: 0.4151251079435019, Avg Validation Loss: 18.708577657461852\n",
            "Epoch 279/2048, Avg Training Loss: 0.41468682909424015, Avg Validation Loss: 18.68693711674298\n",
            "Epoch 280/2048, Avg Training Loss: 0.414251622742152, Avg Validation Loss: 18.66547500602771\n",
            "Epoch 281/2048, Avg Training Loss: 0.41381970716414246, Avg Validation Loss: 18.64420086503395\n",
            "Epoch 282/2048, Avg Training Loss: 0.4133911540846226, Avg Validation Loss: 18.623117123523638\n",
            "Epoch 283/2048, Avg Training Loss: 0.4129659695766531, Avg Validation Loss: 18.60221821469175\n",
            "Epoch 284/2048, Avg Training Loss: 0.4125442581864156, Avg Validation Loss: 18.581497610107245\n",
            "Epoch 285/2048, Avg Training Loss: 0.4121259702996501, Avg Validation Loss: 18.56095528976596\n",
            "Epoch 286/2048, Avg Training Loss: 0.4117109561271893, Avg Validation Loss: 18.540589047401483\n",
            "Epoch 287/2048, Avg Training Loss: 0.4112991015599466, Avg Validation Loss: 18.520394097742216\n",
            "Epoch 288/2048, Avg Training Loss: 0.4108901931239939, Avg Validation Loss: 18.500322803500588\n",
            "Epoch 289/2048, Avg Training Loss: 0.41048346426869947, Avg Validation Loss: 18.480331468237967\n",
            "Epoch 290/2048, Avg Training Loss: 0.41007915675042617, Avg Validation Loss: 18.460457482760248\n",
            "Epoch 291/2048, Avg Training Loss: 0.40967743438301896, Avg Validation Loss: 18.440718980909278\n",
            "Epoch 292/2048, Avg Training Loss: 0.4092783506915327, Avg Validation Loss: 18.42112377626396\n",
            "Epoch 293/2048, Avg Training Loss: 0.4088819001073111, Avg Validation Loss: 18.401674205953775\n",
            "Epoch 294/2048, Avg Training Loss: 0.4084880437513586, Avg Validation Loss: 18.382369682359993\n",
            "Epoch 295/2048, Avg Training Loss: 0.4080967329812992, Avg Validation Loss: 18.363208195903685\n",
            "Epoch 296/2048, Avg Training Loss: 0.40770791396359113, Avg Validation Loss: 18.344186988652506\n",
            "Epoch 297/2048, Avg Training Loss: 0.4073215258162327, Avg Validation Loss: 18.325302908087945\n",
            "Epoch 298/2048, Avg Training Loss: 0.4069375061304593, Avg Validation Loss: 18.30655262093392\n",
            "Epoch 299/2048, Avg Training Loss: 0.40655579843005607, Avg Validation Loss: 18.287932774007658\n",
            "Epoch 300/2048, Avg Training Loss: 0.40617639341210493, Avg Validation Loss: 18.269446346522205\n",
            "Epoch 301/2048, Avg Training Loss: 0.4057996439484544, Avg Validation Loss: 18.251117153616622\n",
            "Epoch 302/2048, Avg Training Loss: 0.4054257673874977, Avg Validation Loss: 18.23293532606414\n",
            "Epoch 303/2048, Avg Training Loss: 0.40505442351818516, Avg Validation Loss: 18.214885179082472\n",
            "Epoch 304/2048, Avg Training Loss: 0.4046853570501088, Avg Validation Loss: 18.196957046340827\n",
            "Epoch 305/2048, Avg Training Loss: 0.40431840940218383, Avg Validation Loss: 18.17914440757682\n",
            "Epoch 306/2048, Avg Training Loss: 0.4039534696166346, Avg Validation Loss: 18.161442396672978\n",
            "Epoch 307/2048, Avg Training Loss: 0.4035904580499128, Avg Validation Loss: 18.143846396915116\n",
            "Epoch 308/2048, Avg Training Loss: 0.40322932237870357, Avg Validation Loss: 18.12635060357141\n",
            "Epoch 309/2048, Avg Training Loss: 0.4028700468225848, Avg Validation Loss: 18.108952815572565\n",
            "Epoch 310/2048, Avg Training Loss: 0.4025125628640195, Avg Validation Loss: 18.091651061004022\n",
            "Epoch 311/2048, Avg Training Loss: 0.40215679747017763, Avg Validation Loss: 18.07443413300189\n",
            "Epoch 312/2048, Avg Training Loss: 0.4018019290211139, Avg Validation Loss: 18.057290516412554\n",
            "Epoch 313/2048, Avg Training Loss: 0.4014478354279968, Avg Validation Loss: 18.040192997724034\n",
            "Epoch 314/2048, Avg Training Loss: 0.40109439570562155, Avg Validation Loss: 18.023147971309143\n",
            "Epoch 315/2048, Avg Training Loss: 0.4007416339959336, Avg Validation Loss: 18.00601451341555\n",
            "Epoch 316/2048, Avg Training Loss: 0.400387729236625, Avg Validation Loss: 17.988819412477778\n",
            "Epoch 317/2048, Avg Training Loss: 0.4000338354343712, Avg Validation Loss: 17.971629767535156\n",
            "Epoch 318/2048, Avg Training Loss: 0.3996806120516074, Avg Validation Loss: 17.954481331918522\n",
            "Epoch 319/2048, Avg Training Loss: 0.3993284504988285, Avg Validation Loss: 17.93739437348914\n",
            "Epoch 320/2048, Avg Training Loss: 0.3989774511937907, Avg Validation Loss: 17.92037249368865\n",
            "Epoch 321/2048, Avg Training Loss: 0.3986274858245837, Avg Validation Loss: 17.90342180149837\n",
            "Epoch 322/2048, Avg Training Loss: 0.39827868691699997, Avg Validation Loss: 17.886545691792502\n",
            "Epoch 323/2048, Avg Training Loss: 0.39793049727879387, Avg Validation Loss: 17.869590513363764\n",
            "Epoch 324/2048, Avg Training Loss: 0.3975816375440928, Avg Validation Loss: 17.852608444888574\n",
            "Epoch 325/2048, Avg Training Loss: 0.3972330443216831, Avg Validation Loss: 17.835646094551635\n",
            "Epoch 326/2048, Avg Training Loss: 0.39688508961298147, Avg Validation Loss: 17.818720436920735\n",
            "Epoch 327/2048, Avg Training Loss: 0.396537883831912, Avg Validation Loss: 17.801848089377092\n",
            "Epoch 328/2048, Avg Training Loss: 0.39619169322781583, Avg Validation Loss: 17.784980400300817\n",
            "Epoch 329/2048, Avg Training Loss: 0.3958449419568465, Avg Validation Loss: 17.767998087383145\n",
            "Epoch 330/2048, Avg Training Loss: 0.39549735186630197, Avg Validation Loss: 17.750942763420714\n",
            "Epoch 331/2048, Avg Training Loss: 0.39514923015308856, Avg Validation Loss: 17.733838739677374\n",
            "Epoch 332/2048, Avg Training Loss: 0.39480140647804013, Avg Validation Loss: 17.716742916158104\n",
            "Epoch 333/2048, Avg Training Loss: 0.39445431711177753, Avg Validation Loss: 17.699684685036942\n",
            "Epoch 334/2048, Avg Training Loss: 0.394108175650462, Avg Validation Loss: 17.682678713111315\n",
            "Epoch 335/2048, Avg Training Loss: 0.39376307573285463, Avg Validation Loss: 17.66573168777134\n",
            "Epoch 336/2048, Avg Training Loss: 0.39341903980444615, Avg Validation Loss: 17.64884600574156\n",
            "Epoch 337/2048, Avg Training Loss: 0.3930760605589263, Avg Validation Loss: 17.63202187237018\n",
            "Epoch 338/2048, Avg Training Loss: 0.39273411250225426, Avg Validation Loss: 17.615258267458707\n",
            "Epoch 339/2048, Avg Training Loss: 0.39239315506558353, Avg Validation Loss: 17.5985534771867\n",
            "Epoch 340/2048, Avg Training Loss: 0.39205315205175045, Avg Validation Loss: 17.581905589336323\n",
            "Epoch 341/2048, Avg Training Loss: 0.3917140651788141, Avg Validation Loss: 17.565312908326792\n",
            "Epoch 342/2048, Avg Training Loss: 0.3913758577068362, Avg Validation Loss: 17.548773961201306\n",
            "Epoch 343/2048, Avg Training Loss: 0.3910384928345442, Avg Validation Loss: 17.532286630584927\n",
            "Epoch 344/2048, Avg Training Loss: 0.3907019327062179, Avg Validation Loss: 17.515848844637368\n",
            "Epoch 345/2048, Avg Training Loss: 0.39036614420465465, Avg Validation Loss: 17.499458660450234\n",
            "Epoch 346/2048, Avg Training Loss: 0.3900310326021265, Avg Validation Loss: 17.48301780863598\n",
            "Epoch 347/2048, Avg Training Loss: 0.3896947979031223, Avg Validation Loss: 17.46650183403806\n",
            "Epoch 348/2048, Avg Training Loss: 0.3893580897383679, Avg Validation Loss: 17.449968354738708\n",
            "Epoch 349/2048, Avg Training Loss: 0.38902146326485143, Avg Validation Loss: 17.433447179080503\n",
            "Epoch 350/2048, Avg Training Loss: 0.38868519794057316, Avg Validation Loss: 17.41695331914771\n",
            "Epoch 351/2048, Avg Training Loss: 0.38834942799116373, Avg Validation Loss: 17.40049389110878\n",
            "Epoch 352/2048, Avg Training Loss: 0.3880142086063791, Avg Validation Loss: 17.38407180421438\n",
            "Epoch 353/2048, Avg Training Loss: 0.3876795474487958, Avg Validation Loss: 17.3676876142102\n",
            "Epoch 354/2048, Avg Training Loss: 0.3873454239839335, Avg Validation Loss: 17.35134050312591\n",
            "Epoch 355/2048, Avg Training Loss: 0.387011815713541, Avg Validation Loss: 17.335028934865466\n",
            "Epoch 356/2048, Avg Training Loss: 0.3866786888496692, Avg Validation Loss: 17.318748820159023\n",
            "Epoch 357/2048, Avg Training Loss: 0.38634596705299906, Avg Validation Loss: 17.302499050026153\n",
            "Epoch 358/2048, Avg Training Loss: 0.38601364820142203, Avg Validation Loss: 17.28627916143755\n",
            "Epoch 359/2048, Avg Training Loss: 0.38568170869352747, Avg Validation Loss: 17.2700836142003\n",
            "Epoch 360/2048, Avg Training Loss: 0.38534982551740554, Avg Validation Loss: 17.253906393572183\n",
            "Epoch 361/2048, Avg Training Loss: 0.38501797322095455, Avg Validation Loss: 17.23774983454149\n",
            "Epoch 362/2048, Avg Training Loss: 0.3846862615755497, Avg Validation Loss: 17.221614386071877\n",
            "Epoch 363/2048, Avg Training Loss: 0.38435481683783057, Avg Validation Loss: 17.205510615126126\n",
            "Epoch 364/2048, Avg Training Loss: 0.3840235684034732, Avg Validation Loss: 17.18940044797292\n",
            "Epoch 365/2048, Avg Training Loss: 0.3836916281309586, Avg Validation Loss: 17.17329483899079\n",
            "Epoch 366/2048, Avg Training Loss: 0.3833594923379236, Avg Validation Loss: 17.15720498827502\n",
            "Epoch 367/2048, Avg Training Loss: 0.38302749335143277, Avg Validation Loss: 17.14113004049945\n",
            "Epoch 368/2048, Avg Training Loss: 0.38269562819791036, Avg Validation Loss: 17.125068782816875\n",
            "Epoch 369/2048, Avg Training Loss: 0.38236385874127454, Avg Validation Loss: 17.10902131921943\n",
            "Epoch 370/2048, Avg Training Loss: 0.3820321005693956, Avg Validation Loss: 17.092985986981247\n",
            "Epoch 371/2048, Avg Training Loss: 0.38170039527257915, Avg Validation Loss: 17.076965364044238\n",
            "Epoch 372/2048, Avg Training Loss: 0.3813689764732194, Avg Validation Loss: 17.06095927436504\n",
            "Epoch 373/2048, Avg Training Loss: 0.3810377833554035, Avg Validation Loss: 17.044963937082933\n",
            "Epoch 374/2048, Avg Training Loss: 0.3807067062037188, Avg Validation Loss: 17.02897632876155\n",
            "Epoch 375/2048, Avg Training Loss: 0.3803756648707337, Avg Validation Loss: 17.012994148354796\n",
            "Epoch 376/2048, Avg Training Loss: 0.38004461264595246, Avg Validation Loss: 16.99701583722905\n",
            "Epoch 377/2048, Avg Training Loss: 0.3797135234060768, Avg Validation Loss: 16.9810396482875\n",
            "Epoch 378/2048, Avg Training Loss: 0.3793823766322012, Avg Validation Loss: 16.965064260428694\n",
            "Epoch 379/2048, Avg Training Loss: 0.37905114707338483, Avg Validation Loss: 16.94908761806079\n",
            "Epoch 380/2048, Avg Training Loss: 0.37871981062983584, Avg Validation Loss: 16.9331092422797\n",
            "Epoch 381/2048, Avg Training Loss: 0.37838836021370925, Avg Validation Loss: 16.917128488507167\n",
            "Epoch 382/2048, Avg Training Loss: 0.37805678560406036, Avg Validation Loss: 16.901144614548794\n",
            "Epoch 383/2048, Avg Training Loss: 0.3777252369226027, Avg Validation Loss: 16.885173980175868\n",
            "Epoch 384/2048, Avg Training Loss: 0.37739426702082796, Avg Validation Loss: 16.86923127169307\n",
            "Epoch 385/2048, Avg Training Loss: 0.3770638784506451, Avg Validation Loss: 16.853306972887662\n",
            "Epoch 386/2048, Avg Training Loss: 0.3767336885620786, Avg Validation Loss: 16.8374784591526\n",
            "Epoch 387/2048, Avg Training Loss: 0.37640490996089454, Avg Validation Loss: 16.82182356987269\n",
            "Epoch 388/2048, Avg Training Loss: 0.3760769568135077, Avg Validation Loss: 16.80625428566859\n",
            "Epoch 389/2048, Avg Training Loss: 0.3757493336556464, Avg Validation Loss: 16.790723067745105\n",
            "Epoch 390/2048, Avg Training Loss: 0.37542177423244033, Avg Validation Loss: 16.775204344509735\n",
            "Epoch 391/2048, Avg Training Loss: 0.3750942189638951, Avg Validation Loss: 16.759597595189057\n",
            "Epoch 392/2048, Avg Training Loss: 0.374765110757812, Avg Validation Loss: 16.743896207657098\n",
            "Epoch 393/2048, Avg Training Loss: 0.37443620965793684, Avg Validation Loss: 16.728263761390544\n",
            "Epoch 394/2048, Avg Training Loss: 0.3741075590379247, Avg Validation Loss: 16.71257289954065\n",
            "Epoch 395/2048, Avg Training Loss: 0.37377746687536123, Avg Validation Loss: 16.696802590596512\n",
            "Epoch 396/2048, Avg Training Loss: 0.37344762964500466, Avg Validation Loss: 16.681108466407217\n",
            "Epoch 397/2048, Avg Training Loss: 0.37311803748966477, Avg Validation Loss: 16.665384760569122\n",
            "Epoch 398/2048, Avg Training Loss: 0.3727872775993235, Avg Validation Loss: 16.649633742880397\n",
            "Epoch 399/2048, Avg Training Loss: 0.37245689147863015, Avg Validation Loss: 16.63398564532874\n",
            "Epoch 400/2048, Avg Training Loss: 0.37212674787429545, Avg Validation Loss: 16.61838210701123\n",
            "Epoch 401/2048, Avg Training Loss: 0.371796669067674, Avg Validation Loss: 16.602705333667313\n",
            "Epoch 402/2048, Avg Training Loss: 0.37146502313550633, Avg Validation Loss: 16.58693902364212\n",
            "Epoch 403/2048, Avg Training Loss: 0.37113355351403304, Avg Validation Loss: 16.571240961554434\n",
            "Epoch 404/2048, Avg Training Loss: 0.3708022144975397, Avg Validation Loss: 16.555567894198145\n",
            "Epoch 405/2048, Avg Training Loss: 0.37047087314529575, Avg Validation Loss: 16.539811058134028\n",
            "Epoch 406/2048, Avg Training Loss: 0.3701379284176104, Avg Validation Loss: 16.52395874859388\n",
            "Epoch 407/2048, Avg Training Loss: 0.3698051334382181, Avg Validation Loss: 16.50817075507375\n",
            "Epoch 408/2048, Avg Training Loss: 0.3694724484489108, Avg Validation Loss: 16.492405129311845\n",
            "Epoch 409/2048, Avg Training Loss: 0.36913973411675116, Avg Validation Loss: 16.476553438170047\n",
            "Epoch 410/2048, Avg Training Loss: 0.3688053677625285, Avg Validation Loss: 16.46060451800126\n",
            "Epoch 411/2048, Avg Training Loss: 0.36847111311415626, Avg Validation Loss: 16.44471820748136\n",
            "Epoch 412/2048, Avg Training Loss: 0.3681369394028946, Avg Validation Loss: 16.4288528264973\n",
            "Epoch 413/2048, Avg Training Loss: 0.36780270852538477, Avg Validation Loss: 16.412900252176783\n",
            "Epoch 414/2048, Avg Training Loss: 0.3674667914242488, Avg Validation Loss: 16.39684935445349\n",
            "Epoch 415/2048, Avg Training Loss: 0.3671309538223282, Avg Validation Loss: 16.380859718736122\n",
            "Epoch 416/2048, Avg Training Loss: 0.36679516706920867, Avg Validation Loss: 16.364889701065728\n",
            "Epoch 417/2048, Avg Training Loss: 0.36645920714509234, Avg Validation Loss: 16.34891693219698\n",
            "Epoch 418/2048, Avg Training Loss: 0.3661230382439269, Avg Validation Loss: 16.3328438659231\n",
            "Epoch 419/2048, Avg Training Loss: 0.36578508349122574, Avg Validation Loss: 16.316665156320433\n",
            "Epoch 420/2048, Avg Training Loss: 0.36544717774480134, Avg Validation Loss: 16.300547580017888\n",
            "Epoch 421/2048, Avg Training Loss: 0.3651093347708648, Avg Validation Loss: 16.284449388765534\n",
            "Epoch 422/2048, Avg Training Loss: 0.3647714042745955, Avg Validation Loss: 16.268262766518923\n",
            "Epoch 423/2048, Avg Training Loss: 0.36443174442005294, Avg Validation Loss: 16.25197696234927\n",
            "Epoch 424/2048, Avg Training Loss: 0.3640921413908662, Avg Validation Loss: 16.235751611162968\n",
            "Epoch 425/2048, Avg Training Loss: 0.3637525811585087, Avg Validation Loss: 16.219545494003594\n",
            "Epoch 426/2048, Avg Training Loss: 0.363412840128793, Avg Validation Loss: 16.20333651910734\n",
            "Epoch 427/2048, Avg Training Loss: 0.36307280266743686, Avg Validation Loss: 16.187113218921702\n",
            "Epoch 428/2048, Avg Training Loss: 0.3627324947691752, Avg Validation Loss: 16.170783962000943\n",
            "Epoch 429/2048, Avg Training Loss: 0.36239035611487064, Avg Validation Loss: 16.15434642717575\n",
            "Epoch 430/2048, Avg Training Loss: 0.362048226412908, Avg Validation Loss: 16.13796409361905\n",
            "Epoch 431/2048, Avg Training Loss: 0.36170612228785914, Avg Validation Loss: 16.121598339717696\n",
            "Epoch 432/2048, Avg Training Loss: 0.36136383268301225, Avg Validation Loss: 16.105228509225704\n",
            "Epoch 433/2048, Avg Training Loss: 0.36102132922194413, Avg Validation Loss: 16.088758564409588\n",
            "Epoch 434/2048, Avg Training Loss: 0.36067702546292396, Avg Validation Loss: 16.072183824536772\n",
            "Epoch 435/2048, Avg Training Loss: 0.36033275562053235, Avg Validation Loss: 16.0556662091597\n",
            "Epoch 436/2048, Avg Training Loss: 0.3599885349013785, Avg Validation Loss: 16.03916652320257\n",
            "Epoch 437/2048, Avg Training Loss: 0.35964414829694513, Avg Validation Loss: 16.022663827070595\n",
            "Epoch 438/2048, Avg Training Loss: 0.3592994835880366, Avg Validation Loss: 16.0061469834577\n",
            "Epoch 439/2048, Avg Training Loss: 0.3589544827459295, Avg Validation Loss: 15.989610156179191\n",
            "Epoch 440/2048, Avg Training Loss: 0.3586091985638202, Avg Validation Loss: 15.972965518051948\n",
            "Epoch 441/2048, Avg Training Loss: 0.35826208340513044, Avg Validation Loss: 15.956212710081676\n",
            "Epoch 442/2048, Avg Training Loss: 0.3579150010655399, Avg Validation Loss: 15.939515627591657\n",
            "Epoch 443/2048, Avg Training Loss: 0.3575679824126096, Avg Validation Loss: 15.922836601101965\n",
            "Epoch 444/2048, Avg Training Loss: 0.35722081559677465, Avg Validation Loss: 15.90615541121985\n",
            "Epoch 445/2048, Avg Training Loss: 0.35687339217360375, Avg Validation Loss: 15.889461342277881\n",
            "Epoch 446/2048, Avg Training Loss: 0.3565256592846447, Avg Validation Loss: 15.87274878002908\n",
            "Epoch 447/2048, Avg Training Loss: 0.3561776718385785, Avg Validation Loss: 15.855930298509886\n",
            "Epoch 448/2048, Avg Training Loss: 0.3558278799080132, Avg Validation Loss: 15.839005677262097\n",
            "Epoch 449/2048, Avg Training Loss: 0.35547812318525984, Avg Validation Loss: 15.822133814391094\n",
            "Epoch 450/2048, Avg Training Loss: 0.3551282779109509, Avg Validation Loss: 15.805277608105508\n",
            "Epoch 451/2048, Avg Training Loss: 0.3547782014959686, Avg Validation Loss: 15.788419002216292\n",
            "Epoch 452/2048, Avg Training Loss: 0.3544278470446024, Avg Validation Loss: 15.771548404626753\n",
            "Epoch 453/2048, Avg Training Loss: 0.35407719285320166, Avg Validation Loss: 15.754660808738272\n",
            "Epoch 454/2048, Avg Training Loss: 0.3537262305957427, Avg Validation Loss: 15.737753691075486\n",
            "Epoch 455/2048, Avg Training Loss: 0.3533750399536752, Avg Validation Loss: 15.72074159939433\n",
            "Epoch 456/2048, Avg Training Loss: 0.35302208218585146, Avg Validation Loss: 15.703625262384374\n",
            "Epoch 457/2048, Avg Training Loss: 0.3526692380638053, Avg Validation Loss: 15.686568104564861\n",
            "Epoch 458/2048, Avg Training Loss: 0.3523165540723462, Avg Validation Loss: 15.669532928536354\n",
            "Epoch 459/2048, Avg Training Loss: 0.351963822350166, Avg Validation Loss: 15.652499955122327\n",
            "Epoch 460/2048, Avg Training Loss: 0.3516109360762232, Avg Validation Loss: 15.635458754760588\n",
            "Epoch 461/2048, Avg Training Loss: 0.3512578173105064, Avg Validation Loss: 15.618377536522473\n",
            "Epoch 462/2048, Avg Training Loss: 0.3509042947651009, Avg Validation Loss: 15.601256508104276\n",
            "Epoch 463/2048, Avg Training Loss: 0.35055044975159777, Avg Validation Loss: 15.584105653496676\n",
            "Epoch 464/2048, Avg Training Loss: 0.3501963175272553, Avg Validation Loss: 15.566930522655015\n",
            "Epoch 465/2048, Avg Training Loss: 0.3498420010609068, Avg Validation Loss: 15.549650367136772\n",
            "Epoch 466/2048, Avg Training Loss: 0.34948597470176335, Avg Validation Loss: 15.53226831980685\n",
            "Epoch 467/2048, Avg Training Loss: 0.3491301272967422, Avg Validation Loss: 15.514948394074075\n",
            "Epoch 468/2048, Avg Training Loss: 0.348774512246919, Avg Validation Loss: 15.497654246730692\n",
            "Epoch 469/2048, Avg Training Loss: 0.34841890057171493, Avg Validation Loss: 15.48035735763297\n",
            "Epoch 470/2048, Avg Training Loss: 0.3480631597278304, Avg Validation Loss: 15.463050593481961\n",
            "Epoch 471/2048, Avg Training Loss: 0.34770711627847234, Avg Validation Loss: 15.445689011841504\n",
            "Epoch 472/2048, Avg Training Loss: 0.34735048285204345, Avg Validation Loss: 15.428285943782736\n",
            "Epoch 473/2048, Avg Training Loss: 0.3469934628504678, Avg Validation Loss: 15.41085496388806\n",
            "Epoch 474/2048, Avg Training Loss: 0.346636168929681, Avg Validation Loss: 15.393403532067707\n",
            "Epoch 475/2048, Avg Training Loss: 0.3462786651195126, Avg Validation Loss: 15.37593583899453\n",
            "Epoch 476/2048, Avg Training Loss: 0.3459209896335674, Avg Validation Loss: 15.358454377609945\n",
            "Epoch 477/2048, Avg Training Loss: 0.34556316699205414, Avg Validation Loss: 15.340960658029891\n",
            "Epoch 478/2048, Avg Training Loss: 0.3452052144632108, Avg Validation Loss: 15.323455672342437\n",
            "Epoch 479/2048, Avg Training Loss: 0.3448472255357623, Avg Validation Loss: 15.305856804576422\n",
            "Epoch 480/2048, Avg Training Loss: 0.34448767838218836, Avg Validation Loss: 15.288165928355644\n",
            "Epoch 481/2048, Avg Training Loss: 0.34412845122753943, Avg Validation Loss: 15.27054523560667\n",
            "Epoch 482/2048, Avg Training Loss: 0.34376959630301773, Avg Validation Loss: 15.252958249854537\n",
            "Epoch 483/2048, Avg Training Loss: 0.34341094246144915, Avg Validation Loss: 15.235390554559213\n",
            "Epoch 484/2048, Avg Training Loss: 0.34305258702458236, Avg Validation Loss: 15.21785651120427\n",
            "Epoch 485/2048, Avg Training Loss: 0.34269484777384446, Avg Validation Loss: 15.200370846701613\n",
            "Epoch 486/2048, Avg Training Loss: 0.34233745879624244, Avg Validation Loss: 15.182907486100758\n",
            "Epoch 487/2048, Avg Training Loss: 0.3419801727246572, Avg Validation Loss: 15.165423998874909\n",
            "Epoch 488/2048, Avg Training Loss: 0.3416228119319984, Avg Validation Loss: 15.147922640633528\n",
            "Epoch 489/2048, Avg Training Loss: 0.34126541372293145, Avg Validation Loss: 15.130409017423975\n",
            "Epoch 490/2048, Avg Training Loss: 0.3409079949617898, Avg Validation Loss: 15.112886347289754\n",
            "Epoch 491/2048, Avg Training Loss: 0.340550568947037, Avg Validation Loss: 15.09535656222907\n",
            "Epoch 492/2048, Avg Training Loss: 0.34019314710092846, Avg Validation Loss: 15.077820901835345\n",
            "Epoch 493/2048, Avg Training Loss: 0.33983573986147486, Avg Validation Loss: 15.060280233067823\n",
            "Epoch 494/2048, Avg Training Loss: 0.33947835715235947, Avg Validation Loss: 15.04273522247178\n",
            "Epoch 495/2048, Avg Training Loss: 0.33912100862985806, Avg Validation Loss: 15.025186428910336\n",
            "Epoch 496/2048, Avg Training Loss: 0.33876370400987516, Avg Validation Loss: 15.007634351243535\n",
            "Epoch 497/2048, Avg Training Loss: 0.3384064532605139, Avg Validation Loss: 14.9900794546201\n",
            "Epoch 498/2048, Avg Training Loss: 0.3380492658221338, Avg Validation Loss: 14.972524192334488\n",
            "Epoch 499/2048, Avg Training Loss: 0.33769228767980064, Avg Validation Loss: 14.954982753757447\n",
            "Epoch 500/2048, Avg Training Loss: 0.33733570217074543, Avg Validation Loss: 14.937450682445752\n",
            "Epoch 501/2048, Avg Training Loss: 0.3369793854470658, Avg Validation Loss: 14.919922939553796\n",
            "Epoch 502/2048, Avg Training Loss: 0.3366232567475935, Avg Validation Loss: 14.902397109100074\n",
            "Epoch 503/2048, Avg Training Loss: 0.336267280000859, Avg Validation Loss: 14.884872115140475\n",
            "Epoch 504/2048, Avg Training Loss: 0.33591144045594723, Avg Validation Loss: 14.867347642583296\n",
            "Epoch 505/2048, Avg Training Loss: 0.33555573464892663, Avg Validation Loss: 14.84982372866753\n",
            "Epoch 506/2048, Avg Training Loss: 0.3352001651215837, Avg Validation Loss: 14.832300597894024\n",
            "Epoch 507/2048, Avg Training Loss: 0.3348445447503239, Avg Validation Loss: 14.814764016524473\n",
            "Epoch 508/2048, Avg Training Loss: 0.33448861115820333, Avg Validation Loss: 14.797221280479036\n",
            "Epoch 509/2048, Avg Training Loss: 0.3341325956787743, Avg Validation Loss: 14.77967642990415\n",
            "Epoch 510/2048, Avg Training Loss: 0.3337766231418232, Avg Validation Loss: 14.762131387389603\n",
            "Epoch 511/2048, Avg Training Loss: 0.33342076314789143, Avg Validation Loss: 14.744587764862096\n",
            "Epoch 512/2048, Avg Training Loss: 0.3330650603198835, Avg Validation Loss: 14.727046620464526\n",
            "Epoch 513/2048, Avg Training Loss: 0.3327095426987209, Avg Validation Loss: 14.709508720471002\n",
            "Epoch 514/2048, Avg Training Loss: 0.33235422949833293, Avg Validation Loss: 14.691974677080236\n",
            "Epoch 515/2048, Avg Training Loss: 0.33199913522700375, Avg Validation Loss: 14.674445020874023\n",
            "Epoch 516/2048, Avg Training Loss: 0.3316442718774388, Avg Validation Loss: 14.656920232531927\n",
            "Epoch 517/2048, Avg Training Loss: 0.3312896500902863, Avg Validation Loss: 14.639400748835289\n",
            "Epoch 518/2048, Avg Training Loss: 0.33093527517253746, Avg Validation Loss: 14.621885924286977\n",
            "Epoch 519/2048, Avg Training Loss: 0.3305811446343039, Avg Validation Loss: 14.604376490820922\n",
            "Epoch 520/2048, Avg Training Loss: 0.3302272739016384, Avg Validation Loss: 14.586872536428414\n",
            "Epoch 521/2048, Avg Training Loss: 0.32987363324523533, Avg Validation Loss: 14.5693738707109\n",
            "Epoch 522/2048, Avg Training Loss: 0.3295202282330833, Avg Validation Loss: 14.551881674800748\n",
            "Epoch 523/2048, Avg Training Loss: 0.3291670918744239, Avg Validation Loss: 14.534396812835462\n",
            "Epoch 524/2048, Avg Training Loss: 0.3288142468710993, Avg Validation Loss: 14.516919945241188\n",
            "Epoch 525/2048, Avg Training Loss: 0.32846170982895867, Avg Validation Loss: 14.499451622580471\n",
            "Epoch 526/2048, Avg Training Loss: 0.3281094449460289, Avg Validation Loss: 14.481988855684005\n",
            "Epoch 527/2048, Avg Training Loss: 0.32775723460982564, Avg Validation Loss: 14.464521509178796\n",
            "Epoch 528/2048, Avg Training Loss: 0.32740485151878296, Avg Validation Loss: 14.447054948304038\n",
            "Epoch 529/2048, Avg Training Loss: 0.3270525319784097, Avg Validation Loss: 14.429593865055304\n",
            "Epoch 530/2048, Avg Training Loss: 0.32670042601101373, Avg Validation Loss: 14.412140925082468\n",
            "Epoch 531/2048, Avg Training Loss: 0.3263486174671113, Avg Validation Loss: 14.394697729644312\n",
            "Epoch 532/2048, Avg Training Loss: 0.32599715490266656, Avg Validation Loss: 14.377265321443515\n",
            "Epoch 533/2048, Avg Training Loss: 0.325646068088365, Avg Validation Loss: 14.359844449504722\n",
            "Epoch 534/2048, Avg Training Loss: 0.3252953981815884, Avg Validation Loss: 14.342442098470576\n",
            "Epoch 535/2048, Avg Training Loss: 0.32494516616716207, Avg Validation Loss: 14.325047168574013\n",
            "Epoch 536/2048, Avg Training Loss: 0.3245952814450313, Avg Validation Loss: 14.307656346773198\n",
            "Epoch 537/2048, Avg Training Loss: 0.3242457900921418, Avg Validation Loss: 14.290274411691913\n",
            "Epoch 538/2048, Avg Training Loss: 0.3238967186456497, Avg Validation Loss: 14.272904119136511\n",
            "Epoch 539/2048, Avg Training Loss: 0.32354808502193233, Avg Validation Loss: 14.255547137151687\n",
            "Epoch 540/2048, Avg Training Loss: 0.3231999018369533, Avg Validation Loss: 14.238203717208268\n",
            "Epoch 541/2048, Avg Training Loss: 0.322852162002028, Avg Validation Loss: 14.220874378190537\n",
            "Epoch 542/2048, Avg Training Loss: 0.3225048801236887, Avg Validation Loss: 14.203560176656334\n",
            "Epoch 543/2048, Avg Training Loss: 0.3221580705694952, Avg Validation Loss: 14.186261862375659\n",
            "Epoch 544/2048, Avg Training Loss: 0.3218117446936788, Avg Validation Loss: 14.168980019551258\n",
            "Epoch 545/2048, Avg Training Loss: 0.32146578640888035, Avg Validation Loss: 14.151704683594879\n",
            "Epoch 546/2048, Avg Training Loss: 0.3211200171435592, Avg Validation Loss: 14.134441283960165\n",
            "Epoch 547/2048, Avg Training Loss: 0.3207745963968538, Avg Validation Loss: 14.11719285568481\n",
            "Epoch 548/2048, Avg Training Loss: 0.32042961855964297, Avg Validation Loss: 14.099962090636303\n",
            "Epoch 549/2048, Avg Training Loss: 0.32008519096063387, Avg Validation Loss: 14.082750354234369\n",
            "Epoch 550/2048, Avg Training Loss: 0.3197413291594923, Avg Validation Loss: 14.065557891634192\n",
            "Epoch 551/2048, Avg Training Loss: 0.31939802996145916, Avg Validation Loss: 14.048385013683816\n",
            "Epoch 552/2048, Avg Training Loss: 0.3190552951408947, Avg Validation Loss: 14.031232062866582\n",
            "Epoch 553/2048, Avg Training Loss: 0.31871311720051676, Avg Validation Loss: 14.014164848808996\n",
            "Epoch 554/2048, Avg Training Loss: 0.31837230095062036, Avg Validation Loss: 13.997200204419716\n",
            "Epoch 555/2048, Avg Training Loss: 0.31803247243282695, Avg Validation Loss: 13.980298894663678\n",
            "Epoch 556/2048, Avg Training Loss: 0.31769344439108854, Avg Validation Loss: 13.963440014299483\n",
            "Epoch 557/2048, Avg Training Loss: 0.3173551206729222, Avg Validation Loss: 13.946612508080749\n",
            "Epoch 558/2048, Avg Training Loss: 0.3170175523858046, Avg Validation Loss: 13.929896301418067\n",
            "Epoch 559/2048, Avg Training Loss: 0.31668132972056295, Avg Validation Loss: 13.913278895650016\n",
            "Epoch 560/2048, Avg Training Loss: 0.3163460796065722, Avg Validation Loss: 13.896722279698471\n",
            "Epoch 561/2048, Avg Training Loss: 0.3160116379779919, Avg Validation Loss: 13.880206197076706\n",
            "Epoch 562/2048, Avg Training Loss: 0.31567792051340643, Avg Validation Loss: 13.86371993857827\n",
            "Epoch 563/2048, Avg Training Loss: 0.31534488517284287, Avg Validation Loss: 13.847258124177076\n",
            "Epoch 564/2048, Avg Training Loss: 0.3150125179012616, Avg Validation Loss: 13.830818140043538\n",
            "Epoch 565/2048, Avg Training Loss: 0.3146808104435551, Avg Validation Loss: 13.81439848215164\n",
            "Epoch 566/2048, Avg Training Loss: 0.3143497590676071, Avg Validation Loss: 13.797998525058214\n",
            "Epoch 567/2048, Avg Training Loss: 0.3140193644563487, Avg Validation Loss: 13.781618114686744\n",
            "Epoch 568/2048, Avg Training Loss: 0.31368962981992515, Avg Validation Loss: 13.765257350230108\n",
            "Epoch 569/2048, Avg Training Loss: 0.3133605605320291, Avg Validation Loss: 13.748916469713421\n",
            "Epoch 570/2048, Avg Training Loss: 0.31303216200146067, Avg Validation Loss: 13.732595783007945\n",
            "Epoch 571/2048, Avg Training Loss: 0.3127044462463956, Avg Validation Loss: 13.716295968636667\n",
            "Epoch 572/2048, Avg Training Loss: 0.31237742580329175, Avg Validation Loss: 13.700017273411484\n",
            "Epoch 573/2048, Avg Training Loss: 0.31205110086580634, Avg Validation Loss: 13.683760027271752\n",
            "Epoch 574/2048, Avg Training Loss: 0.3117254732546573, Avg Validation Loss: 13.667524513779547\n",
            "Epoch 575/2048, Avg Training Loss: 0.31140054602772177, Avg Validation Loss: 13.651311056421164\n",
            "Epoch 576/2048, Avg Training Loss: 0.3110763233147645, Avg Validation Loss: 13.635119998114703\n",
            "Epoch 577/2048, Avg Training Loss: 0.3107528097945511, Avg Validation Loss: 13.618951690405122\n",
            "Epoch 578/2048, Avg Training Loss: 0.3104300104167661, Avg Validation Loss: 13.602806487768408\n",
            "Epoch 579/2048, Avg Training Loss: 0.3101079304384585, Avg Validation Loss: 13.586684755398617\n",
            "Epoch 580/2048, Avg Training Loss: 0.30978657545113464, Avg Validation Loss: 13.5705868798031\n",
            "Epoch 581/2048, Avg Training Loss: 0.3094659506896225, Avg Validation Loss: 13.554513211396861\n",
            "Epoch 582/2048, Avg Training Loss: 0.3091460459653414, Avg Validation Loss: 13.538483968466602\n",
            "Epoch 583/2048, Avg Training Loss: 0.30882716093092927, Avg Validation Loss: 13.522519192787014\n",
            "Epoch 584/2048, Avg Training Loss: 0.30850919371189034, Avg Validation Loss: 13.506600332969015\n",
            "Epoch 585/2048, Avg Training Loss: 0.30819206609882854, Avg Validation Loss: 13.490717237705079\n",
            "Epoch 586/2048, Avg Training Loss: 0.30787573783273153, Avg Validation Loss: 13.474864925942905\n",
            "Epoch 587/2048, Avg Training Loss: 0.30756019261293205, Avg Validation Loss: 13.459040872165652\n",
            "Epoch 588/2048, Avg Training Loss: 0.30724542392370763, Avg Validation Loss: 13.443243871272658\n",
            "Epoch 589/2048, Avg Training Loss: 0.3069314304291439, Avg Validation Loss: 13.427473427530384\n",
            "Epoch 590/2048, Avg Training Loss: 0.30661821352611096, Avg Validation Loss: 13.411729425598415\n",
            "Epoch 591/2048, Avg Training Loss: 0.30630577604363013, Avg Validation Loss: 13.396011953401537\n",
            "Epoch 592/2048, Avg Training Loss: 0.30599412155139716, Avg Validation Loss: 13.380321206118571\n",
            "Epoch 593/2048, Avg Training Loss: 0.30568325399204893, Avg Validation Loss: 13.364657432658928\n",
            "Epoch 594/2048, Avg Training Loss: 0.3053731774855093, Avg Validation Loss: 13.34902091369368\n",
            "Epoch 595/2048, Avg Training Loss: 0.30506389622480456, Avg Validation Loss: 13.333411944378408\n",
            "Epoch 596/2048, Avg Training Loss: 0.3047554144205064, Avg Validation Loss: 13.317830825824307\n",
            "Epoch 597/2048, Avg Training Loss: 0.3044477362710311, Avg Validation Loss: 13.30227786078088\n",
            "Epoch 598/2048, Avg Training Loss: 0.30414086594668893, Avg Validation Loss: 13.28675335131309\n",
            "Epoch 599/2048, Avg Training Loss: 0.3038348075810513, Avg Validation Loss: 13.271257597553062\n",
            "Epoch 600/2048, Avg Training Loss: 0.30352956526621533, Avg Validation Loss: 13.255790897030726\n",
            "Epoch 601/2048, Avg Training Loss: 0.30322514305014653, Avg Validation Loss: 13.240353544316612\n",
            "Epoch 602/2048, Avg Training Loss: 0.3029215449351353, Avg Validation Loss: 13.224945830832974\n",
            "Epoch 603/2048, Avg Training Loss: 0.3026187748768515, Avg Validation Loss: 13.209568417169633\n",
            "Epoch 604/2048, Avg Training Loss: 0.30231691517859277, Avg Validation Loss: 13.194228525049223\n",
            "Epoch 605/2048, Avg Training Loss: 0.30201616635819595, Avg Validation Loss: 13.178924632482016\n",
            "Epoch 606/2048, Avg Training Loss: 0.3017164345657471, Avg Validation Loss: 13.16365428038044\n",
            "Epoch 607/2048, Avg Training Loss: 0.3014176351863262, Avg Validation Loss: 13.148416310336296\n",
            "Epoch 608/2048, Avg Training Loss: 0.3011197249249617, Avg Validation Loss: 13.133210243967424\n",
            "Epoch 609/2048, Avg Training Loss: 0.3008226824730945, Avg Validation Loss: 13.118035957484116\n",
            "Epoch 610/2048, Avg Training Loss: 0.3005262962560377, Avg Validation Loss: 13.102816878351282\n",
            "Epoch 611/2048, Avg Training Loss: 0.3002302967990207, Avg Validation Loss: 13.087578556767207\n",
            "Epoch 612/2048, Avg Training Loss: 0.29993492335921257, Avg Validation Loss: 13.07234606741283\n",
            "Epoch 613/2048, Avg Training Loss: 0.29964029923988056, Avg Validation Loss: 13.05713729211233\n",
            "Epoch 614/2048, Avg Training Loss: 0.29934650181470823, Avg Validation Loss: 13.041958267374344\n",
            "Epoch 615/2048, Avg Training Loss: 0.29905355113980575, Avg Validation Loss: 13.026811733162218\n",
            "Epoch 616/2048, Avg Training Loss: 0.29876146046896174, Avg Validation Loss: 13.011699231794825\n",
            "Epoch 617/2048, Avg Training Loss: 0.29847023847857745, Avg Validation Loss: 12.996621690609658\n",
            "Epoch 618/2048, Avg Training Loss: 0.29817989138814355, Avg Validation Loss: 12.981579702842508\n",
            "Epoch 619/2048, Avg Training Loss: 0.29789042408610866, Avg Validation Loss: 12.966573683151564\n",
            "Epoch 620/2048, Avg Training Loss: 0.29760184072760776, Avg Validation Loss: 12.951603948121418\n",
            "Epoch 621/2048, Avg Training Loss: 0.2973141441546824, Avg Validation Loss: 12.936670435658188\n",
            "Epoch 622/2048, Avg Training Loss: 0.2970273330452053, Avg Validation Loss: 12.921773435324528\n",
            "Epoch 623/2048, Avg Training Loss: 0.29674141321534536, Avg Validation Loss: 12.906913289801787\n",
            "Epoch 624/2048, Avg Training Loss: 0.29645638945793557, Avg Validation Loss: 12.89209027114339\n",
            "Epoch 625/2048, Avg Training Loss: 0.2961722657489128, Avg Validation Loss: 12.877304612295738\n",
            "Epoch 626/2048, Avg Training Loss: 0.29588904560318896, Avg Validation Loss: 12.862556523973913\n",
            "Epoch 627/2048, Avg Training Loss: 0.2956067322640604, Avg Validation Loss: 12.847846203699557\n",
            "Epoch 628/2048, Avg Training Loss: 0.29532532880405593, Avg Validation Loss: 12.83317384064375\n",
            "Epoch 629/2048, Avg Training Loss: 0.29504483817866983, Avg Validation Loss: 12.818539618224698\n",
            "Epoch 630/2048, Avg Training Loss: 0.29476526325503144, Avg Validation Loss: 12.80394371550355\n",
            "Epoch 631/2048, Avg Training Loss: 0.29448660682724054, Avg Validation Loss: 12.789386307937002\n",
            "Epoch 632/2048, Avg Training Loss: 0.29420887162461207, Avg Validation Loss: 12.77486756778554\n",
            "Epoch 633/2048, Avg Training Loss: 0.29393206031614955, Avg Validation Loss: 12.760387664337403\n",
            "Epoch 634/2048, Avg Training Loss: 0.2936561755130142, Avg Validation Loss: 12.74594676403393\n",
            "Epoch 635/2048, Avg Training Loss: 0.29338121976992926, Avg Validation Loss: 12.731545030542145\n",
            "Epoch 636/2048, Avg Training Loss: 0.2931071955860199, Avg Validation Loss: 12.717182624799081\n",
            "Epoch 637/2048, Avg Training Loss: 0.29283410540535304, Avg Validation Loss: 12.702859705041105\n",
            "Epoch 638/2048, Avg Training Loss: 0.29256195161732146, Avg Validation Loss: 12.688576426825147\n",
            "Epoch 639/2048, Avg Training Loss: 0.2922907365569444, Avg Validation Loss: 12.674332943045691\n",
            "Epoch 640/2048, Avg Training Loss: 0.2920204625051263, Avg Validation Loss: 12.660129403949536\n",
            "Epoch 641/2048, Avg Training Loss: 0.291751131688895, Avg Validation Loss: 12.645965957149324\n",
            "Epoch 642/2048, Avg Training Loss: 0.29148274628162846, Avg Validation Loss: 12.631842747636517\n",
            "Epoch 643/2048, Avg Training Loss: 0.29121530840327986, Avg Validation Loss: 12.617759917651487\n",
            "Epoch 644/2048, Avg Training Loss: 0.29094882011951245, Avg Validation Loss: 12.603717606258702\n",
            "Epoch 645/2048, Avg Training Loss: 0.2906832832721392, Avg Validation Loss: 12.589715949793554\n",
            "Epoch 646/2048, Avg Training Loss: 0.2904186996099596, Avg Validation Loss: 12.575755082013307\n",
            "Epoch 647/2048, Avg Training Loss: 0.2901550709679943, Avg Validation Loss: 12.561835134591085\n",
            "Epoch 648/2048, Avg Training Loss: 0.2898923978873927, Avg Validation Loss: 12.547954662939182\n",
            "Epoch 649/2048, Avg Training Loss: 0.28963064922352294, Avg Validation Loss: 12.534113382731942\n",
            "Epoch 650/2048, Avg Training Loss: 0.28936983741935235, Avg Validation Loss: 12.520312349504778\n",
            "Epoch 651/2048, Avg Training Loss: 0.28910977127513376, Avg Validation Loss: 12.506494046866472\n",
            "Epoch 652/2048, Avg Training Loss: 0.28885040819103236, Avg Validation Loss: 12.492686511809854\n",
            "Epoch 653/2048, Avg Training Loss: 0.28859187482055293, Avg Validation Loss: 12.478904881734048\n",
            "Epoch 654/2048, Avg Training Loss: 0.2883342667329298, Avg Validation Loss: 12.465166071688667\n",
            "Epoch 655/2048, Avg Training Loss: 0.28807762688785044, Avg Validation Loss: 12.451470707989655\n",
            "Epoch 656/2048, Avg Training Loss: 0.2878219650564495, Avg Validation Loss: 12.437816574421094\n",
            "Epoch 657/2048, Avg Training Loss: 0.2875672169944626, Avg Validation Loss: 12.424201300866873\n",
            "Epoch 658/2048, Avg Training Loss: 0.2873133918167318, Avg Validation Loss: 12.410627150366373\n",
            "Epoch 659/2048, Avg Training Loss: 0.28706051661996207, Avg Validation Loss: 12.397095372475643\n",
            "Epoch 660/2048, Avg Training Loss: 0.28680860641516553, Avg Validation Loss: 12.383606673685932\n",
            "Epoch 661/2048, Avg Training Loss: 0.28655766599559857, Avg Validation Loss: 12.370156610000516\n",
            "Epoch 662/2048, Avg Training Loss: 0.2863076013490743, Avg Validation Loss: 12.356744175857411\n",
            "Epoch 663/2048, Avg Training Loss: 0.2860585299567025, Avg Validation Loss: 12.343457476833748\n",
            "Epoch 664/2048, Avg Training Loss: 0.2858110797716111, Avg Validation Loss: 12.330288443136073\n",
            "Epoch 665/2048, Avg Training Loss: 0.28556492069725725, Avg Validation Loss: 12.317201489081468\n",
            "Epoch 666/2048, Avg Training Loss: 0.28531990481848846, Avg Validation Loss: 12.30417752174254\n",
            "Epoch 667/2048, Avg Training Loss: 0.28507595406115194, Avg Validation Loss: 12.291206319687314\n",
            "Epoch 668/2048, Avg Training Loss: 0.28483302737422433, Avg Validation Loss: 12.278282434316852\n",
            "Epoch 669/2048, Avg Training Loss: 0.2845911033378996, Avg Validation Loss: 12.265402984728883\n",
            "Epoch 670/2048, Avg Training Loss: 0.2843501709361312, Avg Validation Loss: 12.252566471017131\n",
            "Epoch 671/2048, Avg Training Loss: 0.28411022465788344, Avg Validation Loss: 12.239772135533153\n",
            "Epoch 672/2048, Avg Training Loss: 0.28387126189560974, Avg Validation Loss: 12.227019619044194\n",
            "Epoch 673/2048, Avg Training Loss: 0.2836332815634928, Avg Validation Loss: 12.214308775620918\n",
            "Epoch 674/2048, Avg Training Loss: 0.2833962820175073, Avg Validation Loss: 12.201637799837199\n",
            "Epoch 675/2048, Avg Training Loss: 0.2831602269961081, Avg Validation Loss: 12.189006258273386\n",
            "Epoch 676/2048, Avg Training Loss: 0.28292514641334215, Avg Validation Loss: 12.176377367418588\n",
            "Epoch 677/2048, Avg Training Loss: 0.2826906588177434, Avg Validation Loss: 12.163742269274152\n",
            "Epoch 678/2048, Avg Training Loss: 0.28245697435596845, Avg Validation Loss: 12.151124337907634\n",
            "Epoch 679/2048, Avg Training Loss: 0.28222414874669755, Avg Validation Loss: 12.138533829804144\n",
            "Epoch 680/2048, Avg Training Loss: 0.2819921552932851, Avg Validation Loss: 12.125978340834854\n",
            "Epoch 681/2048, Avg Training Loss: 0.28176102129976754, Avg Validation Loss: 12.113449537927158\n",
            "Epoch 682/2048, Avg Training Loss: 0.2815307412475138, Avg Validation Loss: 12.101041877097025\n",
            "Epoch 683/2048, Avg Training Loss: 0.28130221804097544, Avg Validation Loss: 12.088781509459663\n",
            "Epoch 684/2048, Avg Training Loss: 0.28107504172582104, Avg Validation Loss: 12.076618799743054\n",
            "Epoch 685/2048, Avg Training Loss: 0.28084904000864247, Avg Validation Loss: 12.0645270591231\n",
            "Epoch 686/2048, Avg Training Loss: 0.28062412147018606, Avg Validation Loss: 12.052491955888081\n",
            "Epoch 687/2048, Avg Training Loss: 0.2804003178650872, Avg Validation Loss: 12.040544692270181\n",
            "Epoch 688/2048, Avg Training Loss: 0.280177700329203, Avg Validation Loss: 12.028668289296794\n",
            "Epoch 689/2048, Avg Training Loss: 0.279956158884234, Avg Validation Loss: 12.016847955044094\n",
            "Epoch 690/2048, Avg Training Loss: 0.27973564598695994, Avg Validation Loss: 12.0050757692746\n",
            "Epoch 691/2048, Avg Training Loss: 0.27951613624120963, Avg Validation Loss: 11.993347514081758\n",
            "Epoch 692/2048, Avg Training Loss: 0.279297616005129, Avg Validation Loss: 11.981660966040037\n",
            "Epoch 693/2048, Avg Training Loss: 0.27908007787177525, Avg Validation Loss: 11.970014975702508\n",
            "Epoch 694/2048, Avg Training Loss: 0.27886351773568846, Avg Validation Loss: 11.958408971390638\n",
            "Epoch 695/2048, Avg Training Loss: 0.2786479332331971, Avg Validation Loss: 11.946842692088355\n",
            "Epoch 696/2048, Avg Training Loss: 0.2784333229129351, Avg Validation Loss: 11.935316042938908\n",
            "Epoch 697/2048, Avg Training Loss: 0.2782196857946345, Avg Validation Loss: 11.923829016766184\n",
            "Epoch 698/2048, Avg Training Loss: 0.27800682470207894, Avg Validation Loss: 11.91232214088727\n",
            "Epoch 699/2048, Avg Training Loss: 0.27779465663356345, Avg Validation Loss: 11.900870601216297\n",
            "Epoch 700/2048, Avg Training Loss: 0.2775839241727069, Avg Validation Loss: 11.889536184249852\n",
            "Epoch 701/2048, Avg Training Loss: 0.27737445801947724, Avg Validation Loss: 11.878282429004624\n",
            "Epoch 702/2048, Avg Training Loss: 0.27716611625922616, Avg Validation Loss: 11.867089730258467\n",
            "Epoch 703/2048, Avg Training Loss: 0.27695882311985576, Avg Validation Loss: 11.85594756474691\n",
            "Epoch 704/2048, Avg Training Loss: 0.276752537942643, Avg Validation Loss: 11.844850303602158\n",
            "Epoch 705/2048, Avg Training Loss: 0.27654720264602983, Avg Validation Loss: 11.83378869103979\n",
            "Epoch 706/2048, Avg Training Loss: 0.2763427830484485, Avg Validation Loss: 11.822745130330496\n",
            "Epoch 707/2048, Avg Training Loss: 0.27613911749019227, Avg Validation Loss: 11.811715597110299\n",
            "Epoch 708/2048, Avg Training Loss: 0.27593631700242316, Avg Validation Loss: 11.800712016359629\n",
            "Epoch 709/2048, Avg Training Loss: 0.27573442732483994, Avg Validation Loss: 11.789740848967552\n",
            "Epoch 710/2048, Avg Training Loss: 0.2755334724031883, Avg Validation Loss: 11.778805609277917\n",
            "Epoch 711/2048, Avg Training Loss: 0.27533346456890995, Avg Validation Loss: 11.76790822213501\n",
            "Epoch 712/2048, Avg Training Loss: 0.2751344099595551, Avg Validation Loss: 11.757049754387058\n",
            "Epoch 713/2048, Avg Training Loss: 0.27493631140370717, Avg Validation Loss: 11.746230809158444\n",
            "Epoch 714/2048, Avg Training Loss: 0.27473916995605835, Avg Validation Loss: 11.735451738395433\n",
            "Epoch 715/2048, Avg Training Loss: 0.2745429857141263, Avg Validation Loss: 11.72471275745318\n",
            "Epoch 716/2048, Avg Training Loss: 0.27434775825275765, Avg Validation Loss: 11.714014006879557\n",
            "Epoch 717/2048, Avg Training Loss: 0.2741534868552971, Avg Validation Loss: 11.703355585734842\n",
            "Epoch 718/2048, Avg Training Loss: 0.27396017063659145, Avg Validation Loss: 11.692737569566342\n",
            "Epoch 719/2048, Avg Training Loss: 0.27376781266499883, Avg Validation Loss: 11.68215245672883\n",
            "Epoch 720/2048, Avg Training Loss: 0.27357633621942834, Avg Validation Loss: 11.671598379763282\n",
            "Epoch 721/2048, Avg Training Loss: 0.2733857476661495, Avg Validation Loss: 11.661144136824216\n",
            "Epoch 722/2048, Avg Training Loss: 0.2731967173540162, Avg Validation Loss: 11.650808420609112\n",
            "Epoch 723/2048, Avg Training Loss: 0.27300890914074666, Avg Validation Loss: 11.640553745035628\n",
            "Epoch 724/2048, Avg Training Loss: 0.2728222396808214, Avg Validation Loss: 11.630360961915782\n",
            "Epoch 725/2048, Avg Training Loss: 0.272636692937972, Avg Validation Loss: 11.62021654403912\n",
            "Epoch 726/2048, Avg Training Loss: 0.27245213245845074, Avg Validation Loss: 11.610178004505801\n",
            "Epoch 727/2048, Avg Training Loss: 0.2722691552389677, Avg Validation Loss: 11.600258523968972\n",
            "Epoch 728/2048, Avg Training Loss: 0.27208741157514416, Avg Validation Loss: 11.590418912451371\n",
            "Epoch 729/2048, Avg Training Loss: 0.27190674934056175, Avg Validation Loss: 11.580638070333915\n",
            "Epoch 730/2048, Avg Training Loss: 0.2717270867148207, Avg Validation Loss: 11.570904656300273\n",
            "Epoch 731/2048, Avg Training Loss: 0.27154837929297404, Avg Validation Loss: 11.561212593995238\n",
            "Epoch 732/2048, Avg Training Loss: 0.2713706016178459, Avg Validation Loss: 11.551559887489788\n",
            "Epoch 733/2048, Avg Training Loss: 0.2711937549162454, Avg Validation Loss: 11.541946055859574\n",
            "Epoch 734/2048, Avg Training Loss: 0.271017824512184, Avg Validation Loss: 11.532369029119621\n",
            "Epoch 735/2048, Avg Training Loss: 0.2708428015041043, Avg Validation Loss: 11.52282773138188\n",
            "Epoch 736/2048, Avg Training Loss: 0.2706686803378069, Avg Validation Loss: 11.513321622199015\n",
            "Epoch 737/2048, Avg Training Loss: 0.2704954572423983, Avg Validation Loss: 11.503850448994374\n",
            "Epoch 738/2048, Avg Training Loss: 0.27032312939550973, Avg Validation Loss: 11.494414113419934\n",
            "Epoch 739/2048, Avg Training Loss: 0.27015169447756504, Avg Validation Loss: 11.485012599210162\n",
            "Epoch 740/2048, Avg Training Loss: 0.2699811504336462, Avg Validation Loss: 11.475645933233036\n",
            "Epoch 741/2048, Avg Training Loss: 0.2698114953461546, Avg Validation Loss: 11.466314164463343\n",
            "Epoch 742/2048, Avg Training Loss: 0.2696427273666153, Avg Validation Loss: 11.457017352632585\n",
            "Epoch 743/2048, Avg Training Loss: 0.2694748446790587, Avg Validation Loss: 11.447755562103982\n",
            "Epoch 744/2048, Avg Training Loss: 0.26930784548026904, Avg Validation Loss: 11.438528858569184\n",
            "Epoch 745/2048, Avg Training Loss: 0.26914172796904823, Avg Validation Loss: 11.42933730726907\n",
            "Epoch 746/2048, Avg Training Loss: 0.26897649034030163, Avg Validation Loss: 11.420180972037857\n",
            "Epoch 747/2048, Avg Training Loss: 0.2688121307817093, Avg Validation Loss: 11.411059914792178\n",
            "Epoch 748/2048, Avg Training Loss: 0.268648647471788, Avg Validation Loss: 11.401974195260697\n",
            "Epoch 749/2048, Avg Training Loss: 0.2684860385787045, Avg Validation Loss: 11.392923870843953\n",
            "Epoch 750/2048, Avg Training Loss: 0.2683243022595005, Avg Validation Loss: 11.383908996544724\n",
            "Epoch 751/2048, Avg Training Loss: 0.26816343665954606, Avg Validation Loss: 11.374929624958996\n",
            "Epoch 752/2048, Avg Training Loss: 0.26800343543879723, Avg Validation Loss: 11.36599422706698\n",
            "Epoch 753/2048, Avg Training Loss: 0.26784438341063654, Avg Validation Loss: 11.357104981083866\n",
            "Epoch 754/2048, Avg Training Loss: 0.26768623379322176, Avg Validation Loss: 11.348256844073504\n",
            "Epoch 755/2048, Avg Training Loss: 0.26752896635206896, Avg Validation Loss: 11.339447113354307\n",
            "Epoch 756/2048, Avg Training Loss: 0.2673725693766896, Avg Validation Loss: 11.330674349095492\n",
            "Epoch 757/2048, Avg Training Loss: 0.26721703569920047, Avg Validation Loss: 11.3219377924899\n",
            "Epoch 758/2048, Avg Training Loss: 0.26706236057039745, Avg Validation Loss: 11.313237051517575\n",
            "Epoch 759/2048, Avg Training Loss: 0.2669085405263449, Avg Validation Loss: 11.304571931218671\n",
            "Epoch 760/2048, Avg Training Loss: 0.26675557278339496, Avg Validation Loss: 11.295942342015746\n",
            "Epoch 761/2048, Avg Training Loss: 0.26660345491519283, Avg Validation Loss: 11.287348250194198\n",
            "Epoch 762/2048, Avg Training Loss: 0.2664521846801832, Avg Validation Loss: 11.278789651156128\n",
            "Epoch 763/2048, Avg Training Loss: 0.2663017599294565, Avg Validation Loss: 11.270266554977182\n",
            "Epoch 764/2048, Avg Training Loss: 0.2661521785574849, Avg Validation Loss: 11.261778978610346\n",
            "Epoch 765/2048, Avg Training Loss: 0.2660034384757648, Avg Validation Loss: 11.253326941681149\n",
            "Epoch 766/2048, Avg Training Loss: 0.26585553759868985, Avg Validation Loss: 11.244910464223631\n",
            "Epoch 767/2048, Avg Training Loss: 0.2657084738359638, Avg Validation Loss: 11.236529565465062\n",
            "Epoch 768/2048, Avg Training Loss: 0.2655622450885089, Avg Validation Loss: 11.228184263177624\n",
            "Epoch 769/2048, Avg Training Loss: 0.26541684924624825, Avg Validation Loss: 11.219874573336515\n",
            "Epoch 770/2048, Avg Training Loss: 0.26527228418689147, Avg Validation Loss: 11.211600509943798\n",
            "Epoch 771/2048, Avg Training Loss: 0.26512854777526534, Avg Validation Loss: 11.203362084941961\n",
            "Epoch 772/2048, Avg Training Loss: 0.2649856378629372, Avg Validation Loss: 11.195159308176038\n",
            "Epoch 773/2048, Avg Training Loss: 0.26484355228800155, Avg Validation Loss: 11.186992187382032\n",
            "Epoch 774/2048, Avg Training Loss: 0.26470228887495817, Avg Validation Loss: 11.178860728189724\n",
            "Epoch 775/2048, Avg Training Loss: 0.26456184543464395, Avg Validation Loss: 11.170764934133222\n",
            "Epoch 776/2048, Avg Training Loss: 0.2644222405590805, Avg Validation Loss: 11.162705645819054\n",
            "Epoch 777/2048, Avg Training Loss: 0.2642835376481462, Avg Validation Loss: 11.154682560317205\n",
            "Epoch 778/2048, Avg Training Loss: 0.2641456982698377, Avg Validation Loss: 11.146695413664094\n",
            "Epoch 779/2048, Avg Training Loss: 0.2640086943919118, Avg Validation Loss: 11.138744075921315\n",
            "Epoch 780/2048, Avg Training Loss: 0.2638725100973177, Avg Validation Loss: 11.130828481386784\n",
            "Epoch 781/2048, Avg Training Loss: 0.26373712393308313, Avg Validation Loss: 11.122948033599908\n",
            "Epoch 782/2048, Avg Training Loss: 0.26360246831976136, Avg Validation Loss: 11.115102790014738\n",
            "Epoch 783/2048, Avg Training Loss: 0.2634685574342231, Avg Validation Loss: 11.107292805207393\n",
            "Epoch 784/2048, Avg Training Loss: 0.26333538114116495, Avg Validation Loss: 11.099517718757943\n",
            "Epoch 785/2048, Avg Training Loss: 0.2632029509490221, Avg Validation Loss: 11.0917775498667\n",
            "Epoch 786/2048, Avg Training Loss: 0.2630712881821442, Avg Validation Loss: 11.08407259772349\n",
            "Epoch 787/2048, Avg Training Loss: 0.2629404040502391, Avg Validation Loss: 11.076403005193189\n",
            "Epoch 788/2048, Avg Training Loss: 0.2628103033917061, Avg Validation Loss: 11.068768834120322\n",
            "Epoch 789/2048, Avg Training Loss: 0.262680987655442, Avg Validation Loss: 11.06117010402469\n",
            "Epoch 790/2048, Avg Training Loss: 0.2625524564843151, Avg Validation Loss: 11.053605288798279\n",
            "Epoch 791/2048, Avg Training Loss: 0.26242460988893623, Avg Validation Loss: 11.046044111275032\n",
            "Epoch 792/2048, Avg Training Loss: 0.2622971606582521, Avg Validation Loss: 11.038491755003442\n",
            "Epoch 793/2048, Avg Training Loss: 0.26217025093828805, Avg Validation Loss: 11.030961018409052\n",
            "Epoch 794/2048, Avg Training Loss: 0.26204399696758696, Avg Validation Loss: 11.02345872213058\n",
            "Epoch 795/2048, Avg Training Loss: 0.2619184593954691, Avg Validation Loss: 11.015988491162695\n",
            "Epoch 796/2048, Avg Training Loss: 0.26179371231802656, Avg Validation Loss: 11.008614859506025\n",
            "Epoch 797/2048, Avg Training Loss: 0.26167012290681585, Avg Validation Loss: 11.00133166722465\n",
            "Epoch 798/2048, Avg Training Loss: 0.26154746581521854, Avg Validation Loss: 10.994112466953823\n",
            "Epoch 799/2048, Avg Training Loss: 0.26142566190433253, Avg Validation Loss: 10.986942377929498\n",
            "Epoch 800/2048, Avg Training Loss: 0.2613046546490433, Avg Validation Loss: 10.979813097499768\n",
            "Epoch 801/2048, Avg Training Loss: 0.26118442195646696, Avg Validation Loss: 10.972720990973446\n",
            "Epoch 802/2048, Avg Training Loss: 0.2610649545473659, Avg Validation Loss: 10.965664084118075\n",
            "Epoch 803/2048, Avg Training Loss: 0.26094624621217694, Avg Validation Loss: 10.958641302583743\n",
            "Epoch 804/2048, Avg Training Loss: 0.260828292388465, Avg Validation Loss: 10.951652058683623\n",
            "Epoch 805/2048, Avg Training Loss: 0.26071108939671306, Avg Validation Loss: 10.94469602752261\n",
            "Epoch 806/2048, Avg Training Loss: 0.26059463402954236, Avg Validation Loss: 10.937773025707791\n",
            "Epoch 807/2048, Avg Training Loss: 0.2604789233308272, Avg Validation Loss: 10.93088294563792\n",
            "Epoch 808/2048, Avg Training Loss: 0.2603639544768857, Avg Validation Loss: 10.924025719907211\n",
            "Epoch 809/2048, Avg Training Loss: 0.26024972471256164, Avg Validation Loss: 10.91720130202664\n",
            "Epoch 810/2048, Avg Training Loss: 0.2601362313168278, Avg Validation Loss: 10.910409655987241\n",
            "Epoch 811/2048, Avg Training Loss: 0.26002349031571637, Avg Validation Loss: 10.903650882607105\n",
            "Epoch 812/2048, Avg Training Loss: 0.2599115384198311, Avg Validation Loss: 10.896924885619587\n",
            "Epoch 813/2048, Avg Training Loss: 0.2598003484272685, Avg Validation Loss: 10.89023160153658\n",
            "Epoch 814/2048, Avg Training Loss: 0.2596899003144949, Avg Validation Loss: 10.883570995259781\n",
            "Epoch 815/2048, Avg Training Loss: 0.2595801822055966, Avg Validation Loss: 10.876943039415593\n",
            "Epoch 816/2048, Avg Training Loss: 0.25947118313010126, Avg Validation Loss: 10.870347772725397\n",
            "Epoch 817/2048, Avg Training Loss: 0.2593628865522162, Avg Validation Loss: 10.863785163479884\n",
            "Epoch 818/2048, Avg Training Loss: 0.2592552933404713, Avg Validation Loss: 10.857255148683736\n",
            "Epoch 819/2048, Avg Training Loss: 0.25914840499474984, Avg Validation Loss: 10.850757672094325\n",
            "Epoch 820/2048, Avg Training Loss: 0.25904222103752367, Avg Validation Loss: 10.844292681819233\n",
            "Epoch 821/2048, Avg Training Loss: 0.25893673993958966, Avg Validation Loss: 10.837860128582268\n",
            "Epoch 822/2048, Avg Training Loss: 0.25883195961114, Avg Validation Loss: 10.831459964238311\n",
            "Epoch 823/2048, Avg Training Loss: 0.25872787766227046, Avg Validation Loss: 10.825092140765411\n",
            "Epoch 824/2048, Avg Training Loss: 0.25862449154119227, Avg Validation Loss: 10.818756609642344\n",
            "Epoch 825/2048, Avg Training Loss: 0.2585217986075798, Avg Validation Loss: 10.812453321464032\n",
            "Epoch 826/2048, Avg Training Loss: 0.2584197961715155, Avg Validation Loss: 10.806182225712776\n",
            "Epoch 827/2048, Avg Training Loss: 0.2583184815141902, Avg Validation Loss: 10.79994327062822\n",
            "Epoch 828/2048, Avg Training Loss: 0.2582178518989292, Avg Validation Loss: 10.793736403194558\n",
            "Epoch 829/2048, Avg Training Loss: 0.2581179045770894, Avg Validation Loss: 10.78756156977305\n",
            "Epoch 830/2048, Avg Training Loss: 0.25801863679123715, Avg Validation Loss: 10.781418714829496\n",
            "Epoch 831/2048, Avg Training Loss: 0.25792004577688776, Avg Validation Loss: 10.775307781227928\n",
            "Epoch 832/2048, Avg Training Loss: 0.2578221287634808, Avg Validation Loss: 10.769228710493959\n",
            "Epoch 833/2048, Avg Training Loss: 0.2577248829749545, Avg Validation Loss: 10.763181442830374\n",
            "Epoch 834/2048, Avg Training Loss: 0.257628305630107, Avg Validation Loss: 10.757165917135227\n",
            "Epoch 835/2048, Avg Training Loss: 0.2575323939428463, Avg Validation Loss: 10.751182071021384\n",
            "Epoch 836/2048, Avg Training Loss: 0.2574371451223832, Avg Validation Loss: 10.74522984083676\n",
            "Epoch 837/2048, Avg Training Loss: 0.25734255637339365, Avg Validation Loss: 10.739309161684988\n",
            "Epoch 838/2048, Avg Training Loss: 0.2572486248961679, Avg Validation Loss: 10.733419967446217\n",
            "Epoch 839/2048, Avg Training Loss: 0.2571553478867514, Avg Validation Loss: 10.727562190797947\n",
            "Epoch 840/2048, Avg Training Loss: 0.25706272253708423, Avg Validation Loss: 10.721735763235841\n",
            "Epoch 841/2048, Avg Training Loss: 0.2569707460351397, Avg Validation Loss: 10.715940615094434\n",
            "Epoch 842/2048, Avg Training Loss: 0.25687941556506266, Avg Validation Loss: 10.710176675567725\n",
            "Epoch 843/2048, Avg Training Loss: 0.25678872830731014, Avg Validation Loss: 10.704443872729687\n",
            "Epoch 844/2048, Avg Training Loss: 0.25669868143879226, Avg Validation Loss: 10.69874213355461\n",
            "Epoch 845/2048, Avg Training Loss: 0.2566092721330147, Avg Validation Loss: 10.693071383937301\n",
            "Epoch 846/2048, Avg Training Loss: 0.2565204840725313, Avg Validation Loss: 10.687425191332826\n",
            "Epoch 847/2048, Avg Training Loss: 0.2564320499918646, Avg Validation Loss: 10.681793581283584\n",
            "Epoch 848/2048, Avg Training Loss: 0.25634394395857824, Avg Validation Loss: 10.676183920149457\n",
            "Epoch 849/2048, Avg Training Loss: 0.25625630725980514, Avg Validation Loss: 10.670600501466836\n",
            "Epoch 850/2048, Avg Training Loss: 0.25616921619961047, Avg Validation Loss: 10.66504556620574\n",
            "Epoch 851/2048, Avg Training Loss: 0.2560827099976606, Avg Validation Loss: 10.659520264489052\n",
            "Epoch 852/2048, Avg Training Loss: 0.2559968081399231, Avg Validation Loss: 10.654025166560615\n",
            "Epoch 853/2048, Avg Training Loss: 0.25591151961165387, Avg Validation Loss: 10.648560534239826\n",
            "Epoch 854/2048, Avg Training Loss: 0.25582684781033904, Avg Validation Loss: 10.643126465127382\n",
            "Epoch 855/2048, Avg Training Loss: 0.2557427931598558, Avg Validation Loss: 10.637722969212588\n",
            "Epoch 856/2048, Avg Training Loss: 0.2556593545014725, Avg Validation Loss: 10.632350009572363\n",
            "Epoch 857/2048, Avg Training Loss: 0.2555765298340316, Avg Validation Loss: 10.627007523996628\n",
            "Epoch 858/2048, Avg Training Loss: 0.25549431670785033, Avg Validation Loss: 10.621695436482431\n",
            "Epoch 859/2048, Avg Training Loss: 0.2554127089928743, Avg Validation Loss: 10.616412798384113\n",
            "Epoch 860/2048, Avg Training Loss: 0.2553317035673593, Avg Validation Loss: 10.611159922541665\n",
            "Epoch 861/2048, Avg Training Loss: 0.2552513000606949, Avg Validation Loss: 10.605936945856353\n",
            "Epoch 862/2048, Avg Training Loss: 0.2551714960994105, Avg Validation Loss: 10.600743898645876\n",
            "Epoch 863/2048, Avg Training Loss: 0.2550922890390029, Avg Validation Loss: 10.595580753272916\n",
            "Epoch 864/2048, Avg Training Loss: 0.2550136760900539, Avg Validation Loss: 10.590447450451235\n",
            "Epoch 865/2048, Avg Training Loss: 0.254935654385765, Avg Validation Loss: 10.585343913480616\n",
            "Epoch 866/2048, Avg Training Loss: 0.25485822101815586, Avg Validation Loss: 10.580270055953637\n",
            "Epoch 867/2048, Avg Training Loss: 0.25478137305750204, Avg Validation Loss: 10.575225785931703\n",
            "Epoch 868/2048, Avg Training Loss: 0.25470510756280407, Avg Validation Loss: 10.57021100821134\n",
            "Epoch 869/2048, Avg Training Loss: 0.25462941758219104, Avg Validation Loss: 10.565225963615788\n",
            "Epoch 870/2048, Avg Training Loss: 0.25455426842987233, Avg Validation Loss: 10.560271378769324\n",
            "Epoch 871/2048, Avg Training Loss: 0.2544796237017212, Avg Validation Loss: 10.55534659711818\n",
            "Epoch 872/2048, Avg Training Loss: 0.2544055150302908, Avg Validation Loss: 10.550452588736949\n",
            "Epoch 873/2048, Avg Training Loss: 0.25433200404524553, Avg Validation Loss: 10.545604022877095\n",
            "Epoch 874/2048, Avg Training Loss: 0.25425910455171996, Avg Validation Loss: 10.540794915180749\n",
            "Epoch 875/2048, Avg Training Loss: 0.25418678357374463, Avg Validation Loss: 10.536019740522935\n",
            "Epoch 876/2048, Avg Training Loss: 0.2541150285848303, Avg Validation Loss: 10.531275475109222\n",
            "Epoch 877/2048, Avg Training Loss: 0.25404383153373994, Avg Validation Loss: 10.526560447549231\n",
            "Epoch 878/2048, Avg Training Loss: 0.2539731867707641, Avg Validation Loss: 10.521873717522313\n",
            "Epoch 879/2048, Avg Training Loss: 0.25390308993607125, Avg Validation Loss: 10.517214739742029\n",
            "Epoch 880/2048, Avg Training Loss: 0.2538335373633313, Avg Validation Loss: 10.512583182210893\n",
            "Epoch 881/2048, Avg Training Loss: 0.25376452575963415, Avg Validation Loss: 10.50797882792164\n",
            "Epoch 882/2048, Avg Training Loss: 0.25369605203366913, Avg Validation Loss: 10.503401521692677\n",
            "Epoch 883/2048, Avg Training Loss: 0.2536281132034934, Avg Validation Loss: 10.498851141416841\n",
            "Epoch 884/2048, Avg Training Loss: 0.25356070634703987, Avg Validation Loss: 10.494327582542375\n",
            "Epoch 885/2048, Avg Training Loss: 0.25349382857558095, Avg Validation Loss: 10.489830749760825\n",
            "Epoch 886/2048, Avg Training Loss: 0.25342747701952334, Avg Validation Loss: 10.485360552230794\n",
            "Epoch 887/2048, Avg Training Loss: 0.2533616488208304, Avg Validation Loss: 10.480916901235007\n",
            "Epoch 888/2048, Avg Training Loss: 0.2532963411290037, Avg Validation Loss: 10.476499708887872\n",
            "Epoch 889/2048, Avg Training Loss: 0.25323155109897866, Avg Validation Loss: 10.472108887432949\n",
            "Epoch 890/2048, Avg Training Loss: 0.2531672758900491, Avg Validation Loss: 10.4677443488687\n",
            "Epoch 891/2048, Avg Training Loss: 0.25310351266534437, Avg Validation Loss: 10.46340600474441\n",
            "Epoch 892/2048, Avg Training Loss: 0.2530402585916038, Avg Validation Loss: 10.45909376608679\n",
            "Epoch 893/2048, Avg Training Loss: 0.2529775108391109, Avg Validation Loss: 10.454807543349578\n",
            "Epoch 894/2048, Avg Training Loss: 0.2529152665817136, Avg Validation Loss: 10.450547246396425\n",
            "Epoch 895/2048, Avg Training Loss: 0.2528535229968899, Avg Validation Loss: 10.446312784500776\n",
            "Epoch 896/2048, Avg Training Loss: 0.252792277265839, Avg Validation Loss: 10.442104066353853\n",
            "Epoch 897/2048, Avg Training Loss: 0.25273152657358455, Avg Validation Loss: 10.437921000076866\n",
            "Epoch 898/2048, Avg Training Loss: 0.2526712681090855, Avg Validation Loss: 10.433763493235523\n",
            "Epoch 899/2048, Avg Training Loss: 0.25261149906534985, Avg Validation Loss: 10.429631452855565\n",
            "Epoch 900/2048, Avg Training Loss: 0.2525522166395503, Avg Validation Loss: 10.425524785438848\n",
            "Epoch 901/2048, Avg Training Loss: 0.2524934661894912, Avg Validation Loss: 10.42137496957958\n",
            "Epoch 902/2048, Avg Training Loss: 0.2524346823669065, Avg Validation Loss: 10.417164757207418\n",
            "Epoch 903/2048, Avg Training Loss: 0.2523762066389377, Avg Validation Loss: 10.412935669589016\n",
            "Epoch 904/2048, Avg Training Loss: 0.25231812671956017, Avg Validation Loss: 10.408710111602192\n",
            "Epoch 905/2048, Avg Training Loss: 0.2522604882435754, Avg Validation Loss: 10.404500129156268\n",
            "Epoch 906/2048, Avg Training Loss: 0.25220331420375985, Avg Validation Loss: 10.400312157059659\n",
            "Epoch 907/2048, Avg Training Loss: 0.2521466154453516, Avg Validation Loss: 10.396149590782777\n",
            "Epoch 908/2048, Avg Training Loss: 0.25209039630864255, Avg Validation Loss: 10.392014180549099\n",
            "Epoch 909/2048, Avg Training Loss: 0.25203465765606126, Avg Validation Loss: 10.387906786241416\n",
            "Epoch 910/2048, Avg Training Loss: 0.2519793984941691, Avg Validation Loss: 10.383827786247084\n",
            "Epoch 911/2048, Avg Training Loss: 0.25192461684221973, Avg Validation Loss: 10.379777298881862\n",
            "Epoch 912/2048, Avg Training Loss: 0.25187031019710343, Avg Validation Loss: 10.375755302304848\n",
            "Epoch 913/2048, Avg Training Loss: 0.251816468742045, Avg Validation Loss: 10.371764061273653\n",
            "Epoch 914/2048, Avg Training Loss: 0.251763042481306, Avg Validation Loss: 10.367802654520112\n",
            "Epoch 915/2048, Avg Training Loss: 0.25171004591751106, Avg Validation Loss: 10.363870123999638\n",
            "Epoch 916/2048, Avg Training Loss: 0.2516574947832614, Avg Validation Loss: 10.359965852480396\n",
            "Epoch 917/2048, Avg Training Loss: 0.2516053959934679, Avg Validation Loss: 10.356089416855546\n",
            "Epoch 918/2048, Avg Training Loss: 0.25155375178641765, Avg Validation Loss: 10.35224050451713\n",
            "Epoch 919/2048, Avg Training Loss: 0.25150256192296894, Avg Validation Loss: 10.348418865813551\n",
            "Epoch 920/2048, Avg Training Loss: 0.2514518248543177, Avg Validation Loss: 10.34462428709013\n",
            "Epoch 921/2048, Avg Training Loss: 0.25140153834202167, Avg Validation Loss: 10.34085657543642\n",
            "Epoch 922/2048, Avg Training Loss: 0.2513516997871771, Avg Validation Loss: 10.337115550073637\n",
            "Epoch 923/2048, Avg Training Loss: 0.25130230640516976, Avg Validation Loss: 10.333401037499037\n",
            "Epoch 924/2048, Avg Training Loss: 0.2512533553184419, Avg Validation Loss: 10.329712868751008\n",
            "Epoch 925/2048, Avg Training Loss: 0.2512048436057327, Avg Validation Loss: 10.326050877868996\n",
            "Epoch 926/2048, Avg Training Loss: 0.2511567683282141, Avg Validation Loss: 10.322414901025175\n",
            "Epoch 927/2048, Avg Training Loss: 0.2511091265433615, Avg Validation Loss: 10.318804776033321\n",
            "Epoch 928/2048, Avg Training Loss: 0.2510619153123149, Avg Validation Loss: 10.315220342069107\n",
            "Epoch 929/2048, Avg Training Loss: 0.2510151317037848, Avg Validation Loss: 10.311661439508834\n",
            "Epoch 930/2048, Avg Training Loss: 0.25096877279612534, Avg Validation Loss: 10.308127909834475\n",
            "Epoch 931/2048, Avg Training Loss: 0.2509228356784335, Avg Validation Loss: 10.304619595575769\n",
            "Epoch 932/2048, Avg Training Loss: 0.2508773174511349, Avg Validation Loss: 10.301136340273159\n",
            "Epoch 933/2048, Avg Training Loss: 0.2508322152262934, Avg Validation Loss: 10.297677988452405\n",
            "Epoch 934/2048, Avg Training Loss: 0.25078752612777866, Avg Validation Loss: 10.294244385605818\n",
            "Epoch 935/2048, Avg Training Loss: 0.25074324729135516, Avg Validation Loss: 10.29083537817728\n",
            "Epoch 936/2048, Avg Training Loss: 0.2506993758647328, Avg Validation Loss: 10.287450813549466\n",
            "Epoch 937/2048, Avg Training Loss: 0.25065590900759493, Avg Validation Loss: 10.284090540032413\n",
            "Epoch 938/2048, Avg Training Loss: 0.2506128438916162, Avg Validation Loss: 10.280754406852903\n",
            "Epoch 939/2048, Avg Training Loss: 0.25057017400382664, Avg Validation Loss: 10.277443559607025\n",
            "Epoch 940/2048, Avg Training Loss: 0.25052787414421057, Avg Validation Loss: 10.274157230233126\n",
            "Epoch 941/2048, Avg Training Loss: 0.25048595434750504, Avg Validation Loss: 10.270894924060913\n",
            "Epoch 942/2048, Avg Training Loss: 0.25044439510452826, Avg Validation Loss: 10.267595028233568\n",
            "Epoch 943/2048, Avg Training Loss: 0.25040294906642657, Avg Validation Loss: 10.264264479368151\n",
            "Epoch 944/2048, Avg Training Loss: 0.25036180823351545, Avg Validation Loss: 10.260929633112735\n",
            "Epoch 945/2048, Avg Training Loss: 0.2503210376231647, Avg Validation Loss: 10.257665768759765\n",
            "Epoch 946/2048, Avg Training Loss: 0.2502809027242291, Avg Validation Loss: 10.254473279758644\n",
            "Epoch 947/2048, Avg Training Loss: 0.2502411996841564, Avg Validation Loss: 10.251268664742929\n",
            "Epoch 948/2048, Avg Training Loss: 0.25020166973809044, Avg Validation Loss: 10.248107819715445\n",
            "Epoch 949/2048, Avg Training Loss: 0.2501627004101165, Avg Validation Loss: 10.24494241640738\n",
            "Epoch 950/2048, Avg Training Loss: 0.2501238903466989, Avg Validation Loss: 10.241763787505205\n",
            "Epoch 951/2048, Avg Training Loss: 0.25008540148274955, Avg Validation Loss: 10.23858971443788\n",
            "Epoch 952/2048, Avg Training Loss: 0.2500472806332143, Avg Validation Loss: 10.235490500395015\n",
            "Epoch 953/2048, Avg Training Loss: 0.25000977684548154, Avg Validation Loss: 10.232464076757346\n",
            "Epoch 954/2048, Avg Training Loss: 0.24997268645067092, Avg Validation Loss: 10.22942607146331\n",
            "Epoch 955/2048, Avg Training Loss: 0.24993573583989312, Avg Validation Loss: 10.226370719535534\n",
            "Epoch 956/2048, Avg Training Loss: 0.2498990898972113, Avg Validation Loss: 10.223317289562486\n",
            "Epoch 957/2048, Avg Training Loss: 0.24986277498143053, Avg Validation Loss: 10.220276127429289\n",
            "Epoch 958/2048, Avg Training Loss: 0.24982680389264353, Avg Validation Loss: 10.217252745711216\n",
            "Epoch 959/2048, Avg Training Loss: 0.24979120243582495, Avg Validation Loss: 10.214310469318983\n",
            "Epoch 960/2048, Avg Training Loss: 0.24975618162503405, Avg Validation Loss: 10.211383290546681\n",
            "Epoch 961/2048, Avg Training Loss: 0.24972132834175076, Avg Validation Loss: 10.208452818113\n",
            "Epoch 962/2048, Avg Training Loss: 0.24968678745530481, Avg Validation Loss: 10.205531374232415\n",
            "Epoch 963/2048, Avg Training Loss: 0.24965257396759993, Avg Validation Loss: 10.20262554663487\n",
            "Epoch 964/2048, Avg Training Loss: 0.24961869463333322, Avg Validation Loss: 10.19973881720251\n",
            "Epoch 965/2048, Avg Training Loss: 0.24958515179049479, Avg Validation Loss: 10.196872986221498\n",
            "Epoch 966/2048, Avg Training Loss: 0.24955194541859504, Avg Validation Loss: 10.194028943952171\n",
            "Epoch 967/2048, Avg Training Loss: 0.2495190742425057, Avg Validation Loss: 10.191207088582205\n",
            "Epoch 968/2048, Avg Training Loss: 0.2494865363238354, Avg Validation Loss: 10.18840755261608\n",
            "Epoch 969/2048, Avg Training Loss: 0.24945432937755943, Avg Validation Loss: 10.185630325494909\n",
            "Epoch 970/2048, Avg Training Loss: 0.24942244997017252, Avg Validation Loss: 10.182876394460559\n",
            "Epoch 971/2048, Avg Training Loss: 0.24939090402485176, Avg Validation Loss: 10.180146699356897\n",
            "Epoch 972/2048, Avg Training Loss: 0.24935968318359783, Avg Validation Loss: 10.177440078344295\n",
            "Epoch 973/2048, Avg Training Loss: 0.24932878388451624, Avg Validation Loss: 10.174755823709544\n",
            "Epoch 974/2048, Avg Training Loss: 0.2492982030157546, Avg Validation Loss: 10.172093474369374\n",
            "Epoch 975/2048, Avg Training Loss: 0.24926793771488073, Avg Validation Loss: 10.169452703557045\n",
            "Epoch 976/2048, Avg Training Loss: 0.2492379852612118, Avg Validation Loss: 10.166833257985006\n",
            "Epoch 977/2048, Avg Training Loss: 0.2492083430179824, Avg Validation Loss: 10.164234924897734\n",
            "Epoch 978/2048, Avg Training Loss: 0.24917900411129085, Avg Validation Loss: 10.161653803023453\n",
            "Epoch 979/2048, Avg Training Loss: 0.24915002123838934, Avg Validation Loss: 10.15909150130964\n",
            "Epoch 980/2048, Avg Training Loss: 0.2491213667720253, Avg Validation Loss: 10.15654884591018\n",
            "Epoch 981/2048, Avg Training Loss: 0.2490930248873622, Avg Validation Loss: 10.154026224349655\n",
            "Epoch 982/2048, Avg Training Loss: 0.2490649845909676, Avg Validation Loss: 10.151525040582616\n",
            "Epoch 983/2048, Avg Training Loss: 0.24903723638318584, Avg Validation Loss: 10.149044710493321\n",
            "Epoch 984/2048, Avg Training Loss: 0.24900977826120155, Avg Validation Loss: 10.146584842105373\n",
            "Epoch 985/2048, Avg Training Loss: 0.24898260803564012, Avg Validation Loss: 10.144145148786976\n",
            "Epoch 986/2048, Avg Training Loss: 0.24895572342794198, Avg Validation Loss: 10.141725401990286\n",
            "Epoch 987/2048, Avg Training Loss: 0.2489291221209712, Avg Validation Loss: 10.139325405514558\n",
            "Epoch 988/2048, Avg Training Loss: 0.2489028017852927, Avg Validation Loss: 10.136944981487032\n",
            "Epoch 989/2048, Avg Training Loss: 0.24887676009278012, Avg Validation Loss: 10.134583962722484\n",
            "Epoch 990/2048, Avg Training Loss: 0.2488509947236312, Avg Validation Loss: 10.132242188554752\n",
            "Epoch 991/2048, Avg Training Loss: 0.2488255033699541, Avg Validation Loss: 10.129919502557838\n",
            "Epoch 992/2048, Avg Training Loss: 0.24880028373757376, Avg Validation Loss: 10.127615751295409\n",
            "Epoch 993/2048, Avg Training Loss: 0.24877533354691467, Avg Validation Loss: 10.125330783629966\n",
            "Epoch 994/2048, Avg Training Loss: 0.24875065053340453, Avg Validation Loss: 10.123064450336733\n",
            "Epoch 995/2048, Avg Training Loss: 0.24872623244763212, Avg Validation Loss: 10.120816603883442\n",
            "Epoch 996/2048, Avg Training Loss: 0.24870207705537878, Avg Validation Loss: 10.118587098300615\n",
            "Epoch 997/2048, Avg Training Loss: 0.24867818213758497, Avg Validation Loss: 10.116375789101225\n",
            "Epoch 998/2048, Avg Training Loss: 0.24865454549028576, Avg Validation Loss: 10.114182533227451\n",
            "Epoch 999/2048, Avg Training Loss: 0.24863116492453083, Avg Validation Loss: 10.11200718901237\n",
            "Epoch 1000/2048, Avg Training Loss: 0.2486080382662973, Avg Validation Loss: 10.10984961614995\n",
            "Epoch 1001/2048, Avg Training Loss: 0.24858516335640077, Avg Validation Loss: 10.107709675669787\n",
            "Epoch 1002/2048, Avg Training Loss: 0.24856253805040657, Avg Validation Loss: 10.10558722991461\n",
            "Epoch 1003/2048, Avg Training Loss: 0.2485401602185411, Avg Validation Loss: 10.103482142519464\n",
            "Epoch 1004/2048, Avg Training Loss: 0.24851813178649287, Avg Validation Loss: 10.101339105322525\n",
            "Epoch 1005/2048, Avg Training Loss: 0.24849639873303367, Avg Validation Loss: 10.099159510453415\n",
            "Epoch 1006/2048, Avg Training Loss: 0.24847520200853176, Avg Validation Loss: 10.097050682645326\n",
            "Epoch 1007/2048, Avg Training Loss: 0.24845403991212717, Avg Validation Loss: 10.09498789013978\n",
            "Epoch 1008/2048, Avg Training Loss: 0.24843300591559553, Avg Validation Loss: 10.092957605413403\n",
            "Epoch 1009/2048, Avg Training Loss: 0.24841214880016932, Avg Validation Loss: 10.090952404286597\n",
            "Epoch 1010/2048, Avg Training Loss: 0.24839149354566534, Avg Validation Loss: 10.088968187633034\n",
            "Epoch 1011/2048, Avg Training Loss: 0.24837105248252417, Avg Validation Loss: 10.08700266795889\n",
            "Epoch 1012/2048, Avg Training Loss: 0.24835083122871976, Avg Validation Loss: 10.085054544977977\n",
            "Epoch 1013/2048, Avg Training Loss: 0.24833093636057968, Avg Validation Loss: 10.08306822992617\n",
            "Epoch 1014/2048, Avg Training Loss: 0.24831133982166775, Avg Validation Loss: 10.081044763105744\n",
            "Epoch 1015/2048, Avg Training Loss: 0.24829223843792916, Avg Validation Loss: 10.079109886136731\n",
            "Epoch 1016/2048, Avg Training Loss: 0.24827323501948115, Avg Validation Loss: 10.077243772768757\n",
            "Epoch 1017/2048, Avg Training Loss: 0.24825434391111842, Avg Validation Loss: 10.07542123032933\n",
            "Epoch 1018/2048, Avg Training Loss: 0.2482356106274218, Avg Validation Loss: 10.073628530770478\n",
            "Epoch 1019/2048, Avg Training Loss: 0.24821705847911277, Avg Validation Loss: 10.071858176300378\n",
            "Epoch 1020/2048, Avg Training Loss: 0.2481987062944076, Avg Validation Loss: 10.07009821135618\n",
            "Epoch 1021/2048, Avg Training Loss: 0.24818050656192678, Avg Validation Loss: 10.068344332105593\n",
            "Epoch 1022/2048, Avg Training Loss: 0.248162602814234, Avg Validation Loss: 10.066545438603066\n",
            "Epoch 1023/2048, Avg Training Loss: 0.24814499316462005, Avg Validation Loss: 10.06470493227551\n",
            "Epoch 1024/2048, Avg Training Loss: 0.24812787155206398, Avg Validation Loss: 10.062931185900174\n",
            "Epoch 1025/2048, Avg Training Loss: 0.2481107238794378, Avg Validation Loss: 10.061200021519303\n",
            "Epoch 1026/2048, Avg Training Loss: 0.24809365354870952, Avg Validation Loss: 10.059498051272271\n",
            "Epoch 1027/2048, Avg Training Loss: 0.24807671575572396, Avg Validation Loss: 10.057817938595397\n",
            "Epoch 1028/2048, Avg Training Loss: 0.24805993901367668, Avg Validation Loss: 10.056155646352064\n",
            "Epoch 1029/2048, Avg Training Loss: 0.24804333765995354, Avg Validation Loss: 10.054508936394722\n",
            "Epoch 1030/2048, Avg Training Loss: 0.24802691851062758, Avg Validation Loss: 10.052876551459763\n",
            "Epoch 1031/2048, Avg Training Loss: 0.24801078974758353, Avg Validation Loss: 10.051203510864383\n",
            "Epoch 1032/2048, Avg Training Loss: 0.24799496381686736, Avg Validation Loss: 10.049490862756695\n",
            "Epoch 1033/2048, Avg Training Loss: 0.2479796109659903, Avg Validation Loss: 10.047844042315463\n",
            "Epoch 1034/2048, Avg Training Loss: 0.24796420860722668, Avg Validation Loss: 10.04623869008699\n",
            "Epoch 1035/2048, Avg Training Loss: 0.24794886488182494, Avg Validation Loss: 10.044661492393125\n",
            "Epoch 1036/2048, Avg Training Loss: 0.24793363653705916, Avg Validation Loss: 10.043105157103826\n",
            "Epoch 1037/2048, Avg Training Loss: 0.24791855296346724, Avg Validation Loss: 10.041565675031098\n",
            "Epoch 1038/2048, Avg Training Loss: 0.24790362901695634, Avg Validation Loss: 10.040040826828331\n",
            "Epoch 1039/2048, Avg Training Loss: 0.24788897754808217, Avg Validation Loss: 10.038475341206293\n",
            "Epoch 1040/2048, Avg Training Loss: 0.24787463008166252, Avg Validation Loss: 10.036869975428708\n",
            "Epoch 1041/2048, Avg Training Loss: 0.24786073919050755, Avg Validation Loss: 10.035329572508612\n",
            "Epoch 1042/2048, Avg Training Loss: 0.2478467772049899, Avg Validation Loss: 10.033829801056049\n",
            "Epoch 1043/2048, Avg Training Loss: 0.2478328562982851, Avg Validation Loss: 10.032357363361307\n",
            "Epoch 1044/2048, Avg Training Loss: 0.2478190354068234, Avg Validation Loss: 10.030904977947548\n",
            "Epoch 1045/2048, Avg Training Loss: 0.2478053451298048, Avg Validation Loss: 10.029468643869572\n",
            "Epoch 1046/2048, Avg Training Loss: 0.2477919070193175, Avg Validation Loss: 10.027992314045411\n",
            "Epoch 1047/2048, Avg Training Loss: 0.2477787713446091, Avg Validation Loss: 10.026476211611232\n",
            "Epoch 1048/2048, Avg Training Loss: 0.24776607696251474, Avg Validation Loss: 10.025024525490096\n",
            "Epoch 1049/2048, Avg Training Loss: 0.24775329260723108, Avg Validation Loss: 10.02361286125439\n",
            "Epoch 1050/2048, Avg Training Loss: 0.24774053425273354, Avg Validation Loss: 10.022227887830084\n",
            "Epoch 1051/2048, Avg Training Loss: 0.24772786289544754, Avg Validation Loss: 10.020862307325665\n",
            "Epoch 1052/2048, Avg Training Loss: 0.24771531026803165, Avg Validation Loss: 10.019512112022335\n",
            "Epoch 1053/2048, Avg Training Loss: 0.24770289255430572, Avg Validation Loss: 10.01817508842558\n",
            "Epoch 1054/2048, Avg Training Loss: 0.2476907240395779, Avg Validation Loss: 10.016796368794006\n",
            "Epoch 1055/2048, Avg Training Loss: 0.2476788664419607, Avg Validation Loss: 10.015376702623678\n",
            "Epoch 1056/2048, Avg Training Loss: 0.2476674390383145, Avg Validation Loss: 10.014020184674353\n",
            "Epoch 1057/2048, Avg Training Loss: 0.24765590414647778, Avg Validation Loss: 10.012702676517186\n",
            "Epoch 1058/2048, Avg Training Loss: 0.24764438054534738, Avg Validation Loss: 10.011410987236196\n",
            "Epoch 1059/2048, Avg Training Loss: 0.2476329307636001, Avg Validation Loss: 10.010137896851608\n",
            "Epoch 1060/2048, Avg Training Loss: 0.2476215873901333, Avg Validation Loss: 10.008879442087007\n",
            "Epoch 1061/2048, Avg Training Loss: 0.24761047375093032, Avg Validation Loss: 10.007579973024198\n",
            "Epoch 1062/2048, Avg Training Loss: 0.24759967006860836, Avg Validation Loss: 10.006237922274716\n",
            "Epoch 1063/2048, Avg Training Loss: 0.24758926469426457, Avg Validation Loss: 10.00495633098608\n",
            "Epoch 1064/2048, Avg Training Loss: 0.24757872600044692, Avg Validation Loss: 10.00371207156082\n",
            "Epoch 1065/2048, Avg Training Loss: 0.24756818064940186, Avg Validation Loss: 10.002492496862917\n",
            "Epoch 1066/2048, Avg Training Loss: 0.24755769539612954, Avg Validation Loss: 10.001290680213833\n",
            "Epoch 1067/2048, Avg Training Loss: 0.24754730511059173, Avg Validation Loss: 10.000102817247331\n",
            "Epoch 1068/2048, Avg Training Loss: 0.24753713465366933, Avg Validation Loss: 9.998873506721953\n",
            "Epoch 1069/2048, Avg Training Loss: 0.24752727713546194, Avg Validation Loss: 9.997603015352306\n",
            "Epoch 1070/2048, Avg Training Loss: 0.247517825284589, Avg Validation Loss: 9.996394548003352\n",
            "Epoch 1071/2048, Avg Training Loss: 0.24750823403086175, Avg Validation Loss: 9.995223984418882\n",
            "Epoch 1072/2048, Avg Training Loss: 0.24749862865204303, Avg Validation Loss: 9.99407814433842\n",
            "Epoch 1073/2048, Avg Training Loss: 0.24748903785935938, Avg Validation Loss: 9.992986722985306\n",
            "Epoch 1074/2048, Avg Training Loss: 0.24747970268711675, Avg Validation Loss: 9.991955055563736\n",
            "Epoch 1075/2048, Avg Training Loss: 0.24747058470558883, Avg Validation Loss: 9.990905381464005\n",
            "Epoch 1076/2048, Avg Training Loss: 0.24746184082030298, Avg Validation Loss: 9.989765140237909\n",
            "Epoch 1077/2048, Avg Training Loss: 0.24745314916485034, Avg Validation Loss: 9.988577736575143\n",
            "Epoch 1078/2048, Avg Training Loss: 0.24744425175680593, Avg Validation Loss: 9.987365077438767\n",
            "Epoch 1079/2048, Avg Training Loss: 0.2474353240707313, Avg Validation Loss: 9.986144453564801\n",
            "Epoch 1080/2048, Avg Training Loss: 0.24742644052504234, Avg Validation Loss: 9.984925128100132\n",
            "Epoch 1081/2048, Avg Training Loss: 0.24741774763216462, Avg Validation Loss: 9.983659039084452\n",
            "Epoch 1082/2048, Avg Training Loss: 0.24740937446293457, Avg Validation Loss: 9.98235025937127\n",
            "Epoch 1083/2048, Avg Training Loss: 0.24740139312058976, Avg Validation Loss: 9.981103413940914\n",
            "Epoch 1084/2048, Avg Training Loss: 0.24739325245937568, Avg Validation Loss: 9.979895555749613\n",
            "Epoch 1085/2048, Avg Training Loss: 0.24738508410223417, Avg Validation Loss: 9.978714106652234\n",
            "Epoch 1086/2048, Avg Training Loss: 0.24737695753554478, Avg Validation Loss: 9.977552149601348\n",
            "Epoch 1087/2048, Avg Training Loss: 0.24736901693317764, Avg Validation Loss: 9.976353045451205\n",
            "Epoch 1088/2048, Avg Training Loss: 0.2473614045147306, Avg Validation Loss: 9.97511611400872\n",
            "Epoch 1089/2048, Avg Training Loss: 0.24735417510530766, Avg Validation Loss: 9.973943061313118\n",
            "Epoch 1090/2048, Avg Training Loss: 0.24734677206961725, Avg Validation Loss: 9.97280964420047\n",
            "Epoch 1091/2048, Avg Training Loss: 0.24733933006677378, Avg Validation Loss: 9.97170258771674\n",
            "Epoch 1092/2048, Avg Training Loss: 0.24733192024683603, Avg Validation Loss: 9.970614602894539\n",
            "Epoch 1093/2048, Avg Training Loss: 0.24732468801946464, Avg Validation Loss: 9.96948902578818\n",
            "Epoch 1094/2048, Avg Training Loss: 0.247317789939345, Avg Validation Loss: 9.96832506403423\n",
            "Epoch 1095/2048, Avg Training Loss: 0.24731126543793952, Avg Validation Loss: 9.967224046599002\n",
            "Epoch 1096/2048, Avg Training Loss: 0.2473045532746168, Avg Validation Loss: 9.96616179019961\n",
            "Epoch 1097/2048, Avg Training Loss: 0.2472977912911415, Avg Validation Loss: 9.965125056816923\n",
            "Epoch 1098/2048, Avg Training Loss: 0.24729105236914173, Avg Validation Loss: 9.964106582389777\n",
            "Epoch 1099/2048, Avg Training Loss: 0.24728448317425722, Avg Validation Loss: 9.96304987939316\n",
            "Epoch 1100/2048, Avg Training Loss: 0.24727825388378707, Avg Validation Loss: 9.961954159795326\n",
            "Epoch 1101/2048, Avg Training Loss: 0.24727238936448184, Avg Validation Loss: 9.96092046259604\n",
            "Epoch 1102/2048, Avg Training Loss: 0.24726632402817472, Avg Validation Loss: 9.959924691043991\n",
            "Epoch 1103/2048, Avg Training Loss: 0.24726019872985716, Avg Validation Loss: 9.958953657921949\n",
            "Epoch 1104/2048, Avg Training Loss: 0.24725408798879672, Avg Validation Loss: 9.958000130999682\n",
            "Epoch 1105/2048, Avg Training Loss: 0.24724813966406747, Avg Validation Loss: 9.957007790373654\n",
            "Epoch 1106/2048, Avg Training Loss: 0.2472425368690077, Avg Validation Loss: 9.95597585442736\n",
            "Epoch 1107/2048, Avg Training Loss: 0.24723729063093935, Avg Validation Loss: 9.955005095186186\n",
            "Epoch 1108/2048, Avg Training Loss: 0.24723187710547648, Avg Validation Loss: 9.954025126336123\n",
            "Epoch 1109/2048, Avg Training Loss: 0.24722611648188206, Avg Validation Loss: 9.952977147727523\n",
            "Epoch 1110/2048, Avg Training Loss: 0.24722043898663904, Avg Validation Loss: 9.95184632983517\n",
            "Epoch 1111/2048, Avg Training Loss: 0.2472150811485252, Avg Validation Loss: 9.950654292994278\n",
            "Epoch 1112/2048, Avg Training Loss: 0.2472100638907697, Avg Validation Loss: 9.949513585393493\n",
            "Epoch 1113/2048, Avg Training Loss: 0.24720482398044583, Avg Validation Loss: 9.948406719274587\n",
            "Epoch 1114/2048, Avg Training Loss: 0.24719951297715775, Avg Validation Loss: 9.94732404700156\n",
            "Epoch 1115/2048, Avg Training Loss: 0.2471942110745835, Avg Validation Loss: 9.946260214370174\n",
            "Epoch 1116/2048, Avg Training Loss: 0.24718906975262409, Avg Validation Loss: 9.945160241622384\n",
            "Epoch 1117/2048, Avg Training Loss: 0.24718430330057112, Avg Validation Loss: 9.94402387459764\n",
            "Epoch 1118/2048, Avg Training Loss: 0.24717989172689003, Avg Validation Loss: 9.942951402320972\n",
            "Epoch 1119/2048, Avg Training Loss: 0.2471752542017102, Avg Validation Loss: 9.9419190346502\n",
            "Epoch 1120/2048, Avg Training Loss: 0.24717054020561569, Avg Validation Loss: 9.940913720263744\n",
            "Epoch 1121/2048, Avg Training Loss: 0.24716593857437946, Avg Validation Loss: 9.939876467648748\n",
            "Epoch 1122/2048, Avg Training Loss: 0.2471616974040728, Avg Validation Loss: 9.9388047370229\n",
            "Epoch 1123/2048, Avg Training Loss: 0.247157791332151, Avg Validation Loss: 9.937797253258736\n",
            "Epoch 1124/2048, Avg Training Loss: 0.24715364029216233, Avg Validation Loss: 9.936829646500072\n",
            "Epoch 1125/2048, Avg Training Loss: 0.24714939994299856, Avg Validation Loss: 9.935888555960855\n",
            "Epoch 1126/2048, Avg Training Loss: 0.24714526275916332, Avg Validation Loss: 9.934914988572471\n",
            "Epoch 1127/2048, Avg Training Loss: 0.2471414919102829, Avg Validation Loss: 9.93390631076774\n",
            "Epoch 1128/2048, Avg Training Loss: 0.24713804791076924, Avg Validation Loss: 9.932960902937735\n",
            "Epoch 1129/2048, Avg Training Loss: 0.24713434663705175, Avg Validation Loss: 9.932054458132518\n",
            "Epoch 1130/2048, Avg Training Loss: 0.24713054703752885, Avg Validation Loss: 9.931173655188477\n",
            "Epoch 1131/2048, Avg Training Loss: 0.24712684366350732, Avg Validation Loss: 9.930259676529419\n",
            "Epoch 1132/2048, Avg Training Loss: 0.24712351320073372, Avg Validation Loss: 9.929309898199184\n",
            "Epoch 1133/2048, Avg Training Loss: 0.24712050241556174, Avg Validation Loss: 9.928422434613946\n",
            "Epoch 1134/2048, Avg Training Loss: 0.24711722314341447, Avg Validation Loss: 9.927573065028428\n",
            "Epoch 1135/2048, Avg Training Loss: 0.24711383732104406, Avg Validation Loss: 9.92674851994435\n",
            "Epoch 1136/2048, Avg Training Loss: 0.247110541401632, Avg Validation Loss: 9.92589015196432\n",
            "Epoch 1137/2048, Avg Training Loss: 0.2471076248335967, Avg Validation Loss: 9.924995350293182\n",
            "Epoch 1138/2048, Avg Training Loss: 0.2471050213340802, Avg Validation Loss: 9.9241619856092\n",
            "Epoch 1139/2048, Avg Training Loss: 0.2471021389000097, Avg Validation Loss: 9.923365917733182\n",
            "Epoch 1140/2048, Avg Training Loss: 0.24709925299327892, Avg Validation Loss: 9.92254267499263\n",
            "Epoch 1141/2048, Avg Training Loss: 0.2470967044428923, Avg Validation Loss: 9.921686403903767\n",
            "Epoch 1142/2048, Avg Training Loss: 0.24709444264836808, Avg Validation Loss: 9.920894360381189\n",
            "Epoch 1143/2048, Avg Training Loss: 0.24709188399930118, Avg Validation Loss: 9.920140852739543\n",
            "Epoch 1144/2048, Avg Training Loss: 0.2470891999597458, Avg Validation Loss: 9.919411821071265\n",
            "Epoch 1145/2048, Avg Training Loss: 0.24708659259243498, Avg Validation Loss: 9.918648422370488\n",
            "Epoch 1146/2048, Avg Training Loss: 0.24708437428777028, Avg Validation Loss: 9.91784780859558\n",
            "Epoch 1147/2048, Avg Training Loss: 0.24708245724930672, Avg Validation Loss: 9.917107310388914\n",
            "Epoch 1148/2048, Avg Training Loss: 0.24708024336610088, Avg Validation Loss: 9.91640284463228\n",
            "Epoch 1149/2048, Avg Training Loss: 0.24707801351501255, Avg Validation Loss: 9.915670187999911\n",
            "Epoch 1150/2048, Avg Training Loss: 0.24707613054113572, Avg Validation Loss: 9.914903498796214\n",
            "Epoch 1151/2048, Avg Training Loss: 0.24707451936281155, Avg Validation Loss: 9.914198300399368\n",
            "Epoch 1152/2048, Avg Training Loss: 0.2470725908592302, Avg Validation Loss: 9.913529667722571\n",
            "Epoch 1153/2048, Avg Training Loss: 0.2470705229522164, Avg Validation Loss: 9.912883960660743\n",
            "Epoch 1154/2048, Avg Training Loss: 0.24706852162690127, Avg Validation Loss: 9.912202763600526\n",
            "Epoch 1155/2048, Avg Training Loss: 0.24706691958353202, Avg Validation Loss: 9.911483346945962\n",
            "Epoch 1156/2048, Avg Training Loss: 0.24706561067500635, Avg Validation Loss: 9.910821184475678\n",
            "Epoch 1157/2048, Avg Training Loss: 0.2470639825913343, Avg Validation Loss: 9.910191949322945\n",
            "Epoch 1158/2048, Avg Training Loss: 0.24706233034366862, Avg Validation Loss: 9.909533716363065\n",
            "Epoch 1159/2048, Avg Training Loss: 0.24706104687119806, Avg Validation Loss: 9.908842261964487\n",
            "Epoch 1160/2048, Avg Training Loss: 0.24706003131659537, Avg Validation Loss: 9.908212049245058\n",
            "Epoch 1161/2048, Avg Training Loss: 0.2470586861492974, Avg Validation Loss: 9.907617863309632\n",
            "Epoch 1162/2048, Avg Training Loss: 0.2470573945192594, Avg Validation Loss: 9.906912320972072\n",
            "Epoch 1163/2048, Avg Training Loss: 0.24705621775213538, Avg Validation Loss: 9.906068621403435\n",
            "Epoch 1164/2048, Avg Training Loss: 0.24705539427781245, Avg Validation Loss: 9.905238668864836\n",
            "Epoch 1165/2048, Avg Training Loss: 0.24705429020804034, Avg Validation Loss: 9.904421742589916\n",
            "Epoch 1166/2048, Avg Training Loss: 0.24705318234341728, Avg Validation Loss: 9.903566600958976\n",
            "Epoch 1167/2048, Avg Training Loss: 0.24705246909168413, Avg Validation Loss: 9.902674195406332\n",
            "Epoch 1168/2048, Avg Training Loss: 0.24705203235609885, Avg Validation Loss: 9.901842865485587\n",
            "Epoch 1169/2048, Avg Training Loss: 0.24705126753587597, Avg Validation Loss: 9.901049741468176\n",
            "Epoch 1170/2048, Avg Training Loss: 0.247050471655964, Avg Validation Loss: 9.90023174855773\n",
            "Epoch 1171/2048, Avg Training Loss: 0.24705006620844372, Avg Validation Loss: 9.899383360464276\n",
            "Epoch 1172/2048, Avg Training Loss: 0.24704992506436743, Avg Validation Loss: 9.898599161602588\n",
            "Epoch 1173/2048, Avg Training Loss: 0.2470494429101737, Avg Validation Loss: 9.897854069462559\n",
            "Epoch 1174/2048, Avg Training Loss: 0.2470489191806531, Avg Validation Loss: 9.89708406640658\n",
            "Epoch 1175/2048, Avg Training Loss: 0.24704879049727013, Avg Validation Loss: 9.896284031741713\n",
            "Epoch 1176/2048, Avg Training Loss: 0.24704892380149104, Avg Validation Loss: 9.895548332214128\n",
            "Epoch 1177/2048, Avg Training Loss: 0.24704870499780757, Avg Validation Loss: 9.894851852162228\n",
            "Epoch 1178/2048, Avg Training Loss: 0.24704843961689427, Avg Validation Loss: 9.894130562288234\n",
            "Epoch 1179/2048, Avg Training Loss: 0.24704857626322702, Avg Validation Loss: 9.893378276068242\n",
            "Epoch 1180/2048, Avg Training Loss: 0.24704896509786384, Avg Validation Loss: 9.892688773942636\n",
            "Epoch 1181/2048, Avg Training Loss: 0.2470489938248137, Avg Validation Loss: 9.892037326828923\n",
            "Epoch 1182/2048, Avg Training Loss: 0.24704897060647177, Avg Validation Loss: 9.891360231563313\n",
            "Epoch 1183/2048, Avg Training Loss: 0.24704935587374902, Avg Validation Loss: 9.890651414761681\n",
            "Epoch 1184/2048, Avg Training Loss: 0.24704998840070302, Avg Validation Loss: 9.890004519350125\n",
            "Epoch 1185/2048, Avg Training Loss: 0.2470503659351694, Avg Validation Loss: 9.889344851883804\n",
            "Epoch 1186/2048, Avg Training Loss: 0.24705105053247114, Avg Validation Loss: 9.888660287935167\n",
            "Epoch 1187/2048, Avg Training Loss: 0.24705192216248623, Avg Validation Loss: 9.888041011014836\n",
            "Epoch 1188/2048, Avg Training Loss: 0.2470523892798191, Avg Validation Loss: 9.887460634526384\n",
            "Epoch 1189/2048, Avg Training Loss: 0.24705277902730213, Avg Validation Loss: 9.886854737971692\n",
            "Epoch 1190/2048, Avg Training Loss: 0.24705357951128626, Avg Validation Loss: 9.886216750683994\n",
            "Epoch 1191/2048, Avg Training Loss: 0.24705461453404615, Avg Validation Loss: 9.885639720468694\n",
            "Epoch 1192/2048, Avg Training Loss: 0.2470552653263822, Avg Validation Loss: 9.885098975052836\n",
            "Epoch 1193/2048, Avg Training Loss: 0.24705584877547235, Avg Validation Loss: 9.884531122424333\n",
            "Epoch 1194/2048, Avg Training Loss: 0.24705685669311345, Avg Validation Loss: 9.883930100576016\n",
            "Epoch 1195/2048, Avg Training Loss: 0.24705809922063157, Avg Validation Loss: 9.88338907379151\n",
            "Epoch 1196/2048, Avg Training Loss: 0.24705906675687664, Avg Validation Loss: 9.882833762365333\n",
            "Epoch 1197/2048, Avg Training Loss: 0.24706035424874614, Avg Validation Loss: 9.882252117066162\n",
            "Epoch 1198/2048, Avg Training Loss: 0.2470618152734552, Avg Validation Loss: 9.881733904213897\n",
            "Epoch 1199/2048, Avg Training Loss: 0.24706285132901892, Avg Validation Loss: 9.88125279542329\n",
            "Epoch 1200/2048, Avg Training Loss: 0.2470637960948729, Avg Validation Loss: 9.880744598140405\n",
            "Epoch 1201/2048, Avg Training Loss: 0.24706516759943706, Avg Validation Loss: 9.880202897050545\n",
            "Epoch 1202/2048, Avg Training Loss: 0.2470667627529057, Avg Validation Loss: 9.879720399396218\n",
            "Epoch 1203/2048, Avg Training Loss: 0.24706806956042773, Avg Validation Loss: 9.879222942324793\n",
            "Epoch 1204/2048, Avg Training Loss: 0.2470697031921826, Avg Validation Loss: 9.878698457972092\n",
            "Epoch 1205/2048, Avg Training Loss: 0.24707150224499988, Avg Validation Loss: 9.878236453001003\n",
            "Epoch 1206/2048, Avg Training Loss: 0.24707297882511456, Avg Validation Loss: 9.877761168443824\n",
            "Epoch 1207/2048, Avg Training Loss: 0.24707476997539934, Avg Validation Loss: 9.87725963129182\n",
            "Epoch 1208/2048, Avg Training Loss: 0.2470767150804542, Avg Validation Loss: 9.876820751621732\n",
            "Epoch 1209/2048, Avg Training Loss: 0.24707821395718518, Avg Validation Loss: 9.876418151755761\n",
            "Epoch 1210/2048, Avg Training Loss: 0.24707960968883208, Avg Validation Loss: 9.875987847606394\n",
            "Epoch 1211/2048, Avg Training Loss: 0.2470814444548285, Avg Validation Loss: 9.875523259831319\n",
            "Epoch 1212/2048, Avg Training Loss: 0.24708360817340674, Avg Validation Loss: 9.875067168268268\n",
            "Epoch 1213/2048, Avg Training Loss: 0.2470858933233173, Avg Validation Loss: 9.874596462244757\n",
            "Epoch 1214/2048, Avg Training Loss: 0.24708822025587862, Avg Validation Loss: 9.874194317748872\n",
            "Epoch 1215/2048, Avg Training Loss: 0.24709003522081838, Avg Validation Loss: 9.873831389667844\n",
            "Epoch 1216/2048, Avg Training Loss: 0.24709171123361776, Avg Validation Loss: 9.873442173380285\n",
            "Epoch 1217/2048, Avg Training Loss: 0.24709381836833064, Avg Validation Loss: 9.873019201561839\n",
            "Epoch 1218/2048, Avg Training Loss: 0.24709612640057754, Avg Validation Loss: 9.872654109121468\n",
            "Epoch 1219/2048, Avg Training Loss: 0.24709811880236904, Avg Validation Loss: 9.872272840528026\n",
            "Epoch 1220/2048, Avg Training Loss: 0.24710045332733171, Avg Validation Loss: 9.87186322989136\n",
            "Epoch 1221/2048, Avg Training Loss: 0.24710293710355055, Avg Validation Loss: 9.871514261712289\n",
            "Epoch 1222/2048, Avg Training Loss: 0.24710507516966893, Avg Validation Loss: 9.871150543643477\n",
            "Epoch 1223/2048, Avg Training Loss: 0.24710754459827453, Avg Validation Loss: 9.870759154871344\n",
            "Epoch 1224/2048, Avg Training Loss: 0.24711015333159644, Avg Validation Loss: 9.870428592715301\n",
            "Epoch 1225/2048, Avg Training Loss: 0.24711229310843463, Avg Validation Loss: 9.870132638473143\n",
            "Epoch 1226/2048, Avg Training Loss: 0.247114316005897, Avg Validation Loss: 9.869807629433177\n",
            "Epoch 1227/2048, Avg Training Loss: 0.24711691440369205, Avg Validation Loss: 9.86939772489273\n",
            "Epoch 1228/2048, Avg Training Loss: 0.24712028355778706, Avg Validation Loss: 9.86899613654215\n",
            "Epoch 1229/2048, Avg Training Loss: 0.24712345629224222, Avg Validation Loss: 9.868674703961506\n",
            "Epoch 1230/2048, Avg Training Loss: 0.24712597673325368, Avg Validation Loss: 9.868398223103933\n",
            "Epoch 1231/2048, Avg Training Loss: 0.24712816624302325, Avg Validation Loss: 9.868147457930041\n",
            "Epoch 1232/2048, Avg Training Loss: 0.24713031210095254, Avg Validation Loss: 9.867862634232178\n",
            "Epoch 1233/2048, Avg Training Loss: 0.24713296714253127, Avg Validation Loss: 9.86753929452425\n",
            "Epoch 1234/2048, Avg Training Loss: 0.24713595892891538, Avg Validation Loss: 9.867221129037937\n",
            "Epoch 1235/2048, Avg Training Loss: 0.2471390979767145, Avg Validation Loss: 9.866885805212661\n",
            "Epoch 1236/2048, Avg Training Loss: 0.24714226255991695, Avg Validation Loss: 9.866616413678484\n",
            "Epoch 1237/2048, Avg Training Loss: 0.24714488775435342, Avg Validation Loss: 9.86638400643933\n",
            "Epoch 1238/2048, Avg Training Loss: 0.24714735766246793, Avg Validation Loss: 9.866123566314362\n",
            "Epoch 1239/2048, Avg Training Loss: 0.24715040289280882, Avg Validation Loss: 9.865778559725026\n",
            "Epoch 1240/2048, Avg Training Loss: 0.24715422440488238, Avg Validation Loss: 9.865441500764648\n",
            "Epoch 1241/2048, Avg Training Loss: 0.24715782904219122, Avg Validation Loss: 9.865183819934646\n",
            "Epoch 1242/2048, Avg Training Loss: 0.24716076109165133, Avg Validation Loss: 9.864970289401365\n",
            "Epoch 1243/2048, Avg Training Loss: 0.24716335015333757, Avg Validation Loss: 9.864781664785342\n",
            "Epoch 1244/2048, Avg Training Loss: 0.24716588846280144, Avg Validation Loss: 9.86455828739917\n",
            "Epoch 1245/2048, Avg Training Loss: 0.24716906756724352, Avg Validation Loss: 9.864246570174796\n",
            "Epoch 1246/2048, Avg Training Loss: 0.24717306050772095, Avg Validation Loss: 9.863940508628689\n",
            "Epoch 1247/2048, Avg Training Loss: 0.2471768455675183, Avg Validation Loss: 9.863712332311218\n",
            "Epoch 1248/2048, Avg Training Loss: 0.2471799584990582, Avg Validation Loss: 9.863527330922414\n",
            "Epoch 1249/2048, Avg Training Loss: 0.24718284475068716, Avg Validation Loss: 9.863317443224359\n",
            "Epoch 1250/2048, Avg Training Loss: 0.24718616971693083, Avg Validation Loss: 9.863073585727333\n",
            "Epoch 1251/2048, Avg Training Loss: 0.247189657307586, Avg Validation Loss: 9.86288586228719\n",
            "Epoch 1252/2048, Avg Training Loss: 0.24719278426819002, Avg Validation Loss: 9.862680285497799\n",
            "Epoch 1253/2048, Avg Training Loss: 0.24719628283600917, Avg Validation Loss: 9.862444519317432\n",
            "Epoch 1254/2048, Avg Training Loss: 0.24720002177016143, Avg Validation Loss: 9.862217789641324\n",
            "Epoch 1255/2048, Avg Training Loss: 0.24720387507046743, Avg Validation Loss: 9.861975499089054\n",
            "Epoch 1256/2048, Avg Training Loss: 0.2472077125589471, Avg Validation Loss: 9.861799188901658\n",
            "Epoch 1257/2048, Avg Training Loss: 0.24721109103160366, Avg Validation Loss: 9.8616102979453\n",
            "Epoch 1258/2048, Avg Training Loss: 0.2472147964377851, Avg Validation Loss: 9.861393968762629\n",
            "Epoch 1259/2048, Avg Training Loss: 0.24721859582924643, Avg Validation Loss: 9.861237039490517\n",
            "Epoch 1260/2048, Avg Training Loss: 0.24722199284621438, Avg Validation Loss: 9.86106388957662\n",
            "Epoch 1261/2048, Avg Training Loss: 0.24722575139005498, Avg Validation Loss: 9.860861256311418\n",
            "Epoch 1262/2048, Avg Training Loss: 0.24722961911035946, Avg Validation Loss: 9.860716818933358\n",
            "Epoch 1263/2048, Avg Training Loss: 0.24723309076146674, Avg Validation Loss: 9.860555459842674\n",
            "Epoch 1264/2048, Avg Training Loss: 0.24723704872772578, Avg Validation Loss: 9.860315192211909\n",
            "Epoch 1265/2048, Avg Training Loss: 0.2472417469948353, Avg Validation Loss: 9.860085079075715\n",
            "Epoch 1266/2048, Avg Training Loss: 0.2472461651321213, Avg Validation Loss: 9.859934628803586\n",
            "Epoch 1267/2048, Avg Training Loss: 0.24724985965654922, Avg Validation Loss: 9.859827857042784\n",
            "Epoch 1268/2048, Avg Training Loss: 0.24725329956988068, Avg Validation Loss: 9.859696135081697\n",
            "Epoch 1269/2048, Avg Training Loss: 0.24725719178243596, Avg Validation Loss: 9.859529992582003\n",
            "Epoch 1270/2048, Avg Training Loss: 0.24726123275246895, Avg Validation Loss: 9.859419104711645\n",
            "Epoch 1271/2048, Avg Training Loss: 0.24726488537540503, Avg Validation Loss: 9.85928115217489\n",
            "Epoch 1272/2048, Avg Training Loss: 0.24726905823829906, Avg Validation Loss: 9.859058838675603\n",
            "Epoch 1273/2048, Avg Training Loss: 0.24727399327822444, Avg Validation Loss: 9.85884366790531\n",
            "Epoch 1274/2048, Avg Training Loss: 0.24727864680053915, Avg Validation Loss: 9.858706455592012\n",
            "Epoch 1275/2048, Avg Training Loss: 0.24728257089836186, Avg Validation Loss: 9.858611985357472\n",
            "Epoch 1276/2048, Avg Training Loss: 0.2472862372003797, Avg Validation Loss: 9.858492085614252\n",
            "Epoch 1277/2048, Avg Training Loss: 0.24729036602792354, Avg Validation Loss: 9.858337509386624\n",
            "Epoch 1278/2048, Avg Training Loss: 0.24729475840427045, Avg Validation Loss: 9.858189050880254\n",
            "Epoch 1279/2048, Avg Training Loss: 0.24729929908605003, Avg Validation Loss: 9.858023103102092\n",
            "Epoch 1280/2048, Avg Training Loss: 0.24730381597135018, Avg Validation Loss: 9.85792149807205\n",
            "Epoch 1281/2048, Avg Training Loss: 0.2473078550721879, Avg Validation Loss: 9.857806178129179\n",
            "Epoch 1282/2048, Avg Training Loss: 0.24731224678965955, Avg Validation Loss: 9.857662478868743\n",
            "Epoch 1283/2048, Avg Training Loss: 0.2473168383911795, Avg Validation Loss: 9.857528209387445\n",
            "Epoch 1284/2048, Avg Training Loss: 0.2473215489570074, Avg Validation Loss: 9.857378157541282\n",
            "Epoch 1285/2048, Avg Training Loss: 0.24732621469388158, Avg Validation Loss: 9.857293234569923\n",
            "Epoch 1286/2048, Avg Training Loss: 0.24733038848193328, Avg Validation Loss: 9.857194942714798\n",
            "Epoch 1287/2048, Avg Training Loss: 0.24733491483467904, Avg Validation Loss: 9.857068362654545\n",
            "Epoch 1288/2048, Avg Training Loss: 0.24733962097243867, Avg Validation Loss: 9.856958001890893\n",
            "Epoch 1289/2048, Avg Training Loss: 0.24734422129868483, Avg Validation Loss: 9.856860161057293\n",
            "Epoch 1290/2048, Avg Training Loss: 0.24734829980443682, Avg Validation Loss: 9.856843734811587\n",
            "Epoch 1291/2048, Avg Training Loss: 0.24735161434513364, Avg Validation Loss: 9.856821420439584\n",
            "Epoch 1292/2048, Avg Training Loss: 0.24735514682298687, Avg Validation Loss: 9.856773475883308\n",
            "Epoch 1293/2048, Avg Training Loss: 0.24735880194275137, Avg Validation Loss: 9.856734950070896\n",
            "Epoch 1294/2048, Avg Training Loss: 0.24736254552554593, Avg Validation Loss: 9.856679165635795\n",
            "Epoch 1295/2048, Avg Training Loss: 0.2473662229481738, Avg Validation Loss: 9.856686266200946\n",
            "Epoch 1296/2048, Avg Training Loss: 0.24736939676263203, Avg Validation Loss: 9.856677388601197\n",
            "Epoch 1297/2048, Avg Training Loss: 0.24737304870176605, Avg Validation Loss: 9.85658852965271\n",
            "Epoch 1298/2048, Avg Training Loss: 0.24737745237705897, Avg Validation Loss: 9.856507405734847\n",
            "Epoch 1299/2048, Avg Training Loss: 0.2473815367531417, Avg Validation Loss: 9.856502846113049\n",
            "Epoch 1300/2048, Avg Training Loss: 0.24738486238066254, Avg Validation Loss: 9.856538690898264\n",
            "Epoch 1301/2048, Avg Training Loss: 0.2473879185106007, Avg Validation Loss: 9.856546319778563\n",
            "Epoch 1302/2048, Avg Training Loss: 0.2473915814277985, Avg Validation Loss: 9.85646732464023\n",
            "Epoch 1303/2048, Avg Training Loss: 0.2473960655244269, Avg Validation Loss: 9.856392506945024\n",
            "Epoch 1304/2048, Avg Training Loss: 0.2474002626199019, Avg Validation Loss: 9.856392385129443\n",
            "Epoch 1305/2048, Avg Training Loss: 0.24740383501976268, Avg Validation Loss: 9.856382797051992\n",
            "Epoch 1306/2048, Avg Training Loss: 0.24740770971058917, Avg Validation Loss: 9.856345736508606\n",
            "Epoch 1307/2048, Avg Training Loss: 0.24741162425922802, Avg Validation Loss: 9.856366133819677\n",
            "Epoch 1308/2048, Avg Training Loss: 0.2474150871649752, Avg Validation Loss: 9.856367683097064\n",
            "Epoch 1309/2048, Avg Training Loss: 0.24741906564863933, Avg Validation Loss: 9.856287767394\n",
            "Epoch 1310/2048, Avg Training Loss: 0.24742381860629964, Avg Validation Loss: 9.85621490852883\n",
            "Epoch 1311/2048, Avg Training Loss: 0.24742825273876443, Avg Validation Loss: 9.856218378829785\n",
            "Epoch 1312/2048, Avg Training Loss: 0.24743204251063844, Avg Validation Loss: 9.856213310937603\n",
            "Epoch 1313/2048, Avg Training Loss: 0.24743613019207475, Avg Validation Loss: 9.856181317108879\n",
            "Epoch 1314/2048, Avg Training Loss: 0.24744036966501526, Avg Validation Loss: 9.856158218375104\n",
            "Epoch 1315/2048, Avg Training Loss: 0.24744472337098652, Avg Validation Loss: 9.856117724913704\n",
            "Epoch 1316/2048, Avg Training Loss: 0.24744901063446667, Avg Validation Loss: 9.856140247181642\n",
            "Epoch 1317/2048, Avg Training Loss: 0.2474527860545926, Avg Validation Loss: 9.856146987229847\n",
            "Epoch 1318/2048, Avg Training Loss: 0.24745705366379728, Avg Validation Loss: 9.856073961892173\n",
            "Epoch 1319/2048, Avg Training Loss: 0.2474620859831773, Avg Validation Loss: 9.856008961394267\n",
            "Epoch 1320/2048, Avg Training Loss: 0.24746678447303927, Avg Validation Loss: 9.856020859699322\n",
            "Epoch 1321/2048, Avg Training Loss: 0.24747082693990846, Avg Validation Loss: 9.856024553398754\n",
            "Epoch 1322/2048, Avg Training Loss: 0.2474751698330847, Avg Validation Loss: 9.85600152766367\n",
            "Epoch 1323/2048, Avg Training Loss: 0.24747954025468705, Avg Validation Loss: 9.85603645818907\n",
            "Epoch 1324/2048, Avg Training Loss: 0.24748344603733555, Avg Validation Loss: 9.856052863656599\n",
            "Epoch 1325/2048, Avg Training Loss: 0.24748787621693932, Avg Validation Loss: 9.855988022846923\n",
            "Epoch 1326/2048, Avg Training Loss: 0.24749309041930284, Avg Validation Loss: 9.85593042716867\n",
            "Epoch 1327/2048, Avg Training Loss: 0.24749797330997086, Avg Validation Loss: 9.855949335021249\n",
            "Epoch 1328/2048, Avg Training Loss: 0.2475021986834407, Avg Validation Loss: 9.855959839741608\n",
            "Epoch 1329/2048, Avg Training Loss: 0.24750685017881965, Avg Validation Loss: 9.855894617412114\n",
            "Epoch 1330/2048, Avg Training Loss: 0.24751223492424843, Avg Validation Loss: 9.855839639477756\n",
            "Epoch 1331/2048, Avg Training Loss: 0.24751725633652308, Avg Validation Loss: 9.855862787190546\n",
            "Epoch 1332/2048, Avg Training Loss: 0.2475214816563206, Avg Validation Loss: 9.8559273271248\n",
            "Epoch 1333/2048, Avg Training Loss: 0.24752541655646862, Avg Validation Loss: 9.85596429434151\n",
            "Epoch 1334/2048, Avg Training Loss: 0.24752997860586953, Avg Validation Loss: 9.855915072842937\n",
            "Epoch 1335/2048, Avg Training Loss: 0.24753538238273998, Avg Validation Loss: 9.855870406804641\n",
            "Epoch 1336/2048, Avg Training Loss: 0.24754059470884007, Avg Validation Loss: 9.855851867431246\n",
            "Epoch 1337/2048, Avg Training Loss: 0.2475457523782015, Avg Validation Loss: 9.855825364321186\n",
            "Epoch 1338/2048, Avg Training Loss: 0.24755073514322373, Avg Validation Loss: 9.85587527276525\n",
            "Epoch 1339/2048, Avg Training Loss: 0.24755517868700538, Avg Validation Loss: 9.855920685562646\n",
            "Epoch 1340/2048, Avg Training Loss: 0.24756010725992808, Avg Validation Loss: 9.855892226136875\n",
            "Epoch 1341/2048, Avg Training Loss: 0.247565804239736, Avg Validation Loss: 9.855874775573886\n",
            "Epoch 1342/2048, Avg Training Loss: 0.24757114357205992, Avg Validation Loss: 9.855935647332377\n",
            "Epoch 1343/2048, Avg Training Loss: 0.24757580521693126, Avg Validation Loss: 9.855988867058612\n",
            "Epoch 1344/2048, Avg Training Loss: 0.24758077923819244, Avg Validation Loss: 9.85601544766174\n",
            "Epoch 1345/2048, Avg Training Loss: 0.24758589067317846, Avg Validation Loss: 9.856050907375113\n",
            "Epoch 1346/2048, Avg Training Loss: 0.2475912485598605, Avg Validation Loss: 9.856019837714497\n",
            "Epoch 1347/2048, Avg Training Loss: 0.24759725003537508, Avg Validation Loss: 9.856003819781973\n",
            "Epoch 1348/2048, Avg Training Loss: 0.24760282061416392, Avg Validation Loss: 9.856068357097685\n",
            "Epoch 1349/2048, Avg Training Loss: 0.24760755175020802, Avg Validation Loss: 9.856175401171077\n",
            "Epoch 1350/2048, Avg Training Loss: 0.247611969658308, Avg Validation Loss: 9.856255243347661\n",
            "Epoch 1351/2048, Avg Training Loss: 0.24761702138789513, Avg Validation Loss: 9.856248842471365\n",
            "Epoch 1352/2048, Avg Training Loss: 0.24762304445455047, Avg Validation Loss: 9.856197824133194\n",
            "Epoch 1353/2048, Avg Training Loss: 0.24762947497251345, Avg Validation Loss: 9.856173834685391\n",
            "Epoch 1354/2048, Avg Training Loss: 0.2476353420396648, Avg Validation Loss: 9.856236942832274\n",
            "Epoch 1355/2048, Avg Training Loss: 0.2476402962623827, Avg Validation Loss: 9.856346117380635\n",
            "Epoch 1356/2048, Avg Training Loss: 0.24764489808413087, Avg Validation Loss: 9.856430024344956\n",
            "Epoch 1357/2048, Avg Training Loss: 0.24765011979718551, Avg Validation Loss: 9.856428734074765\n",
            "Epoch 1358/2048, Avg Training Loss: 0.24765618737218956, Avg Validation Loss: 9.856432337222508\n",
            "Epoch 1359/2048, Avg Training Loss: 0.24766203959548297, Avg Validation Loss: 9.856462046946197\n",
            "Epoch 1360/2048, Avg Training Loss: 0.24766783988101088, Avg Validation Loss: 9.856483548634253\n",
            "Epoch 1361/2048, Avg Training Loss: 0.24767357189032693, Avg Validation Loss: 9.856523992014617\n",
            "Epoch 1362/2048, Avg Training Loss: 0.24767933181216176, Avg Validation Loss: 9.856552306193498\n",
            "Epoch 1363/2048, Avg Training Loss: 0.2476850643748966, Avg Validation Loss: 9.856597417486729\n",
            "Epoch 1364/2048, Avg Training Loss: 0.24769084810344705, Avg Validation Loss: 9.856629223753771\n",
            "Epoch 1365/2048, Avg Training Loss: 0.2476966152844759, Avg Validation Loss: 9.856677183083754\n",
            "Epoch 1366/2048, Avg Training Loss: 0.24770244084891138, Avg Validation Loss: 9.85671148162922\n",
            "Epoch 1367/2048, Avg Training Loss: 0.24770825214367936, Avg Validation Loss: 9.856761737085305\n",
            "Epoch 1368/2048, Avg Training Loss: 0.247714124501631, Avg Validation Loss: 9.856798220582778\n",
            "Epoch 1369/2048, Avg Training Loss: 0.24771998244765422, Avg Validation Loss: 9.856850598256262\n",
            "Epoch 1370/2048, Avg Training Loss: 0.24772590696708102, Avg Validation Loss: 9.856892208367638\n",
            "Epoch 1371/2048, Avg Training Loss: 0.24773182473516953, Avg Validation Loss: 9.856951716876173\n",
            "Epoch 1372/2048, Avg Training Loss: 0.24773780900696143, Avg Validation Loss: 9.856998428411707\n",
            "Epoch 1373/2048, Avg Training Loss: 0.24774377860828284, Avg Validation Loss: 9.857061491703993\n",
            "Epoch 1374/2048, Avg Training Loss: 0.24774981344835822, Avg Validation Loss: 9.857110917970202\n",
            "Epoch 1375/2048, Avg Training Loss: 0.24775583139599625, Avg Validation Loss: 9.857176241371349\n",
            "Epoch 1376/2048, Avg Training Loss: 0.2477620360701129, Avg Validation Loss: 9.85717871188988\n",
            "Epoch 1377/2048, Avg Training Loss: 0.24776886387409006, Avg Validation Loss: 9.857198169698634\n",
            "Epoch 1378/2048, Avg Training Loss: 0.2477752126070866, Avg Validation Loss: 9.857299187258551\n",
            "Epoch 1379/2048, Avg Training Loss: 0.24778080352250328, Avg Validation Loss: 9.857394179398115\n",
            "Epoch 1380/2048, Avg Training Loss: 0.2477868267896238, Avg Validation Loss: 9.857414323179889\n",
            "Epoch 1381/2048, Avg Training Loss: 0.24779360544049217, Avg Validation Loss: 9.857444901111204\n",
            "Epoch 1382/2048, Avg Training Loss: 0.24779997011071184, Avg Validation Loss: 9.857553466395997\n",
            "Epoch 1383/2048, Avg Training Loss: 0.24780560990643336, Avg Validation Loss: 9.85765404860655\n",
            "Epoch 1384/2048, Avg Training Loss: 0.24781170457402005, Avg Validation Loss: 9.857678703554122\n",
            "Epoch 1385/2048, Avg Training Loss: 0.24781856818539524, Avg Validation Loss: 9.85771320693235\n",
            "Epoch 1386/2048, Avg Training Loss: 0.2478251415170164, Avg Validation Loss: 9.857776404583449\n",
            "Epoch 1387/2048, Avg Training Loss: 0.24783164344452935, Avg Validation Loss: 9.85783265843346\n",
            "Epoch 1388/2048, Avg Training Loss: 0.24783804498055925, Avg Validation Loss: 9.857908427629633\n",
            "Epoch 1389/2048, Avg Training Loss: 0.2478444778647832, Avg Validation Loss: 9.857972233770791\n",
            "Epoch 1390/2048, Avg Training Loss: 0.2478508634014067, Avg Validation Loss: 9.858052813320016\n",
            "Epoch 1391/2048, Avg Training Loss: 0.24785743187135043, Avg Validation Loss: 9.858070945281135\n",
            "Epoch 1392/2048, Avg Training Loss: 0.24786462779261742, Avg Validation Loss: 9.858106254534539\n",
            "Epoch 1393/2048, Avg Training Loss: 0.24787132663833258, Avg Validation Loss: 9.858223206420565\n",
            "Epoch 1394/2048, Avg Training Loss: 0.2478772511050916, Avg Validation Loss: 9.85833411465228\n",
            "Epoch 1395/2048, Avg Training Loss: 0.24788360584570152, Avg Validation Loss: 9.85838102253345\n",
            "Epoch 1396/2048, Avg Training Loss: 0.2478907491405411, Avg Validation Loss: 9.858451955259822\n",
            "Epoch 1397/2048, Avg Training Loss: 0.24789756730144027, Avg Validation Loss: 9.858558838587825\n",
            "Epoch 1398/2048, Avg Training Loss: 0.24790430513890838, Avg Validation Loss: 9.858659615528062\n",
            "Epoch 1399/2048, Avg Training Loss: 0.24791090909995192, Avg Validation Loss: 9.858776097105626\n",
            "Epoch 1400/2048, Avg Training Loss: 0.24791753799590935, Avg Validation Loss: 9.858878236692224\n",
            "Epoch 1401/2048, Avg Training Loss: 0.24792410657134126, Avg Validation Loss: 9.858995572326641\n",
            "Epoch 1402/2048, Avg Training Loss: 0.24793085964533523, Avg Validation Loss: 9.859049270237456\n",
            "Epoch 1403/2048, Avg Training Loss: 0.24793824482546944, Avg Validation Loss: 9.85911923357781\n",
            "Epoch 1404/2048, Avg Training Loss: 0.2479451205016668, Avg Validation Loss: 9.859270107076307\n",
            "Epoch 1405/2048, Avg Training Loss: 0.24795121004511345, Avg Validation Loss: 9.859414231635\n",
            "Epoch 1406/2048, Avg Training Loss: 0.24795774987393282, Avg Validation Loss: 9.859482714810804\n",
            "Epoch 1407/2048, Avg Training Loss: 0.2479651870719773, Avg Validation Loss: 9.859511902873237\n",
            "Epoch 1408/2048, Avg Training Loss: 0.24797298864407724, Avg Validation Loss: 9.859570658797976\n",
            "Epoch 1409/2048, Avg Training Loss: 0.24798013156514992, Avg Validation Loss: 9.859717592941564\n",
            "Epoch 1410/2048, Avg Training Loss: 0.2479864064644164, Avg Validation Loss: 9.859861714900378\n",
            "Epoch 1411/2048, Avg Training Loss: 0.24799309485842075, Avg Validation Loss: 9.85993230785476\n",
            "Epoch 1412/2048, Avg Training Loss: 0.24800054089680199, Avg Validation Loss: 9.860013780459267\n",
            "Epoch 1413/2048, Avg Training Loss: 0.24800765489167623, Avg Validation Loss: 9.860124214663681\n",
            "Epoch 1414/2048, Avg Training Loss: 0.2480146960856421, Avg Validation Loss: 9.86022749746301\n",
            "Epoch 1415/2048, Avg Training Loss: 0.24802161337017814, Avg Validation Loss: 9.86034989622075\n",
            "Epoch 1416/2048, Avg Training Loss: 0.2480286930552747, Avg Validation Loss: 9.860410718098455\n",
            "Epoch 1417/2048, Avg Training Loss: 0.24803639908759217, Avg Validation Loss: 9.860488948295837\n",
            "Epoch 1418/2048, Avg Training Loss: 0.24804369644821578, Avg Validation Loss: 9.860599694344776\n",
            "Epoch 1419/2048, Avg Training Loss: 0.24805088440666753, Avg Validation Loss: 9.8607052054798\n",
            "Epoch 1420/2048, Avg Training Loss: 0.24805792456111433, Avg Validation Loss: 9.860830866683916\n",
            "Epoch 1421/2048, Avg Training Loss: 0.24806511888373914, Avg Validation Loss: 9.86089547868558\n",
            "Epoch 1422/2048, Avg Training Loss: 0.24807293747073375, Avg Validation Loss: 9.860977781118208\n",
            "Epoch 1423/2048, Avg Training Loss: 0.24808021610739056, Avg Validation Loss: 9.86114180697266\n",
            "Epoch 1424/2048, Avg Training Loss: 0.24808668346891702, Avg Validation Loss: 9.861299438802753\n",
            "Epoch 1425/2048, Avg Training Loss: 0.24809361353376153, Avg Validation Loss: 9.861381513130267\n",
            "Epoch 1426/2048, Avg Training Loss: 0.24810145672388112, Avg Validation Loss: 9.861424300084812\n",
            "Epoch 1427/2048, Avg Training Loss: 0.24810967167163656, Avg Validation Loss: 9.861496672157859\n",
            "Epoch 1428/2048, Avg Training Loss: 0.24811720529735387, Avg Validation Loss: 9.8616572669035\n",
            "Epoch 1429/2048, Avg Training Loss: 0.24812384983127006, Avg Validation Loss: 9.861814987435611\n",
            "Epoch 1430/2048, Avg Training Loss: 0.24813092272014853, Avg Validation Loss: 9.86189903845022\n",
            "Epoch 1431/2048, Avg Training Loss: 0.2481388930240008, Avg Validation Loss: 9.861944815178271\n",
            "Epoch 1432/2048, Avg Training Loss: 0.24814722645812937, Avg Validation Loss: 9.862020724106678\n",
            "Epoch 1433/2048, Avg Training Loss: 0.2481549901511318, Avg Validation Loss: 9.862136065384822\n",
            "Epoch 1434/2048, Avg Training Loss: 0.24816257157096167, Avg Validation Loss: 9.862249878871015\n",
            "Epoch 1435/2048, Avg Training Loss: 0.24816995287202528, Avg Validation Loss: 9.862385831341605\n",
            "Epoch 1436/2048, Avg Training Loss: 0.24817747420661013, Avg Validation Loss: 9.862461715659247\n",
            "Epoch 1437/2048, Avg Training Loss: 0.2481856192841074, Avg Validation Loss: 9.862555808819273\n",
            "Epoch 1438/2048, Avg Training Loss: 0.24819332575679498, Avg Validation Loss: 9.862682809028605\n",
            "Epoch 1439/2048, Avg Training Loss: 0.24820092432445082, Avg Validation Loss: 9.86280469509733\n",
            "Epoch 1440/2048, Avg Training Loss: 0.24820835775316527, Avg Validation Loss: 9.862946751424344\n",
            "Epoch 1441/2048, Avg Training Loss: 0.24821595448714553, Avg Validation Loss: 9.863027632616577\n",
            "Epoch 1442/2048, Avg Training Loss: 0.24822418969061955, Avg Validation Loss: 9.863126114741775\n",
            "Epoch 1443/2048, Avg Training Loss: 0.24823198686967504, Avg Validation Loss: 9.863257163376842\n",
            "Epoch 1444/2048, Avg Training Loss: 0.2482398049338595, Avg Validation Loss: 9.863333776590274\n",
            "Epoch 1445/2048, Avg Training Loss: 0.2482481872431569, Avg Validation Loss: 9.863431656425544\n",
            "Epoch 1446/2048, Avg Training Loss: 0.24825608730341697, Avg Validation Loss: 9.863564083991454\n",
            "Epoch 1447/2048, Avg Training Loss: 0.24826399476798178, Avg Validation Loss: 9.863647139266599\n",
            "Epoch 1448/2048, Avg Training Loss: 0.24827246669832237, Avg Validation Loss: 9.863754684787024\n",
            "Epoch 1449/2048, Avg Training Loss: 0.24828032273108902, Avg Validation Loss: 9.863947536614662\n",
            "Epoch 1450/2048, Avg Training Loss: 0.24828731370422197, Avg Validation Loss: 9.864135678762091\n",
            "Epoch 1451/2048, Avg Training Loss: 0.24829477729129865, Avg Validation Loss: 9.864248885954808\n",
            "Epoch 1452/2048, Avg Training Loss: 0.2483031666168178, Avg Validation Loss: 9.864333278007209\n",
            "Epoch 1453/2048, Avg Training Loss: 0.2483121432466993, Avg Validation Loss: 9.864418621353218\n",
            "Epoch 1454/2048, Avg Training Loss: 0.24832130270530398, Avg Validation Loss: 9.864554858565587\n",
            "Epoch 1455/2048, Avg Training Loss: 0.24832963894952587, Avg Validation Loss: 9.864790392199748\n",
            "Epoch 1456/2048, Avg Training Loss: 0.248336997041085, Avg Validation Loss: 9.865028393409418\n",
            "Epoch 1457/2048, Avg Training Loss: 0.24834477639241578, Avg Validation Loss: 9.865194906205224\n",
            "Epoch 1458/2048, Avg Training Loss: 0.24835346317749288, Avg Validation Loss: 9.865323721785458\n",
            "Epoch 1459/2048, Avg Training Loss: 0.24836251643488097, Avg Validation Loss: 9.865482480634016\n",
            "Epoch 1460/2048, Avg Training Loss: 0.24837096279524074, Avg Validation Loss: 9.865680047768\n",
            "Epoch 1461/2048, Avg Training Loss: 0.24837935578788553, Avg Validation Loss: 9.865825954919233\n",
            "Epoch 1462/2048, Avg Training Loss: 0.2483882810410711, Avg Validation Loss: 9.865994194322885\n",
            "Epoch 1463/2048, Avg Training Loss: 0.24839669527182595, Avg Validation Loss: 9.866185975503193\n",
            "Epoch 1464/2048, Avg Training Loss: 0.24840507962898778, Avg Validation Loss: 9.8663017488784\n",
            "Epoch 1465/2048, Avg Training Loss: 0.2484140650878695, Avg Validation Loss: 9.866427259900062\n",
            "Epoch 1466/2048, Avg Training Loss: 0.24842256030008852, Avg Validation Loss: 9.866581185126181\n",
            "Epoch 1467/2048, Avg Training Loss: 0.24843107014036267, Avg Validation Loss: 9.866678336685624\n",
            "Epoch 1468/2048, Avg Training Loss: 0.2484401518628775, Avg Validation Loss: 9.866795627916735\n",
            "Epoch 1469/2048, Avg Training Loss: 0.24844872284827893, Avg Validation Loss: 9.866946943194835\n",
            "Epoch 1470/2048, Avg Training Loss: 0.2484573006965607, Avg Validation Loss: 9.867044491103627\n",
            "Epoch 1471/2048, Avg Training Loss: 0.2484664479093396, Avg Validation Loss: 9.867163784800374\n",
            "Epoch 1472/2048, Avg Training Loss: 0.2484750780036811, Avg Validation Loss: 9.867317948476256\n",
            "Epoch 1473/2048, Avg Training Loss: 0.24848371476685627, Avg Validation Loss: 9.86741877159398\n",
            "Epoch 1474/2048, Avg Training Loss: 0.2484929224022216, Avg Validation Loss: 9.867541549997746\n",
            "Epoch 1475/2048, Avg Training Loss: 0.2485016086785029, Avg Validation Loss: 9.867699289113409\n",
            "Epoch 1476/2048, Avg Training Loss: 0.2485103025739907, Avg Validation Loss: 9.86780370642763\n",
            "Epoch 1477/2048, Avg Training Loss: 0.24851969504524216, Avg Validation Loss: 9.867880844834191\n",
            "Epoch 1478/2048, Avg Training Loss: 0.24852934247055877, Avg Validation Loss: 9.867994138628426\n",
            "Epoch 1479/2048, Avg Training Loss: 0.2485381675274175, Avg Validation Loss: 9.868199317783574\n",
            "Epoch 1480/2048, Avg Training Loss: 0.24854599826555934, Avg Validation Loss: 9.868406201443028\n",
            "Epoch 1481/2048, Avg Training Loss: 0.2485541308205571, Avg Validation Loss: 9.868562430870762\n",
            "Epoch 1482/2048, Avg Training Loss: 0.24856275728727584, Avg Validation Loss: 9.86869563902366\n",
            "Epoch 1483/2048, Avg Training Loss: 0.24857167945094494, Avg Validation Loss: 9.868876519770906\n",
            "Epoch 1484/2048, Avg Training Loss: 0.24858054507292107, Avg Validation Loss: 9.869140460218349\n",
            "Epoch 1485/2048, Avg Training Loss: 0.2485885097862123, Avg Validation Loss: 9.869470045706539\n",
            "Epoch 1486/2048, Avg Training Loss: 0.24859629221620044, Avg Validation Loss: 9.869760383133634\n",
            "Epoch 1487/2048, Avg Training Loss: 0.24860461128858052, Avg Validation Loss: 9.87012516809275\n",
            "Epoch 1488/2048, Avg Training Loss: 0.24861228494679366, Avg Validation Loss: 9.87054998375434\n",
            "Epoch 1489/2048, Avg Training Loss: 0.24861991324871532, Avg Validation Loss: 9.870931512536366\n",
            "Epoch 1490/2048, Avg Training Loss: 0.2486282175266452, Avg Validation Loss: 9.871287832664994\n",
            "Epoch 1491/2048, Avg Training Loss: 0.2486367642449135, Avg Validation Loss: 9.871678317000539\n",
            "Epoch 1492/2048, Avg Training Loss: 0.24864460065413788, Avg Validation Loss: 9.872107327523794\n",
            "Epoch 1493/2048, Avg Training Loss: 0.24865235673038794, Avg Validation Loss: 9.872481508088079\n",
            "Epoch 1494/2048, Avg Training Loss: 0.24866064338757646, Avg Validation Loss: 9.872873922669394\n",
            "Epoch 1495/2048, Avg Training Loss: 0.24866837260387095, Avg Validation Loss: 9.873296369356623\n",
            "Epoch 1496/2048, Avg Training Loss: 0.24867610241800875, Avg Validation Loss: 9.873659425789857\n",
            "Epoch 1497/2048, Avg Training Loss: 0.24868453133057297, Avg Validation Loss: 9.873988927076672\n",
            "Epoch 1498/2048, Avg Training Loss: 0.24869321419204418, Avg Validation Loss: 9.874348590268268\n",
            "Epoch 1499/2048, Avg Training Loss: 0.24870119185508885, Avg Validation Loss: 9.874745032216714\n",
            "Epoch 1500/2048, Avg Training Loss: 0.24870911323409997, Avg Validation Loss: 9.875100053163452\n",
            "Epoch 1501/2048, Avg Training Loss: 0.2487175500878029, Avg Validation Loss: 9.875480758379668\n",
            "Epoch 1502/2048, Avg Training Loss: 0.24872542026122532, Avg Validation Loss: 9.875895630029486\n",
            "Epoch 1503/2048, Avg Training Loss: 0.24873328636609826, Avg Validation Loss: 9.876253176065315\n",
            "Epoch 1504/2048, Avg Training Loss: 0.24874184859631507, Avg Validation Loss: 9.876578323265324\n",
            "Epoch 1505/2048, Avg Training Loss: 0.24875066260581893, Avg Validation Loss: 9.876934433319686\n",
            "Epoch 1506/2048, Avg Training Loss: 0.24875882583363823, Avg Validation Loss: 9.877366002191712\n",
            "Epoch 1507/2048, Avg Training Loss: 0.24876687965951638, Avg Validation Loss: 9.877761881490967\n",
            "Epoch 1508/2048, Avg Training Loss: 0.24877557367435982, Avg Validation Loss: 9.878136467103127\n",
            "Epoch 1509/2048, Avg Training Loss: 0.24878448933277358, Avg Validation Loss: 9.878547514258402\n",
            "Epoch 1510/2048, Avg Training Loss: 0.24879268070661884, Avg Validation Loss: 9.878998302195226\n",
            "Epoch 1511/2048, Avg Training Loss: 0.2488007846088315, Avg Validation Loss: 9.879394158251968\n",
            "Epoch 1512/2048, Avg Training Loss: 0.2488095396020424, Avg Validation Loss: 9.879758528914794\n",
            "Epoch 1513/2048, Avg Training Loss: 0.24881852153791806, Avg Validation Loss: 9.880154155057676\n",
            "Epoch 1514/2048, Avg Training Loss: 0.2488267813746901, Avg Validation Loss: 9.880586955193609\n",
            "Epoch 1515/2048, Avg Training Loss: 0.24883495481184045, Avg Validation Loss: 9.880963511695873\n",
            "Epoch 1516/2048, Avg Training Loss: 0.24884377958907045, Avg Validation Loss: 9.881308081446187\n",
            "Epoch 1517/2048, Avg Training Loss: 0.2488528311575192, Avg Validation Loss: 9.881683922115574\n",
            "Epoch 1518/2048, Avg Training Loss: 0.24886115955554738, Avg Validation Loss: 9.88209717572323\n",
            "Epoch 1519/2048, Avg Training Loss: 0.2488694012745922, Avg Validation Loss: 9.882454386085138\n",
            "Epoch 1520/2048, Avg Training Loss: 0.24887829413419071, Avg Validation Loss: 9.882779910391545\n",
            "Epoch 1521/2048, Avg Training Loss: 0.24888741344637888, Avg Validation Loss: 9.883137132258982\n",
            "Epoch 1522/2048, Avg Training Loss: 0.2488958079951867, Avg Validation Loss: 9.883532210014309\n",
            "Epoch 1523/2048, Avg Training Loss: 0.24890411566123005, Avg Validation Loss: 9.883871549658213\n",
            "Epoch 1524/2048, Avg Training Loss: 0.24891307453617936, Avg Validation Loss: 9.884179545210179\n",
            "Epoch 1525/2048, Avg Training Loss: 0.2489222596957988, Avg Validation Loss: 9.884519665993412\n",
            "Epoch 1526/2048, Avg Training Loss: 0.24893071818700338, Avg Validation Loss: 9.884898069045498\n",
            "Epoch 1527/2048, Avg Training Loss: 0.24893908974666662, Avg Validation Loss: 9.885221025024292\n",
            "Epoch 1528/2048, Avg Training Loss: 0.24894811286900773, Avg Validation Loss: 9.885512956993264\n",
            "Epoch 1529/2048, Avg Training Loss: 0.24895736227892962, Avg Validation Loss: 9.885837410299311\n",
            "Epoch 1530/2048, Avg Training Loss: 0.24896588242066023, Avg Validation Loss: 9.886199529207424\n",
            "Epoch 1531/2048, Avg Training Loss: 0.24897431067770007, Avg Validation Loss: 9.886505568689374\n",
            "Epoch 1532/2048, Avg Training Loss: 0.24898338937602074, Avg Validation Loss: 9.886780407603355\n",
            "Epoch 1533/2048, Avg Training Loss: 0.24899283451340243, Avg Validation Loss: 9.887042451831658\n",
            "Epoch 1534/2048, Avg Training Loss: 0.24900235257416187, Avg Validation Loss: 9.887347111620944\n",
            "Epoch 1535/2048, Avg Training Loss: 0.24901105641915214, Avg Validation Loss: 9.887696054863575\n",
            "Epoch 1536/2048, Avg Training Loss: 0.2490196301425996, Avg Validation Loss: 9.88799283548233\n",
            "Epoch 1537/2048, Avg Training Loss: 0.24902883410217816, Avg Validation Loss: 9.88826060307128\n",
            "Epoch 1538/2048, Avg Training Loss: 0.2490382524454342, Avg Validation Loss: 9.888562333575907\n",
            "Epoch 1539/2048, Avg Training Loss: 0.2490469287441404, Avg Validation Loss: 9.888903785757\n",
            "Epoch 1540/2048, Avg Training Loss: 0.2490555151156399, Avg Validation Loss: 9.889190637584875\n",
            "Epoch 1541/2048, Avg Training Loss: 0.24906475393413546, Avg Validation Loss: 9.889447276707115\n",
            "Epoch 1542/2048, Avg Training Loss: 0.2490743459879129, Avg Validation Loss: 9.88968705346108\n",
            "Epoch 1543/2048, Avg Training Loss: 0.24908400516069718, Avg Validation Loss: 9.889967640925594\n",
            "Epoch 1544/2048, Avg Training Loss: 0.24909283954702163, Avg Validation Loss: 9.890291849951131\n",
            "Epoch 1545/2048, Avg Training Loss: 0.24910154278960064, Avg Validation Loss: 9.890563619325295\n",
            "Epoch 1546/2048, Avg Training Loss: 0.24911087813007673, Avg Validation Loss: 9.89080358996828\n",
            "Epoch 1547/2048, Avg Training Loss: 0.24912057565152154, Avg Validation Loss: 9.890966790658181\n",
            "Epoch 1548/2048, Avg Training Loss: 0.2491301953713702, Avg Validation Loss: 9.891120530075373\n",
            "Epoch 1549/2048, Avg Training Loss: 0.24913888207506568, Avg Validation Loss: 9.89129213661421\n",
            "Epoch 1550/2048, Avg Training Loss: 0.24914738452881594, Avg Validation Loss: 9.891398685868953\n",
            "Epoch 1551/2048, Avg Training Loss: 0.24915649432293024, Avg Validation Loss: 9.89147087558779\n",
            "Epoch 1552/2048, Avg Training Loss: 0.2491659350667917, Avg Validation Loss: 9.891525432522709\n",
            "Epoch 1553/2048, Avg Training Loss: 0.24917543233641862, Avg Validation Loss: 9.891621908355969\n",
            "Epoch 1554/2048, Avg Training Loss: 0.24918408882369733, Avg Validation Loss: 9.891764032685668\n",
            "Epoch 1555/2048, Avg Training Loss: 0.249192614524198, Avg Validation Loss: 9.891856072598596\n",
            "Epoch 1556/2048, Avg Training Loss: 0.24920177843916508, Avg Validation Loss: 9.891921846883129\n",
            "Epoch 1557/2048, Avg Training Loss: 0.24921128963364758, Avg Validation Loss: 9.891974362779711\n",
            "Epoch 1558/2048, Avg Training Loss: 0.24922086566833362, Avg Validation Loss: 9.892071185459333\n",
            "Epoch 1559/2048, Avg Training Loss: 0.24922959822339213, Avg Validation Loss: 9.892214954982775\n",
            "Epoch 1560/2048, Avg Training Loss: 0.24923820288770332, Avg Validation Loss: 9.892309303841817\n",
            "Epoch 1561/2048, Avg Training Loss: 0.2492474496520575, Avg Validation Loss: 9.892377744021665\n",
            "Epoch 1562/2048, Avg Training Loss: 0.24925704556796052, Avg Validation Loss: 9.892433116325266\n",
            "Epoch 1563/2048, Avg Training Loss: 0.24926683504532793, Avg Validation Loss: 9.892482422526427\n",
            "Epoch 1564/2048, Avg Training Loss: 0.24927660715358158, Avg Validation Loss: 9.892579972890168\n",
            "Epoch 1565/2048, Avg Training Loss: 0.24928548276176443, Avg Validation Loss: 9.892726610471136\n",
            "Epoch 1566/2048, Avg Training Loss: 0.24929420791638718, Avg Validation Loss: 9.892824933489832\n",
            "Epoch 1567/2048, Avg Training Loss: 0.24930356632348888, Avg Validation Loss: 9.89289793167018\n",
            "Epoch 1568/2048, Avg Training Loss: 0.24931326894203185, Avg Validation Loss: 9.892958160203039\n",
            "Epoch 1569/2048, Avg Training Loss: 0.24932316224932827, Avg Validation Loss: 9.8930124647613\n",
            "Epoch 1570/2048, Avg Training Loss: 0.24933303611623098, Avg Validation Loss: 9.893115094518874\n",
            "Epoch 1571/2048, Avg Training Loss: 0.24934200278185822, Avg Validation Loss: 9.893266844217463\n",
            "Epoch 1572/2048, Avg Training Loss: 0.24935081916201735, Avg Validation Loss: 9.893370240770558\n",
            "Epoch 1573/2048, Avg Training Loss: 0.24936027211622386, Avg Validation Loss: 9.893448271071177\n",
            "Epoch 1574/2048, Avg Training Loss: 0.24937007080800502, Avg Validation Loss: 9.893513488578416\n",
            "Epoch 1575/2048, Avg Training Loss: 0.24938005692740284, Avg Validation Loss: 9.893566999520395\n",
            "Epoch 1576/2048, Avg Training Loss: 0.24938999645410087, Avg Validation Loss: 9.893663743879472\n",
            "Epoch 1577/2048, Avg Training Loss: 0.2493990131172604, Avg Validation Loss: 9.893806990899526\n",
            "Epoch 1578/2048, Avg Training Loss: 0.2494078774122066, Avg Validation Loss: 9.89390057008046\n",
            "Epoch 1579/2048, Avg Training Loss: 0.24941738078111772, Avg Validation Loss: 9.893968199552972\n",
            "Epoch 1580/2048, Avg Training Loss: 0.2494272312760311, Avg Validation Loss: 9.894022825109857\n",
            "Epoch 1581/2048, Avg Training Loss: 0.24943727381934536, Avg Validation Loss: 9.894071500583982\n",
            "Epoch 1582/2048, Avg Training Loss: 0.24944742595663566, Avg Validation Loss: 9.894118059089884\n",
            "Epoch 1583/2048, Avg Training Loss: 0.24945751462878446, Avg Validation Loss: 9.89421513264942\n",
            "Epoch 1584/2048, Avg Training Loss: 0.249466650598826, Avg Validation Loss: 9.89436260472585\n",
            "Epoch 1585/2048, Avg Training Loss: 0.24947562598788237, Avg Validation Loss: 9.894462445711703\n",
            "Epoch 1586/2048, Avg Training Loss: 0.24948524036218617, Avg Validation Loss: 9.894537386177351\n",
            "Epoch 1587/2048, Avg Training Loss: 0.24949520151501095, Avg Validation Loss: 9.894599836708657\n",
            "Epoch 1588/2048, Avg Training Loss: 0.24950535421871115, Avg Validation Loss: 9.89465656150007\n",
            "Epoch 1589/2048, Avg Training Loss: 0.24951561593499927, Avg Validation Loss: 9.894711237269386\n",
            "Epoch 1590/2048, Avg Training Loss: 0.249525942733598, Avg Validation Loss: 9.894765853345687\n",
            "Epoch 1591/2048, Avg Training Loss: 0.24953618158327906, Avg Validation Loss: 9.89487204941682\n",
            "Epoch 1592/2048, Avg Training Loss: 0.2495454552243536, Avg Validation Loss: 9.895034010830631\n",
            "Epoch 1593/2048, Avg Training Loss: 0.24955456394717956, Avg Validation Loss: 9.895151057042526\n",
            "Epoch 1594/2048, Avg Training Loss: 0.24956431424682837, Avg Validation Loss: 9.895244503545422\n",
            "Epoch 1595/2048, Avg Training Loss: 0.2495744121205566, Avg Validation Loss: 9.895325996957501\n",
            "Epoch 1596/2048, Avg Training Loss: 0.24958470138709143, Avg Validation Loss: 9.895401890574833\n",
            "Epoch 1597/2048, Avg Training Loss: 0.2495950990086853, Avg Validation Loss: 9.89547564153829\n",
            "Epoch 1598/2048, Avg Training Loss: 0.24960556079739782, Avg Validation Loss: 9.89554912321431\n",
            "Epoch 1599/2048, Avg Training Loss: 0.249616063209403, Avg Validation Loss: 9.895623343311502\n",
            "Epoch 1600/2048, Avg Training Loss: 0.2496264635577412, Avg Validation Loss: 9.895749427591422\n",
            "Epoch 1601/2048, Avg Training Loss: 0.2496358590474296, Avg Validation Loss: 9.89592642440413\n",
            "Epoch 1602/2048, Avg Training Loss: 0.2496450855203697, Avg Validation Loss: 9.89605579032144\n",
            "Epoch 1603/2048, Avg Training Loss: 0.24965495702712528, Avg Validation Loss: 9.896160025534359\n",
            "Epoch 1604/2048, Avg Training Loss: 0.24966517755996526, Avg Validation Loss: 9.896251418862912\n",
            "Epoch 1605/2048, Avg Training Loss: 0.24967558987815616, Avg Validation Loss: 9.89633667112518\n",
            "Epoch 1606/2048, Avg Training Loss: 0.24968611038097369, Avg Validation Loss: 9.896419419571576\n",
            "Epoch 1607/2048, Avg Training Loss: 0.24969669458081512, Avg Validation Loss: 9.896501638087384\n",
            "Epoch 1608/2048, Avg Training Loss: 0.24970731877460967, Avg Validation Loss: 9.896584390843739\n",
            "Epoch 1609/2048, Avg Training Loss: 0.24971797030709084, Avg Validation Loss: 9.89666824390251\n",
            "Epoch 1610/2048, Avg Training Loss: 0.24972864239632558, Avg Validation Loss: 9.896753491539885\n",
            "Epoch 1611/2048, Avg Training Loss: 0.24973920063106886, Avg Validation Loss: 9.896890891626356\n",
            "Epoch 1612/2048, Avg Training Loss: 0.24974872671957657, Avg Validation Loss: 9.897079279130407\n",
            "Epoch 1613/2048, Avg Training Loss: 0.24975808231623484, Avg Validation Loss: 9.897219957107826\n",
            "Epoch 1614/2048, Avg Training Loss: 0.24976808937861197, Avg Validation Loss: 9.89733537380885\n",
            "Epoch 1615/2048, Avg Training Loss: 0.24977844841998648, Avg Validation Loss: 9.897437790786896\n",
            "Epoch 1616/2048, Avg Training Loss: 0.2497890003468795, Avg Validation Loss: 9.897533895858524\n",
            "Epoch 1617/2048, Avg Training Loss: 0.24979966109884863, Avg Validation Loss: 9.897626137303323\n",
            "Epoch 1618/2048, Avg Training Loss: 0.24981037594271965, Avg Validation Loss: 9.897716194386962\n",
            "Epoch 1619/2048, Avg Training Loss: 0.24982112695527664, Avg Validation Loss: 9.8978058578325\n",
            "Epoch 1620/2048, Avg Training Loss: 0.2498319028655376, Avg Validation Loss: 9.897896086800094\n",
            "Epoch 1621/2048, Avg Training Loss: 0.24984269762880226, Avg Validation Loss: 9.897987388337405\n",
            "Epoch 1622/2048, Avg Training Loss: 0.24985350797865735, Avg Validation Loss: 9.898080023829042\n",
            "Epoch 1623/2048, Avg Training Loss: 0.24986433212590975, Avg Validation Loss: 9.898174121300544\n",
            "Epoch 1624/2048, Avg Training Loss: 0.2498751690670135, Avg Validation Loss: 9.898269736498698\n",
            "Epoch 1625/2048, Avg Training Loss: 0.24988601821644685, Avg Validation Loss: 9.898366886105785\n",
            "Epoch 1626/2048, Avg Training Loss: 0.24989687921128378, Avg Validation Loss: 9.898465565796633\n",
            "Epoch 1627/2048, Avg Training Loss: 0.24990762006694592, Avg Validation Loss: 9.89861639649048\n",
            "Epoch 1628/2048, Avg Training Loss: 0.2499172930915583, Avg Validation Loss: 9.898818122856069\n",
            "Epoch 1629/2048, Avg Training Loss: 0.24992679684463742, Avg Validation Loss: 9.89897195568262\n",
            "Epoch 1630/2048, Avg Training Loss: 0.2499369639767479, Avg Validation Loss: 9.899100330657587\n",
            "Epoch 1631/2048, Avg Training Loss: 0.24994748878537743, Avg Validation Loss: 9.899215503660981\n",
            "Epoch 1632/2048, Avg Training Loss: 0.24995820886602113, Avg Validation Loss: 9.89932416061078\n",
            "Epoch 1633/2048, Avg Training Loss: 0.24996903786854202, Avg Validation Loss: 9.89942994156976\n",
            "Epoch 1634/2048, Avg Training Loss: 0.24997992984285622, Avg Validation Loss: 9.899534821828455\n",
            "Epoch 1635/2048, Avg Training Loss: 0.24999086030949821, Avg Validation Loss: 9.8996398671826\n",
            "Epoch 1636/2048, Avg Training Loss: 0.25000181620258677, Avg Validation Loss: 9.899745646804746\n",
            "Epoch 1637/2048, Avg Training Loss: 0.2500127905245709, Avg Validation Loss: 9.89985245886115\n",
            "Epoch 1638/2048, Avg Training Loss: 0.2500237795046725, Avg Validation Loss: 9.899960453762764\n",
            "Epoch 1639/2048, Avg Training Loss: 0.2500347810881365, Avg Validation Loss: 9.90006970147498\n",
            "Epoch 1640/2048, Avg Training Loss: 0.2500457941329714, Avg Validation Loss: 9.900180228265416\n",
            "Epoch 1641/2048, Avg Training Loss: 0.2500568179828455, Avg Validation Loss: 9.90029203675989\n",
            "Epoch 1642/2048, Avg Training Loss: 0.25006786073403864, Avg Validation Loss: 9.900412246014806\n",
            "Epoch 1643/2048, Avg Training Loss: 0.25007891048762915, Avg Validation Loss: 9.900538445244132\n",
            "Epoch 1644/2048, Avg Training Loss: 0.2500899645781727, Avg Validation Loss: 9.900668271188584\n",
            "Epoch 1645/2048, Avg Training Loss: 0.2501010252946228, Avg Validation Loss: 9.90080043683519\n",
            "Epoch 1646/2048, Avg Training Loss: 0.2501120938041191, Avg Validation Loss: 9.900934237912429\n",
            "Epoch 1647/2048, Avg Training Loss: 0.2501231706809067, Avg Validation Loss: 9.901069285733222\n",
            "Epoch 1648/2048, Avg Training Loss: 0.25013425618610413, Avg Validation Loss: 9.901205362566367\n",
            "Epoch 1649/2048, Avg Training Loss: 0.2501453504155854, Avg Validation Loss: 9.901342343338813\n",
            "Epoch 1650/2048, Avg Training Loss: 0.2501564533781152, Avg Validation Loss: 9.901480153246865\n",
            "Epoch 1651/2048, Avg Training Loss: 0.2501675650366215, Avg Validation Loss: 9.901618744807053\n",
            "Epoch 1652/2048, Avg Training Loss: 0.2501786853299893, Avg Validation Loss: 9.901758085430828\n",
            "Epoch 1653/2048, Avg Training Loss: 0.25018981418456665, Avg Validation Loss: 9.901898150696452\n",
            "Epoch 1654/2048, Avg Training Loss: 0.25020095152023747, Avg Validation Loss: 9.90203892070503\n",
            "Epoch 1655/2048, Avg Training Loss: 0.25021209725362425, Avg Validation Loss: 9.902180378106134\n",
            "Epoch 1656/2048, Avg Training Loss: 0.2502232512997779, Avg Validation Loss: 9.902322507027343\n",
            "Epoch 1657/2048, Avg Training Loss: 0.2502344135730658, Avg Validation Loss: 9.902465292493012\n",
            "Epoch 1658/2048, Avg Training Loss: 0.2502455839876402, Avg Validation Loss: 9.902608720108026\n",
            "Epoch 1659/2048, Avg Training Loss: 0.25025676245768175, Avg Validation Loss: 9.90275277588492\n",
            "Epoch 1660/2048, Avg Training Loss: 0.2502679488975275, Avg Validation Loss: 9.902897446148714\n",
            "Epoch 1661/2048, Avg Training Loss: 0.250279143221736, Avg Validation Loss: 9.903042717483778\n",
            "Epoch 1662/2048, Avg Training Loss: 0.2502903453451202, Avg Validation Loss: 9.903188576703563\n",
            "Epoch 1663/2048, Avg Training Loss: 0.2503015551827625, Avg Validation Loss: 9.903335010832615\n",
            "Epoch 1664/2048, Avg Training Loss: 0.2503127726500221, Avg Validation Loss: 9.903482007095418\n",
            "Epoch 1665/2048, Avg Training Loss: 0.2503239976625368, Avg Validation Loss: 9.903629552908809\n",
            "Epoch 1666/2048, Avg Training Loss: 0.250335230136222, Avg Validation Loss: 9.90377763592567\n",
            "Epoch 1667/2048, Avg Training Loss: 0.25034646998726945, Avg Validation Loss: 9.903926244964502\n",
            "Epoch 1668/2048, Avg Training Loss: 0.25035771713214505, Avg Validation Loss: 9.904075368401612\n",
            "Epoch 1669/2048, Avg Training Loss: 0.2503689714875859, Avg Validation Loss: 9.904224994374953\n",
            "Epoch 1670/2048, Avg Training Loss: 0.2503802329705978, Avg Validation Loss: 9.904375111194517\n",
            "Epoch 1671/2048, Avg Training Loss: 0.2503915014984529, Avg Validation Loss: 9.90452570733929\n",
            "Epoch 1672/2048, Avg Training Loss: 0.2504027769886864, Avg Validation Loss: 9.904676771454314\n",
            "Epoch 1673/2048, Avg Training Loss: 0.25041405935909444, Avg Validation Loss: 9.904828292347831\n",
            "Epoch 1674/2048, Avg Training Loss: 0.25042534852773146, Avg Validation Loss: 9.904980258988497\n",
            "Epoch 1675/2048, Avg Training Loss: 0.2504366444129077, Avg Validation Loss: 9.905132660502636\n",
            "Epoch 1676/2048, Avg Training Loss: 0.2504479469331871, Avg Validation Loss: 9.905285486171575\n",
            "Epoch 1677/2048, Avg Training Loss: 0.2504592560073848, Avg Validation Loss: 9.905438725429025\n",
            "Epoch 1678/2048, Avg Training Loss: 0.25047057155456465, Avg Validation Loss: 9.905592367858489\n",
            "Epoch 1679/2048, Avg Training Loss: 0.2504818934940377, Avg Validation Loss: 9.905746403190749\n",
            "Epoch 1680/2048, Avg Training Loss: 0.2504932217453596, Avg Validation Loss: 9.905900821301385\n",
            "Epoch 1681/2048, Avg Training Loss: 0.250504556228329, Avg Validation Loss: 9.906055612208323\n",
            "Epoch 1682/2048, Avg Training Loss: 0.2505158968629852, Avg Validation Loss: 9.906210766069472\n",
            "Epoch 1683/2048, Avg Training Loss: 0.2505272435696066, Avg Validation Loss: 9.906366273180371\n",
            "Epoch 1684/2048, Avg Training Loss: 0.2505385962687088, Avg Validation Loss: 9.906522123971868\n",
            "Epoch 1685/2048, Avg Training Loss: 0.25054995488104254, Avg Validation Loss: 9.906678309007894\n",
            "Epoch 1686/2048, Avg Training Loss: 0.25056130847868135, Avg Validation Loss: 9.906782498514877\n",
            "Epoch 1687/2048, Avg Training Loss: 0.2505725949986814, Avg Validation Loss: 9.906840820030757\n",
            "Epoch 1688/2048, Avg Training Loss: 0.25058391448876377, Avg Validation Loss: 9.906875905358465\n",
            "Epoch 1689/2048, Avg Training Loss: 0.2505952564294211, Avg Validation Loss: 9.906899949348576\n",
            "Epoch 1690/2048, Avg Training Loss: 0.25060661518300203, Avg Validation Loss: 9.906919508651647\n",
            "Epoch 1691/2048, Avg Training Loss: 0.2506179876712096, Avg Validation Loss: 9.906938094821992\n",
            "Epoch 1692/2048, Avg Training Loss: 0.2506293721668666, Avg Validation Loss: 9.906957575182105\n",
            "Epoch 1693/2048, Avg Training Loss: 0.25064076765938026, Avg Validation Loss: 9.906978929506263\n",
            "Epoch 1694/2048, Avg Training Loss: 0.2506521735197018, Avg Validation Loss: 9.907002658719358\n",
            "Epoch 1695/2048, Avg Training Loss: 0.2506635893203518, Avg Validation Loss: 9.90702900426326\n",
            "Epoch 1696/2048, Avg Training Loss: 0.25067501471087567, Avg Validation Loss: 9.907058065023755\n",
            "Epoch 1697/2048, Avg Training Loss: 0.25068644944922375, Avg Validation Loss: 9.907089864379337\n",
            "Epoch 1698/2048, Avg Training Loss: 0.2506978933462336, Avg Validation Loss: 9.907124388819513\n",
            "Epoch 1699/2048, Avg Training Loss: 0.2507093462172985, Avg Validation Loss: 9.907161605404307\n",
            "Epoch 1700/2048, Avg Training Loss: 0.25072080788158885, Avg Validation Loss: 9.907201471182308\n",
            "Epoch 1701/2048, Avg Training Loss: 0.25073227816161, Avg Validation Loss: 9.907243938266838\n",
            "Epoch 1702/2048, Avg Training Loss: 0.250743756882934, Avg Validation Loss: 9.907288956567808\n",
            "Epoch 1703/2048, Avg Training Loss: 0.2507552438740254, Avg Validation Loss: 9.907336475257823\n",
            "Epoch 1704/2048, Avg Training Loss: 0.2507667389661156, Avg Validation Loss: 9.907386443554959\n",
            "Epoch 1705/2048, Avg Training Loss: 0.25077824199310367, Avg Validation Loss: 9.907438811136783\n",
            "Epoch 1706/2048, Avg Training Loss: 0.2507897527914738, Avg Validation Loss: 9.907493528355312\n",
            "Epoch 1707/2048, Avg Training Loss: 0.2508012712002192, Avg Validation Loss: 9.907550546344721\n",
            "Epoch 1708/2048, Avg Training Loss: 0.25081279706077364, Avg Validation Loss: 9.907609817071284\n",
            "Epoch 1709/2048, Avg Training Loss: 0.2508243302169457, Avg Validation Loss: 9.907671293352278\n",
            "Epoch 1710/2048, Avg Training Loss: 0.2508358705148565, Avg Validation Loss: 9.907734928858307\n",
            "Epoch 1711/2048, Avg Training Loss: 0.2508474178028799, Avg Validation Loss: 9.907800678106831\n",
            "Epoch 1712/2048, Avg Training Loss: 0.25085897193158374, Avg Validation Loss: 9.907868496451108\n",
            "Epoch 1713/2048, Avg Training Loss: 0.2508705327536747, Avg Validation Loss: 9.907938340066805\n",
            "Epoch 1714/2048, Avg Training Loss: 0.25088210012394374, Avg Validation Loss: 9.908010165937512\n",
            "Epoch 1715/2048, Avg Training Loss: 0.25089367389921363, Avg Validation Loss: 9.908083931839847\n",
            "Epoch 1716/2048, Avg Training Loss: 0.2509052539382882, Avg Validation Loss: 9.908159596328407\n",
            "Epoch 1717/2048, Avg Training Loss: 0.2509168401019029, Avg Validation Loss: 9.908237118720857\n",
            "Epoch 1718/2048, Avg Training Loss: 0.2509284322526774, Avg Validation Loss: 9.90831645908322\n",
            "Epoch 1719/2048, Avg Training Loss: 0.2509400477151812, Avg Validation Loss: 9.908418096994833\n",
            "Epoch 1720/2048, Avg Training Loss: 0.2509516359810086, Avg Validation Loss: 9.908535083532117\n",
            "Epoch 1721/2048, Avg Training Loss: 0.2509632004393599, Avg Validation Loss: 9.908660625539945\n",
            "Epoch 1722/2048, Avg Training Loss: 0.2509747542779107, Avg Validation Loss: 9.908791039125672\n",
            "Epoch 1723/2048, Avg Training Loss: 0.2509862963126662, Avg Validation Loss: 9.908877044537995\n",
            "Epoch 1724/2048, Avg Training Loss: 0.25099781722358616, Avg Validation Loss: 9.908923145658346\n",
            "Epoch 1725/2048, Avg Training Loss: 0.25100939757033575, Avg Validation Loss: 9.90894925896739\n",
            "Epoch 1726/2048, Avg Training Loss: 0.25102101325914644, Avg Validation Loss: 9.908966123214977\n",
            "Epoch 1727/2048, Avg Training Loss: 0.2510326513888116, Avg Validation Loss: 9.908979513216375\n",
            "Epoch 1728/2048, Avg Training Loss: 0.25104430499422936, Avg Validation Loss: 9.90899251973234\n",
            "Epoch 1729/2048, Avg Training Loss: 0.2510559702676136, Avg Validation Loss: 9.909006782638944\n",
            "Epoch 1730/2048, Avg Training Loss: 0.2510676450838015, Avg Validation Loss: 9.909023157834502\n",
            "Epoch 1731/2048, Avg Training Loss: 0.2510793282159305, Avg Validation Loss: 9.909042077874977\n",
            "Epoch 1732/2048, Avg Training Loss: 0.251091018917764, Avg Validation Loss: 9.909063746978317\n",
            "Epoch 1733/2048, Avg Training Loss: 0.25110271670109285, Avg Validation Loss: 9.909088246461312\n",
            "Epoch 1734/2048, Avg Training Loss: 0.25111442121703226, Avg Validation Loss: 9.909115591742825\n",
            "Epoch 1735/2048, Avg Training Loss: 0.25112613219268337, Avg Validation Loss: 9.909145763157024\n",
            "Epoch 1736/2048, Avg Training Loss: 0.2511378493973116, Avg Validation Loss: 9.909178722604588\n",
            "Epoch 1737/2048, Avg Training Loss: 0.2511495726242621, Avg Validation Loss: 9.909214422546134\n",
            "Epoch 1738/2048, Avg Training Loss: 0.25116130168127004, Avg Validation Loss: 9.909252810854856\n",
            "Epoch 1739/2048, Avg Training Loss: 0.2511730309847555, Avg Validation Loss: 9.909299652852935\n",
            "Epoch 1740/2048, Avg Training Loss: 0.25118477384447657, Avg Validation Loss: 9.909356339034769\n",
            "Epoch 1741/2048, Avg Training Loss: 0.25119651004476945, Avg Validation Loss: 9.909419265760466\n",
            "Epoch 1742/2048, Avg Training Loss: 0.25120824460855307, Avg Validation Loss: 9.909486465274075\n",
            "Epoch 1743/2048, Avg Training Loss: 0.2512199801487474, Avg Validation Loss: 9.9095568537329\n",
            "Epoch 1744/2048, Avg Training Loss: 0.2512317179945466, Avg Validation Loss: 9.909629825808933\n",
            "Epoch 1745/2048, Avg Training Loss: 0.25124345874567744, Avg Validation Loss: 9.90970503194601\n",
            "Epoch 1746/2048, Avg Training Loss: 0.251255202668927, Avg Validation Loss: 9.909782262053987\n",
            "Epoch 1747/2048, Avg Training Loss: 0.2512669499028002, Avg Validation Loss: 9.909861384202122\n",
            "Epoch 1748/2048, Avg Training Loss: 0.25127870046221745, Avg Validation Loss: 9.909942309057746\n",
            "Epoch 1749/2048, Avg Training Loss: 0.2512904542971207, Avg Validation Loss: 9.910024970655805\n",
            "Epoch 1750/2048, Avg Training Loss: 0.2513022113236755, Avg Validation Loss: 9.910109315996479\n",
            "Epoch 1751/2048, Avg Training Loss: 0.2513139714408734, Avg Validation Loss: 9.910195299415134\n",
            "Epoch 1752/2048, Avg Training Loss: 0.2513257345393579, Avg Validation Loss: 9.910282879492504\n",
            "Epoch 1753/2048, Avg Training Loss: 0.2513375005061057, Avg Validation Loss: 9.910372017296874\n",
            "Epoch 1754/2048, Avg Training Loss: 0.2513492692269026, Avg Validation Loss: 9.91046267577068\n",
            "Epoch 1755/2048, Avg Training Loss: 0.25136104058764347, Avg Validation Loss: 9.910554819112456\n",
            "Epoch 1756/2048, Avg Training Loss: 0.25137281447500703, Avg Validation Loss: 9.910648412484223\n",
            "Epoch 1757/2048, Avg Training Loss: 0.25138459077679753, Avg Validation Loss: 9.910743421858442\n",
            "Epoch 1758/2048, Avg Training Loss: 0.25139636938210935, Avg Validation Loss: 9.910839813929597\n",
            "Epoch 1759/2048, Avg Training Loss: 0.2514081501813991, Avg Validation Loss: 9.91093755606089\n",
            "Epoch 1760/2048, Avg Training Loss: 0.25141993306650623, Avg Validation Loss: 9.911036616250009\n",
            "Epoch 1761/2048, Avg Training Loss: 0.25143171793065, Avg Validation Loss: 9.9111369631053\n",
            "Epoch 1762/2048, Avg Training Loss: 0.2514435046684115, Avg Validation Loss: 9.911238565827704\n",
            "Epoch 1763/2048, Avg Training Loss: 0.25145529317570975, Avg Validation Loss: 9.911341394195903\n",
            "Epoch 1764/2048, Avg Training Loss: 0.25146708334977486, Avg Validation Loss: 9.911445418553294\n",
            "Epoch 1765/2048, Avg Training Loss: 0.2514788750891189, Avg Validation Loss: 9.911550609796075\n",
            "Epoch 1766/2048, Avg Training Loss: 0.2514906682935082, Avg Validation Loss: 9.911656939362008\n",
            "Epoch 1767/2048, Avg Training Loss: 0.25150246286393396, Avg Validation Loss: 9.911764379219655\n",
            "Epoch 1768/2048, Avg Training Loss: 0.25151425870258537, Avg Validation Loss: 9.911872901857988\n",
            "Epoch 1769/2048, Avg Training Loss: 0.25152605571282155, Avg Validation Loss: 9.911982480276244\n",
            "Epoch 1770/2048, Avg Training Loss: 0.2515378537991458, Avg Validation Loss: 9.912093087974041\n",
            "Epoch 1771/2048, Avg Training Loss: 0.2515496528671791, Avg Validation Loss: 9.912204698941727\n",
            "Epoch 1772/2048, Avg Training Loss: 0.25156145282363557, Avg Validation Loss: 9.912317287650904\n",
            "Epoch 1773/2048, Avg Training Loss: 0.25157325357629773, Avg Validation Loss: 9.91243082904518\n",
            "Epoch 1774/2048, Avg Training Loss: 0.2515850550339932, Avg Validation Loss: 9.912545298531095\n",
            "Epoch 1775/2048, Avg Training Loss: 0.25159685710657115, Avg Validation Loss: 9.912660671969226\n",
            "Epoch 1776/2048, Avg Training Loss: 0.25160865970488083, Avg Validation Loss: 9.912776925665488\n",
            "Epoch 1777/2048, Avg Training Loss: 0.2516204627407488, Avg Validation Loss: 9.9128940363626\n",
            "Epoch 1778/2048, Avg Training Loss: 0.2516322661269587, Avg Validation Loss: 9.913011981231714\n",
            "Epoch 1779/2048, Avg Training Loss: 0.2516440697772301, Avg Validation Loss: 9.913130737864229\n",
            "Epoch 1780/2048, Avg Training Loss: 0.2516558736061991, Avg Validation Loss: 9.913250284263762\n",
            "Epoch 1781/2048, Avg Training Loss: 0.2516676775293987, Avg Validation Loss: 9.913370598838277\n",
            "Epoch 1782/2048, Avg Training Loss: 0.25167948146324, Avg Validation Loss: 9.913491660392358\n",
            "Epoch 1783/2048, Avg Training Loss: 0.2516912853249938, Avg Validation Loss: 9.913613448119673\n",
            "Epoch 1784/2048, Avg Training Loss: 0.25170308903277305, Avg Validation Loss: 9.913735941595554\n",
            "Epoch 1785/2048, Avg Training Loss: 0.2517148925055153, Avg Validation Loss: 9.91385912076973\n",
            "Epoch 1786/2048, Avg Training Loss: 0.251726695662966, Avg Validation Loss: 9.913982965959223\n",
            "Epoch 1787/2048, Avg Training Loss: 0.2517384984256623, Avg Validation Loss: 9.914107457841357\n",
            "Epoch 1788/2048, Avg Training Loss: 0.2517503007149167, Avg Validation Loss: 9.914232577446931\n",
            "Epoch 1789/2048, Avg Training Loss: 0.251762102452802, Avg Validation Loss: 9.914358306153527\n",
            "Epoch 1790/2048, Avg Training Loss: 0.25177390356213597, Avg Validation Loss: 9.914484625678913\n",
            "Epoch 1791/2048, Avg Training Loss: 0.251785703966467, Avg Validation Loss: 9.914611518074633\n",
            "Epoch 1792/2048, Avg Training Loss: 0.2517975035900596, Avg Validation Loss: 9.914738965719678\n",
            "Epoch 1793/2048, Avg Training Loss: 0.2518093023578804, Avg Validation Loss: 9.914866951314304\n",
            "Epoch 1794/2048, Avg Training Loss: 0.25182110019558523, Avg Validation Loss: 9.914995457873957\n",
            "Epoch 1795/2048, Avg Training Loss: 0.25183289702950545, Avg Validation Loss: 9.915124468723358\n",
            "Epoch 1796/2048, Avg Training Loss: 0.2518446927866353, Avg Validation Loss: 9.91525396749063\n",
            "Epoch 1797/2048, Avg Training Loss: 0.2518564873946196, Avg Validation Loss: 9.915383938101627\n",
            "Epoch 1798/2048, Avg Training Loss: 0.25186828078174145, Avg Validation Loss: 9.915514364774312\n",
            "Epoch 1799/2048, Avg Training Loss: 0.2518800728769108, Avg Validation Loss: 9.915645232013254\n",
            "Epoch 1800/2048, Avg Training Loss: 0.2518918636096525, Avg Validation Loss: 9.915776524604267\n",
            "Epoch 1801/2048, Avg Training Loss: 0.2519036529100954, Avg Validation Loss: 9.915908227609139\n",
            "Epoch 1802/2048, Avg Training Loss: 0.25191544070896155, Avg Validation Loss: 9.916040326360422\n",
            "Epoch 1803/2048, Avg Training Loss: 0.2519272269375555, Avg Validation Loss: 9.91617280645639\n",
            "Epoch 1804/2048, Avg Training Loss: 0.2519390115277538, Avg Validation Loss: 9.916305647358323\n",
            "Epoch 1805/2048, Avg Training Loss: 0.25195079441199525, Avg Validation Loss: 9.916438821258813\n",
            "Epoch 1806/2048, Avg Training Loss: 0.25196257552327095, Avg Validation Loss: 9.916572314011391\n",
            "Epoch 1807/2048, Avg Training Loss: 0.2519743547951149, Avg Validation Loss: 9.916706112198716\n",
            "Epoch 1808/2048, Avg Training Loss: 0.2519861321615943, Avg Validation Loss: 9.916840202645965\n",
            "Epoch 1809/2048, Avg Training Loss: 0.25199789662543604, Avg Validation Loss: 9.916961757425904\n",
            "Epoch 1810/2048, Avg Training Loss: 0.25200959870729156, Avg Validation Loss: 9.917072238793914\n",
            "Epoch 1811/2048, Avg Training Loss: 0.252021279639376, Avg Validation Loss: 9.917177154592734\n",
            "Epoch 1812/2048, Avg Training Loss: 0.25203294826724754, Avg Validation Loss: 9.917279471041171\n",
            "Epoch 1813/2048, Avg Training Loss: 0.252044609244732, Avg Validation Loss: 9.917380780414819\n",
            "Epoch 1814/2048, Avg Training Loss: 0.25205626500214756, Avg Validation Loss: 9.917481932413798\n",
            "Epoch 1815/2048, Avg Training Loss: 0.2520679167911443, Avg Validation Loss: 9.9175833755033\n",
            "Epoch 1816/2048, Avg Training Loss: 0.2520795652389556, Avg Validation Loss: 9.917685341443095\n",
            "Epoch 1817/2048, Avg Training Loss: 0.25209121064227985, Avg Validation Loss: 9.91778794504082\n",
            "Epoch 1818/2048, Avg Training Loss: 0.25210285312306063, Avg Validation Loss: 9.917891236743937\n",
            "Epoch 1819/2048, Avg Training Loss: 0.2521144927110428, Avg Validation Loss: 9.917995225723589\n",
            "Epoch 1820/2048, Avg Training Loss: 0.25212612938751505, Avg Validation Loss: 9.918099910148435\n",
            "Epoch 1821/2048, Avg Training Loss: 0.2521377631084792, Avg Validation Loss: 9.918205280154131\n",
            "Epoch 1822/2048, Avg Training Loss: 0.25214939381691864, Avg Validation Loss: 9.918311320860477\n",
            "Epoch 1823/2048, Avg Training Loss: 0.2521610214492906, Avg Validation Loss: 9.918418014854261\n",
            "Epoch 1824/2048, Avg Training Loss: 0.2521726459389565, Avg Validation Loss: 9.918525343528213\n",
            "Epoch 1825/2048, Avg Training Loss: 0.2521842672179932, Avg Validation Loss: 9.918633287801587\n",
            "Epoch 1826/2048, Avg Training Loss: 0.2521958852181434, Avg Validation Loss: 9.918741828506503\n",
            "Epoch 1827/2048, Avg Training Loss: 0.25220742092427473, Avg Validation Loss: 9.918830646175952\n",
            "Epoch 1828/2048, Avg Training Loss: 0.25221875359919627, Avg Validation Loss: 9.91889775236538\n",
            "Epoch 1829/2048, Avg Training Loss: 0.2522300123218991, Avg Validation Loss: 9.918954015738477\n",
            "Epoch 1830/2048, Avg Training Loss: 0.25224123120839714, Avg Validation Loss: 9.919005311113095\n",
            "Epoch 1831/2048, Avg Training Loss: 0.2522524282807135, Avg Validation Loss: 9.919054803803602\n",
            "Epoch 1832/2048, Avg Training Loss: 0.2522636130266077, Avg Validation Loss: 9.919104189582766\n",
            "Epoch 1833/2048, Avg Training Loss: 0.2522747904132992, Avg Validation Loss: 9.919154367500722\n",
            "Epoch 1834/2048, Avg Training Loss: 0.2522859630153574, Avg Validation Loss: 9.919205804883372\n",
            "Epoch 1835/2048, Avg Training Loss: 0.25229713214186206, Avg Validation Loss: 9.919258735310178\n",
            "Epoch 1836/2048, Avg Training Loss: 0.2523082896319822, Avg Validation Loss: 9.91930501955845\n",
            "Epoch 1837/2048, Avg Training Loss: 0.2523194612596251, Avg Validation Loss: 9.919348653232086\n",
            "Epoch 1838/2048, Avg Training Loss: 0.25233063946580947, Avg Validation Loss: 9.919391773126561\n",
            "Epoch 1839/2048, Avg Training Loss: 0.2523418201593571, Avg Validation Loss: 9.91943551216518\n",
            "Epoch 1840/2048, Avg Training Loss: 0.2523530011011494, Avg Validation Loss: 9.919480461313062\n",
            "Epoch 1841/2048, Avg Training Loss: 0.2523641810420988, Avg Validation Loss: 9.919526919133553\n",
            "Epoch 1842/2048, Avg Training Loss: 0.25237535926293336, Avg Validation Loss: 9.919575026610678\n",
            "Epoch 1843/2048, Avg Training Loss: 0.2523865353284062, Avg Validation Loss: 9.919624839983706\n",
            "Epoch 1844/2048, Avg Training Loss: 0.2523977089559848, Avg Validation Loss: 9.919676370091237\n",
            "Epoch 1845/2048, Avg Training Loss: 0.2524088799456789, Avg Validation Loss: 9.919729603621304\n",
            "Epoch 1846/2048, Avg Training Loss: 0.25242004814252456, Avg Validation Loss: 9.919784514585508\n",
            "Epoch 1847/2048, Avg Training Loss: 0.25243121341651453, Avg Validation Loss: 9.919841070511465\n",
            "Epoch 1848/2048, Avg Training Loss: 0.25244237565184996, Avg Validation Loss: 9.919899235781557\n",
            "Epoch 1849/2048, Avg Training Loss: 0.2524535347411732, Avg Validation Loss: 9.919958973430017\n",
            "Epoch 1850/2048, Avg Training Loss: 0.2524646905824636, Avg Validation Loss: 9.920020246107041\n",
            "Epoch 1851/2048, Avg Training Loss: 0.25247584307735615, Avg Validation Loss: 9.92008301659306\n",
            "Epoch 1852/2048, Avg Training Loss: 0.25248699213022174, Avg Validation Loss: 9.920147248069968\n",
            "Epoch 1853/2048, Avg Training Loss: 0.25249813764765494, Avg Validation Loss: 9.920212904261257\n",
            "Epoch 1854/2048, Avg Training Loss: 0.25250927953818053, Avg Validation Loss: 9.92027994950134\n",
            "Epoch 1855/2048, Avg Training Loss: 0.2525204177120769, Avg Validation Loss: 9.920348348309645\n",
            "Epoch 1856/2048, Avg Training Loss: 0.25253155208126316, Avg Validation Loss: 9.920418063402598\n",
            "Epoch 1857/2048, Avg Training Loss: 0.25254268255922097, Avg Validation Loss: 9.920489060528727\n",
            "Epoch 1858/2048, Avg Training Loss: 0.2525538090609342, Avg Validation Loss: 9.920561306594616\n",
            "Epoch 1859/2048, Avg Training Loss: 0.2525649315028409, Avg Validation Loss: 9.920634769144876\n",
            "Epoch 1860/2048, Avg Training Loss: 0.2525760498027898, Avg Validation Loss: 9.920709416351718\n",
            "Epoch 1861/2048, Avg Training Loss: 0.25258716388000213, Avg Validation Loss: 9.920785217003784\n",
            "Epoch 1862/2048, Avg Training Loss: 0.25259827365503473, Avg Validation Loss: 9.920862140494753\n",
            "Epoch 1863/2048, Avg Training Loss: 0.252609379049746, Avg Validation Loss: 9.920940156811861\n",
            "Epoch 1864/2048, Avg Training Loss: 0.2526204799872624, Avg Validation Loss: 9.921019236524556\n",
            "Epoch 1865/2048, Avg Training Loss: 0.2526315763919467, Avg Validation Loss: 9.92109935077328\n",
            "Epoch 1866/2048, Avg Training Loss: 0.25264266818936737, Avg Validation Loss: 9.921180471258428\n",
            "Epoch 1867/2048, Avg Training Loss: 0.2526537553062683, Avg Validation Loss: 9.921262570229523\n",
            "Epoch 1868/2048, Avg Training Loss: 0.2526648376705403, Avg Validation Loss: 9.921345620474593\n",
            "Epoch 1869/2048, Avg Training Loss: 0.2526759152111925, Avg Validation Loss: 9.921429595309746\n",
            "Epoch 1870/2048, Avg Training Loss: 0.2526869878583258, Avg Validation Loss: 9.921514468568947\n",
            "Epoch 1871/2048, Avg Training Loss: 0.2526980555431057, Avg Validation Loss: 9.921600214594033\n",
            "Epoch 1872/2048, Avg Training Loss: 0.25270911425504233, Avg Validation Loss: 9.92169501019225\n",
            "Epoch 1873/2048, Avg Training Loss: 0.2527202263719066, Avg Validation Loss: 9.921800880673214\n",
            "Epoch 1874/2048, Avg Training Loss: 0.2527313503506462, Avg Validation Loss: 9.921912831029513\n",
            "Epoch 1875/2048, Avg Training Loss: 0.25274247805280137, Avg Validation Loss: 9.922028155743906\n",
            "Epoch 1876/2048, Avg Training Loss: 0.25275360515199324, Avg Validation Loss: 9.922145386857014\n",
            "Epoch 1877/2048, Avg Training Loss: 0.25276472933708455, Avg Validation Loss: 9.922263725465541\n",
            "Epoch 1878/2048, Avg Training Loss: 0.25277584936269165, Avg Validation Loss: 9.922382734441548\n",
            "Epoch 1879/2048, Avg Training Loss: 0.2527869645472133, Avg Validation Loss: 9.922502172330436\n",
            "Epoch 1880/2048, Avg Training Loss: 0.25279807450739267, Avg Validation Loss: 9.92262190355848\n",
            "Epoch 1881/2048, Avg Training Loss: 0.25280917901794786, Avg Validation Loss: 9.922741849889855\n",
            "Epoch 1882/2048, Avg Training Loss: 0.2528202779373392, Avg Validation Loss: 9.922861964182214\n",
            "Epoch 1883/2048, Avg Training Loss: 0.25283137116851473, Avg Validation Loss: 9.922982216196736\n",
            "Epoch 1884/2048, Avg Training Loss: 0.25284245863815225, Avg Validation Loss: 9.923102584924917\n",
            "Epoch 1885/2048, Avg Training Loss: 0.2528535402856817, Avg Validation Loss: 9.923223054438466\n",
            "Epoch 1886/2048, Avg Training Loss: 0.25286461605747895, Avg Validation Loss: 9.923343611643894\n",
            "Epoch 1887/2048, Avg Training Loss: 0.25287568590379406, Avg Validation Loss: 9.923464245066882\n",
            "Epoch 1888/2048, Avg Training Loss: 0.2528867497771241, Avg Validation Loss: 9.923584944193426\n",
            "Epoch 1889/2048, Avg Training Loss: 0.25289780666295786, Avg Validation Loss: 9.923707385906269\n",
            "Epoch 1890/2048, Avg Training Loss: 0.2529088749216888, Avg Validation Loss: 9.923833216919698\n",
            "Epoch 1891/2048, Avg Training Loss: 0.2529199377742567, Avg Validation Loss: 9.923960808749044\n",
            "Epoch 1892/2048, Avg Training Loss: 0.2529309947425867, Avg Validation Loss: 9.924089278233968\n",
            "Epoch 1893/2048, Avg Training Loss: 0.2529420455598114, Avg Validation Loss: 9.92421814537904\n",
            "Epoch 1894/2048, Avg Training Loss: 0.25295309006895117, Avg Validation Loss: 9.924347148289801\n",
            "Epoch 1895/2048, Avg Training Loss: 0.2529641281702612, Avg Validation Loss: 9.924476143077735\n",
            "Epoch 1896/2048, Avg Training Loss: 0.25297515979385515, Avg Validation Loss: 9.924605049722171\n",
            "Epoch 1897/2048, Avg Training Loss: 0.25298618488547236, Avg Validation Loss: 9.924733822788532\n",
            "Epoch 1898/2048, Avg Training Loss: 0.2529972033990819, Avg Validation Loss: 9.92486243559043\n",
            "Epoch 1899/2048, Avg Training Loss: 0.2530082152930424, Avg Validation Loss: 9.924990871622956\n",
            "Epoch 1900/2048, Avg Training Loss: 0.25301922052811043, Avg Validation Loss: 9.925119119928597\n",
            "Epoch 1901/2048, Avg Training Loss: 0.2530302190664069, Avg Validation Loss: 9.925247172590211\n",
            "Epoch 1902/2048, Avg Training Loss: 0.2530412108708819, Avg Validation Loss: 9.925375023374365\n",
            "Epoch 1903/2048, Avg Training Loss: 0.2530521959050367, Avg Validation Loss: 9.925502666996904\n",
            "Epoch 1904/2048, Avg Training Loss: 0.25306317413277934, Avg Validation Loss: 9.92563009872504\n",
            "Epoch 1905/2048, Avg Training Loss: 0.25307414551834867, Avg Validation Loss: 9.92575731416149\n",
            "Epoch 1906/2048, Avg Training Loss: 0.2530851100262741, Avg Validation Loss: 9.925884309127037\n",
            "Epoch 1907/2048, Avg Training Loss: 0.25309606305354687, Avg Validation Loss: 9.926006573686141\n",
            "Epoch 1908/2048, Avg Training Loss: 0.25310701747571773, Avg Validation Loss: 9.92612560891392\n",
            "Epoch 1909/2048, Avg Training Loss: 0.2531179717404776, Avg Validation Loss: 9.926242885844276\n",
            "Epoch 1910/2048, Avg Training Loss: 0.2531289226515342, Avg Validation Loss: 9.926359199263505\n",
            "Epoch 1911/2048, Avg Training Loss: 0.25313986848318704, Avg Validation Loss: 9.926474974988258\n",
            "Epoch 1912/2048, Avg Training Loss: 0.253150808295769, Avg Validation Loss: 9.926590440657156\n",
            "Epoch 1913/2048, Avg Training Loss: 0.25316174156989363, Avg Validation Loss: 9.926705717038871\n",
            "Epoch 1914/2048, Avg Training Loss: 0.2531726680109883, Avg Validation Loss: 9.926820865535436\n",
            "Epoch 1915/2048, Avg Training Loss: 0.2531835879330116, Avg Validation Loss: 9.926943130832123\n",
            "Epoch 1916/2048, Avg Training Loss: 0.2531944922360198, Avg Validation Loss: 9.927071667945908\n",
            "Epoch 1917/2048, Avg Training Loss: 0.2532053746826686, Avg Validation Loss: 9.927203350110918\n",
            "Epoch 1918/2048, Avg Training Loss: 0.253216242066143, Avg Validation Loss: 9.927336489356883\n",
            "Epoch 1919/2048, Avg Training Loss: 0.25322709800228216, Avg Validation Loss: 9.927470174905793\n",
            "Epoch 1920/2048, Avg Training Loss: 0.25323794440917957, Avg Validation Loss: 9.927603915807198\n",
            "Epoch 1921/2048, Avg Training Loss: 0.2532487822979326, Avg Validation Loss: 9.927737447895465\n",
            "Epoch 1922/2048, Avg Training Loss: 0.25325961219514115, Avg Validation Loss: 9.927870629506582\n",
            "Epoch 1923/2048, Avg Training Loss: 0.2532704343686244, Avg Validation Loss: 9.928003385141702\n",
            "Epoch 1924/2048, Avg Training Loss: 0.2532812489480044, Avg Validation Loss: 9.928135675032037\n",
            "Epoch 1925/2048, Avg Training Loss: 0.25329205598912546, Avg Validation Loss: 9.928267478696363\n",
            "Epoch 1926/2048, Avg Training Loss: 0.25330285550847087, Avg Validation Loss: 9.92839878605799\n",
            "Epoch 1927/2048, Avg Training Loss: 0.2533136475015499, Avg Validation Loss: 9.928529592645706\n",
            "Epoch 1928/2048, Avg Training Loss: 0.253324431952723, Avg Validation Loss: 9.928659897001253\n",
            "Epoch 1929/2048, Avg Training Loss: 0.2533352088404509, Avg Validation Loss: 9.928789699278907\n",
            "Epoch 1930/2048, Avg Training Loss: 0.25334597814010046, Avg Validation Loss: 9.928919000489149\n",
            "Epoch 1931/2048, Avg Training Loss: 0.25335673982544327, Avg Validation Loss: 9.92904780209034\n",
            "Epoch 1932/2048, Avg Training Loss: 0.2533674952310785, Avg Validation Loss: 9.929170068539042\n",
            "Epoch 1933/2048, Avg Training Loss: 0.2533780944031365, Avg Validation Loss: 9.929244098063531\n",
            "Epoch 1934/2048, Avg Training Loss: 0.2533885220002392, Avg Validation Loss: 9.929284677327086\n",
            "Epoch 1935/2048, Avg Training Loss: 0.2533988680839396, Avg Validation Loss: 9.929307786334983\n",
            "Epoch 1936/2048, Avg Training Loss: 0.25340916786849244, Avg Validation Loss: 9.929322000120166\n",
            "Epoch 1937/2048, Avg Training Loss: 0.253419439981435, Avg Validation Loss: 9.929331914359961\n",
            "Epoch 1938/2048, Avg Training Loss: 0.253429694258082, Avg Validation Loss: 9.929339986389845\n",
            "Epoch 1939/2048, Avg Training Loss: 0.25343993587684455, Avg Validation Loss: 9.929347524562887\n",
            "Epoch 1940/2048, Avg Training Loss: 0.2534501675508544, Avg Validation Loss: 9.929355219934868\n",
            "Epoch 1941/2048, Avg Training Loss: 0.253460390688752, Avg Validation Loss: 9.929363432003687\n",
            "Epoch 1942/2048, Avg Training Loss: 0.2534706060092621, Avg Validation Loss: 9.92937234227914\n",
            "Epoch 1943/2048, Avg Training Loss: 0.2534808138664939, Avg Validation Loss: 9.929382036821984\n",
            "Epoch 1944/2048, Avg Training Loss: 0.25349101442209143, Avg Validation Loss: 9.929392550607329\n",
            "Epoch 1945/2048, Avg Training Loss: 0.25350120773631873, Avg Validation Loss: 9.929403891368976\n",
            "Epoch 1946/2048, Avg Training Loss: 0.25351139381624266, Avg Validation Loss: 9.929416052414194\n",
            "Epoch 1947/2048, Avg Training Loss: 0.25352157264121455, Avg Validation Loss: 9.929429019509389\n",
            "Epoch 1948/2048, Avg Training Loss: 0.2535317441763385, Avg Validation Loss: 9.929442774578305\n",
            "Epoch 1949/2048, Avg Training Loss: 0.25354190837958673, Avg Validation Loss: 9.929457297686524\n",
            "Epoch 1950/2048, Avg Training Loss: 0.25355206520555246, Avg Validation Loss: 9.929472568104623\n",
            "Epoch 1951/2048, Avg Training Loss: 0.2535622146074254, Avg Validation Loss: 9.929488564875978\n",
            "Epoch 1952/2048, Avg Training Loss: 0.253572356538028, Avg Validation Loss: 9.929505267118236\n",
            "Epoch 1953/2048, Avg Training Loss: 0.2535824909503534, Avg Validation Loss: 9.929522654181643\n",
            "Epoch 1954/2048, Avg Training Loss: 0.2535926177978423, Avg Validation Loss: 9.929540705730467\n",
            "Epoch 1955/2048, Avg Training Loss: 0.2536027370345205, Avg Validation Loss: 9.929559401783106\n",
            "Epoch 1956/2048, Avg Training Loss: 0.2536128486150637, Avg Validation Loss: 9.929578722730067\n",
            "Epoch 1957/2048, Avg Training Loss: 0.2536229524948239, Avg Validation Loss: 9.929598649340095\n",
            "Epoch 1958/2048, Avg Training Loss: 0.25363304862983604, Avg Validation Loss: 9.929619162760028\n",
            "Epoch 1959/2048, Avg Training Loss: 0.2536431369768133, Avg Validation Loss: 9.929640244511303\n",
            "Epoch 1960/2048, Avg Training Loss: 0.25365321749313857, Avg Validation Loss: 9.929661876484762\n",
            "Epoch 1961/2048, Avg Training Loss: 0.2536632901368523, Avg Validation Loss: 9.929684040934596\n",
            "Epoch 1962/2048, Avg Training Loss: 0.25367335486663917, Avg Validation Loss: 9.929706720471895\n",
            "Epoch 1963/2048, Avg Training Loss: 0.2536834116418149, Avg Validation Loss: 9.929729898058048\n",
            "Epoch 1964/2048, Avg Training Loss: 0.25369346042231283, Avg Validation Loss: 9.929753556998126\n",
            "Epoch 1965/2048, Avg Training Loss: 0.25370350116867013, Avg Validation Loss: 9.92977768093431\n",
            "Epoch 1966/2048, Avg Training Loss: 0.2537135338420151, Avg Validation Loss: 9.92980225383942\n",
            "Epoch 1967/2048, Avg Training Loss: 0.2537235584040542, Avg Validation Loss: 9.929827260010548\n",
            "Epoch 1968/2048, Avg Training Loss: 0.25373357481705966, Avg Validation Loss: 9.929852684062784\n",
            "Epoch 1969/2048, Avg Training Loss: 0.2537435830438571, Avg Validation Loss: 9.929878510923098\n",
            "Epoch 1970/2048, Avg Training Loss: 0.2537535830478139, Avg Validation Loss: 9.929904725824315\n",
            "Epoch 1971/2048, Avg Training Loss: 0.25376357479282785, Avg Validation Loss: 9.929931314299202\n",
            "Epoch 1972/2048, Avg Training Loss: 0.25377355824331554, Avg Validation Loss: 9.92995826217471\n",
            "Epoch 1973/2048, Avg Training Loss: 0.2537835333642018, Avg Validation Loss: 9.92998555556627\n",
            "Epoch 1974/2048, Avg Training Loss: 0.25379350012090884, Avg Validation Loss: 9.930013180872285\n",
            "Epoch 1975/2048, Avg Training Loss: 0.2538034584793464, Avg Validation Loss: 9.930041124768627\n",
            "Epoch 1976/2048, Avg Training Loss: 0.25381340840590116, Avg Validation Loss: 9.930069374203354\n",
            "Epoch 1977/2048, Avg Training Loss: 0.25382334986742733, Avg Validation Loss: 9.93009791639144\n",
            "Epoch 1978/2048, Avg Training Loss: 0.25383328283123713, Avg Validation Loss: 9.930126738809662\n",
            "Epoch 1979/2048, Avg Training Loss: 0.25384323198557446, Avg Validation Loss: 9.930127441863112\n",
            "Epoch 1980/2048, Avg Training Loss: 0.2538531282977293, Avg Validation Loss: 9.930093010434705\n",
            "Epoch 1981/2048, Avg Training Loss: 0.2538630864122595, Avg Validation Loss: 9.930040794478819\n",
            "Epoch 1982/2048, Avg Training Loss: 0.25387307488723126, Avg Validation Loss: 9.929980142532697\n",
            "Epoch 1983/2048, Avg Training Loss: 0.2538830768101592, Avg Validation Loss: 9.929916079563387\n",
            "Epoch 1984/2048, Avg Training Loss: 0.25389308305247094, Avg Validation Loss: 9.929851294872643\n",
            "Epoch 1985/2048, Avg Training Loss: 0.2539030886629668, Avg Validation Loss: 9.92978721631268\n",
            "Epoch 1986/2048, Avg Training Loss: 0.25391309093484743, Avg Validation Loss: 9.929724590695166\n",
            "Epoch 1987/2048, Avg Training Loss: 0.2539230883683043, Avg Validation Loss: 9.929663797372013\n",
            "Epoch 1988/2048, Avg Training Loss: 0.2539330801133203, Avg Validation Loss: 9.929605017648719\n",
            "Epoch 1989/2048, Avg Training Loss: 0.2539430656702523, Avg Validation Loss: 9.9295483263067\n",
            "Epoch 1990/2048, Avg Training Loss: 0.2539530444881839, Avg Validation Loss: 9.929494664276678\n",
            "Epoch 1991/2048, Avg Training Loss: 0.2539630231120202, Avg Validation Loss: 9.929444244339628\n",
            "Epoch 1992/2048, Avg Training Loss: 0.2539729949226666, Avg Validation Loss: 9.929396473257396\n",
            "Epoch 1993/2048, Avg Training Loss: 0.25398295972247004, Avg Validation Loss: 9.929351009886535\n",
            "Epoch 1994/2048, Avg Training Loss: 0.25399291737421836, Avg Validation Loss: 9.929307649713527\n",
            "Epoch 1995/2048, Avg Training Loss: 0.254002867771632, Avg Validation Loss: 9.929266262459828\n",
            "Epoch 1996/2048, Avg Training Loss: 0.2540128108246143, Avg Validation Loss: 9.929226758360743\n",
            "Epoch 1997/2048, Avg Training Loss: 0.25402274645190076, Avg Validation Loss: 9.9291890699372\n",
            "Epoch 1998/2048, Avg Training Loss: 0.2540326745774051, Avg Validation Loss: 9.929153142138558\n",
            "Epoch 1999/2048, Avg Training Loss: 0.25404259512840344, Avg Validation Loss: 9.929118927008714\n",
            "Epoch 2000/2048, Avg Training Loss: 0.25405250803462726, Avg Validation Loss: 9.92908638079624\n",
            "Epoch 2001/2048, Avg Training Loss: 0.2540624132278021, Avg Validation Loss: 9.929055462385392\n",
            "Epoch 2002/2048, Avg Training Loss: 0.2540723106414056, Avg Validation Loss: 9.929026132440907\n",
            "Epoch 2003/2048, Avg Training Loss: 0.2540822002105295, Avg Validation Loss: 9.928998352938647\n",
            "Epoch 2004/2048, Avg Training Loss: 0.25409208187179355, Avg Validation Loss: 9.92897208690492\n",
            "Epoch 2005/2048, Avg Training Loss: 0.25410195556328385, Avg Validation Loss: 9.928947298268668\n",
            "Epoch 2006/2048, Avg Training Loss: 0.2541118212245022, Avg Validation Loss: 9.928923951774811\n",
            "Epoch 2007/2048, Avg Training Loss: 0.2541216787963226, Avg Validation Loss: 9.928902012930768\n",
            "Epoch 2008/2048, Avg Training Loss: 0.2541315282209502, Avg Validation Loss: 9.928881447971062\n",
            "Epoch 2009/2048, Avg Training Loss: 0.2541413694418829, Avg Validation Loss: 9.928862223831823\n",
            "Epoch 2010/2048, Avg Training Loss: 0.2541512024038738, Avg Validation Loss: 9.928844308130762\n",
            "Epoch 2011/2048, Avg Training Loss: 0.2541610270528952, Avg Validation Loss: 9.928827669150262\n",
            "Epoch 2012/2048, Avg Training Loss: 0.25417084333610407, Avg Validation Loss: 9.928812275822258\n",
            "Epoch 2013/2048, Avg Training Loss: 0.2541806512018081, Avg Validation Loss: 9.928798097714203\n",
            "Epoch 2014/2048, Avg Training Loss: 0.2541904505994329, Avg Validation Loss: 9.928785105015782\n",
            "Epoch 2015/2048, Avg Training Loss: 0.2542002414794905, Avg Validation Loss: 9.9287732685261\n",
            "Epoch 2016/2048, Avg Training Loss: 0.2542100237935484, Avg Validation Loss: 9.928762559641262\n",
            "Epoch 2017/2048, Avg Training Loss: 0.25421979749420026, Avg Validation Loss: 9.928752950342327\n",
            "Epoch 2018/2048, Avg Training Loss: 0.2542295625350367, Avg Validation Loss: 9.928744413183479\n",
            "Epoch 2019/2048, Avg Training Loss: 0.25423931887061757, Avg Validation Loss: 9.928736921280512\n",
            "Epoch 2020/2048, Avg Training Loss: 0.2542490664564449, Avg Validation Loss: 9.928730448299557\n",
            "Epoch 2021/2048, Avg Training Loss: 0.2542588052489366, Avg Validation Loss: 9.928724968446017\n",
            "Epoch 2022/2048, Avg Training Loss: 0.2542685352054016, Avg Validation Loss: 9.928720456453807\n",
            "Epoch 2023/2048, Avg Training Loss: 0.25427825628401457, Avg Validation Loss: 9.928716887574712\n",
            "Epoch 2024/2048, Avg Training Loss: 0.25428796844379287, Avg Validation Loss: 9.92871423756809\n",
            "Epoch 2025/2048, Avg Training Loss: 0.25429767164457295, Avg Validation Loss: 9.928712482690681\n",
            "Epoch 2026/2048, Avg Training Loss: 0.2543073658469882, Avg Validation Loss: 9.928711599686693\n",
            "Epoch 2027/2048, Avg Training Loss: 0.2543170510124474, Avg Validation Loss: 9.928711565778068\n",
            "Epoch 2028/2048, Avg Training Loss: 0.25432672710311377, Avg Validation Loss: 9.928712358654938\n",
            "Epoch 2029/2048, Avg Training Loss: 0.2543363940818846, Avg Validation Loss: 9.928713956466325\n",
            "Epoch 2030/2048, Avg Training Loss: 0.25434605191237136, Avg Validation Loss: 9.928716337810968\n",
            "Epoch 2031/2048, Avg Training Loss: 0.25435570055888107, Avg Validation Loss: 9.928719481728388\n",
            "Epoch 2032/2048, Avg Training Loss: 0.2543653399863973, Avg Validation Loss: 9.928723367690113\n",
            "Epoch 2033/2048, Avg Training Loss: 0.25437497016056276, Avg Validation Loss: 9.928727975591094\n",
            "Epoch 2034/2048, Avg Training Loss: 0.25438459104766165, Avg Validation Loss: 9.928733285741291\n",
            "Epoch 2035/2048, Avg Training Loss: 0.25439420261460266, Avg Validation Loss: 9.928739278857437\n",
            "Epoch 2036/2048, Avg Training Loss: 0.25440381568293513, Avg Validation Loss: 9.928734462448118\n",
            "Epoch 2037/2048, Avg Training Loss: 0.25441340766393583, Avg Validation Loss: 9.92870755802337\n",
            "Epoch 2038/2048, Avg Training Loss: 0.25442305998049963, Avg Validation Loss: 9.928669604861435\n",
            "Epoch 2039/2048, Avg Training Loss: 0.25443274005342575, Avg Validation Loss: 9.928626548516247\n",
            "Epoch 2040/2048, Avg Training Loss: 0.2544424304028302, Avg Validation Loss: 9.928581582140175\n",
            "Epoch 2041/2048, Avg Training Loss: 0.2544521216351717, Avg Validation Loss: 9.928536411915273\n",
            "Epoch 2042/2048, Avg Training Loss: 0.25446180869070106, Avg Validation Loss: 9.928491940895299\n",
            "Epoch 2043/2048, Avg Training Loss: 0.25447148883352044, Avg Validation Loss: 9.928448638530678\n",
            "Epoch 2044/2048, Avg Training Loss: 0.25448116057440917, Avg Validation Loss: 9.928406740340137\n",
            "Epoch 2045/2048, Avg Training Loss: 0.25449081407604196, Avg Validation Loss: 9.928380307605652\n",
            "Epoch 2046/2048, Avg Training Loss: 0.25450062490713155, Avg Validation Loss: 9.928383186066188\n",
            "Epoch 2047/2048, Avg Training Loss: 0.2545104802070072, Avg Validation Loss: 9.928402062494133\n",
            "Epoch 2048/2048, Avg Training Loss: 0.2545203544984955, Avg Validation Loss: 9.92842974481445\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x_tot = np.random.rand(256, 15)\n",
        "target = np.random.rand(256, 3)\n",
        "\n",
        "test_split = 0.2\n",
        "train_split = 0.75\n",
        "\n",
        "layer_one = Layer(15, 8, ELU, d_ELU)\n",
        "layer_two = Layer(8, 4, ELU, d_ELU)\n",
        "layer_out = Layer(4, 3, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "# Split data\n",
        "x_train_val, target_train_val, x_test, target_test = NN.data_split(x_tot, target, test_split)\n",
        "\n",
        "NN.add_layer(layer_one)\n",
        "NN.add_layer(layer_two)\n",
        "NN.add_layer(layer_out)\n",
        "\n",
        "# Parametri di training\n",
        "K = 5\n",
        "epochs = 2048\n",
        "learning_rate_w = 0.0001 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
        "                         # learning_rate of minibatch for number of the examples in the minibatch\n",
        "learning_rate_b = 0.0001\n",
        "batch_size = 32\n",
        "Lambda_t = 0.1\n",
        "Lambda_l = 0.1\n",
        "momentum = 0.9 # tipically from 0.8 and 0.99\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# Cross-validation\n",
        "train_error, val_error = NN.train_val(x_train_val, target_train_val, train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, mean_squared_error, d_mean_squared_error, 'elastic', 'adam')\n",
        "\n",
        "# Plot degli errori\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_error, label='Training Error')\n",
        "plt.plot(val_error, label='Validation Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Error')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
