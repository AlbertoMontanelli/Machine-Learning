{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives.\n",
    "* **Hidden layers**:\n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qv1wwlgWsFM8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
    "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
    "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha):\n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "ELU = np.vectorize(ELU)\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n",
    "\n",
    "d_ELU = np.vectorize(d_ELU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjNvxQOLsFM-"
   },
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters:**\n",
    " * input_size: $k_{i-1}$;\n",
    " * output_size: $k_i$;\n",
    " * activation_function;\n",
    " * activation_derivative. \n",
    "\n",
    "**Constructor attributes:**\n",
    "* self.input_size = input_size;\n",
    "* self.output_size = output_size;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative;\n",
    "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
    "\n",
    "**Methods :**\n",
    "\n",
    "* **initialize_weights**: initialize weights and biases\n",
    "  * attributes:\n",
    "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
    "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
    "    \n",
    "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
    "  * parameter:\n",
    "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
    "  * attributes:\n",
    "    * self.input: input_array;\n",
    "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
    "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
    "  \n",
    "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    "  * parameters:\n",
    "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "    * learning_rate.\n",
    "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "H8Ap9rLxLlNU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.initialize_weights()\n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
    "        self.biases = np.zeros((1, self.output_size))\n",
    "        \n",
    " \n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward_layer(self, d_Ep, learning_rate, Lambda, reg_type):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "        if (reg_type=='tikhonov'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*self.weights # learning rule - tikhonov regularization\n",
    "        elif (reg_type=='lasso'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*np.sign(self.weights) # learning rule - lasso regularization\n",
    "        elif (reg_type=='elastic'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*self.weights - Lambda*np.sign(self.weights) # learning rule:\n",
    "                                                                                                                             # lasso + tikhonov\n",
    "                                                                                                                             # regularization\n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
    "   * parameters:\n",
    "     * x_tot: total data given as input;\n",
    "     * target: total data labels given as input;\n",
    "     * test_split: percentile of test set with respect to the total data.\n",
    "    * return->:\n",
    "      * x_train_val: training & validation set extracted from input data;\n",
    "      * target_train_val: training & validation set labels;\n",
    "      * x_test_val: test set extracted from input data;\n",
    "      * target_test_val: test set for input data labels.\n",
    "\n",
    "     \n",
    "* **add_layer**: appends a layer to the empty list self.layers\n",
    "   * parameter:\n",
    "     * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    "  * parameter:\n",
    "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    "  * return -> input = $o_i$ for layer $L_i$.\n",
    "  \n",
    "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    "  * parameters:\n",
    "    * d_Ep;\n",
    "    * learning_rate.\n",
    "\n",
    "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
    "\n",
    "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
    " * parameters:\n",
    "   * x_train: set of the original dataset used for training;\n",
    "   * target_train: labels corresponding to the training set;\n",
    "   * x_val: set of the original dataset used for validation;\n",
    "   * target_val: labels corresponding to the validation set;\n",
    "   * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
    "   * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
    "   * loss_function: hyperparameter;\n",
    "   * loss_function_derivative: hyperparameter;\n",
    "   * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
    "\n",
    "* **train_val**: actual training and validation process.\n",
    " * parameters:\n",
    "   * x_train_val;\n",
    "   * target_train_val;\n",
    "   * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
    "   * epochs;\n",
    "   * learning_rate;\n",
    "   * loss_function;\n",
    "   * loss_function_derivative;\n",
    "   * batch_size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def data_split(self, x_tot, target, test_split):\n",
    "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
    "        test_size = int(num_samples * test_split) # the number of the examples in the test set        \n",
    "\n",
    "        x_test = x_tot[:test_size]\n",
    "        target_test = target[:test_size]\n",
    "        x_train_val = x_tot[test_size:]\n",
    "        target_train_val = target[test_size:]\n",
    "\n",
    "        return x_train_val, target_train_val, x_test, target_test\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, d_Ep, learning_rate, Lambda, reg_type):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate, Lambda, reg_type)\n",
    "\n",
    "    def reinitialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights() # does it layer-by-layer         \n",
    "\n",
    "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size):\n",
    "        train_error_epoch = np.zeros(epochs)\n",
    "        val_error_epoch = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            # shuffling training data before splitting it into batches.\n",
    "            # done in order to avoid reinforcing neurons in the same way\n",
    "            # in different epochs due to invisible patterns in the data\n",
    "            train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(train_indices)\n",
    "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
    "            target_train = target_train[train_indices]\n",
    "            \n",
    "            # if batch_size=1 we get the online version, \n",
    "            # else we get mini-batch version with batches of size batch_size\n",
    "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
    "                                                             # even if the number of examples is not divisible \n",
    "                                                             # for batch_size the last, smaller batch is processed anyway\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                target_batch = target_train[i:i+batch_size]\n",
    "\n",
    "                # forward propagation\n",
    "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
    "                # computing loss and gradient\n",
    "                loss = loss_function(target_batch, predictions)\n",
    "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "                epoch_tr_loss += np.sum(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(loss_gradient, learning_rate, Lambda, reg_type)\n",
    "\n",
    "            # validation\n",
    "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
    "            val_loss = loss_function(target_val, val_predictions)\n",
    "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished. \n",
    "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
    "            # Store average errors for the epoch\n",
    "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
    "            val_error_epoch[epoch] = epoch_val_loss\n",
    "\n",
    "        return train_error_epoch, val_error_epoch\n",
    "\n",
    "\n",
    "    def train_val(self, x_train_val, target_train_val, K, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_train_val.shape[0]\n",
    "        fold_size = num_samples // K\n",
    "\n",
    "        # error storage for averaging\n",
    "        avg_train_error_epoch = np.zeros(epochs)\n",
    "        avg_val_error_epoch = np.zeros(epochs)\n",
    "        \n",
    "        if K==1: # hold-out validation\n",
    "            train_indices = np.arange(0, int(0.75*num_samples)) # training set is 75% of the training & validation set\n",
    "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set diffrence between the first and the second set\n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets           \n",
    "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size\n",
    "            ) # computation of errors via train_val_setup method\n",
    "            return train_error_epoch, val_error_epoch\n",
    "            \n",
    "\n",
    "        for k in range(K):\n",
    "            # creating fold indices\n",
    "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
    "                                                                        # It contains the indices of the examples used in validation set for\n",
    "                                                                        # this fold\n",
    "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples - \n",
    "                                                                              # len(val_indices). It contains the indices of all the examples\n",
    "                                                                              # but the ones used in the validation set for this fold. \n",
    "                                                                              # It corresponds to the training set for the current fold \n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
    "\n",
    "            # re-initializing weights for each fold\n",
    "            self.reinitialize_weights()\n",
    "\n",
    "            # training on the current fold\n",
    "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "\n",
    "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
    "            avg_train_error_epoch += train_error_epoch\n",
    "            avg_val_error_epoch += val_error_epoch\n",
    "\n",
    "            print(f\"Fold {k+1} completed.\")\n",
    "\n",
    "        # averaging errors across all folds\n",
    "        avg_train_error_epoch /= K\n",
    "        avg_val_error_epoch /= K\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
    "\n",
    "        return avg_train_error_epoch, avg_val_error_epoch\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp0g8MgT3hhO",
    "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Fold 5 completed.\n",
      "Epoch 1/1000, Avg Training Loss: 0.9142087257617486, Avg Validation Loss: 154.5475828308264\n",
      "Epoch 2/1000, Avg Training Loss: 0.7324670942945711, Avg Validation Loss: 124.64177902911975\n",
      "Epoch 3/1000, Avg Training Loss: 0.5984903524595746, Avg Validation Loss: 103.06307462737661\n",
      "Epoch 4/1000, Avg Training Loss: 0.5016812308373756, Avg Validation Loss: 87.55280902110394\n",
      "Epoch 5/1000, Avg Training Loss: 0.4315357985992613, Avg Validation Loss: 76.3649379179501\n",
      "Epoch 6/1000, Avg Training Loss: 0.3807289727280516, Avg Validation Loss: 68.35155594173449\n",
      "Epoch 7/1000, Avg Training Loss: 0.34395173448917676, Avg Validation Loss: 62.603990083263014\n",
      "Epoch 8/1000, Avg Training Loss: 0.3173945090284019, Avg Validation Loss: 58.47195902139898\n",
      "Epoch 9/1000, Avg Training Loss: 0.2980244547372898, Avg Validation Loss: 55.52599247134725\n",
      "Epoch 10/1000, Avg Training Loss: 0.28409847156096146, Avg Validation Loss: 53.4357149849525\n",
      "Epoch 11/1000, Avg Training Loss: 0.274031490446199, Avg Validation Loss: 51.95206239533437\n",
      "Epoch 12/1000, Avg Training Loss: 0.2667991297002045, Avg Validation Loss: 50.891158812556995\n",
      "Epoch 13/1000, Avg Training Loss: 0.26151428784728853, Avg Validation Loss: 50.14846606674341\n",
      "Epoch 14/1000, Avg Training Loss: 0.25775132962037406, Avg Validation Loss: 49.6287055342932\n",
      "Epoch 15/1000, Avg Training Loss: 0.2549565074981492, Avg Validation Loss: 49.26592101881918\n",
      "Epoch 16/1000, Avg Training Loss: 0.25299846245893454, Avg Validation Loss: 49.01253085879447\n",
      "Epoch 17/1000, Avg Training Loss: 0.25150799458736073, Avg Validation Loss: 48.84045728150372\n",
      "Epoch 18/1000, Avg Training Loss: 0.25049370500222673, Avg Validation Loss: 48.723578703513155\n",
      "Epoch 19/1000, Avg Training Loss: 0.24974539293740952, Avg Validation Loss: 48.64426790875364\n",
      "Epoch 20/1000, Avg Training Loss: 0.24911805499428788, Avg Validation Loss: 48.59435037769326\n",
      "Epoch 21/1000, Avg Training Loss: 0.24863550519876598, Avg Validation Loss: 48.5639819383298\n",
      "Epoch 22/1000, Avg Training Loss: 0.2484695264523701, Avg Validation Loss: 48.54656673887667\n",
      "Epoch 23/1000, Avg Training Loss: 0.24815215271026164, Avg Validation Loss: 48.53918955079182\n",
      "Epoch 24/1000, Avg Training Loss: 0.24797944075455028, Avg Validation Loss: 48.535978805302705\n",
      "Epoch 25/1000, Avg Training Loss: 0.2479581548162672, Avg Validation Loss: 48.53770131126139\n",
      "Epoch 26/1000, Avg Training Loss: 0.24788262621294332, Avg Validation Loss: 48.541290191629685\n",
      "Epoch 27/1000, Avg Training Loss: 0.24782597593249306, Avg Validation Loss: 48.54603885951654\n",
      "Epoch 28/1000, Avg Training Loss: 0.24766042843221497, Avg Validation Loss: 48.54996202844791\n",
      "Epoch 29/1000, Avg Training Loss: 0.24772082992151204, Avg Validation Loss: 48.55502810797013\n",
      "Epoch 30/1000, Avg Training Loss: 0.247603441247002, Avg Validation Loss: 48.559195035970546\n",
      "Epoch 31/1000, Avg Training Loss: 0.2476993944879151, Avg Validation Loss: 48.56444463495828\n",
      "Epoch 32/1000, Avg Training Loss: 0.24772302192298226, Avg Validation Loss: 48.56849055939695\n",
      "Epoch 33/1000, Avg Training Loss: 0.24774938012189768, Avg Validation Loss: 48.57213954685581\n",
      "Epoch 34/1000, Avg Training Loss: 0.2475987630826402, Avg Validation Loss: 48.57662088179878\n",
      "Epoch 35/1000, Avg Training Loss: 0.24762293228270793, Avg Validation Loss: 48.58009376547486\n",
      "Epoch 36/1000, Avg Training Loss: 0.24760474887480785, Avg Validation Loss: 48.58390498619967\n",
      "Epoch 37/1000, Avg Training Loss: 0.24768882373705128, Avg Validation Loss: 48.58540436158982\n",
      "Epoch 38/1000, Avg Training Loss: 0.24766117994774203, Avg Validation Loss: 48.58820302800202\n",
      "Epoch 39/1000, Avg Training Loss: 0.24768770031305343, Avg Validation Loss: 48.590880884636796\n",
      "Epoch 40/1000, Avg Training Loss: 0.24771380941536478, Avg Validation Loss: 48.59140316007013\n",
      "Epoch 41/1000, Avg Training Loss: 0.24769023849092542, Avg Validation Loss: 48.59275104885356\n",
      "Epoch 42/1000, Avg Training Loss: 0.24759814704811242, Avg Validation Loss: 48.593858804581686\n",
      "Epoch 43/1000, Avg Training Loss: 0.24771532420716758, Avg Validation Loss: 48.59387072564045\n",
      "Epoch 44/1000, Avg Training Loss: 0.2477090983431401, Avg Validation Loss: 48.59529528981771\n",
      "Epoch 45/1000, Avg Training Loss: 0.24762877010052844, Avg Validation Loss: 48.59668820411733\n",
      "Epoch 46/1000, Avg Training Loss: 0.24758281247229325, Avg Validation Loss: 48.597168731209464\n",
      "Epoch 47/1000, Avg Training Loss: 0.24774004401898964, Avg Validation Loss: 48.597898985080136\n",
      "Epoch 48/1000, Avg Training Loss: 0.24761556274122412, Avg Validation Loss: 48.597421709746484\n",
      "Epoch 49/1000, Avg Training Loss: 0.24769323452228015, Avg Validation Loss: 48.59908517507989\n",
      "Epoch 50/1000, Avg Training Loss: 0.24765188430851207, Avg Validation Loss: 48.60009050228595\n",
      "Epoch 51/1000, Avg Training Loss: 0.24754912920711594, Avg Validation Loss: 48.599842054973095\n",
      "Epoch 52/1000, Avg Training Loss: 0.24763153744886005, Avg Validation Loss: 48.6006182456658\n",
      "Epoch 53/1000, Avg Training Loss: 0.24772887564664728, Avg Validation Loss: 48.601184969658696\n",
      "Epoch 54/1000, Avg Training Loss: 0.24771720002547873, Avg Validation Loss: 48.6004105940264\n",
      "Epoch 55/1000, Avg Training Loss: 0.24766258013665265, Avg Validation Loss: 48.60047878459508\n",
      "Epoch 56/1000, Avg Training Loss: 0.24765493166546654, Avg Validation Loss: 48.60009469473081\n",
      "Epoch 57/1000, Avg Training Loss: 0.24774467708120418, Avg Validation Loss: 48.600236077809676\n",
      "Epoch 58/1000, Avg Training Loss: 0.24767139914793435, Avg Validation Loss: 48.599669011645105\n",
      "Epoch 59/1000, Avg Training Loss: 0.24773927346659266, Avg Validation Loss: 48.60027148871818\n",
      "Epoch 60/1000, Avg Training Loss: 0.2477040438080566, Avg Validation Loss: 48.59958375494173\n",
      "Epoch 61/1000, Avg Training Loss: 0.2476689898082269, Avg Validation Loss: 48.59906625277548\n",
      "Epoch 62/1000, Avg Training Loss: 0.24769881869957389, Avg Validation Loss: 48.60034429652556\n",
      "Epoch 63/1000, Avg Training Loss: 0.24766131553313447, Avg Validation Loss: 48.60026792978776\n",
      "Epoch 64/1000, Avg Training Loss: 0.24775853452940239, Avg Validation Loss: 48.60121615499155\n",
      "Epoch 65/1000, Avg Training Loss: 0.2476835034585471, Avg Validation Loss: 48.60173764357924\n",
      "Epoch 66/1000, Avg Training Loss: 0.24772179547482046, Avg Validation Loss: 48.60195942662608\n",
      "Epoch 67/1000, Avg Training Loss: 0.24770717182765106, Avg Validation Loss: 48.602169016437344\n",
      "Epoch 68/1000, Avg Training Loss: 0.24753369201031958, Avg Validation Loss: 48.60228214737344\n",
      "Epoch 69/1000, Avg Training Loss: 0.24757726120798185, Avg Validation Loss: 48.60212796796307\n",
      "Epoch 70/1000, Avg Training Loss: 0.24774597984836647, Avg Validation Loss: 48.60203851824318\n",
      "Epoch 71/1000, Avg Training Loss: 0.24762422342354612, Avg Validation Loss: 48.60060720652897\n",
      "Epoch 72/1000, Avg Training Loss: 0.2477317413289061, Avg Validation Loss: 48.60131014322132\n",
      "Epoch 73/1000, Avg Training Loss: 0.24754964325681428, Avg Validation Loss: 48.6015123989887\n",
      "Epoch 74/1000, Avg Training Loss: 0.2476830787472008, Avg Validation Loss: 48.601804544364214\n",
      "Epoch 75/1000, Avg Training Loss: 0.24766264289589376, Avg Validation Loss: 48.60196226402019\n",
      "Epoch 76/1000, Avg Training Loss: 0.24776594608161567, Avg Validation Loss: 48.60323343582765\n",
      "Epoch 77/1000, Avg Training Loss: 0.24766931440181963, Avg Validation Loss: 48.60387042493537\n",
      "Epoch 78/1000, Avg Training Loss: 0.2475924018961372, Avg Validation Loss: 48.6034416348407\n",
      "Epoch 79/1000, Avg Training Loss: 0.24770026808494933, Avg Validation Loss: 48.602893505067634\n",
      "Epoch 80/1000, Avg Training Loss: 0.24778008461546538, Avg Validation Loss: 48.60433057374103\n",
      "Epoch 81/1000, Avg Training Loss: 0.24767973270856014, Avg Validation Loss: 48.60530970134246\n",
      "Epoch 82/1000, Avg Training Loss: 0.24758312670883398, Avg Validation Loss: 48.604413177789326\n",
      "Epoch 83/1000, Avg Training Loss: 0.24763399050862353, Avg Validation Loss: 48.60414465496186\n",
      "Epoch 84/1000, Avg Training Loss: 0.24754680157178513, Avg Validation Loss: 48.603177530817575\n",
      "Epoch 85/1000, Avg Training Loss: 0.24765677726059793, Avg Validation Loss: 48.60256797170534\n",
      "Epoch 86/1000, Avg Training Loss: 0.2476307668658706, Avg Validation Loss: 48.60250726740075\n",
      "Epoch 87/1000, Avg Training Loss: 0.24762159998196265, Avg Validation Loss: 48.601555942655615\n",
      "Epoch 88/1000, Avg Training Loss: 0.2477577223757143, Avg Validation Loss: 48.60191495262653\n",
      "Epoch 89/1000, Avg Training Loss: 0.24758071764280715, Avg Validation Loss: 48.601923234415246\n",
      "Epoch 90/1000, Avg Training Loss: 0.24759868106482658, Avg Validation Loss: 48.60140563237391\n",
      "Epoch 91/1000, Avg Training Loss: 0.24774800176933542, Avg Validation Loss: 48.602116991000266\n",
      "Epoch 92/1000, Avg Training Loss: 0.247708250884012, Avg Validation Loss: 48.60310952413104\n",
      "Epoch 93/1000, Avg Training Loss: 0.24760917402031837, Avg Validation Loss: 48.6036399957665\n",
      "Epoch 94/1000, Avg Training Loss: 0.24762305710963153, Avg Validation Loss: 48.603641634110446\n",
      "Epoch 95/1000, Avg Training Loss: 0.24765425189536316, Avg Validation Loss: 48.60394577294014\n",
      "Epoch 96/1000, Avg Training Loss: 0.24766763089183996, Avg Validation Loss: 48.60247654012641\n",
      "Epoch 97/1000, Avg Training Loss: 0.24757705107911263, Avg Validation Loss: 48.601781939202745\n",
      "Epoch 98/1000, Avg Training Loss: 0.24765306093900769, Avg Validation Loss: 48.602483312498464\n",
      "Epoch 99/1000, Avg Training Loss: 0.24771920211554463, Avg Validation Loss: 48.602332344892865\n",
      "Epoch 100/1000, Avg Training Loss: 0.24764444884944375, Avg Validation Loss: 48.601733049072074\n",
      "Epoch 101/1000, Avg Training Loss: 0.24759428289169463, Avg Validation Loss: 48.601346201526496\n",
      "Epoch 102/1000, Avg Training Loss: 0.24764981356187232, Avg Validation Loss: 48.601363911124864\n",
      "Epoch 103/1000, Avg Training Loss: 0.24764030626686057, Avg Validation Loss: 48.60093071735729\n",
      "Epoch 104/1000, Avg Training Loss: 0.24769850098844348, Avg Validation Loss: 48.60074024413056\n",
      "Epoch 105/1000, Avg Training Loss: 0.2476464043442093, Avg Validation Loss: 48.60174863309797\n",
      "Epoch 106/1000, Avg Training Loss: 0.2475882387820355, Avg Validation Loss: 48.60129057103063\n",
      "Epoch 107/1000, Avg Training Loss: 0.24757903186567937, Avg Validation Loss: 48.600884902066014\n",
      "Epoch 108/1000, Avg Training Loss: 0.24761039323212652, Avg Validation Loss: 48.60066367023197\n",
      "Epoch 109/1000, Avg Training Loss: 0.24751259216010343, Avg Validation Loss: 48.6018844300847\n",
      "Epoch 110/1000, Avg Training Loss: 0.2476164737152372, Avg Validation Loss: 48.60163863252637\n",
      "Epoch 111/1000, Avg Training Loss: 0.24758673633104727, Avg Validation Loss: 48.60022596892382\n",
      "Epoch 112/1000, Avg Training Loss: 0.24765267926352272, Avg Validation Loss: 48.60041233018222\n",
      "Epoch 113/1000, Avg Training Loss: 0.24765645467808323, Avg Validation Loss: 48.60145268682872\n",
      "Epoch 114/1000, Avg Training Loss: 0.24758370627120868, Avg Validation Loss: 48.60134760092505\n",
      "Epoch 115/1000, Avg Training Loss: 0.24769267359973837, Avg Validation Loss: 48.601052130007695\n",
      "Epoch 116/1000, Avg Training Loss: 0.24767738763825176, Avg Validation Loss: 48.60135626909852\n",
      "Epoch 117/1000, Avg Training Loss: 0.24766957869839779, Avg Validation Loss: 48.60179557065362\n",
      "Epoch 118/1000, Avg Training Loss: 0.24773886389360375, Avg Validation Loss: 48.60156719186996\n",
      "Epoch 119/1000, Avg Training Loss: 0.24771450397514944, Avg Validation Loss: 48.60161217415961\n",
      "Epoch 120/1000, Avg Training Loss: 0.2477428711891865, Avg Validation Loss: 48.60264478410225\n",
      "Epoch 121/1000, Avg Training Loss: 0.24762239163822902, Avg Validation Loss: 48.60312954271173\n",
      "Epoch 122/1000, Avg Training Loss: 0.24767237819428764, Avg Validation Loss: 48.60282198050138\n",
      "Epoch 123/1000, Avg Training Loss: 0.24771264459497522, Avg Validation Loss: 48.60321737013656\n",
      "Epoch 124/1000, Avg Training Loss: 0.24760872946997245, Avg Validation Loss: 48.60299817235966\n",
      "Epoch 125/1000, Avg Training Loss: 0.2478366741991762, Avg Validation Loss: 48.60266103839589\n",
      "Epoch 126/1000, Avg Training Loss: 0.24758845051096123, Avg Validation Loss: 48.60256816839629\n",
      "Epoch 127/1000, Avg Training Loss: 0.2477116330356642, Avg Validation Loss: 48.603639932803425\n",
      "Epoch 128/1000, Avg Training Loss: 0.24773385849373064, Avg Validation Loss: 48.60316631663585\n",
      "Epoch 129/1000, Avg Training Loss: 0.24769738236569352, Avg Validation Loss: 48.603479390563066\n",
      "Epoch 130/1000, Avg Training Loss: 0.24791052941776534, Avg Validation Loss: 48.601859274944275\n",
      "Epoch 131/1000, Avg Training Loss: 0.24761813203485186, Avg Validation Loss: 48.60177751163098\n",
      "Epoch 132/1000, Avg Training Loss: 0.24761091654073142, Avg Validation Loss: 48.601242983103205\n",
      "Epoch 133/1000, Avg Training Loss: 0.24783921216789817, Avg Validation Loss: 48.60151664570053\n",
      "Epoch 134/1000, Avg Training Loss: 0.24771807693233414, Avg Validation Loss: 48.6020544331038\n",
      "Epoch 135/1000, Avg Training Loss: 0.24757828291932427, Avg Validation Loss: 48.60374292899755\n",
      "Epoch 136/1000, Avg Training Loss: 0.2477014335541236, Avg Validation Loss: 48.60420754728156\n",
      "Epoch 137/1000, Avg Training Loss: 0.24772541508446924, Avg Validation Loss: 48.60340216830744\n",
      "Epoch 138/1000, Avg Training Loss: 0.24759342684082808, Avg Validation Loss: 48.603399664086844\n",
      "Epoch 139/1000, Avg Training Loss: 0.2477195132912192, Avg Validation Loss: 48.60434730711317\n",
      "Epoch 140/1000, Avg Training Loss: 0.2477133527720939, Avg Validation Loss: 48.603897450506416\n",
      "Epoch 141/1000, Avg Training Loss: 0.24769366721708141, Avg Validation Loss: 48.60546337688882\n",
      "Epoch 142/1000, Avg Training Loss: 0.2476345723142625, Avg Validation Loss: 48.604569131608514\n",
      "Epoch 143/1000, Avg Training Loss: 0.24765443445832555, Avg Validation Loss: 48.606010728256756\n",
      "Epoch 144/1000, Avg Training Loss: 0.2477021067030166, Avg Validation Loss: 48.606149225268034\n",
      "Epoch 145/1000, Avg Training Loss: 0.24768686539041176, Avg Validation Loss: 48.606153167436744\n",
      "Epoch 146/1000, Avg Training Loss: 0.24778089808959336, Avg Validation Loss: 48.60770004399752\n",
      "Epoch 147/1000, Avg Training Loss: 0.24777924218488848, Avg Validation Loss: 48.607643425189025\n",
      "Epoch 148/1000, Avg Training Loss: 0.24762923458481276, Avg Validation Loss: 48.60742651297215\n",
      "Epoch 149/1000, Avg Training Loss: 0.24769720328919476, Avg Validation Loss: 48.60697592492768\n",
      "Epoch 150/1000, Avg Training Loss: 0.24746471112077587, Avg Validation Loss: 48.60722183759326\n",
      "Epoch 151/1000, Avg Training Loss: 0.2476115924738606, Avg Validation Loss: 48.60780913430351\n",
      "Epoch 152/1000, Avg Training Loss: 0.2476573581366833, Avg Validation Loss: 48.60780489184744\n",
      "Epoch 153/1000, Avg Training Loss: 0.24767447361663117, Avg Validation Loss: 48.605981338466535\n",
      "Epoch 154/1000, Avg Training Loss: 0.24766473121159316, Avg Validation Loss: 48.60791223839931\n",
      "Epoch 155/1000, Avg Training Loss: 0.2475594395321071, Avg Validation Loss: 48.60763292116215\n",
      "Epoch 156/1000, Avg Training Loss: 0.24767518619291207, Avg Validation Loss: 48.60663198944773\n",
      "Epoch 157/1000, Avg Training Loss: 0.2477593409734519, Avg Validation Loss: 48.60734080200184\n",
      "Epoch 158/1000, Avg Training Loss: 0.24765707332988396, Avg Validation Loss: 48.60601166317618\n",
      "Epoch 159/1000, Avg Training Loss: 0.2477429876786225, Avg Validation Loss: 48.60705100682878\n",
      "Epoch 160/1000, Avg Training Loss: 0.24756041391571437, Avg Validation Loss: 48.60683102550755\n",
      "Epoch 161/1000, Avg Training Loss: 0.24765883778468964, Avg Validation Loss: 48.60702496477107\n",
      "Epoch 162/1000, Avg Training Loss: 0.24764890459504563, Avg Validation Loss: 48.60601484251319\n",
      "Epoch 163/1000, Avg Training Loss: 0.24780733263008792, Avg Validation Loss: 48.607033464137196\n",
      "Epoch 164/1000, Avg Training Loss: 0.2477025594056765, Avg Validation Loss: 48.60680883482635\n",
      "Epoch 165/1000, Avg Training Loss: 0.2476453301536715, Avg Validation Loss: 48.60493309845692\n",
      "Epoch 166/1000, Avg Training Loss: 0.24775192880100463, Avg Validation Loss: 48.60576437275812\n",
      "Epoch 167/1000, Avg Training Loss: 0.24779672810279976, Avg Validation Loss: 48.60798308512219\n",
      "Epoch 168/1000, Avg Training Loss: 0.24765244331311576, Avg Validation Loss: 48.60819773611911\n",
      "Epoch 169/1000, Avg Training Loss: 0.2475722630227073, Avg Validation Loss: 48.60765172975037\n",
      "Epoch 170/1000, Avg Training Loss: 0.24755215664846708, Avg Validation Loss: 48.60765324796394\n",
      "Epoch 171/1000, Avg Training Loss: 0.2478351696458686, Avg Validation Loss: 48.60873261145669\n",
      "Epoch 172/1000, Avg Training Loss: 0.2477041834076061, Avg Validation Loss: 48.60840865524939\n",
      "Epoch 173/1000, Avg Training Loss: 0.2476159318981444, Avg Validation Loss: 48.60783474510188\n",
      "Epoch 174/1000, Avg Training Loss: 0.24772306243403602, Avg Validation Loss: 48.607508474537156\n",
      "Epoch 175/1000, Avg Training Loss: 0.2476358889798501, Avg Validation Loss: 48.60768562710123\n",
      "Epoch 176/1000, Avg Training Loss: 0.24753798608407018, Avg Validation Loss: 48.60778755733144\n",
      "Epoch 177/1000, Avg Training Loss: 0.24774041286730125, Avg Validation Loss: 48.60748349884792\n",
      "Epoch 178/1000, Avg Training Loss: 0.24766609520734764, Avg Validation Loss: 48.6069190666077\n",
      "Epoch 179/1000, Avg Training Loss: 0.2477146697049038, Avg Validation Loss: 48.607204638180924\n",
      "Epoch 180/1000, Avg Training Loss: 0.24773922554948719, Avg Validation Loss: 48.6084518426062\n",
      "Epoch 181/1000, Avg Training Loss: 0.24780519071462728, Avg Validation Loss: 48.60953092128433\n",
      "Epoch 182/1000, Avg Training Loss: 0.24767609408284585, Avg Validation Loss: 48.60877138735197\n",
      "Epoch 183/1000, Avg Training Loss: 0.24751023705276728, Avg Validation Loss: 48.60749367228671\n",
      "Epoch 184/1000, Avg Training Loss: 0.2476639482963059, Avg Validation Loss: 48.607751306359006\n",
      "Epoch 185/1000, Avg Training Loss: 0.2476088938838534, Avg Validation Loss: 48.607666527383614\n",
      "Epoch 186/1000, Avg Training Loss: 0.2475646135781504, Avg Validation Loss: 48.60923919713774\n",
      "Epoch 187/1000, Avg Training Loss: 0.24758007538211133, Avg Validation Loss: 48.6107375668613\n",
      "Epoch 188/1000, Avg Training Loss: 0.24769924292920525, Avg Validation Loss: 48.6113838366834\n",
      "Epoch 189/1000, Avg Training Loss: 0.24755573389191682, Avg Validation Loss: 48.61106757271566\n",
      "Epoch 190/1000, Avg Training Loss: 0.24769142344625675, Avg Validation Loss: 48.609863503670674\n",
      "Epoch 191/1000, Avg Training Loss: 0.24765777148210177, Avg Validation Loss: 48.61018136986241\n",
      "Epoch 192/1000, Avg Training Loss: 0.24771553244545022, Avg Validation Loss: 48.6106207019921\n",
      "Epoch 193/1000, Avg Training Loss: 0.2476597112851422, Avg Validation Loss: 48.61067814111482\n",
      "Epoch 194/1000, Avg Training Loss: 0.24768769414546626, Avg Validation Loss: 48.610192062990265\n",
      "Epoch 195/1000, Avg Training Loss: 0.24760976228942636, Avg Validation Loss: 48.61056565284091\n",
      "Epoch 196/1000, Avg Training Loss: 0.24760298032878453, Avg Validation Loss: 48.608784727010494\n",
      "Epoch 197/1000, Avg Training Loss: 0.24780164980485903, Avg Validation Loss: 48.609219151464416\n",
      "Epoch 198/1000, Avg Training Loss: 0.24772698506995033, Avg Validation Loss: 48.60963997097454\n",
      "Epoch 199/1000, Avg Training Loss: 0.2477824073157512, Avg Validation Loss: 48.61006742205252\n",
      "Epoch 200/1000, Avg Training Loss: 0.24773003209646838, Avg Validation Loss: 48.60945514876596\n",
      "Epoch 201/1000, Avg Training Loss: 0.2476977376429825, Avg Validation Loss: 48.608698868513294\n",
      "Epoch 202/1000, Avg Training Loss: 0.24772191946196487, Avg Validation Loss: 48.60753138149845\n",
      "Epoch 203/1000, Avg Training Loss: 0.24772018828883385, Avg Validation Loss: 48.60681529423047\n",
      "Epoch 204/1000, Avg Training Loss: 0.24765359190377617, Avg Validation Loss: 48.60708520950497\n",
      "Epoch 205/1000, Avg Training Loss: 0.24782573400932467, Avg Validation Loss: 48.60870791545038\n",
      "Epoch 206/1000, Avg Training Loss: 0.24773737798074183, Avg Validation Loss: 48.60885638240542\n",
      "Epoch 207/1000, Avg Training Loss: 0.24761143484761416, Avg Validation Loss: 48.60966610775547\n",
      "Epoch 208/1000, Avg Training Loss: 0.24764504447222113, Avg Validation Loss: 48.609853649942906\n",
      "Epoch 209/1000, Avg Training Loss: 0.24761941814915656, Avg Validation Loss: 48.61032820457618\n",
      "Epoch 210/1000, Avg Training Loss: 0.24761031652585885, Avg Validation Loss: 48.61044049516364\n",
      "Epoch 211/1000, Avg Training Loss: 0.24756576172487185, Avg Validation Loss: 48.61086913825929\n",
      "Epoch 212/1000, Avg Training Loss: 0.24755710614691837, Avg Validation Loss: 48.610722212560084\n",
      "Epoch 213/1000, Avg Training Loss: 0.247634860858847, Avg Validation Loss: 48.611735419908314\n",
      "Epoch 214/1000, Avg Training Loss: 0.24760126095210558, Avg Validation Loss: 48.611431441286356\n",
      "Epoch 215/1000, Avg Training Loss: 0.2476278653681426, Avg Validation Loss: 48.611038325295084\n",
      "Epoch 216/1000, Avg Training Loss: 0.2476786951393965, Avg Validation Loss: 48.609788398465156\n",
      "Epoch 217/1000, Avg Training Loss: 0.24772936338365933, Avg Validation Loss: 48.60985526165901\n",
      "Epoch 218/1000, Avg Training Loss: 0.24765386088900881, Avg Validation Loss: 48.60950200856044\n",
      "Epoch 219/1000, Avg Training Loss: 0.24769484582695633, Avg Validation Loss: 48.6100659896054\n",
      "Epoch 220/1000, Avg Training Loss: 0.24767704495025203, Avg Validation Loss: 48.609838924790125\n",
      "Epoch 221/1000, Avg Training Loss: 0.24782155221716556, Avg Validation Loss: 48.609393892992216\n",
      "Epoch 222/1000, Avg Training Loss: 0.24780300260230817, Avg Validation Loss: 48.60949966176261\n",
      "Epoch 223/1000, Avg Training Loss: 0.2477540569144954, Avg Validation Loss: 48.60936931956148\n",
      "Epoch 224/1000, Avg Training Loss: 0.2475611413109964, Avg Validation Loss: 48.609473675435694\n",
      "Epoch 225/1000, Avg Training Loss: 0.24789720310556493, Avg Validation Loss: 48.610568693124264\n",
      "Epoch 226/1000, Avg Training Loss: 0.24778456252973066, Avg Validation Loss: 48.61086459866425\n",
      "Epoch 227/1000, Avg Training Loss: 0.24780780577799813, Avg Validation Loss: 48.61065423754724\n",
      "Epoch 228/1000, Avg Training Loss: 0.24762868641648145, Avg Validation Loss: 48.61052719407583\n",
      "Epoch 229/1000, Avg Training Loss: 0.24759731354679937, Avg Validation Loss: 48.61051414247284\n",
      "Epoch 230/1000, Avg Training Loss: 0.24771016989026143, Avg Validation Loss: 48.61234097145283\n",
      "Epoch 231/1000, Avg Training Loss: 0.24772578045896967, Avg Validation Loss: 48.613543705471685\n",
      "Epoch 232/1000, Avg Training Loss: 0.24776646506694724, Avg Validation Loss: 48.61313721600505\n",
      "Epoch 233/1000, Avg Training Loss: 0.24762751270652208, Avg Validation Loss: 48.61292880417933\n",
      "Epoch 234/1000, Avg Training Loss: 0.24766551787951316, Avg Validation Loss: 48.61331352056607\n",
      "Epoch 235/1000, Avg Training Loss: 0.24765371589742535, Avg Validation Loss: 48.61349701729659\n",
      "Epoch 236/1000, Avg Training Loss: 0.24762134342105724, Avg Validation Loss: 48.613781812522404\n",
      "Epoch 237/1000, Avg Training Loss: 0.24767029644459973, Avg Validation Loss: 48.61266200779255\n",
      "Epoch 238/1000, Avg Training Loss: 0.24759971213584703, Avg Validation Loss: 48.61171751971449\n",
      "Epoch 239/1000, Avg Training Loss: 0.2476844868422004, Avg Validation Loss: 48.61332350232329\n",
      "Epoch 240/1000, Avg Training Loss: 0.24771638781887667, Avg Validation Loss: 48.61232354119255\n",
      "Epoch 241/1000, Avg Training Loss: 0.24766200406462538, Avg Validation Loss: 48.610399847875485\n",
      "Epoch 242/1000, Avg Training Loss: 0.24768415034405677, Avg Validation Loss: 48.6118652123546\n",
      "Epoch 243/1000, Avg Training Loss: 0.24767927762327857, Avg Validation Loss: 48.611835257084095\n",
      "Epoch 244/1000, Avg Training Loss: 0.24769025039840123, Avg Validation Loss: 48.61342767002993\n",
      "Epoch 245/1000, Avg Training Loss: 0.24748283092492415, Avg Validation Loss: 48.61249934834936\n",
      "Epoch 246/1000, Avg Training Loss: 0.24779737028631765, Avg Validation Loss: 48.61232268381705\n",
      "Epoch 247/1000, Avg Training Loss: 0.24755101130800697, Avg Validation Loss: 48.61247482677275\n",
      "Epoch 248/1000, Avg Training Loss: 0.24757934425188555, Avg Validation Loss: 48.61215547089241\n",
      "Epoch 249/1000, Avg Training Loss: 0.24769355935021506, Avg Validation Loss: 48.612218464426796\n",
      "Epoch 250/1000, Avg Training Loss: 0.2474933251940265, Avg Validation Loss: 48.6115537650046\n",
      "Epoch 251/1000, Avg Training Loss: 0.24769482641494497, Avg Validation Loss: 48.61177956430633\n",
      "Epoch 252/1000, Avg Training Loss: 0.24788747211339365, Avg Validation Loss: 48.61201243533641\n",
      "Epoch 253/1000, Avg Training Loss: 0.2477334919622762, Avg Validation Loss: 48.61298488324598\n",
      "Epoch 254/1000, Avg Training Loss: 0.2476565795016973, Avg Validation Loss: 48.61121548818278\n",
      "Epoch 255/1000, Avg Training Loss: 0.2477423548873573, Avg Validation Loss: 48.611154932196456\n",
      "Epoch 256/1000, Avg Training Loss: 0.24769763377935106, Avg Validation Loss: 48.613143973590184\n",
      "Epoch 257/1000, Avg Training Loss: 0.24764636306636656, Avg Validation Loss: 48.61225841321344\n",
      "Epoch 258/1000, Avg Training Loss: 0.24787190355414146, Avg Validation Loss: 48.61402348064124\n",
      "Epoch 259/1000, Avg Training Loss: 0.2474935227540216, Avg Validation Loss: 48.61489866841062\n",
      "Epoch 260/1000, Avg Training Loss: 0.24762732061227116, Avg Validation Loss: 48.61556062340439\n",
      "Epoch 261/1000, Avg Training Loss: 0.24754008398237842, Avg Validation Loss: 48.61428029759671\n",
      "Epoch 262/1000, Avg Training Loss: 0.2475754323563498, Avg Validation Loss: 48.613248019694\n",
      "Epoch 263/1000, Avg Training Loss: 0.24761689489233824, Avg Validation Loss: 48.61285145815152\n",
      "Epoch 264/1000, Avg Training Loss: 0.24772891299494, Avg Validation Loss: 48.61325950251869\n",
      "Epoch 265/1000, Avg Training Loss: 0.2476135849290052, Avg Validation Loss: 48.612672981194564\n",
      "Epoch 266/1000, Avg Training Loss: 0.2479087332611812, Avg Validation Loss: 48.612907049836394\n",
      "Epoch 267/1000, Avg Training Loss: 0.24773308438296707, Avg Validation Loss: 48.61254282036346\n",
      "Epoch 268/1000, Avg Training Loss: 0.24762087380735423, Avg Validation Loss: 48.61215947744905\n",
      "Epoch 269/1000, Avg Training Loss: 0.24767273092862058, Avg Validation Loss: 48.61302218485493\n",
      "Epoch 270/1000, Avg Training Loss: 0.24774139947165386, Avg Validation Loss: 48.61462283457027\n",
      "Epoch 271/1000, Avg Training Loss: 0.24773469787955693, Avg Validation Loss: 48.61505766667016\n",
      "Epoch 272/1000, Avg Training Loss: 0.24765294968319523, Avg Validation Loss: 48.616642428110325\n",
      "Epoch 273/1000, Avg Training Loss: 0.2476630781287912, Avg Validation Loss: 48.61678059455948\n",
      "Epoch 274/1000, Avg Training Loss: 0.24784583536173518, Avg Validation Loss: 48.61667526568054\n",
      "Epoch 275/1000, Avg Training Loss: 0.24756264372142606, Avg Validation Loss: 48.617076080237126\n",
      "Epoch 276/1000, Avg Training Loss: 0.2477486329928071, Avg Validation Loss: 48.61620207240033\n",
      "Epoch 277/1000, Avg Training Loss: 0.24758098274857915, Avg Validation Loss: 48.61770225296548\n",
      "Epoch 278/1000, Avg Training Loss: 0.2476311704459346, Avg Validation Loss: 48.61642070379912\n",
      "Epoch 279/1000, Avg Training Loss: 0.24757500829030465, Avg Validation Loss: 48.616853788298826\n",
      "Epoch 280/1000, Avg Training Loss: 0.24768810071243677, Avg Validation Loss: 48.618204461929494\n",
      "Epoch 281/1000, Avg Training Loss: 0.24768130986029846, Avg Validation Loss: 48.61788453976506\n",
      "Epoch 282/1000, Avg Training Loss: 0.24791289057990287, Avg Validation Loss: 48.619198674110635\n",
      "Epoch 283/1000, Avg Training Loss: 0.24761782816226907, Avg Validation Loss: 48.61855211429252\n",
      "Epoch 284/1000, Avg Training Loss: 0.24750747007046403, Avg Validation Loss: 48.617563448017705\n",
      "Epoch 285/1000, Avg Training Loss: 0.24792122717150744, Avg Validation Loss: 48.6193566529696\n",
      "Epoch 286/1000, Avg Training Loss: 0.2476318039775009, Avg Validation Loss: 48.61896863028482\n",
      "Epoch 287/1000, Avg Training Loss: 0.24765069609915727, Avg Validation Loss: 48.61828764534634\n",
      "Epoch 288/1000, Avg Training Loss: 0.2476383507505726, Avg Validation Loss: 48.61808953740868\n",
      "Epoch 289/1000, Avg Training Loss: 0.24768520427678173, Avg Validation Loss: 48.618168949593354\n",
      "Epoch 290/1000, Avg Training Loss: 0.2476595229447742, Avg Validation Loss: 48.61873469366454\n",
      "Epoch 291/1000, Avg Training Loss: 0.2478481992716326, Avg Validation Loss: 48.62017480249952\n",
      "Epoch 292/1000, Avg Training Loss: 0.24766280150130016, Avg Validation Loss: 48.619267770270525\n",
      "Epoch 293/1000, Avg Training Loss: 0.2476191675921809, Avg Validation Loss: 48.61803428461922\n",
      "Epoch 294/1000, Avg Training Loss: 0.24780179468040844, Avg Validation Loss: 48.61952053093254\n",
      "Epoch 295/1000, Avg Training Loss: 0.24763808696088682, Avg Validation Loss: 48.61809560730035\n",
      "Epoch 296/1000, Avg Training Loss: 0.24763476299909726, Avg Validation Loss: 48.618110798346905\n",
      "Epoch 297/1000, Avg Training Loss: 0.24787254625281313, Avg Validation Loss: 48.618916884986376\n",
      "Epoch 298/1000, Avg Training Loss: 0.24759205900636364, Avg Validation Loss: 48.61801407981943\n",
      "Epoch 299/1000, Avg Training Loss: 0.24790157060512474, Avg Validation Loss: 48.619496341749596\n",
      "Epoch 300/1000, Avg Training Loss: 0.24786980023466917, Avg Validation Loss: 48.61995650914295\n",
      "Epoch 301/1000, Avg Training Loss: 0.2476373740678434, Avg Validation Loss: 48.62016535194355\n",
      "Epoch 302/1000, Avg Training Loss: 0.24758781531979854, Avg Validation Loss: 48.61977292185152\n",
      "Epoch 303/1000, Avg Training Loss: 0.2477861694013173, Avg Validation Loss: 48.62051899769248\n",
      "Epoch 304/1000, Avg Training Loss: 0.2478145987801768, Avg Validation Loss: 48.6204071581096\n",
      "Epoch 305/1000, Avg Training Loss: 0.2477277347995841, Avg Validation Loss: 48.62024828656199\n",
      "Epoch 306/1000, Avg Training Loss: 0.24778047399937525, Avg Validation Loss: 48.620191565663674\n",
      "Epoch 307/1000, Avg Training Loss: 0.24760250603274106, Avg Validation Loss: 48.619312709614135\n",
      "Epoch 308/1000, Avg Training Loss: 0.247520780757936, Avg Validation Loss: 48.61803174232389\n",
      "Epoch 309/1000, Avg Training Loss: 0.24773508676834743, Avg Validation Loss: 48.619068428625994\n",
      "Epoch 310/1000, Avg Training Loss: 0.2477010073797262, Avg Validation Loss: 48.61868721978192\n",
      "Epoch 311/1000, Avg Training Loss: 0.24777007033187395, Avg Validation Loss: 48.619280739133316\n",
      "Epoch 312/1000, Avg Training Loss: 0.24786822033227446, Avg Validation Loss: 48.62059667431657\n",
      "Epoch 313/1000, Avg Training Loss: 0.24784166979683026, Avg Validation Loss: 48.620641885269876\n",
      "Epoch 314/1000, Avg Training Loss: 0.24783636872649376, Avg Validation Loss: 48.62094878124322\n",
      "Epoch 315/1000, Avg Training Loss: 0.247649272283314, Avg Validation Loss: 48.61945773785794\n",
      "Epoch 316/1000, Avg Training Loss: 0.2479386249633576, Avg Validation Loss: 48.61896897116526\n",
      "Epoch 317/1000, Avg Training Loss: 0.24767978518729716, Avg Validation Loss: 48.61975692840764\n",
      "Epoch 318/1000, Avg Training Loss: 0.2478078465866289, Avg Validation Loss: 48.61976497137653\n",
      "Epoch 319/1000, Avg Training Loss: 0.2478589839358927, Avg Validation Loss: 48.62167496834866\n",
      "Epoch 320/1000, Avg Training Loss: 0.24748854161028888, Avg Validation Loss: 48.62059728595204\n",
      "Epoch 321/1000, Avg Training Loss: 0.24765698313913567, Avg Validation Loss: 48.620341565132584\n",
      "Epoch 322/1000, Avg Training Loss: 0.24756473401165977, Avg Validation Loss: 48.62133439237616\n",
      "Epoch 323/1000, Avg Training Loss: 0.2478723686517614, Avg Validation Loss: 48.620760850187864\n",
      "Epoch 324/1000, Avg Training Loss: 0.24770712675989048, Avg Validation Loss: 48.6213917266944\n",
      "Epoch 325/1000, Avg Training Loss: 0.2476328820453178, Avg Validation Loss: 48.62277620774083\n",
      "Epoch 326/1000, Avg Training Loss: 0.24755784129692873, Avg Validation Loss: 48.62396379547897\n",
      "Epoch 327/1000, Avg Training Loss: 0.24764147054200836, Avg Validation Loss: 48.623333459831414\n",
      "Epoch 328/1000, Avg Training Loss: 0.24774790156997256, Avg Validation Loss: 48.624883311797525\n",
      "Epoch 329/1000, Avg Training Loss: 0.24751798575695413, Avg Validation Loss: 48.6234296539547\n",
      "Epoch 330/1000, Avg Training Loss: 0.24796800300185104, Avg Validation Loss: 48.625309562209694\n",
      "Epoch 331/1000, Avg Training Loss: 0.247671786253585, Avg Validation Loss: 48.623861566952804\n",
      "Epoch 332/1000, Avg Training Loss: 0.24765481565163136, Avg Validation Loss: 48.62508476101726\n",
      "Epoch 333/1000, Avg Training Loss: 0.24767265408350095, Avg Validation Loss: 48.62411113567947\n",
      "Epoch 334/1000, Avg Training Loss: 0.2476200349743193, Avg Validation Loss: 48.62252693657361\n",
      "Epoch 335/1000, Avg Training Loss: 0.24774101565268702, Avg Validation Loss: 48.62231719663078\n",
      "Epoch 336/1000, Avg Training Loss: 0.24776204573496904, Avg Validation Loss: 48.62181074409942\n",
      "Epoch 337/1000, Avg Training Loss: 0.24786386927177254, Avg Validation Loss: 48.621185701306686\n",
      "Epoch 338/1000, Avg Training Loss: 0.24753584535843629, Avg Validation Loss: 48.62094498287091\n",
      "Epoch 339/1000, Avg Training Loss: 0.24782793925978897, Avg Validation Loss: 48.62231107238272\n",
      "Epoch 340/1000, Avg Training Loss: 0.2478878920732279, Avg Validation Loss: 48.62251001235027\n",
      "Epoch 341/1000, Avg Training Loss: 0.2474709860456584, Avg Validation Loss: 48.6215945825507\n",
      "Epoch 342/1000, Avg Training Loss: 0.24763723393099454, Avg Validation Loss: 48.622748638082086\n",
      "Epoch 343/1000, Avg Training Loss: 0.24776266442262976, Avg Validation Loss: 48.623043519141135\n",
      "Epoch 344/1000, Avg Training Loss: 0.24768659334213922, Avg Validation Loss: 48.6223905141078\n",
      "Epoch 345/1000, Avg Training Loss: 0.24771331179554124, Avg Validation Loss: 48.622133910490234\n",
      "Epoch 346/1000, Avg Training Loss: 0.24757596492294381, Avg Validation Loss: 48.62288796808889\n",
      "Epoch 347/1000, Avg Training Loss: 0.24767184828414923, Avg Validation Loss: 48.62214119455703\n",
      "Epoch 348/1000, Avg Training Loss: 0.2476385177066218, Avg Validation Loss: 48.622322220705314\n",
      "Epoch 349/1000, Avg Training Loss: 0.24794256034676215, Avg Validation Loss: 48.623506551824434\n",
      "Epoch 350/1000, Avg Training Loss: 0.24779956440887826, Avg Validation Loss: 48.6225481696395\n",
      "Epoch 351/1000, Avg Training Loss: 0.24768756041843001, Avg Validation Loss: 48.62204460827542\n",
      "Epoch 352/1000, Avg Training Loss: 0.24771726605918962, Avg Validation Loss: 48.62334560409721\n",
      "Epoch 353/1000, Avg Training Loss: 0.2480149034455396, Avg Validation Loss: 48.62412700879581\n",
      "Epoch 354/1000, Avg Training Loss: 0.24765208692109314, Avg Validation Loss: 48.62382698119053\n",
      "Epoch 355/1000, Avg Training Loss: 0.24769622972826805, Avg Validation Loss: 48.62425354613818\n",
      "Epoch 356/1000, Avg Training Loss: 0.24760058088906414, Avg Validation Loss: 48.624586145532156\n",
      "Epoch 357/1000, Avg Training Loss: 0.24777233798103407, Avg Validation Loss: 48.62412314705197\n",
      "Epoch 358/1000, Avg Training Loss: 0.24748680946389928, Avg Validation Loss: 48.624022307066326\n",
      "Epoch 359/1000, Avg Training Loss: 0.24794662161636183, Avg Validation Loss: 48.62464505645568\n",
      "Epoch 360/1000, Avg Training Loss: 0.24754731898210636, Avg Validation Loss: 48.62520024448767\n",
      "Epoch 361/1000, Avg Training Loss: 0.24789570378927928, Avg Validation Loss: 48.625468907668505\n",
      "Epoch 362/1000, Avg Training Loss: 0.2476829414257683, Avg Validation Loss: 48.62575887497284\n",
      "Epoch 363/1000, Avg Training Loss: 0.24766706027482374, Avg Validation Loss: 48.62631453154526\n",
      "Epoch 364/1000, Avg Training Loss: 0.24767862932973486, Avg Validation Loss: 48.62536815993328\n",
      "Epoch 365/1000, Avg Training Loss: 0.24781971583893797, Avg Validation Loss: 48.62546411663473\n",
      "Epoch 366/1000, Avg Training Loss: 0.24761474650876272, Avg Validation Loss: 48.625786276742645\n",
      "Epoch 367/1000, Avg Training Loss: 0.24768218049330096, Avg Validation Loss: 48.626498646812465\n",
      "Epoch 368/1000, Avg Training Loss: 0.24789824709470043, Avg Validation Loss: 48.627191752156236\n",
      "Epoch 369/1000, Avg Training Loss: 0.24786601381679713, Avg Validation Loss: 48.627780522609605\n",
      "Epoch 370/1000, Avg Training Loss: 0.24754236939440386, Avg Validation Loss: 48.628043635417\n",
      "Epoch 371/1000, Avg Training Loss: 0.24748269038602685, Avg Validation Loss: 48.62634822794857\n",
      "Epoch 372/1000, Avg Training Loss: 0.24778496536529063, Avg Validation Loss: 48.62597034961004\n",
      "Epoch 373/1000, Avg Training Loss: 0.2476912383520593, Avg Validation Loss: 48.626613189775966\n",
      "Epoch 374/1000, Avg Training Loss: 0.2477503932142886, Avg Validation Loss: 48.62757039086309\n",
      "Epoch 375/1000, Avg Training Loss: 0.2476824538563759, Avg Validation Loss: 48.62707784725485\n",
      "Epoch 376/1000, Avg Training Loss: 0.24779111789856884, Avg Validation Loss: 48.628848251241706\n",
      "Epoch 377/1000, Avg Training Loss: 0.24765354406852874, Avg Validation Loss: 48.62755820091648\n",
      "Epoch 378/1000, Avg Training Loss: 0.24765578334895264, Avg Validation Loss: 48.62956132058458\n",
      "Epoch 379/1000, Avg Training Loss: 0.2476041515114698, Avg Validation Loss: 48.62939703832126\n",
      "Epoch 380/1000, Avg Training Loss: 0.24795353309259113, Avg Validation Loss: 48.62974870690572\n",
      "Epoch 381/1000, Avg Training Loss: 0.2476858452317269, Avg Validation Loss: 48.628650197551906\n",
      "Epoch 382/1000, Avg Training Loss: 0.24779299035843227, Avg Validation Loss: 48.62876373274493\n",
      "Epoch 383/1000, Avg Training Loss: 0.24762804715668746, Avg Validation Loss: 48.62958026682654\n",
      "Epoch 384/1000, Avg Training Loss: 0.2477299421527029, Avg Validation Loss: 48.62885155692019\n",
      "Epoch 385/1000, Avg Training Loss: 0.24766338277574854, Avg Validation Loss: 48.629175995174464\n",
      "Epoch 386/1000, Avg Training Loss: 0.24783379507304779, Avg Validation Loss: 48.629644921627374\n",
      "Epoch 387/1000, Avg Training Loss: 0.2476349079748703, Avg Validation Loss: 48.628600410482775\n",
      "Epoch 388/1000, Avg Training Loss: 0.24795902076809828, Avg Validation Loss: 48.63024691010412\n",
      "Epoch 389/1000, Avg Training Loss: 0.24760830701950082, Avg Validation Loss: 48.630294717676634\n",
      "Epoch 390/1000, Avg Training Loss: 0.24764454275734576, Avg Validation Loss: 48.628911260609\n",
      "Epoch 391/1000, Avg Training Loss: 0.24776828940707019, Avg Validation Loss: 48.62951941818103\n",
      "Epoch 392/1000, Avg Training Loss: 0.24792945023455196, Avg Validation Loss: 48.63124989457903\n",
      "Epoch 393/1000, Avg Training Loss: 0.24760853895474796, Avg Validation Loss: 48.630725645049374\n",
      "Epoch 394/1000, Avg Training Loss: 0.24790045644775976, Avg Validation Loss: 48.631237543309354\n",
      "Epoch 395/1000, Avg Training Loss: 0.24808627836951383, Avg Validation Loss: 48.63227739816222\n",
      "Epoch 396/1000, Avg Training Loss: 0.24764737155479224, Avg Validation Loss: 48.63194075206893\n",
      "Epoch 397/1000, Avg Training Loss: 0.24798506425717826, Avg Validation Loss: 48.633227242956856\n",
      "Epoch 398/1000, Avg Training Loss: 0.24762433077812432, Avg Validation Loss: 48.63335582208093\n",
      "Epoch 399/1000, Avg Training Loss: 0.24763014811475814, Avg Validation Loss: 48.63414112219385\n",
      "Epoch 400/1000, Avg Training Loss: 0.24772050174030316, Avg Validation Loss: 48.63439990499891\n",
      "Epoch 401/1000, Avg Training Loss: 0.24765727601285079, Avg Validation Loss: 48.63576520641209\n",
      "Epoch 402/1000, Avg Training Loss: 0.24774621606942496, Avg Validation Loss: 48.635622688312736\n",
      "Epoch 403/1000, Avg Training Loss: 0.24775791944950776, Avg Validation Loss: 48.63519015336465\n",
      "Epoch 404/1000, Avg Training Loss: 0.2477410861904175, Avg Validation Loss: 48.63492267233193\n",
      "Epoch 405/1000, Avg Training Loss: 0.24758002367464452, Avg Validation Loss: 48.634428038129414\n",
      "Epoch 406/1000, Avg Training Loss: 0.2476590118942259, Avg Validation Loss: 48.63253352824025\n",
      "Epoch 407/1000, Avg Training Loss: 0.24798428728489408, Avg Validation Loss: 48.632701334421085\n",
      "Epoch 408/1000, Avg Training Loss: 0.24764895802359818, Avg Validation Loss: 48.632142982258\n",
      "Epoch 409/1000, Avg Training Loss: 0.2478505151712483, Avg Validation Loss: 48.63200294443515\n",
      "Epoch 410/1000, Avg Training Loss: 0.24800701056529925, Avg Validation Loss: 48.63192926728078\n",
      "Epoch 411/1000, Avg Training Loss: 0.24763998767095713, Avg Validation Loss: 48.631385766315084\n",
      "Epoch 412/1000, Avg Training Loss: 0.24742925603373347, Avg Validation Loss: 48.632191979724176\n",
      "Epoch 413/1000, Avg Training Loss: 0.24803364355514299, Avg Validation Loss: 48.63269935107905\n",
      "Epoch 414/1000, Avg Training Loss: 0.24743520906797975, Avg Validation Loss: 48.633813067648695\n",
      "Epoch 415/1000, Avg Training Loss: 0.2477279784263864, Avg Validation Loss: 48.635344758871994\n",
      "Epoch 416/1000, Avg Training Loss: 0.24745292995484974, Avg Validation Loss: 48.63502448330317\n",
      "Epoch 417/1000, Avg Training Loss: 0.24781636197278484, Avg Validation Loss: 48.635107994946864\n",
      "Epoch 418/1000, Avg Training Loss: 0.2475256705943763, Avg Validation Loss: 48.63504231982158\n",
      "Epoch 419/1000, Avg Training Loss: 0.24766132865218804, Avg Validation Loss: 48.635749369584104\n",
      "Epoch 420/1000, Avg Training Loss: 0.24747596763330862, Avg Validation Loss: 48.63612194977302\n",
      "Epoch 421/1000, Avg Training Loss: 0.24781307570632444, Avg Validation Loss: 48.63655386969152\n",
      "Epoch 422/1000, Avg Training Loss: 0.2477480826070684, Avg Validation Loss: 48.635898957548164\n",
      "Epoch 423/1000, Avg Training Loss: 0.2474417910870243, Avg Validation Loss: 48.63527279057215\n",
      "Epoch 424/1000, Avg Training Loss: 0.2474389858934645, Avg Validation Loss: 48.63619679792954\n",
      "Epoch 425/1000, Avg Training Loss: 0.24748581012844498, Avg Validation Loss: 48.63703670355204\n",
      "Epoch 426/1000, Avg Training Loss: 0.24755170231449, Avg Validation Loss: 48.63736079112496\n",
      "Epoch 427/1000, Avg Training Loss: 0.24769420136713308, Avg Validation Loss: 48.63729019367537\n",
      "Epoch 428/1000, Avg Training Loss: 0.24777357479137826, Avg Validation Loss: 48.63722222816328\n",
      "Epoch 429/1000, Avg Training Loss: 0.24788151706045924, Avg Validation Loss: 48.6378682867158\n",
      "Epoch 430/1000, Avg Training Loss: 0.2476715088058323, Avg Validation Loss: 48.638239459977896\n",
      "Epoch 431/1000, Avg Training Loss: 0.24758409787104163, Avg Validation Loss: 48.63616575594631\n",
      "Epoch 432/1000, Avg Training Loss: 0.24773571180462628, Avg Validation Loss: 48.635815933021\n",
      "Epoch 433/1000, Avg Training Loss: 0.24806154416182186, Avg Validation Loss: 48.636411716787805\n",
      "Epoch 434/1000, Avg Training Loss: 0.24807946346307994, Avg Validation Loss: 48.63816173831144\n",
      "Epoch 435/1000, Avg Training Loss: 0.24760199225451512, Avg Validation Loss: 48.637621689771144\n",
      "Epoch 436/1000, Avg Training Loss: 0.24800730480665434, Avg Validation Loss: 48.63834277731851\n",
      "Epoch 437/1000, Avg Training Loss: 0.2476784409691758, Avg Validation Loss: 48.6361784750157\n",
      "Epoch 438/1000, Avg Training Loss: 0.24775488971376686, Avg Validation Loss: 48.637239986977875\n",
      "Epoch 439/1000, Avg Training Loss: 0.24768756779290643, Avg Validation Loss: 48.637461299866516\n",
      "Epoch 440/1000, Avg Training Loss: 0.2478214994549652, Avg Validation Loss: 48.63805270037561\n",
      "Epoch 441/1000, Avg Training Loss: 0.2476841030002672, Avg Validation Loss: 48.637240329902276\n",
      "Epoch 442/1000, Avg Training Loss: 0.24740100528007872, Avg Validation Loss: 48.637386423348644\n",
      "Epoch 443/1000, Avg Training Loss: 0.2478959524731826, Avg Validation Loss: 48.63811115486841\n",
      "Epoch 444/1000, Avg Training Loss: 0.2476726970558003, Avg Validation Loss: 48.6376845775079\n",
      "Epoch 445/1000, Avg Training Loss: 0.24775652230869086, Avg Validation Loss: 48.63734366545731\n",
      "Epoch 446/1000, Avg Training Loss: 0.2478505545263242, Avg Validation Loss: 48.63696426599368\n",
      "Epoch 447/1000, Avg Training Loss: 0.24800130467051396, Avg Validation Loss: 48.63737271563341\n",
      "Epoch 448/1000, Avg Training Loss: 0.24761997836945823, Avg Validation Loss: 48.63780598092568\n",
      "Epoch 449/1000, Avg Training Loss: 0.24774013774687853, Avg Validation Loss: 48.639475622339866\n",
      "Epoch 450/1000, Avg Training Loss: 0.24769552100537587, Avg Validation Loss: 48.639731427409174\n",
      "Epoch 451/1000, Avg Training Loss: 0.24771166800570582, Avg Validation Loss: 48.638373695362674\n",
      "Epoch 452/1000, Avg Training Loss: 0.2482079644360558, Avg Validation Loss: 48.63981011513631\n",
      "Epoch 453/1000, Avg Training Loss: 0.24768584559811482, Avg Validation Loss: 48.63999792729497\n",
      "Epoch 454/1000, Avg Training Loss: 0.24770255757691995, Avg Validation Loss: 48.64069530073159\n",
      "Epoch 455/1000, Avg Training Loss: 0.24734312953875928, Avg Validation Loss: 48.639896146651445\n",
      "Epoch 456/1000, Avg Training Loss: 0.24820305249198338, Avg Validation Loss: 48.640553452557995\n",
      "Epoch 457/1000, Avg Training Loss: 0.2476851077212107, Avg Validation Loss: 48.640839998980326\n",
      "Epoch 458/1000, Avg Training Loss: 0.2477702587959947, Avg Validation Loss: 48.641966719524056\n",
      "Epoch 459/1000, Avg Training Loss: 0.2474451814675628, Avg Validation Loss: 48.64209553713011\n",
      "Epoch 460/1000, Avg Training Loss: 0.24757306155062841, Avg Validation Loss: 48.64463083386677\n",
      "Epoch 461/1000, Avg Training Loss: 0.24781003317056757, Avg Validation Loss: 48.645077958638176\n",
      "Epoch 462/1000, Avg Training Loss: 0.24766122860722473, Avg Validation Loss: 48.64270794927383\n",
      "Epoch 463/1000, Avg Training Loss: 0.24774262573325373, Avg Validation Loss: 48.64376436236421\n",
      "Epoch 464/1000, Avg Training Loss: 0.2481654594425614, Avg Validation Loss: 48.64430612241841\n",
      "Epoch 465/1000, Avg Training Loss: 0.2477812535181117, Avg Validation Loss: 48.64420004395759\n",
      "Epoch 466/1000, Avg Training Loss: 0.24796354580425095, Avg Validation Loss: 48.64479966717856\n",
      "Epoch 467/1000, Avg Training Loss: 0.24759347113689034, Avg Validation Loss: 48.6448009531992\n",
      "Epoch 468/1000, Avg Training Loss: 0.24755716252375606, Avg Validation Loss: 48.644543416016965\n",
      "Epoch 469/1000, Avg Training Loss: 0.24765542537938312, Avg Validation Loss: 48.64444416278101\n",
      "Epoch 470/1000, Avg Training Loss: 0.24808217497679386, Avg Validation Loss: 48.64430820287103\n",
      "Epoch 471/1000, Avg Training Loss: 0.24762223256479354, Avg Validation Loss: 48.64467961408597\n",
      "Epoch 472/1000, Avg Training Loss: 0.24753540548780026, Avg Validation Loss: 48.64536504389448\n",
      "Epoch 473/1000, Avg Training Loss: 0.2476535023743446, Avg Validation Loss: 48.64616354301854\n",
      "Epoch 474/1000, Avg Training Loss: 0.24746113827181504, Avg Validation Loss: 48.64629483233756\n",
      "Epoch 475/1000, Avg Training Loss: 0.2479495843180215, Avg Validation Loss: 48.64661454015852\n",
      "Epoch 476/1000, Avg Training Loss: 0.2478103884517133, Avg Validation Loss: 48.64687380069474\n",
      "Epoch 477/1000, Avg Training Loss: 0.24799907464484938, Avg Validation Loss: 48.64706888494129\n",
      "Epoch 478/1000, Avg Training Loss: 0.2477276210356583, Avg Validation Loss: 48.64777731665933\n",
      "Epoch 479/1000, Avg Training Loss: 0.24788153433014956, Avg Validation Loss: 48.64846483972248\n",
      "Epoch 480/1000, Avg Training Loss: 0.2477323079050506, Avg Validation Loss: 48.64718844384123\n",
      "Epoch 481/1000, Avg Training Loss: 0.24789677879006716, Avg Validation Loss: 48.646948102049194\n",
      "Epoch 482/1000, Avg Training Loss: 0.24758649371074448, Avg Validation Loss: 48.6462881424656\n",
      "Epoch 483/1000, Avg Training Loss: 0.24752153654337442, Avg Validation Loss: 48.6464358506844\n",
      "Epoch 484/1000, Avg Training Loss: 0.24780606254383158, Avg Validation Loss: 48.646145261021815\n",
      "Epoch 485/1000, Avg Training Loss: 0.24748214760647713, Avg Validation Loss: 48.64740692204122\n",
      "Epoch 486/1000, Avg Training Loss: 0.24809492336152547, Avg Validation Loss: 48.64798273012397\n",
      "Epoch 487/1000, Avg Training Loss: 0.24798394111356237, Avg Validation Loss: 48.6498711536915\n",
      "Epoch 488/1000, Avg Training Loss: 0.2473846367828184, Avg Validation Loss: 48.6505726334915\n",
      "Epoch 489/1000, Avg Training Loss: 0.24768910551509493, Avg Validation Loss: 48.65072356986154\n",
      "Epoch 490/1000, Avg Training Loss: 0.24775149820924383, Avg Validation Loss: 48.64990328108484\n",
      "Epoch 491/1000, Avg Training Loss: 0.24785841139210793, Avg Validation Loss: 48.65081208316573\n",
      "Epoch 492/1000, Avg Training Loss: 0.24790488815521947, Avg Validation Loss: 48.65243757722298\n",
      "Epoch 493/1000, Avg Training Loss: 0.2476094264011024, Avg Validation Loss: 48.65198970365351\n",
      "Epoch 494/1000, Avg Training Loss: 0.24739922687640412, Avg Validation Loss: 48.651089622994\n",
      "Epoch 495/1000, Avg Training Loss: 0.24800452977492568, Avg Validation Loss: 48.65154390329266\n",
      "Epoch 496/1000, Avg Training Loss: 0.2480961347351838, Avg Validation Loss: 48.65179018125808\n",
      "Epoch 497/1000, Avg Training Loss: 0.24798573768561802, Avg Validation Loss: 48.65206254340757\n",
      "Epoch 498/1000, Avg Training Loss: 0.24783505437688758, Avg Validation Loss: 48.65217008428734\n",
      "Epoch 499/1000, Avg Training Loss: 0.24773688953905715, Avg Validation Loss: 48.65149254849239\n",
      "Epoch 500/1000, Avg Training Loss: 0.2476771456425593, Avg Validation Loss: 48.65155470802307\n",
      "Epoch 501/1000, Avg Training Loss: 0.24764700444234303, Avg Validation Loss: 48.649713889117365\n",
      "Epoch 502/1000, Avg Training Loss: 0.24793942661162205, Avg Validation Loss: 48.65225829668577\n",
      "Epoch 503/1000, Avg Training Loss: 0.24807517564767592, Avg Validation Loss: 48.653009666302765\n",
      "Epoch 504/1000, Avg Training Loss: 0.24759099761084882, Avg Validation Loss: 48.65291949328765\n",
      "Epoch 505/1000, Avg Training Loss: 0.24761793370243804, Avg Validation Loss: 48.65285187564502\n",
      "Epoch 506/1000, Avg Training Loss: 0.24807762032901554, Avg Validation Loss: 48.653989531782585\n",
      "Epoch 507/1000, Avg Training Loss: 0.24810486900733178, Avg Validation Loss: 48.653127339657885\n",
      "Epoch 508/1000, Avg Training Loss: 0.2477852430560069, Avg Validation Loss: 48.6558259242469\n",
      "Epoch 509/1000, Avg Training Loss: 0.24741800605391556, Avg Validation Loss: 48.65461203345491\n",
      "Epoch 510/1000, Avg Training Loss: 0.24798370258907615, Avg Validation Loss: 48.65618770499034\n",
      "Epoch 511/1000, Avg Training Loss: 0.24780636720566485, Avg Validation Loss: 48.65690581280258\n",
      "Epoch 512/1000, Avg Training Loss: 0.24784985041647012, Avg Validation Loss: 48.65770611318676\n",
      "Epoch 513/1000, Avg Training Loss: 0.2477296765065205, Avg Validation Loss: 48.657528740278806\n",
      "Epoch 514/1000, Avg Training Loss: 0.2478349066460443, Avg Validation Loss: 48.657957599854925\n",
      "Epoch 515/1000, Avg Training Loss: 0.24783452529424876, Avg Validation Loss: 48.65818136007639\n",
      "Epoch 516/1000, Avg Training Loss: 0.2477403175999506, Avg Validation Loss: 48.65668797894156\n",
      "Epoch 517/1000, Avg Training Loss: 0.24775045884945548, Avg Validation Loss: 48.657412532839864\n",
      "Epoch 518/1000, Avg Training Loss: 0.2476384683433227, Avg Validation Loss: 48.65994125807999\n",
      "Epoch 519/1000, Avg Training Loss: 0.24784677934953034, Avg Validation Loss: 48.65822552577295\n",
      "Epoch 520/1000, Avg Training Loss: 0.24804814954778856, Avg Validation Loss: 48.65987187407843\n",
      "Epoch 521/1000, Avg Training Loss: 0.24761924284574632, Avg Validation Loss: 48.6601430660327\n",
      "Epoch 522/1000, Avg Training Loss: 0.2479908599195784, Avg Validation Loss: 48.660662093796475\n",
      "Epoch 523/1000, Avg Training Loss: 0.24788803150571126, Avg Validation Loss: 48.66228100092543\n",
      "Epoch 524/1000, Avg Training Loss: 0.24795389882341445, Avg Validation Loss: 48.663191122337935\n",
      "Epoch 525/1000, Avg Training Loss: 0.24842232987724383, Avg Validation Loss: 48.66357567161212\n",
      "Epoch 526/1000, Avg Training Loss: 0.24753911340527668, Avg Validation Loss: 48.663834880294885\n",
      "Epoch 527/1000, Avg Training Loss: 0.24819079684905637, Avg Validation Loss: 48.663163065369275\n",
      "Epoch 528/1000, Avg Training Loss: 0.2475016660022112, Avg Validation Loss: 48.66208445907715\n",
      "Epoch 529/1000, Avg Training Loss: 0.24779333448263346, Avg Validation Loss: 48.66227490824593\n",
      "Epoch 530/1000, Avg Training Loss: 0.2481294944868572, Avg Validation Loss: 48.6609549675119\n",
      "Epoch 531/1000, Avg Training Loss: 0.24775091404547817, Avg Validation Loss: 48.659216604253714\n",
      "Epoch 532/1000, Avg Training Loss: 0.24777397923238775, Avg Validation Loss: 48.66001244212287\n",
      "Epoch 533/1000, Avg Training Loss: 0.2478056082286907, Avg Validation Loss: 48.6605767447672\n",
      "Epoch 534/1000, Avg Training Loss: 0.24785358266572938, Avg Validation Loss: 48.65962122412907\n",
      "Epoch 535/1000, Avg Training Loss: 0.2475509153367009, Avg Validation Loss: 48.66020231084465\n",
      "Epoch 536/1000, Avg Training Loss: 0.24801006429093042, Avg Validation Loss: 48.66063198923881\n",
      "Epoch 537/1000, Avg Training Loss: 0.2474299265910955, Avg Validation Loss: 48.66194402917651\n",
      "Epoch 538/1000, Avg Training Loss: 0.2478579189172457, Avg Validation Loss: 48.662308273772084\n",
      "Epoch 539/1000, Avg Training Loss: 0.2471480163676176, Avg Validation Loss: 48.66153513267075\n",
      "Epoch 540/1000, Avg Training Loss: 0.2480241768323206, Avg Validation Loss: 48.66185585840679\n",
      "Epoch 541/1000, Avg Training Loss: 0.24767177381455205, Avg Validation Loss: 48.66215435185232\n",
      "Epoch 542/1000, Avg Training Loss: 0.24768309672730993, Avg Validation Loss: 48.66157821913747\n",
      "Epoch 543/1000, Avg Training Loss: 0.24757836916290707, Avg Validation Loss: 48.66214744487101\n",
      "Epoch 544/1000, Avg Training Loss: 0.2482544762558384, Avg Validation Loss: 48.661949271666195\n",
      "Epoch 545/1000, Avg Training Loss: 0.2475137572602799, Avg Validation Loss: 48.661705777564535\n",
      "Epoch 546/1000, Avg Training Loss: 0.24806271518388226, Avg Validation Loss: 48.66256668741067\n",
      "Epoch 547/1000, Avg Training Loss: 0.24775035883905505, Avg Validation Loss: 48.663299802549254\n",
      "Epoch 548/1000, Avg Training Loss: 0.2478386455522722, Avg Validation Loss: 48.66440316436379\n",
      "Epoch 549/1000, Avg Training Loss: 0.2475456637407394, Avg Validation Loss: 48.66536010302788\n",
      "Epoch 550/1000, Avg Training Loss: 0.2477452010921894, Avg Validation Loss: 48.66676880229763\n",
      "Epoch 551/1000, Avg Training Loss: 0.2476103440779144, Avg Validation Loss: 48.667596398568925\n",
      "Epoch 552/1000, Avg Training Loss: 0.2481177206723482, Avg Validation Loss: 48.66851736641463\n",
      "Epoch 553/1000, Avg Training Loss: 0.24815962796276408, Avg Validation Loss: 48.66960561553823\n",
      "Epoch 554/1000, Avg Training Loss: 0.2479557736431671, Avg Validation Loss: 48.66943738251837\n",
      "Epoch 555/1000, Avg Training Loss: 0.24800090829040194, Avg Validation Loss: 48.669590857773926\n",
      "Epoch 556/1000, Avg Training Loss: 0.24771882600885, Avg Validation Loss: 48.66922856412337\n",
      "Epoch 557/1000, Avg Training Loss: 0.2481389517102698, Avg Validation Loss: 48.66991547113889\n",
      "Epoch 558/1000, Avg Training Loss: 0.24749427238243332, Avg Validation Loss: 48.66986457914384\n",
      "Epoch 559/1000, Avg Training Loss: 0.24781379814363974, Avg Validation Loss: 48.66945313427439\n",
      "Epoch 560/1000, Avg Training Loss: 0.247619380297625, Avg Validation Loss: 48.668999811259326\n",
      "Epoch 561/1000, Avg Training Loss: 0.24811603072088162, Avg Validation Loss: 48.67111961995698\n",
      "Epoch 562/1000, Avg Training Loss: 0.2481251287281773, Avg Validation Loss: 48.67323272073925\n",
      "Epoch 563/1000, Avg Training Loss: 0.24794562043073362, Avg Validation Loss: 48.67392553957427\n",
      "Epoch 564/1000, Avg Training Loss: 0.2475192832551507, Avg Validation Loss: 48.67341967970348\n",
      "Epoch 565/1000, Avg Training Loss: 0.24790327679247656, Avg Validation Loss: 48.67356976131752\n",
      "Epoch 566/1000, Avg Training Loss: 0.2478567355084398, Avg Validation Loss: 48.676040419233246\n",
      "Epoch 567/1000, Avg Training Loss: 0.24793944134356075, Avg Validation Loss: 48.67598277664844\n",
      "Epoch 568/1000, Avg Training Loss: 0.24791115367274863, Avg Validation Loss: 48.67679692228147\n",
      "Epoch 569/1000, Avg Training Loss: 0.24740351213629996, Avg Validation Loss: 48.67487818571422\n",
      "Epoch 570/1000, Avg Training Loss: 0.24824883526260028, Avg Validation Loss: 48.67458139144271\n",
      "Epoch 571/1000, Avg Training Loss: 0.24787142822512367, Avg Validation Loss: 48.67515553104394\n",
      "Epoch 572/1000, Avg Training Loss: 0.24842765345348802, Avg Validation Loss: 48.675148413661375\n",
      "Epoch 573/1000, Avg Training Loss: 0.2479103749834852, Avg Validation Loss: 48.677442027173285\n",
      "Epoch 574/1000, Avg Training Loss: 0.24780695155942495, Avg Validation Loss: 48.67658438927024\n",
      "Epoch 575/1000, Avg Training Loss: 0.24802057143969092, Avg Validation Loss: 48.677267191641036\n",
      "Epoch 576/1000, Avg Training Loss: 0.24777050169407644, Avg Validation Loss: 48.677500141768\n",
      "Epoch 577/1000, Avg Training Loss: 0.2477174062898465, Avg Validation Loss: 48.6772565476766\n",
      "Epoch 578/1000, Avg Training Loss: 0.2476040864259791, Avg Validation Loss: 48.680919862578\n",
      "Epoch 579/1000, Avg Training Loss: 0.24760626615507797, Avg Validation Loss: 48.68052542105747\n",
      "Epoch 580/1000, Avg Training Loss: 0.2482550125940483, Avg Validation Loss: 48.67970928752702\n",
      "Epoch 581/1000, Avg Training Loss: 0.24780906215025278, Avg Validation Loss: 48.68125152279444\n",
      "Epoch 582/1000, Avg Training Loss: 0.24794845630962642, Avg Validation Loss: 48.682250784643685\n",
      "Epoch 583/1000, Avg Training Loss: 0.2475792251314412, Avg Validation Loss: 48.68202190451271\n",
      "Epoch 584/1000, Avg Training Loss: 0.24822980072536033, Avg Validation Loss: 48.681354502575346\n",
      "Epoch 585/1000, Avg Training Loss: 0.24782728406843735, Avg Validation Loss: 48.68159412367843\n",
      "Epoch 586/1000, Avg Training Loss: 0.24785688388524973, Avg Validation Loss: 48.682273953180214\n",
      "Epoch 587/1000, Avg Training Loss: 0.24866824161723153, Avg Validation Loss: 48.68533052570766\n",
      "Epoch 588/1000, Avg Training Loss: 0.24780256308136359, Avg Validation Loss: 48.68538761785492\n",
      "Epoch 589/1000, Avg Training Loss: 0.24802790347471462, Avg Validation Loss: 48.68471529964471\n",
      "Epoch 590/1000, Avg Training Loss: 0.24783250966410555, Avg Validation Loss: 48.685647799517156\n",
      "Epoch 591/1000, Avg Training Loss: 0.24811105395728197, Avg Validation Loss: 48.684551688236695\n",
      "Epoch 592/1000, Avg Training Loss: 0.24804152777413804, Avg Validation Loss: 48.68395677214845\n",
      "Epoch 593/1000, Avg Training Loss: 0.2479294720794374, Avg Validation Loss: 48.68419395339308\n",
      "Epoch 594/1000, Avg Training Loss: 0.24803247734537318, Avg Validation Loss: 48.68594567388962\n",
      "Epoch 595/1000, Avg Training Loss: 0.2481386232483086, Avg Validation Loss: 48.68554979862235\n",
      "Epoch 596/1000, Avg Training Loss: 0.24772907864642713, Avg Validation Loss: 48.68487600499509\n",
      "Epoch 597/1000, Avg Training Loss: 0.24809905459693216, Avg Validation Loss: 48.685487581497895\n",
      "Epoch 598/1000, Avg Training Loss: 0.24711902305255481, Avg Validation Loss: 48.6841669818555\n",
      "Epoch 599/1000, Avg Training Loss: 0.24774150118923444, Avg Validation Loss: 48.684477813700596\n",
      "Epoch 600/1000, Avg Training Loss: 0.24792125317402566, Avg Validation Loss: 48.68583841324235\n",
      "Epoch 601/1000, Avg Training Loss: 0.24789198750986632, Avg Validation Loss: 48.68648595313132\n",
      "Epoch 602/1000, Avg Training Loss: 0.24719863309180673, Avg Validation Loss: 48.68634170481227\n",
      "Epoch 603/1000, Avg Training Loss: 0.2483914066055787, Avg Validation Loss: 48.68626583335432\n",
      "Epoch 604/1000, Avg Training Loss: 0.24799386447509764, Avg Validation Loss: 48.686833662324744\n",
      "Epoch 605/1000, Avg Training Loss: 0.2479971799513067, Avg Validation Loss: 48.688102945928684\n",
      "Epoch 606/1000, Avg Training Loss: 0.24768470483927227, Avg Validation Loss: 48.688405836067446\n",
      "Epoch 607/1000, Avg Training Loss: 0.24766551525391484, Avg Validation Loss: 48.69011069029504\n",
      "Epoch 608/1000, Avg Training Loss: 0.24839649296539693, Avg Validation Loss: 48.693646696638325\n",
      "Epoch 609/1000, Avg Training Loss: 0.24777466027040887, Avg Validation Loss: 48.69477258885277\n",
      "Epoch 610/1000, Avg Training Loss: 0.2481958740837613, Avg Validation Loss: 48.69474478760505\n",
      "Epoch 611/1000, Avg Training Loss: 0.24801897653301566, Avg Validation Loss: 48.697276103932545\n",
      "Epoch 612/1000, Avg Training Loss: 0.248018690019172, Avg Validation Loss: 48.69811119501574\n",
      "Epoch 613/1000, Avg Training Loss: 0.24803344677427636, Avg Validation Loss: 48.6986365808241\n",
      "Epoch 614/1000, Avg Training Loss: 0.24736360213175373, Avg Validation Loss: 48.69912495976149\n",
      "Epoch 615/1000, Avg Training Loss: 0.2473065105835132, Avg Validation Loss: 48.6967749340995\n",
      "Epoch 616/1000, Avg Training Loss: 0.2478406388780001, Avg Validation Loss: 48.69751070519534\n",
      "Epoch 617/1000, Avg Training Loss: 0.24768355051250182, Avg Validation Loss: 48.69813118565055\n",
      "Epoch 618/1000, Avg Training Loss: 0.24835690138283514, Avg Validation Loss: 48.69749944952871\n",
      "Epoch 619/1000, Avg Training Loss: 0.24737100170495352, Avg Validation Loss: 48.69444959829583\n",
      "Epoch 620/1000, Avg Training Loss: 0.24791601439564448, Avg Validation Loss: 48.695566357512476\n",
      "Epoch 621/1000, Avg Training Loss: 0.24750489658273542, Avg Validation Loss: 48.69339921093821\n",
      "Epoch 622/1000, Avg Training Loss: 0.24779868795213716, Avg Validation Loss: 48.6944470286582\n",
      "Epoch 623/1000, Avg Training Loss: 0.24757708245036908, Avg Validation Loss: 48.6948003034196\n",
      "Epoch 624/1000, Avg Training Loss: 0.2484266590243694, Avg Validation Loss: 48.695601414936036\n",
      "Epoch 625/1000, Avg Training Loss: 0.24751916757939774, Avg Validation Loss: 48.69352491171709\n",
      "Epoch 626/1000, Avg Training Loss: 0.24740021103754878, Avg Validation Loss: 48.6935340353633\n",
      "Epoch 627/1000, Avg Training Loss: 0.24740610716125183, Avg Validation Loss: 48.69393868103761\n",
      "Epoch 628/1000, Avg Training Loss: 0.24809885336025717, Avg Validation Loss: 48.69587025581937\n",
      "Epoch 629/1000, Avg Training Loss: 0.24837895778230187, Avg Validation Loss: 48.695896291040796\n",
      "Epoch 630/1000, Avg Training Loss: 0.24848959857071484, Avg Validation Loss: 48.69681614032387\n",
      "Epoch 631/1000, Avg Training Loss: 0.2481425166684137, Avg Validation Loss: 48.69915250921147\n",
      "Epoch 632/1000, Avg Training Loss: 0.24771222457425388, Avg Validation Loss: 48.70053899085038\n",
      "Epoch 633/1000, Avg Training Loss: 0.24786122060181293, Avg Validation Loss: 48.700896264471496\n",
      "Epoch 634/1000, Avg Training Loss: 0.2484163679646613, Avg Validation Loss: 48.701889229905625\n",
      "Epoch 635/1000, Avg Training Loss: 0.24727302687879085, Avg Validation Loss: 48.70048808859069\n",
      "Epoch 636/1000, Avg Training Loss: 0.24752928941819208, Avg Validation Loss: 48.699373198649624\n",
      "Epoch 637/1000, Avg Training Loss: 0.24795496693550714, Avg Validation Loss: 48.69933684933389\n",
      "Epoch 638/1000, Avg Training Loss: 0.24790100687247993, Avg Validation Loss: 48.70091421782702\n",
      "Epoch 639/1000, Avg Training Loss: 0.24797747189534472, Avg Validation Loss: 48.70154799646237\n",
      "Epoch 640/1000, Avg Training Loss: 0.24788553397288746, Avg Validation Loss: 48.702700545402834\n",
      "Epoch 641/1000, Avg Training Loss: 0.24803673170874305, Avg Validation Loss: 48.70364281707693\n",
      "Epoch 642/1000, Avg Training Loss: 0.24741816876027264, Avg Validation Loss: 48.703711386491705\n",
      "Epoch 643/1000, Avg Training Loss: 0.24806400227748562, Avg Validation Loss: 48.70284198772814\n",
      "Epoch 644/1000, Avg Training Loss: 0.24825287487344233, Avg Validation Loss: 48.705201268717104\n",
      "Epoch 645/1000, Avg Training Loss: 0.24760251510555464, Avg Validation Loss: 48.70439511281258\n",
      "Epoch 646/1000, Avg Training Loss: 0.24815341649081715, Avg Validation Loss: 48.706663703754245\n",
      "Epoch 647/1000, Avg Training Loss: 0.24777686027511975, Avg Validation Loss: 48.70723491291065\n",
      "Epoch 648/1000, Avg Training Loss: 0.24782959544894567, Avg Validation Loss: 48.70764927394557\n",
      "Epoch 649/1000, Avg Training Loss: 0.2483189688454562, Avg Validation Loss: 48.70950464738395\n",
      "Epoch 650/1000, Avg Training Loss: 0.24688954583668468, Avg Validation Loss: 48.70778461332132\n",
      "Epoch 651/1000, Avg Training Loss: 0.24736018823883252, Avg Validation Loss: 48.709258661304396\n",
      "Epoch 652/1000, Avg Training Loss: 0.2487217179761292, Avg Validation Loss: 48.71202975085962\n",
      "Epoch 653/1000, Avg Training Loss: 0.2481243149607925, Avg Validation Loss: 48.71304491541558\n",
      "Epoch 654/1000, Avg Training Loss: 0.24805653333537514, Avg Validation Loss: 48.711952001713776\n",
      "Epoch 655/1000, Avg Training Loss: 0.24787940755897206, Avg Validation Loss: 48.71224864197316\n",
      "Epoch 656/1000, Avg Training Loss: 0.24781919094760177, Avg Validation Loss: 48.71098307480493\n",
      "Epoch 657/1000, Avg Training Loss: 0.24745710051061529, Avg Validation Loss: 48.71123695522528\n",
      "Epoch 658/1000, Avg Training Loss: 0.2474204885352979, Avg Validation Loss: 48.71242136897173\n",
      "Epoch 659/1000, Avg Training Loss: 0.2477614666197514, Avg Validation Loss: 48.71159099028416\n",
      "Epoch 660/1000, Avg Training Loss: 0.24819456575557866, Avg Validation Loss: 48.71228004496855\n",
      "Epoch 661/1000, Avg Training Loss: 0.24739240296376952, Avg Validation Loss: 48.71053475670793\n",
      "Epoch 662/1000, Avg Training Loss: 0.24810203493791927, Avg Validation Loss: 48.712177741592\n",
      "Epoch 663/1000, Avg Training Loss: 0.2484225835081067, Avg Validation Loss: 48.71474662032304\n",
      "Epoch 664/1000, Avg Training Loss: 0.24761898236996557, Avg Validation Loss: 48.71358070792037\n",
      "Epoch 665/1000, Avg Training Loss: 0.24762296066111106, Avg Validation Loss: 48.714674496212695\n",
      "Epoch 666/1000, Avg Training Loss: 0.24774768942311098, Avg Validation Loss: 48.71667449120113\n",
      "Epoch 667/1000, Avg Training Loss: 0.24741114923124882, Avg Validation Loss: 48.716191301074424\n",
      "Epoch 668/1000, Avg Training Loss: 0.24788487653794689, Avg Validation Loss: 48.71771237240423\n",
      "Epoch 669/1000, Avg Training Loss: 0.24857466206695963, Avg Validation Loss: 48.7201623769691\n",
      "Epoch 670/1000, Avg Training Loss: 0.2479568592709981, Avg Validation Loss: 48.719717318501985\n",
      "Epoch 671/1000, Avg Training Loss: 0.2478394193520815, Avg Validation Loss: 48.71992324990095\n",
      "Epoch 672/1000, Avg Training Loss: 0.2483763923316371, Avg Validation Loss: 48.72192442032045\n",
      "Epoch 673/1000, Avg Training Loss: 0.24777848174009254, Avg Validation Loss: 48.721621619932684\n",
      "Epoch 674/1000, Avg Training Loss: 0.24837004098693208, Avg Validation Loss: 48.721811083992414\n",
      "Epoch 675/1000, Avg Training Loss: 0.24787309767871002, Avg Validation Loss: 48.72131980189163\n",
      "Epoch 676/1000, Avg Training Loss: 0.2484956385468242, Avg Validation Loss: 48.72227709961531\n",
      "Epoch 677/1000, Avg Training Loss: 0.24757727778665248, Avg Validation Loss: 48.72289726347998\n",
      "Epoch 678/1000, Avg Training Loss: 0.2477382034859464, Avg Validation Loss: 48.72349785627332\n",
      "Epoch 679/1000, Avg Training Loss: 0.2485237783670196, Avg Validation Loss: 48.724715562737515\n",
      "Epoch 680/1000, Avg Training Loss: 0.24763850554962608, Avg Validation Loss: 48.72649810197523\n",
      "Epoch 681/1000, Avg Training Loss: 0.24779300185660488, Avg Validation Loss: 48.72714771830145\n",
      "Epoch 682/1000, Avg Training Loss: 0.24773109907769503, Avg Validation Loss: 48.72727945474003\n",
      "Epoch 683/1000, Avg Training Loss: 0.24763881456432962, Avg Validation Loss: 48.72477458329045\n",
      "Epoch 684/1000, Avg Training Loss: 0.24743433309831633, Avg Validation Loss: 48.72523167895383\n",
      "Epoch 685/1000, Avg Training Loss: 0.24780349421195527, Avg Validation Loss: 48.72661907493956\n",
      "Epoch 686/1000, Avg Training Loss: 0.24829578620621015, Avg Validation Loss: 48.72704240985489\n",
      "Epoch 687/1000, Avg Training Loss: 0.2475478887550073, Avg Validation Loss: 48.72694185951476\n",
      "Epoch 688/1000, Avg Training Loss: 0.24822408565088772, Avg Validation Loss: 48.727747004977644\n",
      "Epoch 689/1000, Avg Training Loss: 0.24812592539911876, Avg Validation Loss: 48.72836929020559\n",
      "Epoch 690/1000, Avg Training Loss: 0.24749268922161005, Avg Validation Loss: 48.72961118084\n",
      "Epoch 691/1000, Avg Training Loss: 0.2488113998910492, Avg Validation Loss: 48.73173260288119\n",
      "Epoch 692/1000, Avg Training Loss: 0.24803589075225985, Avg Validation Loss: 48.73270508763907\n",
      "Epoch 693/1000, Avg Training Loss: 0.2479477283015438, Avg Validation Loss: 48.73285127591258\n",
      "Epoch 694/1000, Avg Training Loss: 0.24744304848169274, Avg Validation Loss: 48.73254055398722\n",
      "Epoch 695/1000, Avg Training Loss: 0.24800081982173253, Avg Validation Loss: 48.731965499286815\n",
      "Epoch 696/1000, Avg Training Loss: 0.24824295522504414, Avg Validation Loss: 48.73332679345776\n",
      "Epoch 697/1000, Avg Training Loss: 0.24776366331660044, Avg Validation Loss: 48.73465796312745\n",
      "Epoch 698/1000, Avg Training Loss: 0.2480683372653157, Avg Validation Loss: 48.73534000295352\n",
      "Epoch 699/1000, Avg Training Loss: 0.24822578387149796, Avg Validation Loss: 48.734367435023636\n",
      "Epoch 700/1000, Avg Training Loss: 0.2480079014257448, Avg Validation Loss: 48.73683721713861\n",
      "Epoch 701/1000, Avg Training Loss: 0.24826054307647374, Avg Validation Loss: 48.738557377709526\n",
      "Epoch 702/1000, Avg Training Loss: 0.24829048779209448, Avg Validation Loss: 48.74024371582772\n",
      "Epoch 703/1000, Avg Training Loss: 0.24765486277000914, Avg Validation Loss: 48.73941623126498\n",
      "Epoch 704/1000, Avg Training Loss: 0.24792184034665687, Avg Validation Loss: 48.73854625055536\n",
      "Epoch 705/1000, Avg Training Loss: 0.248183567972783, Avg Validation Loss: 48.73940323150596\n",
      "Epoch 706/1000, Avg Training Loss: 0.24781059741606204, Avg Validation Loss: 48.740142484320955\n",
      "Epoch 707/1000, Avg Training Loss: 0.24845031005068072, Avg Validation Loss: 48.740038826920596\n",
      "Epoch 708/1000, Avg Training Loss: 0.2473024420108315, Avg Validation Loss: 48.73962378293206\n",
      "Epoch 709/1000, Avg Training Loss: 0.24806990372483292, Avg Validation Loss: 48.740554702158406\n",
      "Epoch 710/1000, Avg Training Loss: 0.24858719741033566, Avg Validation Loss: 48.74189743433727\n",
      "Epoch 711/1000, Avg Training Loss: 0.2479196199373613, Avg Validation Loss: 48.741264041500784\n",
      "Epoch 712/1000, Avg Training Loss: 0.24816568136605113, Avg Validation Loss: 48.74127684976817\n",
      "Epoch 713/1000, Avg Training Loss: 0.24800658636336878, Avg Validation Loss: 48.74197259054789\n",
      "Epoch 714/1000, Avg Training Loss: 0.24802836964464334, Avg Validation Loss: 48.743122055014936\n",
      "Epoch 715/1000, Avg Training Loss: 0.2475567941505777, Avg Validation Loss: 48.741860255330124\n",
      "Epoch 716/1000, Avg Training Loss: 0.2479496787951813, Avg Validation Loss: 48.74407997624443\n",
      "Epoch 717/1000, Avg Training Loss: 0.2477207430464562, Avg Validation Loss: 48.74323276588141\n",
      "Epoch 718/1000, Avg Training Loss: 0.2480854752853225, Avg Validation Loss: 48.744588981624034\n",
      "Epoch 719/1000, Avg Training Loss: 0.24739330758036857, Avg Validation Loss: 48.746509052445816\n",
      "Epoch 720/1000, Avg Training Loss: 0.24935594631717561, Avg Validation Loss: 48.74929205957428\n",
      "Epoch 721/1000, Avg Training Loss: 0.24853494008895852, Avg Validation Loss: 48.75203775141859\n",
      "Epoch 722/1000, Avg Training Loss: 0.24842268096533984, Avg Validation Loss: 48.75364652119126\n",
      "Epoch 723/1000, Avg Training Loss: 0.24802709959706293, Avg Validation Loss: 48.753679813958975\n",
      "Epoch 724/1000, Avg Training Loss: 0.24870117102782682, Avg Validation Loss: 48.75460230599249\n",
      "Epoch 725/1000, Avg Training Loss: 0.24862654912432777, Avg Validation Loss: 48.75640554747387\n",
      "Epoch 726/1000, Avg Training Loss: 0.24755478914967682, Avg Validation Loss: 48.754651274951726\n",
      "Epoch 727/1000, Avg Training Loss: 0.24846680157721693, Avg Validation Loss: 48.7564981811029\n",
      "Epoch 728/1000, Avg Training Loss: 0.24730047846280231, Avg Validation Loss: 48.7578759308298\n",
      "Epoch 729/1000, Avg Training Loss: 0.24797394535656056, Avg Validation Loss: 48.75876279653449\n",
      "Epoch 730/1000, Avg Training Loss: 0.2480997013481943, Avg Validation Loss: 48.7592790653739\n",
      "Epoch 731/1000, Avg Training Loss: 0.24767182834869472, Avg Validation Loss: 48.75750649929513\n",
      "Epoch 732/1000, Avg Training Loss: 0.24736185363627342, Avg Validation Loss: 48.75683493621975\n",
      "Epoch 733/1000, Avg Training Loss: 0.24751593618692344, Avg Validation Loss: 48.756638647776555\n",
      "Epoch 734/1000, Avg Training Loss: 0.2483381732723445, Avg Validation Loss: 48.75805206445999\n",
      "Epoch 735/1000, Avg Training Loss: 0.24844821098214948, Avg Validation Loss: 48.75940434349765\n",
      "Epoch 736/1000, Avg Training Loss: 0.24823391490444205, Avg Validation Loss: 48.76019910198817\n",
      "Epoch 737/1000, Avg Training Loss: 0.2484778605932987, Avg Validation Loss: 48.76312206536877\n",
      "Epoch 738/1000, Avg Training Loss: 0.24779523510050627, Avg Validation Loss: 48.762426619221706\n",
      "Epoch 739/1000, Avg Training Loss: 0.24863934538240995, Avg Validation Loss: 48.76486512465746\n",
      "Epoch 740/1000, Avg Training Loss: 0.24818885198428775, Avg Validation Loss: 48.765550293938915\n",
      "Epoch 741/1000, Avg Training Loss: 0.2479688998562226, Avg Validation Loss: 48.76745070508938\n",
      "Epoch 742/1000, Avg Training Loss: 0.24779539578908014, Avg Validation Loss: 48.76881518747773\n",
      "Epoch 743/1000, Avg Training Loss: 0.247361350854943, Avg Validation Loss: 48.76705265772308\n",
      "Epoch 744/1000, Avg Training Loss: 0.2484834453840875, Avg Validation Loss: 48.76700026689571\n",
      "Epoch 745/1000, Avg Training Loss: 0.24796539948326607, Avg Validation Loss: 48.77081389147415\n",
      "Epoch 746/1000, Avg Training Loss: 0.24895640324297635, Avg Validation Loss: 48.77127110116834\n",
      "Epoch 747/1000, Avg Training Loss: 0.24894814547777824, Avg Validation Loss: 48.77293086495617\n",
      "Epoch 748/1000, Avg Training Loss: 0.24753450248766376, Avg Validation Loss: 48.774915166779905\n",
      "Epoch 749/1000, Avg Training Loss: 0.24791731153422814, Avg Validation Loss: 48.773683633530226\n",
      "Epoch 750/1000, Avg Training Loss: 0.24829796867199114, Avg Validation Loss: 48.776526601177316\n",
      "Epoch 751/1000, Avg Training Loss: 0.24790492545220283, Avg Validation Loss: 48.778005911677816\n",
      "Epoch 752/1000, Avg Training Loss: 0.2483978381854275, Avg Validation Loss: 48.77876676360545\n",
      "Epoch 753/1000, Avg Training Loss: 0.24758168380739373, Avg Validation Loss: 48.777653431973306\n",
      "Epoch 754/1000, Avg Training Loss: 0.2484355750479199, Avg Validation Loss: 48.77825773768332\n",
      "Epoch 755/1000, Avg Training Loss: 0.2479116160011035, Avg Validation Loss: 48.7799084210627\n",
      "Epoch 756/1000, Avg Training Loss: 0.24780105932979624, Avg Validation Loss: 48.78044420621497\n",
      "Epoch 757/1000, Avg Training Loss: 0.24911495369118378, Avg Validation Loss: 48.78382970142252\n",
      "Epoch 758/1000, Avg Training Loss: 0.2478624262823388, Avg Validation Loss: 48.78235367638096\n",
      "Epoch 759/1000, Avg Training Loss: 0.2487710679122319, Avg Validation Loss: 48.783154149506935\n",
      "Epoch 760/1000, Avg Training Loss: 0.2480333100594774, Avg Validation Loss: 48.78195608764472\n",
      "Epoch 761/1000, Avg Training Loss: 0.24883109084172286, Avg Validation Loss: 48.78157145503836\n",
      "Epoch 762/1000, Avg Training Loss: 0.24764802113243875, Avg Validation Loss: 48.78386690742216\n",
      "Epoch 763/1000, Avg Training Loss: 0.2479704171056919, Avg Validation Loss: 48.78431502948433\n",
      "Epoch 764/1000, Avg Training Loss: 0.24776124367666705, Avg Validation Loss: 48.786965040692834\n",
      "Epoch 765/1000, Avg Training Loss: 0.24885200906433738, Avg Validation Loss: 48.787934918030395\n",
      "Epoch 766/1000, Avg Training Loss: 0.24817188799376322, Avg Validation Loss: 48.788113762892365\n",
      "Epoch 767/1000, Avg Training Loss: 0.24908853067906833, Avg Validation Loss: 48.79243006521288\n",
      "Epoch 768/1000, Avg Training Loss: 0.2479283750661439, Avg Validation Loss: 48.79208739719634\n",
      "Epoch 769/1000, Avg Training Loss: 0.24834252439610893, Avg Validation Loss: 48.792020536189085\n",
      "Epoch 770/1000, Avg Training Loss: 0.24868243952938252, Avg Validation Loss: 48.794815212668226\n",
      "Epoch 771/1000, Avg Training Loss: 0.24694021793456206, Avg Validation Loss: 48.79669575630032\n",
      "Epoch 772/1000, Avg Training Loss: 0.24759960664606054, Avg Validation Loss: 48.796677360270195\n",
      "Epoch 773/1000, Avg Training Loss: 0.24894064222397763, Avg Validation Loss: 48.796109427848634\n",
      "Epoch 774/1000, Avg Training Loss: 0.2491830413250619, Avg Validation Loss: 48.7978709965253\n",
      "Epoch 775/1000, Avg Training Loss: 0.2476438382886899, Avg Validation Loss: 48.797696422927636\n",
      "Epoch 776/1000, Avg Training Loss: 0.24803987985153458, Avg Validation Loss: 48.799772363126905\n",
      "Epoch 777/1000, Avg Training Loss: 0.24722195895652782, Avg Validation Loss: 48.79891254643648\n",
      "Epoch 778/1000, Avg Training Loss: 0.24853584502790532, Avg Validation Loss: 48.801053429217866\n",
      "Epoch 779/1000, Avg Training Loss: 0.24864535847968064, Avg Validation Loss: 48.80089266600686\n",
      "Epoch 780/1000, Avg Training Loss: 0.2490358845065218, Avg Validation Loss: 48.80273339974947\n",
      "Epoch 781/1000, Avg Training Loss: 0.24743724421894295, Avg Validation Loss: 48.803398161278295\n",
      "Epoch 782/1000, Avg Training Loss: 0.2480414730026181, Avg Validation Loss: 48.80521804196353\n",
      "Epoch 783/1000, Avg Training Loss: 0.2485257545945007, Avg Validation Loss: 48.80845462172665\n",
      "Epoch 784/1000, Avg Training Loss: 0.2486763660858894, Avg Validation Loss: 48.80968330541424\n",
      "Epoch 785/1000, Avg Training Loss: 0.24764346267575824, Avg Validation Loss: 48.809619831200465\n",
      "Epoch 786/1000, Avg Training Loss: 0.2482129489476972, Avg Validation Loss: 48.80931312620402\n",
      "Epoch 787/1000, Avg Training Loss: 0.24844592669159354, Avg Validation Loss: 48.80790752114488\n",
      "Epoch 788/1000, Avg Training Loss: 0.24907634119100266, Avg Validation Loss: 48.810676754167474\n",
      "Epoch 789/1000, Avg Training Loss: 0.24778331869612313, Avg Validation Loss: 48.810164329356525\n",
      "Epoch 790/1000, Avg Training Loss: 0.24864248893836854, Avg Validation Loss: 48.810667176333695\n",
      "Epoch 791/1000, Avg Training Loss: 0.24776289481358674, Avg Validation Loss: 48.81124705089668\n",
      "Epoch 792/1000, Avg Training Loss: 0.24965605709561786, Avg Validation Loss: 48.813570661929475\n",
      "Epoch 793/1000, Avg Training Loss: 0.2485655693456353, Avg Validation Loss: 48.81600014819065\n",
      "Epoch 794/1000, Avg Training Loss: 0.24755942793238878, Avg Validation Loss: 48.81600795654401\n",
      "Epoch 795/1000, Avg Training Loss: 0.24855014277769844, Avg Validation Loss: 48.816121978251346\n",
      "Epoch 796/1000, Avg Training Loss: 0.24855788178225055, Avg Validation Loss: 48.81832915725934\n",
      "Epoch 797/1000, Avg Training Loss: 0.24729346071578695, Avg Validation Loss: 48.818290572995565\n",
      "Epoch 798/1000, Avg Training Loss: 0.24689015105168458, Avg Validation Loss: 48.81917089882505\n",
      "Epoch 799/1000, Avg Training Loss: 0.24829992563765807, Avg Validation Loss: 48.81982448582836\n",
      "Epoch 800/1000, Avg Training Loss: 0.2475685706201138, Avg Validation Loss: 48.821976852343425\n",
      "Epoch 801/1000, Avg Training Loss: 0.24859755479463352, Avg Validation Loss: 48.823233116226945\n",
      "Epoch 802/1000, Avg Training Loss: 0.2488831273523904, Avg Validation Loss: 48.824664832702744\n",
      "Epoch 803/1000, Avg Training Loss: 0.2495295001606971, Avg Validation Loss: 48.827252103180335\n",
      "Epoch 804/1000, Avg Training Loss: 0.24833735916998215, Avg Validation Loss: 48.83008555970657\n",
      "Epoch 805/1000, Avg Training Loss: 0.24885152468577532, Avg Validation Loss: 48.83101909131186\n",
      "Epoch 806/1000, Avg Training Loss: 0.2484280410851766, Avg Validation Loss: 48.83091460872484\n",
      "Epoch 807/1000, Avg Training Loss: 0.24927425957015095, Avg Validation Loss: 48.83403208612654\n",
      "Epoch 808/1000, Avg Training Loss: 0.24822261299327741, Avg Validation Loss: 48.83625818597134\n",
      "Epoch 809/1000, Avg Training Loss: 0.2488742791441932, Avg Validation Loss: 48.834812156902174\n",
      "Epoch 810/1000, Avg Training Loss: 0.24829715815070513, Avg Validation Loss: 48.83606427238664\n",
      "Epoch 811/1000, Avg Training Loss: 0.24837220954989006, Avg Validation Loss: 48.83578522456357\n",
      "Epoch 812/1000, Avg Training Loss: 0.24840132108383156, Avg Validation Loss: 48.835088287091814\n",
      "Epoch 813/1000, Avg Training Loss: 0.2481291305069566, Avg Validation Loss: 48.83501532428299\n",
      "Epoch 814/1000, Avg Training Loss: 0.24950946842619356, Avg Validation Loss: 48.838585930053306\n",
      "Epoch 815/1000, Avg Training Loss: 0.24777828646665678, Avg Validation Loss: 48.83768432051491\n",
      "Epoch 816/1000, Avg Training Loss: 0.24884402279849027, Avg Validation Loss: 48.841288353917484\n",
      "Epoch 817/1000, Avg Training Loss: 0.24829167125265733, Avg Validation Loss: 48.8423952044417\n",
      "Epoch 818/1000, Avg Training Loss: 0.24788967073069185, Avg Validation Loss: 48.84316686584448\n",
      "Epoch 819/1000, Avg Training Loss: 0.24871114769771632, Avg Validation Loss: 48.84521280976078\n",
      "Epoch 820/1000, Avg Training Loss: 0.2492677625869997, Avg Validation Loss: 48.84599791538335\n",
      "Epoch 821/1000, Avg Training Loss: 0.24825020447621896, Avg Validation Loss: 48.846838135362766\n",
      "Epoch 822/1000, Avg Training Loss: 0.24870384420176211, Avg Validation Loss: 48.84750470981791\n",
      "Epoch 823/1000, Avg Training Loss: 0.24930592744422653, Avg Validation Loss: 48.85032330679322\n",
      "Epoch 824/1000, Avg Training Loss: 0.24877730222627623, Avg Validation Loss: 48.85176228165862\n",
      "Epoch 825/1000, Avg Training Loss: 0.24848876671515385, Avg Validation Loss: 48.85423314886401\n",
      "Epoch 826/1000, Avg Training Loss: 0.24730304383647062, Avg Validation Loss: 48.85418260460033\n",
      "Epoch 827/1000, Avg Training Loss: 0.24930684435531525, Avg Validation Loss: 48.85762044105331\n",
      "Epoch 828/1000, Avg Training Loss: 0.2485510876689632, Avg Validation Loss: 48.85569302510568\n",
      "Epoch 829/1000, Avg Training Loss: 0.2492925251016762, Avg Validation Loss: 48.85828802460988\n",
      "Epoch 830/1000, Avg Training Loss: 0.24815550326187177, Avg Validation Loss: 48.86055929972687\n",
      "Epoch 831/1000, Avg Training Loss: 0.2477011243314829, Avg Validation Loss: 48.863041531020585\n",
      "Epoch 832/1000, Avg Training Loss: 0.24787514454088877, Avg Validation Loss: 48.861883621553794\n",
      "Epoch 833/1000, Avg Training Loss: 0.2477797625627006, Avg Validation Loss: 48.86236162791257\n",
      "Epoch 834/1000, Avg Training Loss: 0.24871608470858558, Avg Validation Loss: 48.86400094408555\n",
      "Epoch 835/1000, Avg Training Loss: 0.24749100333514815, Avg Validation Loss: 48.86241823335925\n",
      "Epoch 836/1000, Avg Training Loss: 0.2482195618102947, Avg Validation Loss: 48.86556766174214\n",
      "Epoch 837/1000, Avg Training Loss: 0.24812837716088978, Avg Validation Loss: 48.86580950575116\n",
      "Epoch 838/1000, Avg Training Loss: 0.2474251248408949, Avg Validation Loss: 48.86783454965393\n",
      "Epoch 839/1000, Avg Training Loss: 0.24932192431892639, Avg Validation Loss: 48.86914138741286\n",
      "Epoch 840/1000, Avg Training Loss: 0.2494274886189193, Avg Validation Loss: 48.871035230841194\n",
      "Epoch 841/1000, Avg Training Loss: 0.247751830861869, Avg Validation Loss: 48.87173027498561\n",
      "Epoch 842/1000, Avg Training Loss: 0.2491762354636193, Avg Validation Loss: 48.875409042733516\n",
      "Epoch 843/1000, Avg Training Loss: 0.248038368406201, Avg Validation Loss: 48.87558233045636\n",
      "Epoch 844/1000, Avg Training Loss: 0.24907854220578912, Avg Validation Loss: 48.878703604487804\n",
      "Epoch 845/1000, Avg Training Loss: 0.24868873584036405, Avg Validation Loss: 48.87964383659801\n",
      "Epoch 846/1000, Avg Training Loss: 0.24817260690224954, Avg Validation Loss: 48.88139867947959\n",
      "Epoch 847/1000, Avg Training Loss: 0.2483497488459739, Avg Validation Loss: 48.882806480681495\n",
      "Epoch 848/1000, Avg Training Loss: 0.24860413217986635, Avg Validation Loss: 48.88583306476834\n",
      "Epoch 849/1000, Avg Training Loss: 0.2477960673981059, Avg Validation Loss: 48.88589505964838\n",
      "Epoch 850/1000, Avg Training Loss: 0.24949502148287586, Avg Validation Loss: 48.88971244362663\n",
      "Epoch 851/1000, Avg Training Loss: 0.24900748780316248, Avg Validation Loss: 48.8890029398129\n",
      "Epoch 852/1000, Avg Training Loss: 0.24902518013817632, Avg Validation Loss: 48.8886530103502\n",
      "Epoch 853/1000, Avg Training Loss: 0.24822829987060258, Avg Validation Loss: 48.889428085831526\n",
      "Epoch 854/1000, Avg Training Loss: 0.24963086497979892, Avg Validation Loss: 48.89405494255817\n",
      "Epoch 855/1000, Avg Training Loss: 0.24859814802052627, Avg Validation Loss: 48.893949001696775\n",
      "Epoch 856/1000, Avg Training Loss: 0.2480177873043655, Avg Validation Loss: 48.8961708225569\n",
      "Epoch 857/1000, Avg Training Loss: 0.24785218393061256, Avg Validation Loss: 48.89971760366973\n",
      "Epoch 858/1000, Avg Training Loss: 0.24863250966266268, Avg Validation Loss: 48.900731301226344\n",
      "Epoch 859/1000, Avg Training Loss: 0.24759426263607773, Avg Validation Loss: 48.89948377550516\n",
      "Epoch 860/1000, Avg Training Loss: 0.24885544604200743, Avg Validation Loss: 48.90020248467975\n",
      "Epoch 861/1000, Avg Training Loss: 0.24871265103608978, Avg Validation Loss: 48.90347976650042\n",
      "Epoch 862/1000, Avg Training Loss: 0.24819634894096512, Avg Validation Loss: 48.904578376433264\n",
      "Epoch 863/1000, Avg Training Loss: 0.24850848979354404, Avg Validation Loss: 48.90216890493248\n",
      "Epoch 864/1000, Avg Training Loss: 0.2487256644536391, Avg Validation Loss: 48.90288232606687\n",
      "Epoch 865/1000, Avg Training Loss: 0.24904018446297826, Avg Validation Loss: 48.90300364310424\n",
      "Epoch 866/1000, Avg Training Loss: 0.24974047171560418, Avg Validation Loss: 48.90476874256328\n",
      "Epoch 867/1000, Avg Training Loss: 0.24974608553539127, Avg Validation Loss: 48.9075053743764\n",
      "Epoch 868/1000, Avg Training Loss: 0.24884828778900536, Avg Validation Loss: 48.90930562897074\n",
      "Epoch 869/1000, Avg Training Loss: 0.2485489171191879, Avg Validation Loss: 48.91291592343188\n",
      "Epoch 870/1000, Avg Training Loss: 0.24759495971881598, Avg Validation Loss: 48.91434589081379\n",
      "Epoch 871/1000, Avg Training Loss: 0.24931338786313337, Avg Validation Loss: 48.91334712309523\n",
      "Epoch 872/1000, Avg Training Loss: 0.24810958292937463, Avg Validation Loss: 48.91302799298166\n",
      "Epoch 873/1000, Avg Training Loss: 0.2485025776975545, Avg Validation Loss: 48.914958649163324\n",
      "Epoch 874/1000, Avg Training Loss: 0.24819346393618702, Avg Validation Loss: 48.91870021994503\n",
      "Epoch 875/1000, Avg Training Loss: 0.24901342940516938, Avg Validation Loss: 48.91818127741854\n",
      "Epoch 876/1000, Avg Training Loss: 0.24957803515695642, Avg Validation Loss: 48.92222959727143\n",
      "Epoch 877/1000, Avg Training Loss: 0.24953616266887335, Avg Validation Loss: 48.92522374728772\n",
      "Epoch 878/1000, Avg Training Loss: 0.24877007154859965, Avg Validation Loss: 48.92317106195844\n",
      "Epoch 879/1000, Avg Training Loss: 0.24872419544154567, Avg Validation Loss: 48.92301114541382\n",
      "Epoch 880/1000, Avg Training Loss: 0.25015508944633935, Avg Validation Loss: 48.93144274591694\n",
      "Epoch 881/1000, Avg Training Loss: 0.24907567337300995, Avg Validation Loss: 48.93403556198579\n",
      "Epoch 882/1000, Avg Training Loss: 0.24894632368450065, Avg Validation Loss: 48.93631306888642\n",
      "Epoch 883/1000, Avg Training Loss: 0.2490853556848303, Avg Validation Loss: 48.9374647592694\n",
      "Epoch 884/1000, Avg Training Loss: 0.2485965709096094, Avg Validation Loss: 48.9388985408532\n",
      "Epoch 885/1000, Avg Training Loss: 0.24914311120694727, Avg Validation Loss: 48.93602052695087\n",
      "Epoch 886/1000, Avg Training Loss: 0.25001552118320836, Avg Validation Loss: 48.94279966217978\n",
      "Epoch 887/1000, Avg Training Loss: 0.2501923573878037, Avg Validation Loss: 48.946419775275835\n",
      "Epoch 888/1000, Avg Training Loss: 0.24916612377050132, Avg Validation Loss: 48.94910439283845\n",
      "Epoch 889/1000, Avg Training Loss: 0.24896508949746185, Avg Validation Loss: 48.95413686402223\n",
      "Epoch 890/1000, Avg Training Loss: 0.24893811292553247, Avg Validation Loss: 48.95407689499043\n",
      "Epoch 891/1000, Avg Training Loss: 0.24878250731600737, Avg Validation Loss: 48.95746003913221\n",
      "Epoch 892/1000, Avg Training Loss: 0.2492750047570469, Avg Validation Loss: 48.95481585687716\n",
      "Epoch 893/1000, Avg Training Loss: 0.24827166111544202, Avg Validation Loss: 48.955189148188026\n",
      "Epoch 894/1000, Avg Training Loss: 0.2491073004202203, Avg Validation Loss: 48.9560498614121\n",
      "Epoch 895/1000, Avg Training Loss: 0.24863013819582483, Avg Validation Loss: 48.961320561838775\n",
      "Epoch 896/1000, Avg Training Loss: 0.2491827204841709, Avg Validation Loss: 48.96276940682429\n",
      "Epoch 897/1000, Avg Training Loss: 0.2502305209992567, Avg Validation Loss: 48.96535155758065\n",
      "Epoch 898/1000, Avg Training Loss: 0.24874505622100748, Avg Validation Loss: 48.9689936649322\n",
      "Epoch 899/1000, Avg Training Loss: 0.24911871698651944, Avg Validation Loss: 48.97340536974668\n",
      "Epoch 900/1000, Avg Training Loss: 0.24783873669078063, Avg Validation Loss: 48.973093380245345\n",
      "Epoch 901/1000, Avg Training Loss: 0.2485138554499889, Avg Validation Loss: 48.96963434433911\n",
      "Epoch 902/1000, Avg Training Loss: 0.24872337259930408, Avg Validation Loss: 48.9710702977253\n",
      "Epoch 903/1000, Avg Training Loss: 0.24855966681040012, Avg Validation Loss: 48.97770305566759\n",
      "Epoch 904/1000, Avg Training Loss: 0.25043228278533824, Avg Validation Loss: 48.98280946417529\n",
      "Epoch 905/1000, Avg Training Loss: 0.24901410228679435, Avg Validation Loss: 48.98288138183664\n",
      "Epoch 906/1000, Avg Training Loss: 0.24865209610801275, Avg Validation Loss: 48.98470986571358\n",
      "Epoch 907/1000, Avg Training Loss: 0.24952384660937527, Avg Validation Loss: 48.983928377197685\n",
      "Epoch 908/1000, Avg Training Loss: 0.2486924558525228, Avg Validation Loss: 48.984087018183786\n",
      "Epoch 909/1000, Avg Training Loss: 0.24889654096452352, Avg Validation Loss: 48.984486122095035\n",
      "Epoch 910/1000, Avg Training Loss: 0.2485610922353988, Avg Validation Loss: 48.98693698663647\n",
      "Epoch 911/1000, Avg Training Loss: 0.2481960509921259, Avg Validation Loss: 48.98761625787027\n",
      "Epoch 912/1000, Avg Training Loss: 0.25016884933094835, Avg Validation Loss: 48.99330751511278\n",
      "Epoch 913/1000, Avg Training Loss: 0.2500050920592951, Avg Validation Loss: 48.995415384060564\n",
      "Epoch 914/1000, Avg Training Loss: 0.24826854359584538, Avg Validation Loss: 48.996199736110114\n",
      "Epoch 915/1000, Avg Training Loss: 0.24915401318214478, Avg Validation Loss: 48.99916963741987\n",
      "Epoch 916/1000, Avg Training Loss: 0.2489060574743714, Avg Validation Loss: 49.00351116668411\n",
      "Epoch 917/1000, Avg Training Loss: 0.24851904757943566, Avg Validation Loss: 49.0026666994017\n",
      "Epoch 918/1000, Avg Training Loss: 0.25043720059978847, Avg Validation Loss: 49.00985877654931\n",
      "Epoch 919/1000, Avg Training Loss: 0.24837428618254825, Avg Validation Loss: 49.006484256218656\n",
      "Epoch 920/1000, Avg Training Loss: 0.24983136438416975, Avg Validation Loss: 49.00846923986748\n",
      "Epoch 921/1000, Avg Training Loss: 0.24882982738818907, Avg Validation Loss: 49.008007921575\n",
      "Epoch 922/1000, Avg Training Loss: 0.24903067250764324, Avg Validation Loss: 49.007174195275965\n",
      "Epoch 923/1000, Avg Training Loss: 0.2503325140504119, Avg Validation Loss: 49.010328457668685\n",
      "Epoch 924/1000, Avg Training Loss: 0.2500670859579712, Avg Validation Loss: 49.01490864010564\n",
      "Epoch 925/1000, Avg Training Loss: 0.24869353436639002, Avg Validation Loss: 49.018522594057615\n",
      "Epoch 926/1000, Avg Training Loss: 0.24943741893268076, Avg Validation Loss: 49.0219031107103\n",
      "Epoch 927/1000, Avg Training Loss: 0.2503161270630658, Avg Validation Loss: 49.02453378352976\n",
      "Epoch 928/1000, Avg Training Loss: 0.2491944283523649, Avg Validation Loss: 49.028438066618385\n",
      "Epoch 929/1000, Avg Training Loss: 0.2486530349084296, Avg Validation Loss: 49.031280649055695\n",
      "Epoch 930/1000, Avg Training Loss: 0.25065837579835365, Avg Validation Loss: 49.036433885718324\n",
      "Epoch 931/1000, Avg Training Loss: 0.250982234564611, Avg Validation Loss: 49.040173180831495\n",
      "Epoch 932/1000, Avg Training Loss: 0.24794561050569786, Avg Validation Loss: 49.04052598852996\n",
      "Epoch 933/1000, Avg Training Loss: 0.24904802329252168, Avg Validation Loss: 49.041388850321525\n",
      "Epoch 934/1000, Avg Training Loss: 0.24834131479242785, Avg Validation Loss: 49.04487014924469\n",
      "Epoch 935/1000, Avg Training Loss: 0.24930760325260362, Avg Validation Loss: 49.04812872848168\n",
      "Epoch 936/1000, Avg Training Loss: 0.24965625016748022, Avg Validation Loss: 49.04406564852792\n",
      "Epoch 937/1000, Avg Training Loss: 0.24943598718512938, Avg Validation Loss: 49.046862094678076\n",
      "Epoch 938/1000, Avg Training Loss: 0.2494846514718079, Avg Validation Loss: 49.051539578233246\n",
      "Epoch 939/1000, Avg Training Loss: 0.24900465023732038, Avg Validation Loss: 49.05133503703865\n",
      "Epoch 940/1000, Avg Training Loss: 0.2497345373019408, Avg Validation Loss: 49.056059997264626\n",
      "Epoch 941/1000, Avg Training Loss: 0.24996848641950456, Avg Validation Loss: 49.058400142125485\n",
      "Epoch 942/1000, Avg Training Loss: 0.24868863287173695, Avg Validation Loss: 49.05991091419354\n",
      "Epoch 943/1000, Avg Training Loss: 0.2490133782500667, Avg Validation Loss: 49.05876266677679\n",
      "Epoch 944/1000, Avg Training Loss: 0.25089637924044106, Avg Validation Loss: 49.062903005306396\n",
      "Epoch 945/1000, Avg Training Loss: 0.247922519425492, Avg Validation Loss: 49.06607682474158\n",
      "Epoch 946/1000, Avg Training Loss: 0.2492682987695358, Avg Validation Loss: 49.0677368581417\n",
      "Epoch 947/1000, Avg Training Loss: 0.24975021951037263, Avg Validation Loss: 49.07165528249466\n",
      "Epoch 948/1000, Avg Training Loss: 0.24809489178730673, Avg Validation Loss: 49.074326349115466\n",
      "Epoch 949/1000, Avg Training Loss: 0.24994169072311415, Avg Validation Loss: 49.075103009471334\n",
      "Epoch 950/1000, Avg Training Loss: 0.25027628959742976, Avg Validation Loss: 49.0768689758611\n",
      "Epoch 951/1000, Avg Training Loss: 0.2511887326731203, Avg Validation Loss: 49.080267412373246\n",
      "Epoch 952/1000, Avg Training Loss: 0.2483216577917128, Avg Validation Loss: 49.083769299676774\n",
      "Epoch 953/1000, Avg Training Loss: 0.2485898050586558, Avg Validation Loss: 49.084078390140604\n",
      "Epoch 954/1000, Avg Training Loss: 0.24799288448851456, Avg Validation Loss: 49.08592364834829\n",
      "Epoch 955/1000, Avg Training Loss: 0.24923241451955666, Avg Validation Loss: 49.08773685676309\n",
      "Epoch 956/1000, Avg Training Loss: 0.24934755733181851, Avg Validation Loss: 49.092890426804615\n",
      "Epoch 957/1000, Avg Training Loss: 0.25068388353430254, Avg Validation Loss: 49.099850558515016\n",
      "Epoch 958/1000, Avg Training Loss: 0.2486696264984833, Avg Validation Loss: 49.09909260264525\n",
      "Epoch 959/1000, Avg Training Loss: 0.24824878103928047, Avg Validation Loss: 49.10284574225001\n",
      "Epoch 960/1000, Avg Training Loss: 0.24938184519333004, Avg Validation Loss: 49.104623469268354\n",
      "Epoch 961/1000, Avg Training Loss: 0.2498508214981898, Avg Validation Loss: 49.10695274480294\n",
      "Epoch 962/1000, Avg Training Loss: 0.2515552971078995, Avg Validation Loss: 49.10931588965856\n",
      "Epoch 963/1000, Avg Training Loss: 0.24844688085128083, Avg Validation Loss: 49.110288973955065\n",
      "Epoch 964/1000, Avg Training Loss: 0.24938370660829678, Avg Validation Loss: 49.11311282416341\n",
      "Epoch 965/1000, Avg Training Loss: 0.2508445241368009, Avg Validation Loss: 49.11741910793224\n",
      "Epoch 966/1000, Avg Training Loss: 0.24848692431571665, Avg Validation Loss: 49.117681204627715\n",
      "Epoch 967/1000, Avg Training Loss: 0.25034412430705083, Avg Validation Loss: 49.1242128826906\n",
      "Epoch 968/1000, Avg Training Loss: 0.2505452607658021, Avg Validation Loss: 49.12820155407831\n",
      "Epoch 969/1000, Avg Training Loss: 0.2503276342577391, Avg Validation Loss: 49.128761775420664\n",
      "Epoch 970/1000, Avg Training Loss: 0.24983152676449003, Avg Validation Loss: 49.130673069468294\n",
      "Epoch 971/1000, Avg Training Loss: 0.24886224120831296, Avg Validation Loss: 49.13395991514709\n",
      "Epoch 972/1000, Avg Training Loss: 0.24921929866669315, Avg Validation Loss: 49.13960351315771\n",
      "Epoch 973/1000, Avg Training Loss: 0.24989928467747877, Avg Validation Loss: 49.143557140425415\n",
      "Epoch 974/1000, Avg Training Loss: 0.2489697256501814, Avg Validation Loss: 49.14517285149095\n",
      "Epoch 975/1000, Avg Training Loss: 0.24986386520247827, Avg Validation Loss: 49.144916052126895\n",
      "Epoch 976/1000, Avg Training Loss: 0.24830143648546574, Avg Validation Loss: 49.146163906299\n",
      "Epoch 977/1000, Avg Training Loss: 0.2501421543207124, Avg Validation Loss: 49.15195568820499\n",
      "Epoch 978/1000, Avg Training Loss: 0.2506715739651693, Avg Validation Loss: 49.15428114795258\n",
      "Epoch 979/1000, Avg Training Loss: 0.24966420777002285, Avg Validation Loss: 49.15644602036248\n",
      "Epoch 980/1000, Avg Training Loss: 0.24913390080045225, Avg Validation Loss: 49.16248405265618\n",
      "Epoch 981/1000, Avg Training Loss: 0.24951952967452326, Avg Validation Loss: 49.167288832445834\n",
      "Epoch 982/1000, Avg Training Loss: 0.249972025390215, Avg Validation Loss: 49.164912165555464\n",
      "Epoch 983/1000, Avg Training Loss: 0.24828700196162312, Avg Validation Loss: 49.167527024893836\n",
      "Epoch 984/1000, Avg Training Loss: 0.24943267537505104, Avg Validation Loss: 49.1700931566623\n",
      "Epoch 985/1000, Avg Training Loss: 0.2503637545033216, Avg Validation Loss: 49.17598018226029\n",
      "Epoch 986/1000, Avg Training Loss: 0.25012110414367206, Avg Validation Loss: 49.178208741638755\n",
      "Epoch 987/1000, Avg Training Loss: 0.24954151950548328, Avg Validation Loss: 49.18247701889236\n",
      "Epoch 988/1000, Avg Training Loss: 0.24998784207803973, Avg Validation Loss: 49.18340250729916\n",
      "Epoch 989/1000, Avg Training Loss: 0.24982595027989013, Avg Validation Loss: 49.18526546586817\n",
      "Epoch 990/1000, Avg Training Loss: 0.24828872395574714, Avg Validation Loss: 49.185898892132656\n",
      "Epoch 991/1000, Avg Training Loss: 0.2496789246109321, Avg Validation Loss: 49.18623661552671\n",
      "Epoch 992/1000, Avg Training Loss: 0.24947682402331497, Avg Validation Loss: 49.18981514127239\n",
      "Epoch 993/1000, Avg Training Loss: 0.24871336534246502, Avg Validation Loss: 49.1883679775909\n",
      "Epoch 994/1000, Avg Training Loss: 0.2506003920887193, Avg Validation Loss: 49.19234336335322\n",
      "Epoch 995/1000, Avg Training Loss: 0.24910745527539016, Avg Validation Loss: 49.19436222764291\n",
      "Epoch 996/1000, Avg Training Loss: 0.24787034946864192, Avg Validation Loss: 49.19483668531453\n",
      "Epoch 997/1000, Avg Training Loss: 0.24927280351752415, Avg Validation Loss: 49.19551820899835\n",
      "Epoch 998/1000, Avg Training Loss: 0.24875733108848117, Avg Validation Loss: 49.198509679837116\n",
      "Epoch 999/1000, Avg Training Loss: 0.24968280518626748, Avg Validation Loss: 49.200013770853275\n",
      "Epoch 1000/1000, Avg Training Loss: 0.25084887383984655, Avg Validation Loss: 49.2067670767241\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x_tot = np.random.rand(1000, 15)\n",
    "target = np.random.rand(1000, 3)\n",
    "\n",
    "layer_one = Layer(15, 8, ELU, d_ELU)\n",
    "layer_two = Layer(8, 5, ELU, d_ELU)\n",
    "layer_out = Layer(5, 3, linear, d_linear)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "NN.add_layer(layer_one)\n",
    "NN.add_layer(layer_two)\n",
    "NN.add_layer(layer_out)\n",
    "\n",
    "# Parametri di training\n",
    "K = 5\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "Lambda = 0.1\n",
    "\n",
    "# Cross-validation\n",
    "train_error, val_error = NN.train_val(x_tot, target, K, epochs, learning_rate, Lambda, 'elastic', mean_squared_error, d_mean_squared_error, batch_size)\n",
    "\n",
    "# Plot degli errori\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_error, label='Training Error')\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
