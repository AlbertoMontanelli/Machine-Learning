{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zBZ8ZOoCBL"
      },
      "source": [
        "# Cose da fare\n",
        "* inserire documentazione per le regolarizzazioni, adam, momento e ricontrollare tutto (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
        "* inserire loss function per problemi di classificazione: BCE o altro\n",
        "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
        "* analisi training error vs validation error vs test error\n",
        "### Novelties\n",
        "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
        "* early stopping\n",
        "* learning rate variabile (?)\n",
        "* standarditation e normalization (FACOLTATIVO FORSE)\n",
        "# Cose da fare secondo le (!) Micheli\n",
        "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "qv1wwlgWsFM8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n",
        "\n",
        "# np.vectorize returns an object that acts like pyfunc, but takes arrays as input\n",
        "d_ReLU = np.vectorize(d_ReLU)\n",
        "d_leaky_relu = np.vectorize(d_leaky_relu)\n",
        "ELU = np.vectorize(ELU)\n",
        "d_ELU = np.vectorize(d_ELU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return - 2 * (y_true - y_pred)\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return - (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return - (y_true - y_pred) if(np.abs(y_true-y_pred)<=delta) else - delta * np.sign(y_true-y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters:**\n",
        " * input_size: $k_{i-1}$;\n",
        " * output_size: $k_i$;\n",
        " * activation_function;\n",
        " * activation_derivative.\n",
        "\n",
        "**Constructor attributes:**\n",
        "* self.input_size = input_size;\n",
        "* self.output_size = output_size;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative;\n",
        "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
        "\n",
        "**Methods :**\n",
        "\n",
        "* **initialize_weights**: initialize weights and biases\n",
        "  * attributes:\n",
        "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
        "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
        "    \n",
        "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
        "  * parameter:\n",
        "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
        "  * attributes:\n",
        "    * self.input: input_array;\n",
        "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
        "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
        "  \n",
        "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        "  * parameters:\n",
        "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "    * learning_rate.\n",
        "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "H8Ap9rLxLlNU",
        "outputId": "da307f31-1bbc-4d71-826e-718b36ac131a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "        self.initialize_weights()\n",
        "        self.t = 1 # number of iterations for adam\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialization of the parameters of the network\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
        "        self.biases = np.zeros((1, self.output_size))\n",
        "\n",
        "        # Initialization of the parameters for Nesterov optimization\n",
        "        self.velocity_weights = np.zeros_like(self.weights) # zeros array with dim = dim(self.weights)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        # Initialization of the parameters for Adam optimization\n",
        "        self.m_weights = np.zeros_like(self.weights)\n",
        "        self.v_weights = np.zeros_like(self.weights)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "    # Tikhonov, lasso, elastic regularization.\n",
        "    def regularization_func(self, reg_type, Lambda_t, Lambda_l, ww):\n",
        "        if reg_type == 'tikhonov':\n",
        "            reg_term = 2 * Lambda_t * ww # learning rule of tikhonov regularization,\n",
        "                                                   # the minus sign is due to the application of gradient descent algorithm.\n",
        "                                                   # We use the predicted weights in the Nesterov optimization in the regularization too\n",
        "        elif reg_type == 'lasso':\n",
        "            reg_term = Lambda_l * np.sign(ww) # learning rule of lasso regularization\n",
        "        elif reg_type == 'elastic':\n",
        "            reg_term = (2 * Lambda_t * ww + Lambda_l * np.sign(ww)) # lasso + tikhonov regularization\n",
        "        return reg_term\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        self.delta = - d_Ep * self.activation_derivative(self.net)\n",
        "        if opt_type == 'NAG':\n",
        "            weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights in order to use the Nesterov momentum,\n",
        "                                                                            # used to evaluate the gradient after the momentum is applied\n",
        "            bias_pred = self.biases + momentum * self.velocity_biases # same thing for the biases\n",
        "            net_pred = np.dot(self.input, weights_pred) + bias_pred  #  Net has been computed with respect to the predicted weights and the predicted biases\n",
        "            delta_pred = - d_Ep * self.activation_derivative(net_pred)  # Loss gradient with respect to net, minus sign due to the definition\n",
        "            grad_weights = learning_rate_w * np.dot(self.input.T, delta_pred)  # Loss gradient multiplied by the learning rate.\n",
        "                                                                            # The gradient has been computed with respect to the predicted weights and biases\n",
        "            \n",
        "            reg_term = self.regularization_func(reg_type, Lambda_t, Lambda_l, weights_pred)\n",
        "            self.velocity_weights = momentum * self.velocity_weights + grad_weights - reg_term  # Delta w new \n",
        "            self.weights += self.velocity_weights  # Updating the weights\n",
        "            self.velocity_biases = momentum * self.velocity_biases + learning_rate_b * np.sum(delta_pred, axis=0, keepdims=True)\n",
        "            self.biases += self.velocity_biases # Updating the biases\n",
        "\n",
        "        elif opt_type == 'adam':\n",
        "            reg_term = self.regularization_func(reg_type, Lambda_t, Lambda_l, self.weights)\n",
        "            self.m_weights = beta_1 * self.m_weights + (1 - beta_1) * (- np.dot(self.input.T, self.delta) - reg_term) # np.dot(self.input.T, delta) is dLoss/dw,\n",
        "                                                                                                                        # since self.delta is defined with a minus sign\n",
        "                                                                                                                        # and the formula is with a plus sign, we put a minus sign\n",
        "                                                                                                                        # in front of np.dot(xxx)\n",
        "            self.v_weights = beta_2* self.v_weights + (1 - beta_2) * ((- np.dot(self.input.T, self.delta) - reg_term)**2) # here we have a plus sign in front of (1 - beta_2) since\n",
        "                                                                                                                        # self.delta is squared\n",
        "            m_weights_hat = self.m_weights / (1 - beta_1**self.t)\n",
        "            v_weights_hat = self.v_weights / (1 - beta_2**self.t)\n",
        "\n",
        "            self.m_biases = beta_1 * self.m_biases - (1 - beta_1) * np.sum(self.delta, axis=0, keepdims=True)\n",
        "            self.v_biases = beta_2* self.v_biases + (1 - beta_2) * np.sum(self.delta**2, axis=0, keepdims=True)\n",
        "            m_biases_hat = self.m_biases / (1 - beta_1**self.t)\n",
        "            v_biases_hat = self.v_biases / (1 - beta_2**self.t)\n",
        "\n",
        "            self.weights -= learning_rate_w * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "            self.biases -= learning_rate_b * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)\n",
        "\n",
        "        sum_delta_weights = np.dot(self.delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
        "   * parameters:\n",
        "     * x_tot: total data given as input;\n",
        "     * target: total data labels given as input;\n",
        "     * test_split: percentile of test set with respect to the total data.\n",
        "    * return->:\n",
        "      * x_train_val: training & validation set extracted from input data;\n",
        "      * target_train_val: training & validation set labels;\n",
        "      * x_test_val: test set extracted from input data;\n",
        "      * target_test_val: test set for input data labels.\n",
        "\n",
        "     \n",
        "* **add_layer**: appends a layer to the empty list self.layers\n",
        "   * parameter:\n",
        "     * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        "  * parameter:\n",
        "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        "  * return -> input = $o_i$ for layer $L_i$.\n",
        "  \n",
        "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        "  * parameters:\n",
        "    * d_Ep;\n",
        "    * learning_rate.\n",
        "\n",
        "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
        "\n",
        "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
        "  * parameters:\n",
        "    * x_train: set of the original dataset used for training;\n",
        "    * target_train: labels corresponding to the training set;\n",
        "    * x_val: set of the original dataset used for validation;\n",
        "    * target_val: labels corresponding to the validation set;\n",
        "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
        "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
        "    * loss_function: hyperparameter;\n",
        "    * loss_function_derivative: hyperparameter;\n",
        "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
        "\n",
        "* **train_val**: actual training and validation process.\n",
        "  * parameters:\n",
        "    * x_train_val;\n",
        "    * target_train_val;\n",
        "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
        "    * epochs;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def data_split(self, x_tot, target, test_split):\n",
        "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
        "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
        "\n",
        "        x_test = x_tot[:test_size]\n",
        "        target_test = target[:test_size]\n",
        "        x_train_val = x_tot[test_size:]\n",
        "        target_train_val = target[test_size:]\n",
        "\n",
        "        return x_train_val, target_train_val, x_test, target_test\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "\n",
        "    def reinitialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.initialize_weights() # does it layer-by-layer\n",
        "\n",
        "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        train_error_epoch = np.zeros(epochs)\n",
        "        val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
        "            epoch_val_loss = 0\n",
        "\n",
        "            # shuffling training data before splitting it into batches.\n",
        "            # done in order to avoid reinforcing neurons in the same way\n",
        "            # in different epochs due to invisible patterns in the data\n",
        "            train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(train_indices)\n",
        "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
        "            target_train = target_train[train_indices]\n",
        "\n",
        "            # if batch_size=1 we get the online version,\n",
        "            # else we get mini-batch version with batches of size batch_size\n",
        "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
        "                                                             # Even if the number of examples is not divisible\n",
        "                                                             # for batch_size the last, smaller batch is processed anyway\n",
        "\n",
        "                x_batch = x_train[i:i+batch_size]\n",
        "                target_batch = target_train[i:i+batch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
        "                # computing loss and gradient\n",
        "                loss = loss_function(target_batch, predictions)\n",
        "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "                epoch_tr_loss += np.sum(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(loss_gradient, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "                Layer.t += 1 # number of iterations for adam\n",
        "\n",
        "            # validation\n",
        "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
        "            val_loss = loss_function(target_val, val_predictions)\n",
        "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished.\n",
        "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
        "            # Store average errors for the epoch\n",
        "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
        "            val_error_epoch[epoch] = epoch_val_loss\n",
        "\n",
        "        return train_error_epoch, val_error_epoch\n",
        "\n",
        "\n",
        "    def train_val(self, x_train_val, target_train_val, train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        num_samples = x_train_val.shape[0]\n",
        "        fold_size = num_samples // K\n",
        "\n",
        "        # error storage for averaging\n",
        "        avg_train_error_epoch = np.zeros(epochs)\n",
        "        avg_val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        if K==1: # hold-out validation\n",
        "            train_indices = np.arange(0, int(train_split*num_samples)) # training set is 75% of the training & validation set\n",
        "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            ) # computation of errors via train_val_setup method\n",
        "            return train_error_epoch, val_error_epoch\n",
        "\n",
        "        for k in range(K):\n",
        "            # creating fold indices\n",
        "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
        "                                                                        # It contains the indices of the examples used in validation set for\n",
        "                                                                        # this fold.\n",
        "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples -\n",
        "                                                                              # len(val_indices). It contains the indices of all the examples\n",
        "                                                                              # but the ones used in the validation set for this fold.\n",
        "                                                                              # It corresponds to the training set for the current fold.\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
        "\n",
        "            # re-initializing weights for each fold\n",
        "            self.reinitialize_weights()\n",
        "            Layer.t = 1\n",
        "            # training on the current fold\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            )\n",
        "\n",
        "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
        "            avg_train_error_epoch += train_error_epoch\n",
        "            avg_val_error_epoch += val_error_epoch\n",
        "            print(f\"Fold {k+1} completed, t = {Layer.t}\")\n",
        "\n",
        "        # averaging errors across all folds\n",
        "        avg_train_error_epoch /= K\n",
        "        avg_val_error_epoch /= K\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
        "\n",
        "        return avg_train_error_epoch, avg_val_error_epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "# Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "5be15b38-62a4-439a-dd86-57790daaffe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 completed, t = 7001\n",
            "Fold 2 completed, t = 7001\n",
            "Fold 3 completed, t = 7001\n",
            "Fold 4 completed, t = 7001\n",
            "Fold 5 completed, t = 7001\n",
            "Epoch 1/1000, Avg Training Loss: 0.8668432244584338, Avg Validation Loss: 120.92871596900068\n",
            "Epoch 2/1000, Avg Training Loss: 0.7432315649003642, Avg Validation Loss: 102.24496477892822\n",
            "Epoch 3/1000, Avg Training Loss: 0.6353073128006742, Avg Validation Loss: 88.34160264322091\n",
            "Epoch 4/1000, Avg Training Loss: 0.5572952778692082, Avg Validation Loss: 78.85805233607337\n",
            "Epoch 5/1000, Avg Training Loss: 0.5055004275364976, Avg Validation Loss: 72.85154402991304\n",
            "Epoch 6/1000, Avg Training Loss: 0.47241562582545715, Avg Validation Loss: 69.36874446174869\n",
            "Epoch 7/1000, Avg Training Loss: 0.45362689158344294, Avg Validation Loss: 67.30702507294858\n",
            "Epoch 8/1000, Avg Training Loss: 0.4432738592581468, Avg Validation Loss: 66.56168952233648\n",
            "Epoch 9/1000, Avg Training Loss: 0.44006066510988917, Avg Validation Loss: 66.16337457577377\n",
            "Epoch 10/1000, Avg Training Loss: 0.43651438248122315, Avg Validation Loss: 66.01206658139398\n",
            "Epoch 11/1000, Avg Training Loss: 0.43717537807283324, Avg Validation Loss: 66.444756638405\n",
            "Epoch 12/1000, Avg Training Loss: 0.4396226248497682, Avg Validation Loss: 66.9510664317882\n",
            "Epoch 13/1000, Avg Training Loss: 0.4418078299060021, Avg Validation Loss: 67.23986508102442\n",
            "Epoch 14/1000, Avg Training Loss: 0.44252604795321426, Avg Validation Loss: 67.1713731627012\n",
            "Epoch 15/1000, Avg Training Loss: 0.4405375043718031, Avg Validation Loss: 66.74063810899128\n",
            "Epoch 16/1000, Avg Training Loss: 0.43650085836706254, Avg Validation Loss: 65.96964243709837\n",
            "Epoch 17/1000, Avg Training Loss: 0.4305694071099039, Avg Validation Loss: 64.91973051442264\n",
            "Epoch 18/1000, Avg Training Loss: 0.4229687030003417, Avg Validation Loss: 63.66702840340162\n",
            "Epoch 19/1000, Avg Training Loss: 0.4142786546519567, Avg Validation Loss: 62.25735721216196\n",
            "Epoch 20/1000, Avg Training Loss: 0.40478196625392415, Avg Validation Loss: 60.73025027267071\n",
            "Epoch 21/1000, Avg Training Loss: 0.3947953772354772, Avg Validation Loss: 59.115772740108206\n",
            "Epoch 22/1000, Avg Training Loss: 0.38408049100950986, Avg Validation Loss: 57.476137176846976\n",
            "Epoch 23/1000, Avg Training Loss: 0.3733780426129618, Avg Validation Loss: 55.82219590639348\n",
            "Epoch 24/1000, Avg Training Loss: 0.3628644652075373, Avg Validation Loss: 54.176362789093716\n",
            "Epoch 25/1000, Avg Training Loss: 0.3523443719617191, Avg Validation Loss: 52.58061693054917\n",
            "Epoch 26/1000, Avg Training Loss: 0.3422958206736182, Avg Validation Loss: 51.047743734398615\n",
            "Epoch 27/1000, Avg Training Loss: 0.3327266416033432, Avg Validation Loss: 49.59927633862387\n",
            "Epoch 28/1000, Avg Training Loss: 0.32356295880350444, Avg Validation Loss: 48.25385063283504\n",
            "Epoch 29/1000, Avg Training Loss: 0.31531902160328207, Avg Validation Loss: 47.00101447817665\n",
            "Epoch 30/1000, Avg Training Loss: 0.3075328586685418, Avg Validation Loss: 45.867281678715344\n",
            "Epoch 31/1000, Avg Training Loss: 0.30061654950567396, Avg Validation Loss: 44.8289445760456\n",
            "Epoch 32/1000, Avg Training Loss: 0.2942790389515576, Avg Validation Loss: 43.900277782655564\n",
            "Epoch 33/1000, Avg Training Loss: 0.2884345556715174, Avg Validation Loss: 43.07990044237175\n",
            "Epoch 34/1000, Avg Training Loss: 0.2835458029109128, Avg Validation Loss: 42.32952021823971\n",
            "Epoch 35/1000, Avg Training Loss: 0.27892959026118935, Avg Validation Loss: 41.68239987483754\n",
            "Epoch 36/1000, Avg Training Loss: 0.2750303751000328, Avg Validation Loss: 41.09557705563036\n",
            "Epoch 37/1000, Avg Training Loss: 0.27144543097200063, Avg Validation Loss: 40.58349528720008\n",
            "Epoch 38/1000, Avg Training Loss: 0.2682226083267836, Avg Validation Loss: 40.143713132521256\n",
            "Epoch 39/1000, Avg Training Loss: 0.26560198597781703, Avg Validation Loss: 39.753539270440584\n",
            "Epoch 40/1000, Avg Training Loss: 0.263279014151259, Avg Validation Loss: 39.406120066859216\n",
            "Epoch 41/1000, Avg Training Loss: 0.26109511657770895, Avg Validation Loss: 39.11163529888165\n",
            "Epoch 42/1000, Avg Training Loss: 0.2593091963201085, Avg Validation Loss: 38.84912888369065\n",
            "Epoch 43/1000, Avg Training Loss: 0.25767972818133644, Avg Validation Loss: 38.63096121939199\n",
            "Epoch 44/1000, Avg Training Loss: 0.25643969594247035, Avg Validation Loss: 38.42931421553149\n",
            "Epoch 45/1000, Avg Training Loss: 0.25509440179463444, Avg Validation Loss: 38.26594482536058\n",
            "Epoch 46/1000, Avg Training Loss: 0.25414473403643245, Avg Validation Loss: 38.1194361944545\n",
            "Epoch 47/1000, Avg Training Loss: 0.2531911053024817, Avg Validation Loss: 37.99983581077842\n",
            "Epoch 48/1000, Avg Training Loss: 0.25240062649949846, Avg Validation Loss: 37.89400491761121\n",
            "Epoch 49/1000, Avg Training Loss: 0.2517553739548061, Avg Validation Loss: 37.80219883469505\n",
            "Epoch 50/1000, Avg Training Loss: 0.2511809800612863, Avg Validation Loss: 37.72610020070478\n",
            "Epoch 51/1000, Avg Training Loss: 0.2507099066995778, Avg Validation Loss: 37.65676902895481\n",
            "Epoch 52/1000, Avg Training Loss: 0.25029627843879715, Avg Validation Loss: 37.6011204033076\n",
            "Epoch 53/1000, Avg Training Loss: 0.24993535836360184, Avg Validation Loss: 37.54511859505935\n",
            "Epoch 54/1000, Avg Training Loss: 0.24960140104238643, Avg Validation Loss: 37.50528927047431\n",
            "Epoch 55/1000, Avg Training Loss: 0.24928555623804421, Avg Validation Loss: 37.46837554931871\n",
            "Epoch 56/1000, Avg Training Loss: 0.24908911432555927, Avg Validation Loss: 37.43621617014297\n",
            "Epoch 57/1000, Avg Training Loss: 0.2488788731510511, Avg Validation Loss: 37.40912471885233\n",
            "Epoch 58/1000, Avg Training Loss: 0.24870197392009072, Avg Validation Loss: 37.38642946444607\n",
            "Epoch 59/1000, Avg Training Loss: 0.24852431255123322, Avg Validation Loss: 37.37156810592619\n",
            "Epoch 60/1000, Avg Training Loss: 0.24844951035714255, Avg Validation Loss: 37.350873088142336\n",
            "Epoch 61/1000, Avg Training Loss: 0.24830553122083546, Avg Validation Loss: 37.33855677404024\n",
            "Epoch 62/1000, Avg Training Loss: 0.24821953053664408, Avg Validation Loss: 37.324911583451254\n",
            "Epoch 63/1000, Avg Training Loss: 0.24811580149555024, Avg Validation Loss: 37.31785847203527\n",
            "Epoch 64/1000, Avg Training Loss: 0.2480690884783562, Avg Validation Loss: 37.30903929367883\n",
            "Epoch 65/1000, Avg Training Loss: 0.24800039454934408, Avg Validation Loss: 37.30090858389454\n",
            "Epoch 66/1000, Avg Training Loss: 0.2479420368785345, Avg Validation Loss: 37.29435617673738\n",
            "Epoch 67/1000, Avg Training Loss: 0.24791165066830972, Avg Validation Loss: 37.29090320553652\n",
            "Epoch 68/1000, Avg Training Loss: 0.24787858072326774, Avg Validation Loss: 37.290978411248744\n",
            "Epoch 69/1000, Avg Training Loss: 0.2478363726236627, Avg Validation Loss: 37.285769370618326\n",
            "Epoch 70/1000, Avg Training Loss: 0.2478081467332231, Avg Validation Loss: 37.28246295299099\n",
            "Epoch 71/1000, Avg Training Loss: 0.24775923225974816, Avg Validation Loss: 37.27983387755707\n",
            "Epoch 72/1000, Avg Training Loss: 0.24774810336347425, Avg Validation Loss: 37.27865962179248\n",
            "Epoch 73/1000, Avg Training Loss: 0.24772987744932068, Avg Validation Loss: 37.27332808620052\n",
            "Epoch 74/1000, Avg Training Loss: 0.24770836297359472, Avg Validation Loss: 37.270382070055746\n",
            "Epoch 75/1000, Avg Training Loss: 0.24769341108367166, Avg Validation Loss: 37.27067022642339\n",
            "Epoch 76/1000, Avg Training Loss: 0.24767752759402512, Avg Validation Loss: 37.27082835829858\n",
            "Epoch 77/1000, Avg Training Loss: 0.24766213848619584, Avg Validation Loss: 37.26895150094817\n",
            "Epoch 78/1000, Avg Training Loss: 0.24765318733148295, Avg Validation Loss: 37.26756898995406\n",
            "Epoch 79/1000, Avg Training Loss: 0.2476470717219253, Avg Validation Loss: 37.26672669453601\n",
            "Epoch 80/1000, Avg Training Loss: 0.24766347226915125, Avg Validation Loss: 37.26923104778072\n",
            "Epoch 81/1000, Avg Training Loss: 0.24764608878341687, Avg Validation Loss: 37.27071061004314\n",
            "Epoch 82/1000, Avg Training Loss: 0.2476324870293905, Avg Validation Loss: 37.26389065088658\n",
            "Epoch 83/1000, Avg Training Loss: 0.24761612085634335, Avg Validation Loss: 37.263136784737384\n",
            "Epoch 84/1000, Avg Training Loss: 0.24761061046695323, Avg Validation Loss: 37.261016538860844\n",
            "Epoch 85/1000, Avg Training Loss: 0.24760435142596635, Avg Validation Loss: 37.260792252802275\n",
            "Epoch 86/1000, Avg Training Loss: 0.2475990144083525, Avg Validation Loss: 37.26018572866853\n",
            "Epoch 87/1000, Avg Training Loss: 0.24759323266796712, Avg Validation Loss: 37.259581996629265\n",
            "Epoch 88/1000, Avg Training Loss: 0.2475848239241097, Avg Validation Loss: 37.26078325145309\n",
            "Epoch 89/1000, Avg Training Loss: 0.24758825375438476, Avg Validation Loss: 37.26360556950627\n",
            "Epoch 90/1000, Avg Training Loss: 0.2475765617348508, Avg Validation Loss: 37.26162483134523\n",
            "Epoch 91/1000, Avg Training Loss: 0.24757776519916774, Avg Validation Loss: 37.26331365365054\n",
            "Epoch 92/1000, Avg Training Loss: 0.24756930700358848, Avg Validation Loss: 37.2629322733183\n",
            "Epoch 93/1000, Avg Training Loss: 0.24756287016961576, Avg Validation Loss: 37.26038782655522\n",
            "Epoch 94/1000, Avg Training Loss: 0.24757027049300445, Avg Validation Loss: 37.25967822569122\n",
            "Epoch 95/1000, Avg Training Loss: 0.24755964241175232, Avg Validation Loss: 37.25757269885996\n",
            "Epoch 96/1000, Avg Training Loss: 0.24755895244014506, Avg Validation Loss: 37.25669872355428\n",
            "Epoch 97/1000, Avg Training Loss: 0.24755398775067056, Avg Validation Loss: 37.25837321854394\n",
            "Epoch 98/1000, Avg Training Loss: 0.2475525525237367, Avg Validation Loss: 37.259739483474036\n",
            "Epoch 99/1000, Avg Training Loss: 0.24753940434008523, Avg Validation Loss: 37.26008971423175\n",
            "Epoch 100/1000, Avg Training Loss: 0.24754718348777435, Avg Validation Loss: 37.25875453401108\n",
            "Epoch 101/1000, Avg Training Loss: 0.24753735910062394, Avg Validation Loss: 37.25728962767732\n",
            "Epoch 102/1000, Avg Training Loss: 0.2475414001327647, Avg Validation Loss: 37.25912216606467\n",
            "Epoch 103/1000, Avg Training Loss: 0.2475329275160132, Avg Validation Loss: 37.25666865873811\n",
            "Epoch 104/1000, Avg Training Loss: 0.24752892351287498, Avg Validation Loss: 37.25494252295981\n",
            "Epoch 105/1000, Avg Training Loss: 0.24751995249754902, Avg Validation Loss: 37.25509855266594\n",
            "Epoch 106/1000, Avg Training Loss: 0.24753879237529616, Avg Validation Loss: 37.258038938932295\n",
            "Epoch 107/1000, Avg Training Loss: 0.24751557796229212, Avg Validation Loss: 37.25635129349458\n",
            "Epoch 108/1000, Avg Training Loss: 0.24751291019545235, Avg Validation Loss: 37.25618425847888\n",
            "Epoch 109/1000, Avg Training Loss: 0.2475317678508039, Avg Validation Loss: 37.25397525876237\n",
            "Epoch 110/1000, Avg Training Loss: 0.24751542470856897, Avg Validation Loss: 37.25553206594961\n",
            "Epoch 111/1000, Avg Training Loss: 0.2475125151624742, Avg Validation Loss: 37.2534384747689\n",
            "Epoch 112/1000, Avg Training Loss: 0.2474984195593782, Avg Validation Loss: 37.25337961271673\n",
            "Epoch 113/1000, Avg Training Loss: 0.24750499839769158, Avg Validation Loss: 37.25554799105153\n",
            "Epoch 114/1000, Avg Training Loss: 0.24748973860694154, Avg Validation Loss: 37.2546877851061\n",
            "Epoch 115/1000, Avg Training Loss: 0.2475018593501749, Avg Validation Loss: 37.256656356165195\n",
            "Epoch 116/1000, Avg Training Loss: 0.24748450063729427, Avg Validation Loss: 37.25514342355703\n",
            "Epoch 117/1000, Avg Training Loss: 0.24748111333592565, Avg Validation Loss: 37.25396553763451\n",
            "Epoch 118/1000, Avg Training Loss: 0.24748368113111532, Avg Validation Loss: 37.25236084356668\n",
            "Epoch 119/1000, Avg Training Loss: 0.24750480550867565, Avg Validation Loss: 37.24900721341403\n",
            "Epoch 120/1000, Avg Training Loss: 0.2474879211544012, Avg Validation Loss: 37.251864782959174\n",
            "Epoch 121/1000, Avg Training Loss: 0.24748065808816935, Avg Validation Loss: 37.25362412524744\n",
            "Epoch 122/1000, Avg Training Loss: 0.24746452744463793, Avg Validation Loss: 37.25241763521568\n",
            "Epoch 123/1000, Avg Training Loss: 0.24747583483968943, Avg Validation Loss: 37.24903769707302\n",
            "Epoch 124/1000, Avg Training Loss: 0.24746574416194939, Avg Validation Loss: 37.251296414828815\n",
            "Epoch 125/1000, Avg Training Loss: 0.24745931238075972, Avg Validation Loss: 37.25212170030993\n",
            "Epoch 126/1000, Avg Training Loss: 0.2474638677069927, Avg Validation Loss: 37.25173089062578\n",
            "Epoch 127/1000, Avg Training Loss: 0.2474552709904919, Avg Validation Loss: 37.24911822238765\n",
            "Epoch 128/1000, Avg Training Loss: 0.24745530626927154, Avg Validation Loss: 37.24934688856855\n",
            "Epoch 129/1000, Avg Training Loss: 0.2474529063069884, Avg Validation Loss: 37.24805952408772\n",
            "Epoch 130/1000, Avg Training Loss: 0.24745178531506026, Avg Validation Loss: 37.247608615926154\n",
            "Epoch 131/1000, Avg Training Loss: 0.24745027481900744, Avg Validation Loss: 37.24941167486739\n",
            "Epoch 132/1000, Avg Training Loss: 0.247442170332855, Avg Validation Loss: 37.24931383339869\n",
            "Epoch 133/1000, Avg Training Loss: 0.2474481160639684, Avg Validation Loss: 37.2454633752532\n",
            "Epoch 134/1000, Avg Training Loss: 0.24743903247523277, Avg Validation Loss: 37.243491810233365\n",
            "Epoch 135/1000, Avg Training Loss: 0.24742840078240871, Avg Validation Loss: 37.24522362801706\n",
            "Epoch 136/1000, Avg Training Loss: 0.24742477757422368, Avg Validation Loss: 37.24652755800791\n",
            "Epoch 137/1000, Avg Training Loss: 0.24742221315922136, Avg Validation Loss: 37.24557712835039\n",
            "Epoch 138/1000, Avg Training Loss: 0.2474170601052812, Avg Validation Loss: 37.245809045687196\n",
            "Epoch 139/1000, Avg Training Loss: 0.24741570011953523, Avg Validation Loss: 37.245536532790936\n",
            "Epoch 140/1000, Avg Training Loss: 0.24741230842609613, Avg Validation Loss: 37.243606464674954\n",
            "Epoch 141/1000, Avg Training Loss: 0.24741214749755783, Avg Validation Loss: 37.244556352059135\n",
            "Epoch 142/1000, Avg Training Loss: 0.2474066687888154, Avg Validation Loss: 37.243560492387054\n",
            "Epoch 143/1000, Avg Training Loss: 0.24740683010396336, Avg Validation Loss: 37.24445680631797\n",
            "Epoch 144/1000, Avg Training Loss: 0.2474014591867682, Avg Validation Loss: 37.245330131350705\n",
            "Epoch 145/1000, Avg Training Loss: 0.24741088327264263, Avg Validation Loss: 37.246408943812526\n",
            "Epoch 146/1000, Avg Training Loss: 0.24740053444975466, Avg Validation Loss: 37.244125554060425\n",
            "Epoch 147/1000, Avg Training Loss: 0.2473907813361822, Avg Validation Loss: 37.24529280806096\n",
            "Epoch 148/1000, Avg Training Loss: 0.24739737938708628, Avg Validation Loss: 37.24252194229564\n",
            "Epoch 149/1000, Avg Training Loss: 0.2473876338006015, Avg Validation Loss: 37.2431858006896\n",
            "Epoch 150/1000, Avg Training Loss: 0.24738714987025454, Avg Validation Loss: 37.241633391537775\n",
            "Epoch 151/1000, Avg Training Loss: 0.247376321983378, Avg Validation Loss: 37.242383596979124\n",
            "Epoch 152/1000, Avg Training Loss: 0.24737530448155728, Avg Validation Loss: 37.242574545033804\n",
            "Epoch 153/1000, Avg Training Loss: 0.24737887045056578, Avg Validation Loss: 37.24139388282701\n",
            "Epoch 154/1000, Avg Training Loss: 0.2473704753416909, Avg Validation Loss: 37.24084575236088\n",
            "Epoch 155/1000, Avg Training Loss: 0.24737036547176747, Avg Validation Loss: 37.24298540567858\n",
            "Epoch 156/1000, Avg Training Loss: 0.2473609323159544, Avg Validation Loss: 37.24173170999941\n",
            "Epoch 157/1000, Avg Training Loss: 0.24737570478500465, Avg Validation Loss: 37.24459879707951\n",
            "Epoch 158/1000, Avg Training Loss: 0.24735699062552113, Avg Validation Loss: 37.243146751079635\n",
            "Epoch 159/1000, Avg Training Loss: 0.24735860752165656, Avg Validation Loss: 37.240582440433684\n",
            "Epoch 160/1000, Avg Training Loss: 0.24735522186790454, Avg Validation Loss: 37.23862245410302\n",
            "Epoch 161/1000, Avg Training Loss: 0.2473471396770997, Avg Validation Loss: 37.23973482558863\n",
            "Epoch 162/1000, Avg Training Loss: 0.24734509843062208, Avg Validation Loss: 37.239133198161696\n",
            "Epoch 163/1000, Avg Training Loss: 0.24735392007687604, Avg Validation Loss: 37.24106956441756\n",
            "Epoch 164/1000, Avg Training Loss: 0.2473460799554557, Avg Validation Loss: 37.23827507990197\n",
            "Epoch 165/1000, Avg Training Loss: 0.2473455546068955, Avg Validation Loss: 37.23658331181693\n",
            "Epoch 166/1000, Avg Training Loss: 0.24733607319706213, Avg Validation Loss: 37.23793009451035\n",
            "Epoch 167/1000, Avg Training Loss: 0.24733599845457982, Avg Validation Loss: 37.23719111386937\n",
            "Epoch 168/1000, Avg Training Loss: 0.24733566721419892, Avg Validation Loss: 37.2376291104\n",
            "Epoch 169/1000, Avg Training Loss: 0.247332926788259, Avg Validation Loss: 37.237710228907005\n",
            "Epoch 170/1000, Avg Training Loss: 0.2473277327460735, Avg Validation Loss: 37.23717977650794\n",
            "Epoch 171/1000, Avg Training Loss: 0.24731824843159203, Avg Validation Loss: 37.23791753807392\n",
            "Epoch 172/1000, Avg Training Loss: 0.24732314460921306, Avg Validation Loss: 37.23730299725663\n",
            "Epoch 173/1000, Avg Training Loss: 0.24731853223803063, Avg Validation Loss: 37.238050599028554\n",
            "Epoch 174/1000, Avg Training Loss: 0.2473201867010475, Avg Validation Loss: 37.23544037735017\n",
            "Epoch 175/1000, Avg Training Loss: 0.2473096373816533, Avg Validation Loss: 37.23514916064515\n",
            "Epoch 176/1000, Avg Training Loss: 0.24730992365630952, Avg Validation Loss: 37.23489413371263\n",
            "Epoch 177/1000, Avg Training Loss: 0.24730608187086012, Avg Validation Loss: 37.23640938390879\n",
            "Epoch 178/1000, Avg Training Loss: 0.24730611409336917, Avg Validation Loss: 37.2366934577622\n",
            "Epoch 179/1000, Avg Training Loss: 0.2473115445880439, Avg Validation Loss: 37.23924372994571\n",
            "Epoch 180/1000, Avg Training Loss: 0.24729797078165391, Avg Validation Loss: 37.236926047411906\n",
            "Epoch 181/1000, Avg Training Loss: 0.24729422258184522, Avg Validation Loss: 37.23499586118948\n",
            "Epoch 182/1000, Avg Training Loss: 0.24730049885549138, Avg Validation Loss: 37.23321399138111\n",
            "Epoch 183/1000, Avg Training Loss: 0.24729044190732882, Avg Validation Loss: 37.23251888200704\n",
            "Epoch 184/1000, Avg Training Loss: 0.24730154167224927, Avg Validation Loss: 37.23650536347839\n",
            "Epoch 185/1000, Avg Training Loss: 0.24728589428380618, Avg Validation Loss: 37.23540276402788\n",
            "Epoch 186/1000, Avg Training Loss: 0.24728245124039105, Avg Validation Loss: 37.23395420622549\n",
            "Epoch 187/1000, Avg Training Loss: 0.24728074093707889, Avg Validation Loss: 37.23181029495181\n",
            "Epoch 188/1000, Avg Training Loss: 0.24729351920573653, Avg Validation Loss: 37.23609425955708\n",
            "Epoch 189/1000, Avg Training Loss: 0.24727493106808116, Avg Validation Loss: 37.234086169937996\n",
            "Epoch 190/1000, Avg Training Loss: 0.2472735114928743, Avg Validation Loss: 37.23287532716827\n",
            "Epoch 191/1000, Avg Training Loss: 0.24727549111322272, Avg Validation Loss: 37.232158112970346\n",
            "Epoch 192/1000, Avg Training Loss: 0.24727948202767497, Avg Validation Loss: 37.23571519192214\n",
            "Epoch 193/1000, Avg Training Loss: 0.24726454606058618, Avg Validation Loss: 37.23246837226437\n",
            "Epoch 194/1000, Avg Training Loss: 0.24726192309653716, Avg Validation Loss: 37.23344961665097\n",
            "Epoch 195/1000, Avg Training Loss: 0.2472581643578121, Avg Validation Loss: 37.2319587134609\n",
            "Epoch 196/1000, Avg Training Loss: 0.24725390234093475, Avg Validation Loss: 37.233010633567346\n",
            "Epoch 197/1000, Avg Training Loss: 0.24725399922729618, Avg Validation Loss: 37.23230009381771\n",
            "Epoch 198/1000, Avg Training Loss: 0.24725201768214985, Avg Validation Loss: 37.230542820947974\n",
            "Epoch 199/1000, Avg Training Loss: 0.24724847711493464, Avg Validation Loss: 37.22958602666004\n",
            "Epoch 200/1000, Avg Training Loss: 0.24725769204537382, Avg Validation Loss: 37.233107398077195\n",
            "Epoch 201/1000, Avg Training Loss: 0.24724513354787855, Avg Validation Loss: 37.23270071569586\n",
            "Epoch 202/1000, Avg Training Loss: 0.24724046839941832, Avg Validation Loss: 37.23104719278722\n",
            "Epoch 203/1000, Avg Training Loss: 0.24723981075944104, Avg Validation Loss: 37.23224348922464\n",
            "Epoch 204/1000, Avg Training Loss: 0.2472373767673492, Avg Validation Loss: 37.23130680453823\n",
            "Epoch 205/1000, Avg Training Loss: 0.24723662055720413, Avg Validation Loss: 37.231534864743175\n",
            "Epoch 206/1000, Avg Training Loss: 0.2472320440482949, Avg Validation Loss: 37.23070504281502\n",
            "Epoch 207/1000, Avg Training Loss: 0.24723384155613798, Avg Validation Loss: 37.23321254760734\n",
            "Epoch 208/1000, Avg Training Loss: 0.24722332458883794, Avg Validation Loss: 37.23181455215853\n",
            "Epoch 209/1000, Avg Training Loss: 0.24722407823324835, Avg Validation Loss: 37.23131458025374\n",
            "Epoch 210/1000, Avg Training Loss: 0.24721899673945202, Avg Validation Loss: 37.229791599907244\n",
            "Epoch 211/1000, Avg Training Loss: 0.24721519371369025, Avg Validation Loss: 37.22959925292912\n",
            "Epoch 212/1000, Avg Training Loss: 0.2472204194131978, Avg Validation Loss: 37.23053132317776\n",
            "Epoch 213/1000, Avg Training Loss: 0.2472089239136405, Avg Validation Loss: 37.23026031523849\n",
            "Epoch 214/1000, Avg Training Loss: 0.24720974803655263, Avg Validation Loss: 37.230270255688936\n",
            "Epoch 215/1000, Avg Training Loss: 0.24721265928870267, Avg Validation Loss: 37.231201138598436\n",
            "Epoch 216/1000, Avg Training Loss: 0.2472153239261728, Avg Validation Loss: 37.23183944855647\n",
            "Epoch 217/1000, Avg Training Loss: 0.24720289277978108, Avg Validation Loss: 37.230268097697085\n",
            "Epoch 218/1000, Avg Training Loss: 0.24720326613806928, Avg Validation Loss: 37.229255303069905\n",
            "Epoch 219/1000, Avg Training Loss: 0.24720014605868273, Avg Validation Loss: 37.22732264163903\n",
            "Epoch 220/1000, Avg Training Loss: 0.24720750520875576, Avg Validation Loss: 37.23041044947554\n",
            "Epoch 221/1000, Avg Training Loss: 0.2472013962757187, Avg Validation Loss: 37.22762247132677\n",
            "Epoch 222/1000, Avg Training Loss: 0.24720309933108084, Avg Validation Loss: 37.23067188464072\n",
            "Epoch 223/1000, Avg Training Loss: 0.24718986555162076, Avg Validation Loss: 37.23089449660777\n",
            "Epoch 224/1000, Avg Training Loss: 0.2471892861144554, Avg Validation Loss: 37.22817911881266\n",
            "Epoch 225/1000, Avg Training Loss: 0.2471848316060994, Avg Validation Loss: 37.227452481057874\n",
            "Epoch 226/1000, Avg Training Loss: 0.24719076947681956, Avg Validation Loss: 37.22667893346055\n",
            "Epoch 227/1000, Avg Training Loss: 0.2471944461852984, Avg Validation Loss: 37.227775461993375\n",
            "Epoch 228/1000, Avg Training Loss: 0.2471914344460504, Avg Validation Loss: 37.2312987963811\n",
            "Epoch 229/1000, Avg Training Loss: 0.2471801782792027, Avg Validation Loss: 37.22969708771505\n",
            "Epoch 230/1000, Avg Training Loss: 0.24717924225607693, Avg Validation Loss: 37.23016391315375\n",
            "Epoch 231/1000, Avg Training Loss: 0.24717961598413069, Avg Validation Loss: 37.22869401956406\n",
            "Epoch 232/1000, Avg Training Loss: 0.24718256454611, Avg Validation Loss: 37.23091918960226\n",
            "Epoch 233/1000, Avg Training Loss: 0.24717199175633317, Avg Validation Loss: 37.2283701963237\n",
            "Epoch 234/1000, Avg Training Loss: 0.24716726173676787, Avg Validation Loss: 37.22791984287084\n",
            "Epoch 235/1000, Avg Training Loss: 0.2471681449370675, Avg Validation Loss: 37.22858851582501\n",
            "Epoch 236/1000, Avg Training Loss: 0.24717957563061516, Avg Validation Loss: 37.22859202702229\n",
            "Epoch 237/1000, Avg Training Loss: 0.24717698652982595, Avg Validation Loss: 37.23146430860005\n",
            "Epoch 238/1000, Avg Training Loss: 0.24716530379195936, Avg Validation Loss: 37.230766910232944\n",
            "Epoch 239/1000, Avg Training Loss: 0.24717993058305857, Avg Validation Loss: 37.22848847428601\n",
            "Epoch 240/1000, Avg Training Loss: 0.24715457395908497, Avg Validation Loss: 37.229109805417295\n",
            "Epoch 241/1000, Avg Training Loss: 0.2471528020915414, Avg Validation Loss: 37.22797389630399\n",
            "Epoch 242/1000, Avg Training Loss: 0.24715430784093922, Avg Validation Loss: 37.2279932247541\n",
            "Epoch 243/1000, Avg Training Loss: 0.24714803565783136, Avg Validation Loss: 37.22817288563205\n",
            "Epoch 244/1000, Avg Training Loss: 0.2471540487530611, Avg Validation Loss: 37.2306413010111\n",
            "Epoch 245/1000, Avg Training Loss: 0.2471492719679909, Avg Validation Loss: 37.227752895439494\n",
            "Epoch 246/1000, Avg Training Loss: 0.24714776296093235, Avg Validation Loss: 37.2265151213738\n",
            "Epoch 247/1000, Avg Training Loss: 0.247147882860098, Avg Validation Loss: 37.22843462600492\n",
            "Epoch 248/1000, Avg Training Loss: 0.24713863354981483, Avg Validation Loss: 37.22849125388728\n",
            "Epoch 249/1000, Avg Training Loss: 0.24713655851149544, Avg Validation Loss: 37.22872541605547\n",
            "Epoch 250/1000, Avg Training Loss: 0.24713795190777468, Avg Validation Loss: 37.22974402724758\n",
            "Epoch 251/1000, Avg Training Loss: 0.24713755793004172, Avg Validation Loss: 37.23128624739505\n",
            "Epoch 252/1000, Avg Training Loss: 0.2471397250342476, Avg Validation Loss: 37.228457939434534\n",
            "Epoch 253/1000, Avg Training Loss: 0.24713488309121284, Avg Validation Loss: 37.22811203262344\n",
            "Epoch 254/1000, Avg Training Loss: 0.24713125919494944, Avg Validation Loss: 37.22873365879815\n",
            "Epoch 255/1000, Avg Training Loss: 0.24712900543120844, Avg Validation Loss: 37.22937607677986\n",
            "Epoch 256/1000, Avg Training Loss: 0.24714235514475816, Avg Validation Loss: 37.231521870732195\n",
            "Epoch 257/1000, Avg Training Loss: 0.24713115066428518, Avg Validation Loss: 37.233751262649676\n",
            "Epoch 258/1000, Avg Training Loss: 0.24711912769245528, Avg Validation Loss: 37.23137767140979\n",
            "Epoch 259/1000, Avg Training Loss: 0.24712077963157944, Avg Validation Loss: 37.2306532685315\n",
            "Epoch 260/1000, Avg Training Loss: 0.2471242527392842, Avg Validation Loss: 37.231655861225775\n",
            "Epoch 261/1000, Avg Training Loss: 0.2471189767106882, Avg Validation Loss: 37.231251331152905\n",
            "Epoch 262/1000, Avg Training Loss: 0.24712005195484027, Avg Validation Loss: 37.230299733660104\n",
            "Epoch 263/1000, Avg Training Loss: 0.24711223784357475, Avg Validation Loss: 37.22940096753133\n",
            "Epoch 264/1000, Avg Training Loss: 0.24711208743129479, Avg Validation Loss: 37.23099986717093\n",
            "Epoch 265/1000, Avg Training Loss: 0.24711539974870997, Avg Validation Loss: 37.23002077331092\n",
            "Epoch 266/1000, Avg Training Loss: 0.24713499659082552, Avg Validation Loss: 37.23041948407601\n",
            "Epoch 267/1000, Avg Training Loss: 0.2471064303848008, Avg Validation Loss: 37.23071458606734\n",
            "Epoch 268/1000, Avg Training Loss: 0.2471225206411415, Avg Validation Loss: 37.22906667925132\n",
            "Epoch 269/1000, Avg Training Loss: 0.2471063876975388, Avg Validation Loss: 37.22959827785747\n",
            "Epoch 270/1000, Avg Training Loss: 0.24711271375307708, Avg Validation Loss: 37.22943888258686\n",
            "Epoch 271/1000, Avg Training Loss: 0.24710153728391887, Avg Validation Loss: 37.23017071488145\n",
            "Epoch 272/1000, Avg Training Loss: 0.24710456441785372, Avg Validation Loss: 37.232012710717726\n",
            "Epoch 273/1000, Avg Training Loss: 0.24710218236793707, Avg Validation Loss: 37.23154257672831\n",
            "Epoch 274/1000, Avg Training Loss: 0.24710525436116879, Avg Validation Loss: 37.230801029568475\n",
            "Epoch 275/1000, Avg Training Loss: 0.24709374848100701, Avg Validation Loss: 37.23109579590903\n",
            "Epoch 276/1000, Avg Training Loss: 0.2470987762073152, Avg Validation Loss: 37.23266749790664\n",
            "Epoch 277/1000, Avg Training Loss: 0.24709618568076913, Avg Validation Loss: 37.233897087958425\n",
            "Epoch 278/1000, Avg Training Loss: 0.24709128412326148, Avg Validation Loss: 37.23265156606807\n",
            "Epoch 279/1000, Avg Training Loss: 0.24710626348070477, Avg Validation Loss: 37.235703905934116\n",
            "Epoch 280/1000, Avg Training Loss: 0.24709612735564873, Avg Validation Loss: 37.232476919031086\n",
            "Epoch 281/1000, Avg Training Loss: 0.24708807419559506, Avg Validation Loss: 37.23276781685584\n",
            "Epoch 282/1000, Avg Training Loss: 0.24710435158019967, Avg Validation Loss: 37.23404503070732\n",
            "Epoch 283/1000, Avg Training Loss: 0.24709208855832446, Avg Validation Loss: 37.233308803325485\n",
            "Epoch 284/1000, Avg Training Loss: 0.24709073273609197, Avg Validation Loss: 37.23325300410242\n",
            "Epoch 285/1000, Avg Training Loss: 0.24708863708921647, Avg Validation Loss: 37.23571902406091\n",
            "Epoch 286/1000, Avg Training Loss: 0.2470904446363364, Avg Validation Loss: 37.23343287075035\n",
            "Epoch 287/1000, Avg Training Loss: 0.2470829906364252, Avg Validation Loss: 37.23420714974891\n",
            "Epoch 288/1000, Avg Training Loss: 0.2470900902338083, Avg Validation Loss: 37.234454696164775\n",
            "Epoch 289/1000, Avg Training Loss: 0.2470989078681663, Avg Validation Loss: 37.23628897462096\n",
            "Epoch 290/1000, Avg Training Loss: 0.2470828451665843, Avg Validation Loss: 37.23727779882579\n",
            "Epoch 291/1000, Avg Training Loss: 0.24709315929914477, Avg Validation Loss: 37.23933612367956\n",
            "Epoch 292/1000, Avg Training Loss: 0.2470755251670665, Avg Validation Loss: 37.23939750776148\n",
            "Epoch 293/1000, Avg Training Loss: 0.24707755856583877, Avg Validation Loss: 37.23774196778811\n",
            "Epoch 294/1000, Avg Training Loss: 0.24708206656021794, Avg Validation Loss: 37.23502586928885\n",
            "Epoch 295/1000, Avg Training Loss: 0.24708185676800606, Avg Validation Loss: 37.23588028356676\n",
            "Epoch 296/1000, Avg Training Loss: 0.24708046411676707, Avg Validation Loss: 37.236226733666015\n",
            "Epoch 297/1000, Avg Training Loss: 0.2470786564939475, Avg Validation Loss: 37.23375906212074\n",
            "Epoch 298/1000, Avg Training Loss: 0.24707696660197914, Avg Validation Loss: 37.23423692171799\n",
            "Epoch 299/1000, Avg Training Loss: 0.2470789544288207, Avg Validation Loss: 37.236921441676174\n",
            "Epoch 300/1000, Avg Training Loss: 0.24706961507031716, Avg Validation Loss: 37.23769212886029\n",
            "Epoch 301/1000, Avg Training Loss: 0.24706768165408466, Avg Validation Loss: 37.23839694626213\n",
            "Epoch 302/1000, Avg Training Loss: 0.24707293104221403, Avg Validation Loss: 37.240136096549534\n",
            "Epoch 303/1000, Avg Training Loss: 0.2470799360051589, Avg Validation Loss: 37.24183995901408\n",
            "Epoch 304/1000, Avg Training Loss: 0.24707276511788892, Avg Validation Loss: 37.237969670792694\n",
            "Epoch 305/1000, Avg Training Loss: 0.24708206051871437, Avg Validation Loss: 37.23991949813057\n",
            "Epoch 306/1000, Avg Training Loss: 0.24706551353154546, Avg Validation Loss: 37.23903116918347\n",
            "Epoch 307/1000, Avg Training Loss: 0.2470725018126012, Avg Validation Loss: 37.23870238072221\n",
            "Epoch 308/1000, Avg Training Loss: 0.2470682095263253, Avg Validation Loss: 37.2371930659728\n",
            "Epoch 309/1000, Avg Training Loss: 0.2470664827089689, Avg Validation Loss: 37.23765290114112\n",
            "Epoch 310/1000, Avg Training Loss: 0.24707016959636258, Avg Validation Loss: 37.24012530571669\n",
            "Epoch 311/1000, Avg Training Loss: 0.24708446872375847, Avg Validation Loss: 37.24463400835002\n",
            "Epoch 312/1000, Avg Training Loss: 0.24706306816695434, Avg Validation Loss: 37.24314341709483\n",
            "Epoch 313/1000, Avg Training Loss: 0.2470696114947597, Avg Validation Loss: 37.241123016842366\n",
            "Epoch 314/1000, Avg Training Loss: 0.2470675484955926, Avg Validation Loss: 37.24245819574308\n",
            "Epoch 315/1000, Avg Training Loss: 0.24707362048856454, Avg Validation Loss: 37.24054337429003\n",
            "Epoch 316/1000, Avg Training Loss: 0.24706833722844773, Avg Validation Loss: 37.243454963054646\n",
            "Epoch 317/1000, Avg Training Loss: 0.24706166933051113, Avg Validation Loss: 37.24326936905847\n",
            "Epoch 318/1000, Avg Training Loss: 0.24706792393205984, Avg Validation Loss: 37.24224772634693\n",
            "Epoch 319/1000, Avg Training Loss: 0.24706121900558448, Avg Validation Loss: 37.24263188834868\n",
            "Epoch 320/1000, Avg Training Loss: 0.24706025464165804, Avg Validation Loss: 37.243898510712974\n",
            "Epoch 321/1000, Avg Training Loss: 0.24706495117030486, Avg Validation Loss: 37.24450262363962\n",
            "Epoch 322/1000, Avg Training Loss: 0.24706986309094434, Avg Validation Loss: 37.24748152822673\n",
            "Epoch 323/1000, Avg Training Loss: 0.24706250683914646, Avg Validation Loss: 37.24784749049814\n",
            "Epoch 324/1000, Avg Training Loss: 0.24706287349728803, Avg Validation Loss: 37.24917033607478\n",
            "Epoch 325/1000, Avg Training Loss: 0.24705678293075758, Avg Validation Loss: 37.24755487255999\n",
            "Epoch 326/1000, Avg Training Loss: 0.24706389689933425, Avg Validation Loss: 37.24688436523256\n",
            "Epoch 327/1000, Avg Training Loss: 0.24706292946619487, Avg Validation Loss: 37.24730221208115\n",
            "Epoch 328/1000, Avg Training Loss: 0.2470726047439523, Avg Validation Loss: 37.250028799601004\n",
            "Epoch 329/1000, Avg Training Loss: 0.24705686684298783, Avg Validation Loss: 37.24851166212324\n",
            "Epoch 330/1000, Avg Training Loss: 0.2470672346443418, Avg Validation Loss: 37.24995633154879\n",
            "Epoch 331/1000, Avg Training Loss: 0.247058988169052, Avg Validation Loss: 37.2502869723503\n",
            "Epoch 332/1000, Avg Training Loss: 0.24706136875920803, Avg Validation Loss: 37.24976257665423\n",
            "Epoch 333/1000, Avg Training Loss: 0.24706741561863046, Avg Validation Loss: 37.250969539005695\n",
            "Epoch 334/1000, Avg Training Loss: 0.2470558523947944, Avg Validation Loss: 37.249558919174106\n",
            "Epoch 335/1000, Avg Training Loss: 0.24707087440622827, Avg Validation Loss: 37.248597836546736\n",
            "Epoch 336/1000, Avg Training Loss: 0.24706592921289725, Avg Validation Loss: 37.24929107621008\n",
            "Epoch 337/1000, Avg Training Loss: 0.24707212822746444, Avg Validation Loss: 37.249337480080506\n",
            "Epoch 338/1000, Avg Training Loss: 0.2470687245086633, Avg Validation Loss: 37.253693941719256\n",
            "Epoch 339/1000, Avg Training Loss: 0.2470622811819073, Avg Validation Loss: 37.25356940238605\n",
            "Epoch 340/1000, Avg Training Loss: 0.2470646788783391, Avg Validation Loss: 37.25321864820396\n",
            "Epoch 341/1000, Avg Training Loss: 0.2470619553812745, Avg Validation Loss: 37.25276095825852\n",
            "Epoch 342/1000, Avg Training Loss: 0.2470618494516089, Avg Validation Loss: 37.2548413917453\n",
            "Epoch 343/1000, Avg Training Loss: 0.24706316393189, Avg Validation Loss: 37.25511521173847\n",
            "Epoch 344/1000, Avg Training Loss: 0.24707545098710954, Avg Validation Loss: 37.25447027487503\n",
            "Epoch 345/1000, Avg Training Loss: 0.24706032066644196, Avg Validation Loss: 37.25540175837718\n",
            "Epoch 346/1000, Avg Training Loss: 0.2470747531317136, Avg Validation Loss: 37.25667483918345\n",
            "Epoch 347/1000, Avg Training Loss: 0.24707560327993994, Avg Validation Loss: 37.25900059170336\n",
            "Epoch 348/1000, Avg Training Loss: 0.2470645736344507, Avg Validation Loss: 37.258718422592914\n",
            "Epoch 349/1000, Avg Training Loss: 0.2470609919841285, Avg Validation Loss: 37.25899981259233\n",
            "Epoch 350/1000, Avg Training Loss: 0.24707760770330287, Avg Validation Loss: 37.25958322473213\n",
            "Epoch 351/1000, Avg Training Loss: 0.24706669190573088, Avg Validation Loss: 37.261593454742844\n",
            "Epoch 352/1000, Avg Training Loss: 0.24706772461041543, Avg Validation Loss: 37.262828369277784\n",
            "Epoch 353/1000, Avg Training Loss: 0.24706414078306568, Avg Validation Loss: 37.26279500018022\n",
            "Epoch 354/1000, Avg Training Loss: 0.24706357767870624, Avg Validation Loss: 37.26386521421216\n",
            "Epoch 355/1000, Avg Training Loss: 0.24707255247416623, Avg Validation Loss: 37.26165749843092\n",
            "Epoch 356/1000, Avg Training Loss: 0.247068373282792, Avg Validation Loss: 37.26253034103668\n",
            "Epoch 357/1000, Avg Training Loss: 0.24707602722340263, Avg Validation Loss: 37.262039194489596\n",
            "Epoch 358/1000, Avg Training Loss: 0.24707641675349037, Avg Validation Loss: 37.2613881268384\n",
            "Epoch 359/1000, Avg Training Loss: 0.2470698904463559, Avg Validation Loss: 37.261902108290585\n",
            "Epoch 360/1000, Avg Training Loss: 0.2470811811483003, Avg Validation Loss: 37.2664842467261\n",
            "Epoch 361/1000, Avg Training Loss: 0.24707996519479442, Avg Validation Loss: 37.267091981620815\n",
            "Epoch 362/1000, Avg Training Loss: 0.247073357413612, Avg Validation Loss: 37.267107778519716\n",
            "Epoch 363/1000, Avg Training Loss: 0.24707195149109543, Avg Validation Loss: 37.26842897852056\n",
            "Epoch 364/1000, Avg Training Loss: 0.24707393641352032, Avg Validation Loss: 37.2670280053443\n",
            "Epoch 365/1000, Avg Training Loss: 0.24708193584086682, Avg Validation Loss: 37.26740652302655\n",
            "Epoch 366/1000, Avg Training Loss: 0.24708313334043064, Avg Validation Loss: 37.26779042385853\n",
            "Epoch 367/1000, Avg Training Loss: 0.24710367241099646, Avg Validation Loss: 37.264914872990694\n",
            "Epoch 368/1000, Avg Training Loss: 0.2470878780720879, Avg Validation Loss: 37.27012408851278\n",
            "Epoch 369/1000, Avg Training Loss: 0.24710548119116932, Avg Validation Loss: 37.2724732113867\n",
            "Epoch 370/1000, Avg Training Loss: 0.2470783084143783, Avg Validation Loss: 37.27343924865149\n",
            "Epoch 371/1000, Avg Training Loss: 0.247079898430059, Avg Validation Loss: 37.274076175482946\n",
            "Epoch 372/1000, Avg Training Loss: 0.24708890723780227, Avg Validation Loss: 37.27279755198603\n",
            "Epoch 373/1000, Avg Training Loss: 0.24708347547630724, Avg Validation Loss: 37.27252243860865\n",
            "Epoch 374/1000, Avg Training Loss: 0.24707980634177354, Avg Validation Loss: 37.27322320936274\n",
            "Epoch 375/1000, Avg Training Loss: 0.2470841099440939, Avg Validation Loss: 37.27388942988867\n",
            "Epoch 376/1000, Avg Training Loss: 0.24709196699611674, Avg Validation Loss: 37.27544661661932\n",
            "Epoch 377/1000, Avg Training Loss: 0.24708727672946998, Avg Validation Loss: 37.27637066542385\n",
            "Epoch 378/1000, Avg Training Loss: 0.24708615041075857, Avg Validation Loss: 37.275893744976315\n",
            "Epoch 379/1000, Avg Training Loss: 0.24710353800002566, Avg Validation Loss: 37.2794410847834\n",
            "Epoch 380/1000, Avg Training Loss: 0.24708727350061438, Avg Validation Loss: 37.27869477628747\n",
            "Epoch 381/1000, Avg Training Loss: 0.24708952551692004, Avg Validation Loss: 37.27895677764465\n",
            "Epoch 382/1000, Avg Training Loss: 0.24710371530431557, Avg Validation Loss: 37.278379619374064\n",
            "Epoch 383/1000, Avg Training Loss: 0.24709644940864414, Avg Validation Loss: 37.28097793466307\n",
            "Epoch 384/1000, Avg Training Loss: 0.24710084036452146, Avg Validation Loss: 37.28151212820574\n",
            "Epoch 385/1000, Avg Training Loss: 0.24709161216034503, Avg Validation Loss: 37.28237269138735\n",
            "Epoch 386/1000, Avg Training Loss: 0.24710066833372232, Avg Validation Loss: 37.28426132139366\n",
            "Epoch 387/1000, Avg Training Loss: 0.24710023407082207, Avg Validation Loss: 37.284611845465555\n",
            "Epoch 388/1000, Avg Training Loss: 0.2470999053794642, Avg Validation Loss: 37.28386508667705\n",
            "Epoch 389/1000, Avg Training Loss: 0.24710817278040603, Avg Validation Loss: 37.283376363178064\n",
            "Epoch 390/1000, Avg Training Loss: 0.24712298225569235, Avg Validation Loss: 37.28456929402674\n",
            "Epoch 391/1000, Avg Training Loss: 0.24711414197067808, Avg Validation Loss: 37.28708432431983\n",
            "Epoch 392/1000, Avg Training Loss: 0.24711293288053135, Avg Validation Loss: 37.288656536925906\n",
            "Epoch 393/1000, Avg Training Loss: 0.24711411965406369, Avg Validation Loss: 37.28758336875104\n",
            "Epoch 394/1000, Avg Training Loss: 0.2471197373945775, Avg Validation Loss: 37.289042279599656\n",
            "Epoch 395/1000, Avg Training Loss: 0.24712113917387285, Avg Validation Loss: 37.2884414890888\n",
            "Epoch 396/1000, Avg Training Loss: 0.24711522407355804, Avg Validation Loss: 37.29043753423805\n",
            "Epoch 397/1000, Avg Training Loss: 0.24711952491412129, Avg Validation Loss: 37.2903607875739\n",
            "Epoch 398/1000, Avg Training Loss: 0.24712617409399087, Avg Validation Loss: 37.2935280439115\n",
            "Epoch 399/1000, Avg Training Loss: 0.2471340329166937, Avg Validation Loss: 37.293842763437226\n",
            "Epoch 400/1000, Avg Training Loss: 0.24711663200400844, Avg Validation Loss: 37.295656116594785\n",
            "Epoch 401/1000, Avg Training Loss: 0.24712521431578055, Avg Validation Loss: 37.293852614110506\n",
            "Epoch 402/1000, Avg Training Loss: 0.24712675796339215, Avg Validation Loss: 37.29381261955375\n",
            "Epoch 403/1000, Avg Training Loss: 0.24713640638557935, Avg Validation Loss: 37.29655328082997\n",
            "Epoch 404/1000, Avg Training Loss: 0.24713188990249274, Avg Validation Loss: 37.295743035892244\n",
            "Epoch 405/1000, Avg Training Loss: 0.247127410386635, Avg Validation Loss: 37.29655603666775\n",
            "Epoch 406/1000, Avg Training Loss: 0.2471314380969411, Avg Validation Loss: 37.29815087484259\n",
            "Epoch 407/1000, Avg Training Loss: 0.24713972764896525, Avg Validation Loss: 37.3016775625418\n",
            "Epoch 408/1000, Avg Training Loss: 0.24713820332718983, Avg Validation Loss: 37.30110027550485\n",
            "Epoch 409/1000, Avg Training Loss: 0.2471446143735164, Avg Validation Loss: 37.30442915668797\n",
            "Epoch 410/1000, Avg Training Loss: 0.2471366758969588, Avg Validation Loss: 37.30401576033451\n",
            "Epoch 411/1000, Avg Training Loss: 0.24714316121542637, Avg Validation Loss: 37.303357802884804\n",
            "Epoch 412/1000, Avg Training Loss: 0.2471502524755381, Avg Validation Loss: 37.30634570967182\n",
            "Epoch 413/1000, Avg Training Loss: 0.24715835928216334, Avg Validation Loss: 37.30903967161318\n",
            "Epoch 414/1000, Avg Training Loss: 0.2471734424059336, Avg Validation Loss: 37.304726145637005\n",
            "Epoch 415/1000, Avg Training Loss: 0.24715309838646055, Avg Validation Loss: 37.30697714920311\n",
            "Epoch 416/1000, Avg Training Loss: 0.24717094125361383, Avg Validation Loss: 37.30877746779721\n",
            "Epoch 417/1000, Avg Training Loss: 0.2471584178485374, Avg Validation Loss: 37.307639198736595\n",
            "Epoch 418/1000, Avg Training Loss: 0.24715675189304206, Avg Validation Loss: 37.30935372477516\n",
            "Epoch 419/1000, Avg Training Loss: 0.24716744275120636, Avg Validation Loss: 37.31117400237491\n",
            "Epoch 420/1000, Avg Training Loss: 0.24716412438455024, Avg Validation Loss: 37.31133696178718\n",
            "Epoch 421/1000, Avg Training Loss: 0.24716843668526683, Avg Validation Loss: 37.31076520925925\n",
            "Epoch 422/1000, Avg Training Loss: 0.24717004627828554, Avg Validation Loss: 37.31426947187336\n",
            "Epoch 423/1000, Avg Training Loss: 0.2471844449134361, Avg Validation Loss: 37.31360763161639\n",
            "Epoch 424/1000, Avg Training Loss: 0.24717079447055562, Avg Validation Loss: 37.31382248269708\n",
            "Epoch 425/1000, Avg Training Loss: 0.24717836826056291, Avg Validation Loss: 37.31655187995578\n",
            "Epoch 426/1000, Avg Training Loss: 0.24718586733611989, Avg Validation Loss: 37.317121237854806\n",
            "Epoch 427/1000, Avg Training Loss: 0.24718244536857323, Avg Validation Loss: 37.3180891466814\n",
            "Epoch 428/1000, Avg Training Loss: 0.24718377441594538, Avg Validation Loss: 37.32045385588245\n",
            "Epoch 429/1000, Avg Training Loss: 0.24720446927706402, Avg Validation Loss: 37.32364433448144\n",
            "Epoch 430/1000, Avg Training Loss: 0.24718709567676456, Avg Validation Loss: 37.32412358876486\n",
            "Epoch 431/1000, Avg Training Loss: 0.24720281438341535, Avg Validation Loss: 37.321262705821724\n",
            "Epoch 432/1000, Avg Training Loss: 0.2472021174726538, Avg Validation Loss: 37.32518146007755\n",
            "Epoch 433/1000, Avg Training Loss: 0.2472006383596539, Avg Validation Loss: 37.32823057437072\n",
            "Epoch 434/1000, Avg Training Loss: 0.24719730221171163, Avg Validation Loss: 37.326488036912245\n",
            "Epoch 435/1000, Avg Training Loss: 0.24720562885057618, Avg Validation Loss: 37.32602169223608\n",
            "Epoch 436/1000, Avg Training Loss: 0.24721021053117093, Avg Validation Loss: 37.32584971456623\n",
            "Epoch 437/1000, Avg Training Loss: 0.24720992625404645, Avg Validation Loss: 37.32773023140636\n",
            "Epoch 438/1000, Avg Training Loss: 0.24722503679511626, Avg Validation Loss: 37.325981365556345\n",
            "Epoch 439/1000, Avg Training Loss: 0.24721504138254216, Avg Validation Loss: 37.32857628723645\n",
            "Epoch 440/1000, Avg Training Loss: 0.24722326632773833, Avg Validation Loss: 37.33241156055627\n",
            "Epoch 441/1000, Avg Training Loss: 0.24721613130037348, Avg Validation Loss: 37.33391925398603\n",
            "Epoch 442/1000, Avg Training Loss: 0.24722101619770226, Avg Validation Loss: 37.33555388300716\n",
            "Epoch 443/1000, Avg Training Loss: 0.2472300378143201, Avg Validation Loss: 37.335042737123004\n",
            "Epoch 444/1000, Avg Training Loss: 0.24723845792957247, Avg Validation Loss: 37.33848720064343\n",
            "Epoch 445/1000, Avg Training Loss: 0.247235433166966, Avg Validation Loss: 37.338714217432795\n",
            "Epoch 446/1000, Avg Training Loss: 0.24724011730333645, Avg Validation Loss: 37.33957473828055\n",
            "Epoch 447/1000, Avg Training Loss: 0.2472531353740947, Avg Validation Loss: 37.342555399804695\n",
            "Epoch 448/1000, Avg Training Loss: 0.24724265326936834, Avg Validation Loss: 37.34035269255793\n",
            "Epoch 449/1000, Avg Training Loss: 0.24724768291744473, Avg Validation Loss: 37.34234295436275\n",
            "Epoch 450/1000, Avg Training Loss: 0.24724686888229463, Avg Validation Loss: 37.34192122217085\n",
            "Epoch 451/1000, Avg Training Loss: 0.24725516910288747, Avg Validation Loss: 37.34160238444325\n",
            "Epoch 452/1000, Avg Training Loss: 0.24726357134597624, Avg Validation Loss: 37.345215871263704\n",
            "Epoch 453/1000, Avg Training Loss: 0.24725474033775705, Avg Validation Loss: 37.34638586856418\n",
            "Epoch 454/1000, Avg Training Loss: 0.24727277175294846, Avg Validation Loss: 37.344697338073516\n",
            "Epoch 455/1000, Avg Training Loss: 0.2472715672203286, Avg Validation Loss: 37.349658935532766\n",
            "Epoch 456/1000, Avg Training Loss: 0.24727020677716832, Avg Validation Loss: 37.349830951627524\n",
            "Epoch 457/1000, Avg Training Loss: 0.24727289573534056, Avg Validation Loss: 37.35193323392185\n",
            "Epoch 458/1000, Avg Training Loss: 0.2472830541901089, Avg Validation Loss: 37.35587478458952\n",
            "Epoch 459/1000, Avg Training Loss: 0.24728220419823527, Avg Validation Loss: 37.35412014700905\n",
            "Epoch 460/1000, Avg Training Loss: 0.24729092262679964, Avg Validation Loss: 37.35227413393356\n",
            "Epoch 461/1000, Avg Training Loss: 0.2472898308468312, Avg Validation Loss: 37.354106528002696\n",
            "Epoch 462/1000, Avg Training Loss: 0.24728720609859245, Avg Validation Loss: 37.35507286624275\n",
            "Epoch 463/1000, Avg Training Loss: 0.24729036830499423, Avg Validation Loss: 37.35631822265984\n",
            "Epoch 464/1000, Avg Training Loss: 0.24729932192207688, Avg Validation Loss: 37.360561370047165\n",
            "Epoch 465/1000, Avg Training Loss: 0.24730932042064618, Avg Validation Loss: 37.36335194960365\n",
            "Epoch 466/1000, Avg Training Loss: 0.2473061168288353, Avg Validation Loss: 37.364812310313695\n",
            "Epoch 467/1000, Avg Training Loss: 0.24730250091914757, Avg Validation Loss: 37.364910586683116\n",
            "Epoch 468/1000, Avg Training Loss: 0.2473105290579109, Avg Validation Loss: 37.36586751187856\n",
            "Epoch 469/1000, Avg Training Loss: 0.24731048782602044, Avg Validation Loss: 37.367320317109446\n",
            "Epoch 470/1000, Avg Training Loss: 0.24731520737544402, Avg Validation Loss: 37.367871739902085\n",
            "Epoch 471/1000, Avg Training Loss: 0.2473217072322504, Avg Validation Loss: 37.36723669531719\n",
            "Epoch 472/1000, Avg Training Loss: 0.24732117566007603, Avg Validation Loss: 37.36876132092161\n",
            "Epoch 473/1000, Avg Training Loss: 0.24733746808227478, Avg Validation Loss: 37.367764323254505\n",
            "Epoch 474/1000, Avg Training Loss: 0.2473348425171297, Avg Validation Loss: 37.369509790838904\n",
            "Epoch 475/1000, Avg Training Loss: 0.24733812854834408, Avg Validation Loss: 37.372226887639336\n",
            "Epoch 476/1000, Avg Training Loss: 0.2473447375721664, Avg Validation Loss: 37.37189042668632\n",
            "Epoch 477/1000, Avg Training Loss: 0.2473428413823564, Avg Validation Loss: 37.37377932569368\n",
            "Epoch 478/1000, Avg Training Loss: 0.24734524685574372, Avg Validation Loss: 37.375941455217976\n",
            "Epoch 479/1000, Avg Training Loss: 0.24736044322631448, Avg Validation Loss: 37.37703629342965\n",
            "Epoch 480/1000, Avg Training Loss: 0.24735129927231, Avg Validation Loss: 37.37776360486599\n",
            "Epoch 481/1000, Avg Training Loss: 0.247357779305523, Avg Validation Loss: 37.37866330115705\n",
            "Epoch 482/1000, Avg Training Loss: 0.24736970398163946, Avg Validation Loss: 37.37990580217823\n",
            "Epoch 483/1000, Avg Training Loss: 0.24738022817309896, Avg Validation Loss: 37.38485035357705\n",
            "Epoch 484/1000, Avg Training Loss: 0.24737251816970662, Avg Validation Loss: 37.38323777094972\n",
            "Epoch 485/1000, Avg Training Loss: 0.24738754847224062, Avg Validation Loss: 37.38501398730456\n",
            "Epoch 486/1000, Avg Training Loss: 0.24738662570655096, Avg Validation Loss: 37.38490128799351\n",
            "Epoch 487/1000, Avg Training Loss: 0.24738388213908996, Avg Validation Loss: 37.3872605644561\n",
            "Epoch 488/1000, Avg Training Loss: 0.24739050192847123, Avg Validation Loss: 37.38650366321657\n",
            "Epoch 489/1000, Avg Training Loss: 0.2473981639539864, Avg Validation Loss: 37.38919649796651\n",
            "Epoch 490/1000, Avg Training Loss: 0.24739235656001046, Avg Validation Loss: 37.391253634135225\n",
            "Epoch 491/1000, Avg Training Loss: 0.24739751479022468, Avg Validation Loss: 37.392097376662356\n",
            "Epoch 492/1000, Avg Training Loss: 0.24739910540254478, Avg Validation Loss: 37.39235200047396\n",
            "Epoch 493/1000, Avg Training Loss: 0.24740570325431, Avg Validation Loss: 37.39474138018754\n",
            "Epoch 494/1000, Avg Training Loss: 0.24741352424112634, Avg Validation Loss: 37.39578112627672\n",
            "Epoch 495/1000, Avg Training Loss: 0.24742973056727888, Avg Validation Loss: 37.40008898222203\n",
            "Epoch 496/1000, Avg Training Loss: 0.24742870598893857, Avg Validation Loss: 37.39805323614562\n",
            "Epoch 497/1000, Avg Training Loss: 0.24742593268005555, Avg Validation Loss: 37.39746991900587\n",
            "Epoch 498/1000, Avg Training Loss: 0.2474381668399838, Avg Validation Loss: 37.39782401150174\n",
            "Epoch 499/1000, Avg Training Loss: 0.24743541105872274, Avg Validation Loss: 37.400610629574246\n",
            "Epoch 500/1000, Avg Training Loss: 0.24743688789516732, Avg Validation Loss: 37.403732929152184\n",
            "Epoch 501/1000, Avg Training Loss: 0.24745178992890676, Avg Validation Loss: 37.403088808549896\n",
            "Epoch 502/1000, Avg Training Loss: 0.24745376193396038, Avg Validation Loss: 37.40404570200761\n",
            "Epoch 503/1000, Avg Training Loss: 0.2474453902868951, Avg Validation Loss: 37.40679585946002\n",
            "Epoch 504/1000, Avg Training Loss: 0.24745079763689545, Avg Validation Loss: 37.409404817807754\n",
            "Epoch 505/1000, Avg Training Loss: 0.24745502191705024, Avg Validation Loss: 37.41180975556985\n",
            "Epoch 506/1000, Avg Training Loss: 0.24745849824701932, Avg Validation Loss: 37.412350052560804\n",
            "Epoch 507/1000, Avg Training Loss: 0.2474620526677104, Avg Validation Loss: 37.41399079350901\n",
            "Epoch 508/1000, Avg Training Loss: 0.2474697664378922, Avg Validation Loss: 37.41395403292559\n",
            "Epoch 509/1000, Avg Training Loss: 0.24747005662221416, Avg Validation Loss: 37.41678274769198\n",
            "Epoch 510/1000, Avg Training Loss: 0.24747658261421573, Avg Validation Loss: 37.41736639930298\n",
            "Epoch 511/1000, Avg Training Loss: 0.24747884788098226, Avg Validation Loss: 37.41716754531281\n",
            "Epoch 512/1000, Avg Training Loss: 0.2474989407167402, Avg Validation Loss: 37.421362628354984\n",
            "Epoch 513/1000, Avg Training Loss: 0.24748570627194635, Avg Validation Loss: 37.42316271906816\n",
            "Epoch 514/1000, Avg Training Loss: 0.24750232584674275, Avg Validation Loss: 37.42158500702764\n",
            "Epoch 515/1000, Avg Training Loss: 0.24750089086816165, Avg Validation Loss: 37.42099723048976\n",
            "Epoch 516/1000, Avg Training Loss: 0.24750920852379088, Avg Validation Loss: 37.42462134223956\n",
            "Epoch 517/1000, Avg Training Loss: 0.24750132267995628, Avg Validation Loss: 37.425734991944395\n",
            "Epoch 518/1000, Avg Training Loss: 0.24751629895292013, Avg Validation Loss: 37.429287628337846\n",
            "Epoch 519/1000, Avg Training Loss: 0.24751519946160383, Avg Validation Loss: 37.42900637133277\n",
            "Epoch 520/1000, Avg Training Loss: 0.24751534699064615, Avg Validation Loss: 37.428825851162216\n",
            "Epoch 521/1000, Avg Training Loss: 0.2475361163157856, Avg Validation Loss: 37.431807111259744\n",
            "Epoch 522/1000, Avg Training Loss: 0.24752228009213167, Avg Validation Loss: 37.43247889559831\n",
            "Epoch 523/1000, Avg Training Loss: 0.24752645548093258, Avg Validation Loss: 37.434754855975044\n",
            "Epoch 524/1000, Avg Training Loss: 0.2475352723608339, Avg Validation Loss: 37.43744326420622\n",
            "Epoch 525/1000, Avg Training Loss: 0.2475341235522409, Avg Validation Loss: 37.437919051070395\n",
            "Epoch 526/1000, Avg Training Loss: 0.24754514211754577, Avg Validation Loss: 37.43735709946954\n",
            "Epoch 527/1000, Avg Training Loss: 0.2475435217091447, Avg Validation Loss: 37.44003191274753\n",
            "Epoch 528/1000, Avg Training Loss: 0.2475484449629773, Avg Validation Loss: 37.44147223168744\n",
            "Epoch 529/1000, Avg Training Loss: 0.24755685528408772, Avg Validation Loss: 37.44207906328507\n",
            "Epoch 530/1000, Avg Training Loss: 0.24755385590194834, Avg Validation Loss: 37.44299553503964\n",
            "Epoch 531/1000, Avg Training Loss: 0.24757170504221343, Avg Validation Loss: 37.44274005139219\n",
            "Epoch 532/1000, Avg Training Loss: 0.24756522626365257, Avg Validation Loss: 37.44631460542624\n",
            "Epoch 533/1000, Avg Training Loss: 0.24757362440383673, Avg Validation Loss: 37.45034992239439\n",
            "Epoch 534/1000, Avg Training Loss: 0.24756820192689685, Avg Validation Loss: 37.45097873878723\n",
            "Epoch 535/1000, Avg Training Loss: 0.2475723432647457, Avg Validation Loss: 37.45228780783451\n",
            "Epoch 536/1000, Avg Training Loss: 0.24757744174766985, Avg Validation Loss: 37.454647834039676\n",
            "Epoch 537/1000, Avg Training Loss: 0.247584450578865, Avg Validation Loss: 37.45455039022944\n",
            "Epoch 538/1000, Avg Training Loss: 0.24759271308907618, Avg Validation Loss: 37.457485189579614\n",
            "Epoch 539/1000, Avg Training Loss: 0.247590345829357, Avg Validation Loss: 37.4579164730138\n",
            "Epoch 540/1000, Avg Training Loss: 0.24759600545751687, Avg Validation Loss: 37.45720769905242\n",
            "Epoch 541/1000, Avg Training Loss: 0.24759816069449841, Avg Validation Loss: 37.45816781420184\n",
            "Epoch 542/1000, Avg Training Loss: 0.24760965641265242, Avg Validation Loss: 37.45832600141844\n",
            "Epoch 543/1000, Avg Training Loss: 0.24760830440898302, Avg Validation Loss: 37.4609182720258\n",
            "Epoch 544/1000, Avg Training Loss: 0.24761775554409274, Avg Validation Loss: 37.462873231482206\n",
            "Epoch 545/1000, Avg Training Loss: 0.24761683067040635, Avg Validation Loss: 37.465440750779656\n",
            "Epoch 546/1000, Avg Training Loss: 0.24762002374870157, Avg Validation Loss: 37.46586554634885\n",
            "Epoch 547/1000, Avg Training Loss: 0.24762536934993506, Avg Validation Loss: 37.466959638956844\n",
            "Epoch 548/1000, Avg Training Loss: 0.24762856851616638, Avg Validation Loss: 37.46975780257836\n",
            "Epoch 549/1000, Avg Training Loss: 0.2476310021356439, Avg Validation Loss: 37.47043991975474\n",
            "Epoch 550/1000, Avg Training Loss: 0.24763220357458193, Avg Validation Loss: 37.47177637550732\n",
            "Epoch 551/1000, Avg Training Loss: 0.24763880661245258, Avg Validation Loss: 37.47241832454469\n",
            "Epoch 552/1000, Avg Training Loss: 0.2476683823305653, Avg Validation Loss: 37.47762721539596\n",
            "Epoch 553/1000, Avg Training Loss: 0.24764827473139484, Avg Validation Loss: 37.47592509805682\n",
            "Epoch 554/1000, Avg Training Loss: 0.24764918057081609, Avg Validation Loss: 37.47668470876816\n",
            "Epoch 555/1000, Avg Training Loss: 0.24764950578799558, Avg Validation Loss: 37.478766427764555\n",
            "Epoch 556/1000, Avg Training Loss: 0.24765489983407804, Avg Validation Loss: 37.48089153841707\n",
            "Epoch 557/1000, Avg Training Loss: 0.24766078733311997, Avg Validation Loss: 37.48002433810569\n",
            "Epoch 558/1000, Avg Training Loss: 0.24767139323920712, Avg Validation Loss: 37.48342768802762\n",
            "Epoch 559/1000, Avg Training Loss: 0.24767749439273815, Avg Validation Loss: 37.486783437070955\n",
            "Epoch 560/1000, Avg Training Loss: 0.24767123620773562, Avg Validation Loss: 37.48599474884066\n",
            "Epoch 561/1000, Avg Training Loss: 0.24767999114749234, Avg Validation Loss: 37.48688709112899\n",
            "Epoch 562/1000, Avg Training Loss: 0.24768770280619584, Avg Validation Loss: 37.48691801905376\n",
            "Epoch 563/1000, Avg Training Loss: 0.24768494483270195, Avg Validation Loss: 37.48751473150682\n",
            "Epoch 564/1000, Avg Training Loss: 0.2476856927712164, Avg Validation Loss: 37.489725666938746\n",
            "Epoch 565/1000, Avg Training Loss: 0.24769516707578515, Avg Validation Loss: 37.491816146035134\n",
            "Epoch 566/1000, Avg Training Loss: 0.24769139681326474, Avg Validation Loss: 37.494755490404785\n",
            "Epoch 567/1000, Avg Training Loss: 0.24770112897383298, Avg Validation Loss: 37.49615459268037\n",
            "Epoch 568/1000, Avg Training Loss: 0.24769948700089958, Avg Validation Loss: 37.497905636333556\n",
            "Epoch 569/1000, Avg Training Loss: 0.24770551976353214, Avg Validation Loss: 37.49947092300755\n",
            "Epoch 570/1000, Avg Training Loss: 0.2477127347737314, Avg Validation Loss: 37.50264185786198\n",
            "Epoch 571/1000, Avg Training Loss: 0.24770790782514668, Avg Validation Loss: 37.50419026165386\n",
            "Epoch 572/1000, Avg Training Loss: 0.2477139228864605, Avg Validation Loss: 37.50483471872319\n",
            "Epoch 573/1000, Avg Training Loss: 0.2477239281325312, Avg Validation Loss: 37.503570915811736\n",
            "Epoch 574/1000, Avg Training Loss: 0.24773176573306627, Avg Validation Loss: 37.503352497950935\n",
            "Epoch 575/1000, Avg Training Loss: 0.24773096403493974, Avg Validation Loss: 37.507497462228244\n",
            "Epoch 576/1000, Avg Training Loss: 0.2477269254014222, Avg Validation Loss: 37.508442203502796\n",
            "Epoch 577/1000, Avg Training Loss: 0.24773442513837213, Avg Validation Loss: 37.508730972943916\n",
            "Epoch 578/1000, Avg Training Loss: 0.2477369361397895, Avg Validation Loss: 37.5119676431397\n",
            "Epoch 579/1000, Avg Training Loss: 0.2477423075991846, Avg Validation Loss: 37.51378151561339\n",
            "Epoch 580/1000, Avg Training Loss: 0.24774725500500222, Avg Validation Loss: 37.513958170017034\n",
            "Epoch 581/1000, Avg Training Loss: 0.2477457845555636, Avg Validation Loss: 37.51463305415231\n",
            "Epoch 582/1000, Avg Training Loss: 0.24776051026184956, Avg Validation Loss: 37.5157693116477\n",
            "Epoch 583/1000, Avg Training Loss: 0.24776314263007054, Avg Validation Loss: 37.51850351680486\n",
            "Epoch 584/1000, Avg Training Loss: 0.24775881072356523, Avg Validation Loss: 37.52110570522386\n",
            "Epoch 585/1000, Avg Training Loss: 0.24776232215463959, Avg Validation Loss: 37.52319596389076\n",
            "Epoch 586/1000, Avg Training Loss: 0.24776659504751858, Avg Validation Loss: 37.52385655815337\n",
            "Epoch 587/1000, Avg Training Loss: 0.2477746990156197, Avg Validation Loss: 37.52358844877885\n",
            "Epoch 588/1000, Avg Training Loss: 0.24776721608647606, Avg Validation Loss: 37.525795410300404\n",
            "Epoch 589/1000, Avg Training Loss: 0.24777094472918715, Avg Validation Loss: 37.528518510552225\n",
            "Epoch 590/1000, Avg Training Loss: 0.2477738436897706, Avg Validation Loss: 37.52949861690588\n",
            "Epoch 591/1000, Avg Training Loss: 0.24778113807731694, Avg Validation Loss: 37.53230629201177\n",
            "Epoch 592/1000, Avg Training Loss: 0.24777808510460683, Avg Validation Loss: 37.53259505819972\n",
            "Epoch 593/1000, Avg Training Loss: 0.24778068729178945, Avg Validation Loss: 37.53401616741479\n",
            "Epoch 594/1000, Avg Training Loss: 0.24778817188137592, Avg Validation Loss: 37.53563374955502\n",
            "Epoch 595/1000, Avg Training Loss: 0.24779684657580808, Avg Validation Loss: 37.53872634739038\n",
            "Epoch 596/1000, Avg Training Loss: 0.24779810064759006, Avg Validation Loss: 37.53952881896632\n",
            "Epoch 597/1000, Avg Training Loss: 0.2477989476983637, Avg Validation Loss: 37.542537339759846\n",
            "Epoch 598/1000, Avg Training Loss: 0.24780505894768204, Avg Validation Loss: 37.542109073068104\n",
            "Epoch 599/1000, Avg Training Loss: 0.24780377026964717, Avg Validation Loss: 37.54344097463117\n",
            "Epoch 600/1000, Avg Training Loss: 0.24780499087847302, Avg Validation Loss: 37.54555756682208\n",
            "Epoch 601/1000, Avg Training Loss: 0.24780799242256274, Avg Validation Loss: 37.54871663198462\n",
            "Epoch 602/1000, Avg Training Loss: 0.2478125156313718, Avg Validation Loss: 37.54988470250897\n",
            "Epoch 603/1000, Avg Training Loss: 0.24781715628403309, Avg Validation Loss: 37.55264890803511\n",
            "Epoch 604/1000, Avg Training Loss: 0.24781916925067896, Avg Validation Loss: 37.55261394186575\n",
            "Epoch 605/1000, Avg Training Loss: 0.24782369002437404, Avg Validation Loss: 37.55544498229956\n",
            "Epoch 606/1000, Avg Training Loss: 0.2478204029861467, Avg Validation Loss: 37.557313977663206\n",
            "Epoch 607/1000, Avg Training Loss: 0.24782610168277702, Avg Validation Loss: 37.558401646012214\n",
            "Epoch 608/1000, Avg Training Loss: 0.24783205205934644, Avg Validation Loss: 37.55921082325285\n",
            "Epoch 609/1000, Avg Training Loss: 0.2478420738197975, Avg Validation Loss: 37.55928732564476\n",
            "Epoch 610/1000, Avg Training Loss: 0.24785149401468468, Avg Validation Loss: 37.56469032069316\n",
            "Epoch 611/1000, Avg Training Loss: 0.24784142442562698, Avg Validation Loss: 37.565465979323335\n",
            "Epoch 612/1000, Avg Training Loss: 0.2478387570555563, Avg Validation Loss: 37.56753447600861\n",
            "Epoch 613/1000, Avg Training Loss: 0.24785068631518806, Avg Validation Loss: 37.56803704308965\n",
            "Epoch 614/1000, Avg Training Loss: 0.24786094548728776, Avg Validation Loss: 37.56824937023176\n",
            "Epoch 615/1000, Avg Training Loss: 0.2478519602617748, Avg Validation Loss: 37.57004647389281\n",
            "Epoch 616/1000, Avg Training Loss: 0.2478578187257899, Avg Validation Loss: 37.57331524732167\n",
            "Epoch 617/1000, Avg Training Loss: 0.24786542068108375, Avg Validation Loss: 37.57426345074198\n",
            "Epoch 618/1000, Avg Training Loss: 0.24786599919110291, Avg Validation Loss: 37.57551357539842\n",
            "Epoch 619/1000, Avg Training Loss: 0.24786924576756383, Avg Validation Loss: 37.57939252346475\n",
            "Epoch 620/1000, Avg Training Loss: 0.2478700427760725, Avg Validation Loss: 37.58331250421797\n",
            "Epoch 621/1000, Avg Training Loss: 0.24786955079477807, Avg Validation Loss: 37.585526082879795\n",
            "Epoch 622/1000, Avg Training Loss: 0.2478749822601028, Avg Validation Loss: 37.58559101959065\n",
            "Epoch 623/1000, Avg Training Loss: 0.24788470093997783, Avg Validation Loss: 37.58852451989655\n",
            "Epoch 624/1000, Avg Training Loss: 0.24788140628604313, Avg Validation Loss: 37.5914981209329\n",
            "Epoch 625/1000, Avg Training Loss: 0.24789736339217328, Avg Validation Loss: 37.5934036273278\n",
            "Epoch 626/1000, Avg Training Loss: 0.24789098688475564, Avg Validation Loss: 37.59286300932848\n",
            "Epoch 627/1000, Avg Training Loss: 0.24789106981832779, Avg Validation Loss: 37.595022990474845\n",
            "Epoch 628/1000, Avg Training Loss: 0.2478939282233049, Avg Validation Loss: 37.596822202978906\n",
            "Epoch 629/1000, Avg Training Loss: 0.24789481145712933, Avg Validation Loss: 37.601601864789735\n",
            "Epoch 630/1000, Avg Training Loss: 0.24789569083936205, Avg Validation Loss: 37.604420067925446\n",
            "Epoch 631/1000, Avg Training Loss: 0.2479064262972548, Avg Validation Loss: 37.60507271247027\n",
            "Epoch 632/1000, Avg Training Loss: 0.24789758469365414, Avg Validation Loss: 37.60742962969991\n",
            "Epoch 633/1000, Avg Training Loss: 0.24791039010242238, Avg Validation Loss: 37.608786687150086\n",
            "Epoch 634/1000, Avg Training Loss: 0.2479045301078336, Avg Validation Loss: 37.61204774284188\n",
            "Epoch 635/1000, Avg Training Loss: 0.24791437950398096, Avg Validation Loss: 37.61676675695143\n",
            "Epoch 636/1000, Avg Training Loss: 0.24791265490078024, Avg Validation Loss: 37.61672929785398\n",
            "Epoch 637/1000, Avg Training Loss: 0.24791585102370634, Avg Validation Loss: 37.618388071473746\n",
            "Epoch 638/1000, Avg Training Loss: 0.2479196388189835, Avg Validation Loss: 37.62225592383685\n",
            "Epoch 639/1000, Avg Training Loss: 0.2479239466791264, Avg Validation Loss: 37.624597641839884\n",
            "Epoch 640/1000, Avg Training Loss: 0.2479229865754086, Avg Validation Loss: 37.62718493911406\n",
            "Epoch 641/1000, Avg Training Loss: 0.2479355491677897, Avg Validation Loss: 37.62731532609891\n",
            "Epoch 642/1000, Avg Training Loss: 0.2479378346911804, Avg Validation Loss: 37.63119698833349\n",
            "Epoch 643/1000, Avg Training Loss: 0.24793438090410205, Avg Validation Loss: 37.63496147352317\n",
            "Epoch 644/1000, Avg Training Loss: 0.247935641239237, Avg Validation Loss: 37.63621513681062\n",
            "Epoch 645/1000, Avg Training Loss: 0.24793860801173118, Avg Validation Loss: 37.6396300329686\n",
            "Epoch 646/1000, Avg Training Loss: 0.24793940981318116, Avg Validation Loss: 37.64099979479576\n",
            "Epoch 647/1000, Avg Training Loss: 0.24794973294711417, Avg Validation Loss: 37.642057242785086\n",
            "Epoch 648/1000, Avg Training Loss: 0.24796584453010628, Avg Validation Loss: 37.64698803775897\n",
            "Epoch 649/1000, Avg Training Loss: 0.24795606836396886, Avg Validation Loss: 37.64784430987673\n",
            "Epoch 650/1000, Avg Training Loss: 0.2479507753081705, Avg Validation Loss: 37.65146509692222\n",
            "Epoch 651/1000, Avg Training Loss: 0.2479546347350051, Avg Validation Loss: 37.65458847906348\n",
            "Epoch 652/1000, Avg Training Loss: 0.2479637821484475, Avg Validation Loss: 37.65708290523207\n",
            "Epoch 653/1000, Avg Training Loss: 0.2479698748526439, Avg Validation Loss: 37.66232673373278\n",
            "Epoch 654/1000, Avg Training Loss: 0.24797157685602364, Avg Validation Loss: 37.66437088880861\n",
            "Epoch 655/1000, Avg Training Loss: 0.2479697882944251, Avg Validation Loss: 37.66765565768078\n",
            "Epoch 656/1000, Avg Training Loss: 0.24798091220485627, Avg Validation Loss: 37.67080034986649\n",
            "Epoch 657/1000, Avg Training Loss: 0.2479865940467088, Avg Validation Loss: 37.671749242042864\n",
            "Epoch 658/1000, Avg Training Loss: 0.2479834642645903, Avg Validation Loss: 37.674593605296394\n",
            "Epoch 659/1000, Avg Training Loss: 0.24799538083284486, Avg Validation Loss: 37.67635383209103\n",
            "Epoch 660/1000, Avg Training Loss: 0.24799461185324115, Avg Validation Loss: 37.67919347231814\n",
            "Epoch 661/1000, Avg Training Loss: 0.24800110797657057, Avg Validation Loss: 37.68274190761291\n",
            "Epoch 662/1000, Avg Training Loss: 0.24802089919457523, Avg Validation Loss: 37.68972696031111\n",
            "Epoch 663/1000, Avg Training Loss: 0.24800073807570738, Avg Validation Loss: 37.69311003105044\n",
            "Epoch 664/1000, Avg Training Loss: 0.24800535819392405, Avg Validation Loss: 37.69640919663175\n",
            "Epoch 665/1000, Avg Training Loss: 0.24801603734266492, Avg Validation Loss: 37.699837341974074\n",
            "Epoch 666/1000, Avg Training Loss: 0.24803402238520889, Avg Validation Loss: 37.70046692524667\n",
            "Epoch 667/1000, Avg Training Loss: 0.24803376871980007, Avg Validation Loss: 37.70552001659426\n",
            "Epoch 668/1000, Avg Training Loss: 0.24803254554728277, Avg Validation Loss: 37.709635421375914\n",
            "Epoch 669/1000, Avg Training Loss: 0.24804419022651247, Avg Validation Loss: 37.71316548030226\n",
            "Epoch 670/1000, Avg Training Loss: 0.24805259172722477, Avg Validation Loss: 37.714567361929184\n",
            "Epoch 671/1000, Avg Training Loss: 0.24804866544233944, Avg Validation Loss: 37.71804165787641\n",
            "Epoch 672/1000, Avg Training Loss: 0.2480624844943898, Avg Validation Loss: 37.72233134306165\n",
            "Epoch 673/1000, Avg Training Loss: 0.24806950298837135, Avg Validation Loss: 37.72807703057365\n",
            "Epoch 674/1000, Avg Training Loss: 0.24807157823027093, Avg Validation Loss: 37.73075750550737\n",
            "Epoch 675/1000, Avg Training Loss: 0.24807332404272575, Avg Validation Loss: 37.733817984788246\n",
            "Epoch 676/1000, Avg Training Loss: 0.2480818597993842, Avg Validation Loss: 37.738501961989314\n",
            "Epoch 677/1000, Avg Training Loss: 0.24809199575274662, Avg Validation Loss: 37.74189736408516\n",
            "Epoch 678/1000, Avg Training Loss: 0.2480983412539643, Avg Validation Loss: 37.74621631571491\n",
            "Epoch 679/1000, Avg Training Loss: 0.24810420911429049, Avg Validation Loss: 37.75081606674374\n",
            "Epoch 680/1000, Avg Training Loss: 0.2481204591378626, Avg Validation Loss: 37.75623384708059\n",
            "Epoch 681/1000, Avg Training Loss: 0.24813529908813528, Avg Validation Loss: 37.757794025838884\n",
            "Epoch 682/1000, Avg Training Loss: 0.2481340349315985, Avg Validation Loss: 37.76343002562379\n",
            "Epoch 683/1000, Avg Training Loss: 0.24814798298550636, Avg Validation Loss: 37.76953044957494\n",
            "Epoch 684/1000, Avg Training Loss: 0.2481600755707376, Avg Validation Loss: 37.7724136410991\n",
            "Epoch 685/1000, Avg Training Loss: 0.24816592003932558, Avg Validation Loss: 37.77625333010463\n",
            "Epoch 686/1000, Avg Training Loss: 0.24818103186292495, Avg Validation Loss: 37.78019174878699\n",
            "Epoch 687/1000, Avg Training Loss: 0.24818409294300067, Avg Validation Loss: 37.78674780383524\n",
            "Epoch 688/1000, Avg Training Loss: 0.24819819661042705, Avg Validation Loss: 37.790211055728705\n",
            "Epoch 689/1000, Avg Training Loss: 0.24821538551208885, Avg Validation Loss: 37.79591088253291\n",
            "Epoch 690/1000, Avg Training Loss: 0.2482263488613635, Avg Validation Loss: 37.80087943213413\n",
            "Epoch 691/1000, Avg Training Loss: 0.24824071841000012, Avg Validation Loss: 37.80696331466696\n",
            "Epoch 692/1000, Avg Training Loss: 0.24825868497427517, Avg Validation Loss: 37.81000392617403\n",
            "Epoch 693/1000, Avg Training Loss: 0.24827198759283903, Avg Validation Loss: 37.817139979301054\n",
            "Epoch 694/1000, Avg Training Loss: 0.24828200811456944, Avg Validation Loss: 37.822759797280504\n",
            "Epoch 695/1000, Avg Training Loss: 0.24829852729927032, Avg Validation Loss: 37.82845024529399\n",
            "Epoch 696/1000, Avg Training Loss: 0.24831994429552778, Avg Validation Loss: 37.83618000007475\n",
            "Epoch 697/1000, Avg Training Loss: 0.24833219833737305, Avg Validation Loss: 37.84299031321616\n",
            "Epoch 698/1000, Avg Training Loss: 0.2483525284752016, Avg Validation Loss: 37.848334417160174\n",
            "Epoch 699/1000, Avg Training Loss: 0.2483687546103655, Avg Validation Loss: 37.85465927250434\n",
            "Epoch 700/1000, Avg Training Loss: 0.2483997783980037, Avg Validation Loss: 37.85969436031432\n",
            "Epoch 701/1000, Avg Training Loss: 0.2484182526110604, Avg Validation Loss: 37.867944226343255\n",
            "Epoch 702/1000, Avg Training Loss: 0.2484399481930048, Avg Validation Loss: 37.876549284274255\n",
            "Epoch 703/1000, Avg Training Loss: 0.24846906029695312, Avg Validation Loss: 37.88296458353481\n",
            "Epoch 704/1000, Avg Training Loss: 0.24849763350873838, Avg Validation Loss: 37.89186781316271\n",
            "Epoch 705/1000, Avg Training Loss: 0.2485171026948076, Avg Validation Loss: 37.89927087670377\n",
            "Epoch 706/1000, Avg Training Loss: 0.2485488313062015, Avg Validation Loss: 37.90594305747055\n",
            "Epoch 707/1000, Avg Training Loss: 0.24857483473448894, Avg Validation Loss: 37.915962189815126\n",
            "Epoch 708/1000, Avg Training Loss: 0.24860977860169972, Avg Validation Loss: 37.924855919045825\n",
            "Epoch 709/1000, Avg Training Loss: 0.24864029449405817, Avg Validation Loss: 37.93590438003591\n",
            "Epoch 710/1000, Avg Training Loss: 0.2486822491114639, Avg Validation Loss: 37.94372488949202\n",
            "Epoch 711/1000, Avg Training Loss: 0.24872460783862788, Avg Validation Loss: 37.95512087489149\n",
            "Epoch 712/1000, Avg Training Loss: 0.24875648677419243, Avg Validation Loss: 37.96576012501883\n",
            "Epoch 713/1000, Avg Training Loss: 0.24881438582401075, Avg Validation Loss: 37.97528089933743\n",
            "Epoch 714/1000, Avg Training Loss: 0.24884675175349816, Avg Validation Loss: 37.98861625802501\n",
            "Epoch 715/1000, Avg Training Loss: 0.24890303833144428, Avg Validation Loss: 37.9996765277227\n",
            "Epoch 716/1000, Avg Training Loss: 0.24895043499384015, Avg Validation Loss: 38.01280162260305\n",
            "Epoch 717/1000, Avg Training Loss: 0.2490372497548289, Avg Validation Loss: 38.0285744964048\n",
            "Epoch 718/1000, Avg Training Loss: 0.2490572442546574, Avg Validation Loss: 38.04140410176991\n",
            "Epoch 719/1000, Avg Training Loss: 0.24911961743098837, Avg Validation Loss: 38.056426677607305\n",
            "Epoch 720/1000, Avg Training Loss: 0.24918821129724886, Avg Validation Loss: 38.071632830628346\n",
            "Epoch 721/1000, Avg Training Loss: 0.24925771554701845, Avg Validation Loss: 38.08829755875358\n",
            "Epoch 722/1000, Avg Training Loss: 0.2493342073649965, Avg Validation Loss: 38.10664629618937\n",
            "Epoch 723/1000, Avg Training Loss: 0.24943414289989368, Avg Validation Loss: 38.12444402400488\n",
            "Epoch 724/1000, Avg Training Loss: 0.24950584414595367, Avg Validation Loss: 38.14550070692447\n",
            "Epoch 725/1000, Avg Training Loss: 0.24962343412076993, Avg Validation Loss: 38.16625747664099\n",
            "Epoch 726/1000, Avg Training Loss: 0.24972944713277595, Avg Validation Loss: 38.18952176880321\n",
            "Epoch 727/1000, Avg Training Loss: 0.24982793479734716, Avg Validation Loss: 38.215387120976544\n",
            "Epoch 728/1000, Avg Training Loss: 0.2499526823910113, Avg Validation Loss: 38.24163453938196\n",
            "Epoch 729/1000, Avg Training Loss: 0.25008755812106054, Avg Validation Loss: 38.27334443801115\n",
            "Epoch 730/1000, Avg Training Loss: 0.2502407062341363, Avg Validation Loss: 38.303292984990755\n",
            "Epoch 731/1000, Avg Training Loss: 0.2504019162149852, Avg Validation Loss: 38.33621228726257\n",
            "Epoch 732/1000, Avg Training Loss: 0.25057381790387273, Avg Validation Loss: 38.373992124410556\n",
            "Epoch 733/1000, Avg Training Loss: 0.2507679074940009, Avg Validation Loss: 38.41370331952821\n",
            "Epoch 734/1000, Avg Training Loss: 0.25100360070657335, Avg Validation Loss: 38.45700130440992\n",
            "Epoch 735/1000, Avg Training Loss: 0.2512249297301659, Avg Validation Loss: 38.505198078961016\n",
            "Epoch 736/1000, Avg Training Loss: 0.2514848929930455, Avg Validation Loss: 38.55679435088875\n",
            "Epoch 737/1000, Avg Training Loss: 0.2517773186833823, Avg Validation Loss: 38.6118641417432\n",
            "Epoch 738/1000, Avg Training Loss: 0.252084905976975, Avg Validation Loss: 38.67427894789858\n",
            "Epoch 739/1000, Avg Training Loss: 0.2524490734009949, Avg Validation Loss: 38.741256929651755\n",
            "Epoch 740/1000, Avg Training Loss: 0.25280681275344796, Avg Validation Loss: 38.82050452734058\n",
            "Epoch 741/1000, Avg Training Loss: 0.25326144508985804, Avg Validation Loss: 38.905451117723274\n",
            "Epoch 742/1000, Avg Training Loss: 0.25373260056245117, Avg Validation Loss: 39.00245399848544\n",
            "Epoch 743/1000, Avg Training Loss: 0.25427913237441396, Avg Validation Loss: 39.10783619830987\n",
            "Epoch 744/1000, Avg Training Loss: 0.25484146555735954, Avg Validation Loss: 39.23004323244129\n",
            "Epoch 745/1000, Avg Training Loss: 0.2555842878482, Avg Validation Loss: 39.36129914778222\n",
            "Epoch 746/1000, Avg Training Loss: 0.2563452711082025, Avg Validation Loss: 39.50636816809114\n",
            "Epoch 747/1000, Avg Training Loss: 0.25717355994324925, Avg Validation Loss: 39.67410666680739\n",
            "Epoch 748/1000, Avg Training Loss: 0.25817883210308656, Avg Validation Loss: 39.85934663463195\n",
            "Epoch 749/1000, Avg Training Loss: 0.25925357004604205, Avg Validation Loss: 40.07207455825328\n",
            "Epoch 750/1000, Avg Training Loss: 0.26050069619164035, Avg Validation Loss: 40.31400588162067\n",
            "Epoch 751/1000, Avg Training Loss: 0.261960266407343, Avg Validation Loss: 40.580673748262484\n",
            "Epoch 752/1000, Avg Training Loss: 0.263479063660491, Avg Validation Loss: 40.89362605493787\n",
            "Epoch 753/1000, Avg Training Loss: 0.2653998061388337, Avg Validation Loss: 41.237970885983714\n",
            "Epoch 754/1000, Avg Training Loss: 0.26745309980005866, Avg Validation Loss: 41.63194249822213\n",
            "Epoch 755/1000, Avg Training Loss: 0.2697847282244322, Avg Validation Loss: 42.082145523601156\n",
            "Epoch 756/1000, Avg Training Loss: 0.27250050667027537, Avg Validation Loss: 42.58826907568179\n",
            "Epoch 757/1000, Avg Training Loss: 0.27511885945723363, Avg Validation Loss: 42.99380076927696\n",
            "Epoch 758/1000, Avg Training Loss: 0.277633810033031, Avg Validation Loss: 43.41934509073092\n",
            "Epoch 759/1000, Avg Training Loss: 0.28015925249283, Avg Validation Loss: 43.881374616147646\n",
            "Epoch 760/1000, Avg Training Loss: 0.28312304306285274, Avg Validation Loss: 44.449660367487226\n",
            "Epoch 761/1000, Avg Training Loss: 0.2866324861284211, Avg Validation Loss: 45.07500309716467\n",
            "Epoch 762/1000, Avg Training Loss: 0.2904473596727193, Avg Validation Loss: 45.78525035393431\n",
            "Epoch 763/1000, Avg Training Loss: 0.29473427535259594, Avg Validation Loss: 46.52305114418573\n",
            "Epoch 764/1000, Avg Training Loss: 0.2987188147966746, Avg Validation Loss: 47.082272946099536\n",
            "Epoch 765/1000, Avg Training Loss: 0.3023848839834597, Avg Validation Loss: 47.873774077607024\n",
            "Epoch 766/1000, Avg Training Loss: 0.3074848094289845, Avg Validation Loss: 48.688591935707436\n",
            "Epoch 767/1000, Avg Training Loss: 0.31230968319281416, Avg Validation Loss: 49.555298220444236\n",
            "Epoch 768/1000, Avg Training Loss: 0.31715069711851873, Avg Validation Loss: 50.41840148450653\n",
            "Epoch 769/1000, Avg Training Loss: 0.32229421242416334, Avg Validation Loss: 51.25339134911944\n",
            "Epoch 770/1000, Avg Training Loss: 0.32727321976113594, Avg Validation Loss: 52.22929394255216\n",
            "Epoch 771/1000, Avg Training Loss: 0.3334313756245718, Avg Validation Loss: 53.38137963563698\n",
            "Epoch 772/1000, Avg Training Loss: 0.33983862159932055, Avg Validation Loss: 54.38620392098081\n",
            "Epoch 773/1000, Avg Training Loss: 0.3455122216673079, Avg Validation Loss: 55.35565778087247\n",
            "Epoch 774/1000, Avg Training Loss: 0.35136265362641356, Avg Validation Loss: 56.36625826020494\n",
            "Epoch 775/1000, Avg Training Loss: 0.3573383643573521, Avg Validation Loss: 57.40429856727593\n",
            "Epoch 776/1000, Avg Training Loss: 0.3631584025167863, Avg Validation Loss: 58.30052461440923\n",
            "Epoch 777/1000, Avg Training Loss: 0.3682126834189955, Avg Validation Loss: 59.19643191732101\n",
            "Epoch 778/1000, Avg Training Loss: 0.3732715409350123, Avg Validation Loss: 60.002686156633686\n",
            "Epoch 779/1000, Avg Training Loss: 0.3776503256023715, Avg Validation Loss: 60.61146822190655\n",
            "Epoch 780/1000, Avg Training Loss: 0.38076124634812286, Avg Validation Loss: 61.04510646426449\n",
            "Epoch 781/1000, Avg Training Loss: 0.38284977138859716, Avg Validation Loss: 61.314681071702154\n",
            "Epoch 782/1000, Avg Training Loss: 0.38393204407892245, Avg Validation Loss: 61.37044124259028\n",
            "Epoch 783/1000, Avg Training Loss: 0.38375123500399816, Avg Validation Loss: 61.21354795378148\n",
            "Epoch 784/1000, Avg Training Loss: 0.3822785621243924, Avg Validation Loss: 60.919398329625956\n",
            "Epoch 785/1000, Avg Training Loss: 0.38017757783797224, Avg Validation Loss: 60.46680644396804\n",
            "Epoch 786/1000, Avg Training Loss: 0.3768161488656995, Avg Validation Loss: 59.79312677111537\n",
            "Epoch 787/1000, Avg Training Loss: 0.37250619096794696, Avg Validation Loss: 59.044661444038546\n",
            "Epoch 788/1000, Avg Training Loss: 0.3680027332061139, Avg Validation Loss: 58.21000540646178\n",
            "Epoch 789/1000, Avg Training Loss: 0.3630177513094701, Avg Validation Loss: 57.35491102131573\n",
            "Epoch 790/1000, Avg Training Loss: 0.3579443005163158, Avg Validation Loss: 56.54082817734694\n",
            "Epoch 791/1000, Avg Training Loss: 0.3530518858525581, Avg Validation Loss: 55.64708487689254\n",
            "Epoch 792/1000, Avg Training Loss: 0.34782729614841146, Avg Validation Loss: 54.76518811659147\n",
            "Epoch 793/1000, Avg Training Loss: 0.3428466262731578, Avg Validation Loss: 53.92328305964556\n",
            "Epoch 794/1000, Avg Training Loss: 0.3379076444979369, Avg Validation Loss: 53.0888469768166\n",
            "Epoch 795/1000, Avg Training Loss: 0.3332189908762776, Avg Validation Loss: 52.25716991626127\n",
            "Epoch 796/1000, Avg Training Loss: 0.3288130415740001, Avg Validation Loss: 51.54185859808406\n",
            "Epoch 797/1000, Avg Training Loss: 0.3250843979575581, Avg Validation Loss: 50.88316873498566\n",
            "Epoch 798/1000, Avg Training Loss: 0.3215173086514985, Avg Validation Loss: 50.246723600033775\n",
            "Epoch 799/1000, Avg Training Loss: 0.31815477694067884, Avg Validation Loss: 49.71560877728025\n",
            "Epoch 800/1000, Avg Training Loss: 0.315579912767435, Avg Validation Loss: 49.25124539267509\n",
            "Epoch 801/1000, Avg Training Loss: 0.3131561708471506, Avg Validation Loss: 48.782628970492425\n",
            "Epoch 802/1000, Avg Training Loss: 0.31072175143161485, Avg Validation Loss: 48.32896970806478\n",
            "Epoch 803/1000, Avg Training Loss: 0.30840086700685587, Avg Validation Loss: 47.92741256209656\n",
            "Epoch 804/1000, Avg Training Loss: 0.30636957014601535, Avg Validation Loss: 47.52882532861841\n",
            "Epoch 805/1000, Avg Training Loss: 0.30446753189918485, Avg Validation Loss: 47.14270928488757\n",
            "Epoch 806/1000, Avg Training Loss: 0.3025388136516165, Avg Validation Loss: 46.797372829287\n",
            "Epoch 807/1000, Avg Training Loss: 0.3009257487534046, Avg Validation Loss: 46.46733120941184\n",
            "Epoch 808/1000, Avg Training Loss: 0.2993051873782911, Avg Validation Loss: 46.16593991945673\n",
            "Epoch 809/1000, Avg Training Loss: 0.29790820458409184, Avg Validation Loss: 45.90083475952535\n",
            "Epoch 810/1000, Avg Training Loss: 0.29663787222814575, Avg Validation Loss: 45.654700113246335\n",
            "Epoch 811/1000, Avg Training Loss: 0.29554950013082515, Avg Validation Loss: 45.441814975580556\n",
            "Epoch 812/1000, Avg Training Loss: 0.294567182413267, Avg Validation Loss: 45.25057916516591\n",
            "Epoch 813/1000, Avg Training Loss: 0.2937172349623179, Avg Validation Loss: 45.08469087309348\n",
            "Epoch 814/1000, Avg Training Loss: 0.29295947303199565, Avg Validation Loss: 44.905475112589826\n",
            "Epoch 815/1000, Avg Training Loss: 0.2921390335557341, Avg Validation Loss: 44.73813610607424\n",
            "Epoch 816/1000, Avg Training Loss: 0.29135049957568493, Avg Validation Loss: 44.569702602690086\n",
            "Epoch 817/1000, Avg Training Loss: 0.2905571893636572, Avg Validation Loss: 44.42903714886462\n",
            "Epoch 818/1000, Avg Training Loss: 0.28982007636656515, Avg Validation Loss: 44.29322735841484\n",
            "Epoch 819/1000, Avg Training Loss: 0.28917206290606906, Avg Validation Loss: 44.16542289322424\n",
            "Epoch 820/1000, Avg Training Loss: 0.2885152574611587, Avg Validation Loss: 44.03378914964475\n",
            "Epoch 821/1000, Avg Training Loss: 0.2878903203612671, Avg Validation Loss: 43.90793544752076\n",
            "Epoch 822/1000, Avg Training Loss: 0.2872369118307658, Avg Validation Loss: 43.789477226988296\n",
            "Epoch 823/1000, Avg Training Loss: 0.2866117665860135, Avg Validation Loss: 43.6803375634845\n",
            "Epoch 824/1000, Avg Training Loss: 0.28604929762387893, Avg Validation Loss: 43.57453910654427\n",
            "Epoch 825/1000, Avg Training Loss: 0.2854267551018796, Avg Validation Loss: 43.45775185828107\n",
            "Epoch 826/1000, Avg Training Loss: 0.2848227240046518, Avg Validation Loss: 43.35279572841257\n",
            "Epoch 827/1000, Avg Training Loss: 0.28425726212135943, Avg Validation Loss: 43.25783708672283\n",
            "Epoch 828/1000, Avg Training Loss: 0.28376422467986373, Avg Validation Loss: 43.16162944471495\n",
            "Epoch 829/1000, Avg Training Loss: 0.2832093121965808, Avg Validation Loss: 43.06032321263682\n",
            "Epoch 830/1000, Avg Training Loss: 0.28266160038499233, Avg Validation Loss: 42.96148362335974\n",
            "Epoch 831/1000, Avg Training Loss: 0.2821187561531465, Avg Validation Loss: 42.87679937497815\n",
            "Epoch 832/1000, Avg Training Loss: 0.281646202050423, Avg Validation Loss: 42.797182871415764\n",
            "Epoch 833/1000, Avg Training Loss: 0.2811891989957017, Avg Validation Loss: 42.712216422141715\n",
            "Epoch 834/1000, Avg Training Loss: 0.28067098058106454, Avg Validation Loss: 42.63556678145845\n",
            "Epoch 835/1000, Avg Training Loss: 0.2802214547589418, Avg Validation Loss: 42.560105001246356\n",
            "Epoch 836/1000, Avg Training Loss: 0.27973340321905227, Avg Validation Loss: 42.486819060531666\n",
            "Epoch 837/1000, Avg Training Loss: 0.27935265428367395, Avg Validation Loss: 42.415118354203685\n",
            "Epoch 838/1000, Avg Training Loss: 0.27888356166031913, Avg Validation Loss: 42.33915992100832\n",
            "Epoch 839/1000, Avg Training Loss: 0.2784596338293067, Avg Validation Loss: 42.26781208469298\n",
            "Epoch 840/1000, Avg Training Loss: 0.2780235062590026, Avg Validation Loss: 42.19780112695833\n",
            "Epoch 841/1000, Avg Training Loss: 0.2775909101183097, Avg Validation Loss: 42.130877981925565\n",
            "Epoch 842/1000, Avg Training Loss: 0.2771818011630659, Avg Validation Loss: 42.05713700325697\n",
            "Epoch 843/1000, Avg Training Loss: 0.276760000792681, Avg Validation Loss: 41.98473979457981\n",
            "Epoch 844/1000, Avg Training Loss: 0.27636017032470034, Avg Validation Loss: 41.911952526892286\n",
            "Epoch 845/1000, Avg Training Loss: 0.2759058712167253, Avg Validation Loss: 41.84502443229776\n",
            "Epoch 846/1000, Avg Training Loss: 0.2755307320001642, Avg Validation Loss: 41.77240708205474\n",
            "Epoch 847/1000, Avg Training Loss: 0.2751361170540513, Avg Validation Loss: 41.70403428544957\n",
            "Epoch 848/1000, Avg Training Loss: 0.27473324750724937, Avg Validation Loss: 41.644039419823876\n",
            "Epoch 849/1000, Avg Training Loss: 0.27437197405496494, Avg Validation Loss: 41.58019514903772\n",
            "Epoch 850/1000, Avg Training Loss: 0.27399275266425704, Avg Validation Loss: 41.515857971965424\n",
            "Epoch 851/1000, Avg Training Loss: 0.27357688075728903, Avg Validation Loss: 41.452066792883656\n",
            "Epoch 852/1000, Avg Training Loss: 0.2731846213684654, Avg Validation Loss: 41.390045702962986\n",
            "Epoch 853/1000, Avg Training Loss: 0.27282116113915716, Avg Validation Loss: 41.32794719290312\n",
            "Epoch 854/1000, Avg Training Loss: 0.27244053256760076, Avg Validation Loss: 41.26978594997708\n",
            "Epoch 855/1000, Avg Training Loss: 0.27206900613147444, Avg Validation Loss: 41.21615384952223\n",
            "Epoch 856/1000, Avg Training Loss: 0.2717005830373308, Avg Validation Loss: 41.16053671298438\n",
            "Epoch 857/1000, Avg Training Loss: 0.271332914822959, Avg Validation Loss: 41.106248434154594\n",
            "Epoch 858/1000, Avg Training Loss: 0.27097681676042856, Avg Validation Loss: 41.05745590138376\n",
            "Epoch 859/1000, Avg Training Loss: 0.2706096073540708, Avg Validation Loss: 41.006462388221415\n",
            "Epoch 860/1000, Avg Training Loss: 0.2702908434532474, Avg Validation Loss: 40.95397397722779\n",
            "Epoch 861/1000, Avg Training Loss: 0.2699602089778915, Avg Validation Loss: 40.90523461309311\n",
            "Epoch 862/1000, Avg Training Loss: 0.26963170980173806, Avg Validation Loss: 40.8534370816718\n",
            "Epoch 863/1000, Avg Training Loss: 0.26932341677391475, Avg Validation Loss: 40.81085435264378\n",
            "Epoch 864/1000, Avg Training Loss: 0.2690006147733378, Avg Validation Loss: 40.76656592164599\n",
            "Epoch 865/1000, Avg Training Loss: 0.26874072757583073, Avg Validation Loss: 40.720245523745696\n",
            "Epoch 866/1000, Avg Training Loss: 0.2684176700714317, Avg Validation Loss: 40.66965143513995\n",
            "Epoch 867/1000, Avg Training Loss: 0.2681244161772574, Avg Validation Loss: 40.62625954224399\n",
            "Epoch 868/1000, Avg Training Loss: 0.2678304538738242, Avg Validation Loss: 40.58330057771714\n",
            "Epoch 869/1000, Avg Training Loss: 0.26755965205996046, Avg Validation Loss: 40.54490003475086\n",
            "Epoch 870/1000, Avg Training Loss: 0.26724773267499, Avg Validation Loss: 40.50142468457696\n",
            "Epoch 871/1000, Avg Training Loss: 0.2669830332946216, Avg Validation Loss: 40.460460887270216\n",
            "Epoch 872/1000, Avg Training Loss: 0.26668439704580704, Avg Validation Loss: 40.42045977635882\n",
            "Epoch 873/1000, Avg Training Loss: 0.26642097683884136, Avg Validation Loss: 40.37841403940172\n",
            "Epoch 874/1000, Avg Training Loss: 0.2661590181306867, Avg Validation Loss: 40.34305609354813\n",
            "Epoch 875/1000, Avg Training Loss: 0.26590659465771704, Avg Validation Loss: 40.30865766648156\n",
            "Epoch 876/1000, Avg Training Loss: 0.26566589099206317, Avg Validation Loss: 40.271538175714625\n",
            "Epoch 877/1000, Avg Training Loss: 0.26542546617306234, Avg Validation Loss: 40.23609740088578\n",
            "Epoch 878/1000, Avg Training Loss: 0.2651832048765793, Avg Validation Loss: 40.204273441082634\n",
            "Epoch 879/1000, Avg Training Loss: 0.2649551378835794, Avg Validation Loss: 40.174258173493364\n",
            "Epoch 880/1000, Avg Training Loss: 0.2647552960687173, Avg Validation Loss: 40.14802366545741\n",
            "Epoch 881/1000, Avg Training Loss: 0.2645024639988274, Avg Validation Loss: 40.116908505083615\n",
            "Epoch 882/1000, Avg Training Loss: 0.2642968428265225, Avg Validation Loss: 40.086802677205156\n",
            "Epoch 883/1000, Avg Training Loss: 0.2640638330130269, Avg Validation Loss: 40.05673152624775\n",
            "Epoch 884/1000, Avg Training Loss: 0.2638440412340236, Avg Validation Loss: 40.03011379839831\n",
            "Epoch 885/1000, Avg Training Loss: 0.2636403566927761, Avg Validation Loss: 40.004548297698065\n",
            "Epoch 886/1000, Avg Training Loss: 0.2634568340221167, Avg Validation Loss: 39.973825396431906\n",
            "Epoch 887/1000, Avg Training Loss: 0.2632237193598358, Avg Validation Loss: 39.9487498641724\n",
            "Epoch 888/1000, Avg Training Loss: 0.26303598812867085, Avg Validation Loss: 39.92325951899181\n",
            "Epoch 889/1000, Avg Training Loss: 0.26283775274266996, Avg Validation Loss: 39.89918115108231\n",
            "Epoch 890/1000, Avg Training Loss: 0.2626683727180186, Avg Validation Loss: 39.87658261642946\n",
            "Epoch 891/1000, Avg Training Loss: 0.2624851522848242, Avg Validation Loss: 39.85291847557869\n",
            "Epoch 892/1000, Avg Training Loss: 0.26229539634184723, Avg Validation Loss: 39.82919589922113\n",
            "Epoch 893/1000, Avg Training Loss: 0.26212621233152256, Avg Validation Loss: 39.807949268954886\n",
            "Epoch 894/1000, Avg Training Loss: 0.2619535341768074, Avg Validation Loss: 39.783323914193204\n",
            "Epoch 895/1000, Avg Training Loss: 0.2617934251599328, Avg Validation Loss: 39.759676647352485\n",
            "Epoch 896/1000, Avg Training Loss: 0.26161730261095373, Avg Validation Loss: 39.732880779098124\n",
            "Epoch 897/1000, Avg Training Loss: 0.26141957600125554, Avg Validation Loss: 39.71231521720484\n",
            "Epoch 898/1000, Avg Training Loss: 0.26122871408248394, Avg Validation Loss: 39.69250947565216\n",
            "Epoch 899/1000, Avg Training Loss: 0.2610527079252881, Avg Validation Loss: 39.67049386566331\n",
            "Epoch 900/1000, Avg Training Loss: 0.26087787606529894, Avg Validation Loss: 39.651388694085156\n",
            "Epoch 901/1000, Avg Training Loss: 0.2607146780046347, Avg Validation Loss: 39.63290626900755\n",
            "Epoch 902/1000, Avg Training Loss: 0.26056114723582324, Avg Validation Loss: 39.61796704745915\n",
            "Epoch 903/1000, Avg Training Loss: 0.2603952714835402, Avg Validation Loss: 39.6017700146477\n",
            "Epoch 904/1000, Avg Training Loss: 0.2602437700554326, Avg Validation Loss: 39.58126784476667\n",
            "Epoch 905/1000, Avg Training Loss: 0.2600838172707668, Avg Validation Loss: 39.55942459606654\n",
            "Epoch 906/1000, Avg Training Loss: 0.2599557262499853, Avg Validation Loss: 39.53805838080866\n",
            "Epoch 907/1000, Avg Training Loss: 0.25979191951849245, Avg Validation Loss: 39.52327145084246\n",
            "Epoch 908/1000, Avg Training Loss: 0.25965555836041665, Avg Validation Loss: 39.50612989497148\n",
            "Epoch 909/1000, Avg Training Loss: 0.2595259845860114, Avg Validation Loss: 39.48815921332638\n",
            "Epoch 910/1000, Avg Training Loss: 0.25941031441275325, Avg Validation Loss: 39.46833182918643\n",
            "Epoch 911/1000, Avg Training Loss: 0.2592960330313177, Avg Validation Loss: 39.452344548600834\n",
            "Epoch 912/1000, Avg Training Loss: 0.2591606819976214, Avg Validation Loss: 39.43970718885957\n",
            "Epoch 913/1000, Avg Training Loss: 0.2590350027872085, Avg Validation Loss: 39.42409052899335\n",
            "Epoch 914/1000, Avg Training Loss: 0.25892271736433814, Avg Validation Loss: 39.40974400329304\n",
            "Epoch 915/1000, Avg Training Loss: 0.2587874650442463, Avg Validation Loss: 39.39761990098668\n",
            "Epoch 916/1000, Avg Training Loss: 0.2586488059649916, Avg Validation Loss: 39.38568822032367\n",
            "Epoch 917/1000, Avg Training Loss: 0.2585305407050712, Avg Validation Loss: 39.37453860279261\n",
            "Epoch 918/1000, Avg Training Loss: 0.2584095404603634, Avg Validation Loss: 39.35881427257548\n",
            "Epoch 919/1000, Avg Training Loss: 0.2582868737712411, Avg Validation Loss: 39.34793263802456\n",
            "Epoch 920/1000, Avg Training Loss: 0.25815872486375246, Avg Validation Loss: 39.334836278877404\n",
            "Epoch 921/1000, Avg Training Loss: 0.2580608631883586, Avg Validation Loss: 39.321861284058784\n",
            "Epoch 922/1000, Avg Training Loss: 0.25793573842251755, Avg Validation Loss: 39.30765202769952\n",
            "Epoch 923/1000, Avg Training Loss: 0.2578214609236532, Avg Validation Loss: 39.29650914977246\n",
            "Epoch 924/1000, Avg Training Loss: 0.2577202476712769, Avg Validation Loss: 39.28493003519438\n",
            "Epoch 925/1000, Avg Training Loss: 0.25760150398108694, Avg Validation Loss: 39.271508751941326\n",
            "Epoch 926/1000, Avg Training Loss: 0.257499000697799, Avg Validation Loss: 39.26121097692405\n",
            "Epoch 927/1000, Avg Training Loss: 0.25737902441876737, Avg Validation Loss: 39.24923532157581\n",
            "Epoch 928/1000, Avg Training Loss: 0.2572823108272576, Avg Validation Loss: 39.2380196196024\n",
            "Epoch 929/1000, Avg Training Loss: 0.25716649111149636, Avg Validation Loss: 39.22532969834225\n",
            "Epoch 930/1000, Avg Training Loss: 0.25707546480518906, Avg Validation Loss: 39.212689298343705\n",
            "Epoch 931/1000, Avg Training Loss: 0.2569689409277595, Avg Validation Loss: 39.2025690932676\n",
            "Epoch 932/1000, Avg Training Loss: 0.25688190645236825, Avg Validation Loss: 39.19014731151172\n",
            "Epoch 933/1000, Avg Training Loss: 0.2567723943485339, Avg Validation Loss: 39.183810827852135\n",
            "Epoch 934/1000, Avg Training Loss: 0.256671839421445, Avg Validation Loss: 39.17597296329444\n",
            "Epoch 935/1000, Avg Training Loss: 0.25659568492614093, Avg Validation Loss: 39.16683521112486\n",
            "Epoch 936/1000, Avg Training Loss: 0.25649674701209824, Avg Validation Loss: 39.15542475518609\n",
            "Epoch 937/1000, Avg Training Loss: 0.2564198122086956, Avg Validation Loss: 39.14240977839748\n",
            "Epoch 938/1000, Avg Training Loss: 0.2563234716489914, Avg Validation Loss: 39.12997727557716\n",
            "Epoch 939/1000, Avg Training Loss: 0.25622904654156403, Avg Validation Loss: 39.122688367735364\n",
            "Epoch 940/1000, Avg Training Loss: 0.25615193512802625, Avg Validation Loss: 39.115213247193196\n",
            "Epoch 941/1000, Avg Training Loss: 0.25606657749189493, Avg Validation Loss: 39.10700809112592\n",
            "Epoch 942/1000, Avg Training Loss: 0.2559798527653475, Avg Validation Loss: 39.103137102124734\n",
            "Epoch 943/1000, Avg Training Loss: 0.2558870074664198, Avg Validation Loss: 39.09365242370461\n",
            "Epoch 944/1000, Avg Training Loss: 0.2558144909055155, Avg Validation Loss: 39.08164141920008\n",
            "Epoch 945/1000, Avg Training Loss: 0.2557201226097533, Avg Validation Loss: 39.07426388016067\n",
            "Epoch 946/1000, Avg Training Loss: 0.25565534248461025, Avg Validation Loss: 39.06744784306858\n",
            "Epoch 947/1000, Avg Training Loss: 0.2555694968148847, Avg Validation Loss: 39.05887716352984\n",
            "Epoch 948/1000, Avg Training Loss: 0.25548878343493603, Avg Validation Loss: 39.0528739647652\n",
            "Epoch 949/1000, Avg Training Loss: 0.25540516107011835, Avg Validation Loss: 39.04189738044204\n",
            "Epoch 950/1000, Avg Training Loss: 0.2553377940191922, Avg Validation Loss: 39.03411932333851\n",
            "Epoch 951/1000, Avg Training Loss: 0.2552542311971027, Avg Validation Loss: 39.02998296862521\n",
            "Epoch 952/1000, Avg Training Loss: 0.2551808198056188, Avg Validation Loss: 39.0233917446855\n",
            "Epoch 953/1000, Avg Training Loss: 0.2551041161846907, Avg Validation Loss: 39.01581518043878\n",
            "Epoch 954/1000, Avg Training Loss: 0.2550268568105197, Avg Validation Loss: 39.0091217105562\n",
            "Epoch 955/1000, Avg Training Loss: 0.2550106115877965, Avg Validation Loss: 38.99308525461953\n",
            "Epoch 956/1000, Avg Training Loss: 0.2549102126721219, Avg Validation Loss: 38.9880314290867\n",
            "Epoch 957/1000, Avg Training Loss: 0.25484243796325934, Avg Validation Loss: 38.98557637562959\n",
            "Epoch 958/1000, Avg Training Loss: 0.2547713249474746, Avg Validation Loss: 38.97861954243169\n",
            "Epoch 959/1000, Avg Training Loss: 0.2547112165670133, Avg Validation Loss: 38.97061725671844\n",
            "Epoch 960/1000, Avg Training Loss: 0.25464404752282577, Avg Validation Loss: 38.970515139045325\n",
            "Epoch 961/1000, Avg Training Loss: 0.2545796347877879, Avg Validation Loss: 38.96372304635382\n",
            "Epoch 962/1000, Avg Training Loss: 0.2545076588351043, Avg Validation Loss: 38.9603764705418\n",
            "Epoch 963/1000, Avg Training Loss: 0.25445281274210496, Avg Validation Loss: 38.95408346582336\n",
            "Epoch 964/1000, Avg Training Loss: 0.25440046349262574, Avg Validation Loss: 38.94939384469612\n",
            "Epoch 965/1000, Avg Training Loss: 0.2543204369700963, Avg Validation Loss: 38.942461390132266\n",
            "Epoch 966/1000, Avg Training Loss: 0.25428203035504815, Avg Validation Loss: 38.939510174073206\n",
            "Epoch 967/1000, Avg Training Loss: 0.25419788815919875, Avg Validation Loss: 38.9329750490944\n",
            "Epoch 968/1000, Avg Training Loss: 0.2541461882007136, Avg Validation Loss: 38.927629716642905\n",
            "Epoch 969/1000, Avg Training Loss: 0.2540879046652633, Avg Validation Loss: 38.9226879661603\n",
            "Epoch 970/1000, Avg Training Loss: 0.2540353576749722, Avg Validation Loss: 38.9121448381379\n",
            "Epoch 971/1000, Avg Training Loss: 0.25398559139311583, Avg Validation Loss: 38.90933378417467\n",
            "Epoch 972/1000, Avg Training Loss: 0.253938859349603, Avg Validation Loss: 38.90839433273186\n",
            "Epoch 973/1000, Avg Training Loss: 0.2538823118528627, Avg Validation Loss: 38.90400340409616\n",
            "Epoch 974/1000, Avg Training Loss: 0.25382688282338794, Avg Validation Loss: 38.89311385647359\n",
            "Epoch 975/1000, Avg Training Loss: 0.25377695881194046, Avg Validation Loss: 38.89007403183847\n",
            "Epoch 976/1000, Avg Training Loss: 0.25372250019041437, Avg Validation Loss: 38.88685316734291\n",
            "Epoch 977/1000, Avg Training Loss: 0.25367578584462913, Avg Validation Loss: 38.88471107995135\n",
            "Epoch 978/1000, Avg Training Loss: 0.2536088558149975, Avg Validation Loss: 38.87893059744111\n",
            "Epoch 979/1000, Avg Training Loss: 0.25357558749825415, Avg Validation Loss: 38.875326496717534\n",
            "Epoch 980/1000, Avg Training Loss: 0.25352500818832585, Avg Validation Loss: 38.86910921099539\n",
            "Epoch 981/1000, Avg Training Loss: 0.25347945215309176, Avg Validation Loss: 38.86449600490843\n",
            "Epoch 982/1000, Avg Training Loss: 0.25343312602710955, Avg Validation Loss: 38.85745009018939\n",
            "Epoch 983/1000, Avg Training Loss: 0.25338559688759055, Avg Validation Loss: 38.854726530632576\n",
            "Epoch 984/1000, Avg Training Loss: 0.2533307271161008, Avg Validation Loss: 38.85044773823279\n",
            "Epoch 985/1000, Avg Training Loss: 0.2532995671972784, Avg Validation Loss: 38.85073106564454\n",
            "Epoch 986/1000, Avg Training Loss: 0.2532500798124137, Avg Validation Loss: 38.84453247299387\n",
            "Epoch 987/1000, Avg Training Loss: 0.2532065220940064, Avg Validation Loss: 38.84013203597253\n",
            "Epoch 988/1000, Avg Training Loss: 0.253177848497422, Avg Validation Loss: 38.844041504575074\n",
            "Epoch 989/1000, Avg Training Loss: 0.25311613426998153, Avg Validation Loss: 38.8392990173384\n",
            "Epoch 990/1000, Avg Training Loss: 0.25306875321637456, Avg Validation Loss: 38.836078962266384\n",
            "Epoch 991/1000, Avg Training Loss: 0.253034682714234, Avg Validation Loss: 38.83472882733531\n",
            "Epoch 992/1000, Avg Training Loss: 0.2529933026995379, Avg Validation Loss: 38.83113752363802\n",
            "Epoch 993/1000, Avg Training Loss: 0.25295147118794153, Avg Validation Loss: 38.827510614092006\n",
            "Epoch 994/1000, Avg Training Loss: 0.25291029322390723, Avg Validation Loss: 38.82415580616657\n",
            "Epoch 995/1000, Avg Training Loss: 0.2528878574276436, Avg Validation Loss: 38.81933058978633\n",
            "Epoch 996/1000, Avg Training Loss: 0.25283730714920366, Avg Validation Loss: 38.81414731294687\n",
            "Epoch 997/1000, Avg Training Loss: 0.2528112239199695, Avg Validation Loss: 38.815668539992146\n",
            "Epoch 998/1000, Avg Training Loss: 0.25277494967342484, Avg Validation Loss: 38.81298377464826\n",
            "Epoch 999/1000, Avg Training Loss: 0.2527329384998857, Avg Validation Loss: 38.81200980249193\n",
            "Epoch 1000/1000, Avg Training Loss: 0.2527104892031249, Avg Validation Loss: 38.811711505462966\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x_tot = np.random.rand(1000, 15)\n",
        "target = np.random.rand(1000, 3)\n",
        "\n",
        "test_split = 0.2\n",
        "train_split = 0.75\n",
        "\n",
        "layer_one = Layer(15, 8, ELU, d_ELU)\n",
        "layer_two = Layer(8, 5, ELU, d_ELU)\n",
        "layer_out = Layer(5, 3, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "# Split data\n",
        "x_train_val, target_train_val, x_test, target_test = NN.data_split(x_tot, target, test_split)\n",
        "\n",
        "NN.add_layer(layer_one)\n",
        "NN.add_layer(layer_two)\n",
        "NN.add_layer(layer_out)\n",
        "\n",
        "# Parametri di training\n",
        "K = 5\n",
        "epochs = 1000\n",
        "learning_rate_w = 0.0005 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
        "                         # learning_rate of minibatch for number of the examples in the minibatch\n",
        "learning_rate_b = 0.0005\n",
        "batch_size = 100\n",
        "Lambda_t = 0.1\n",
        "Lambda_l = 0.1\n",
        "momentum = 0.9 # tipically from 0.8 and 0.99\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# Cross-validation\n",
        "train_error, val_error = NN.train_val(x_train_val, target_train_val, train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, mean_squared_error, d_mean_squared_error, 'elastic', 'adam')\n",
        "\n",
        "# Plot degli errori\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_error, label='Training Error')\n",
        "plt.plot(val_error, label='Validation Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Error')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
