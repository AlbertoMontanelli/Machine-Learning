{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array; \n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives. \n",
    "* **Hidden layers**: \n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
    "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
    "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha): \n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters :**\n",
    " * input_size : $k_{i-1}$ ;\n",
    " * output_size : $k_i$ ;\n",
    " * activation_function ;\n",
    " * activation_derivative . \\\\\n",
    "\n",
    "**Constructor attributes :**\n",
    "* self.weights : $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix . \\\\\n",
    "Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$ ;\n",
    "* self.biases : $1 \\ \\textrm{x} \\ k_i$ array. Initialized to zeros;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative .\n",
    "\n",
    "**Methods :**\n",
    "* forward_layer : allows to compute the output of the layer for a given input.\n",
    " * parameter :\n",
    "   * input_array : matrix $X$ (see above for the case $L_0$ or $L_i$) .\n",
    " * attributes :\n",
    "   * self.input : input_array ;\n",
    "   * self.net : net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$) .\n",
    " * return -> output = $f(net)$, where $f$ is the activation function; $f(net)$ has the same dimensions of $net$.\n",
    "* backward_layer : computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    " * parameters :\n",
    "   * d_Ep : target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "   * learning_rate.\n",
    " * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "id": "H8Ap9rLxLlNU",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(input_size), high=1/np.sqrt(input_size), size=(input_size, output_size))\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "\n",
    "\n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward_layer(self, d_Ep, learning_rate):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "        self.weights += learning_rate * np.dot(self.input.T, delta) # learning rule for the weights\n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    " * add_layer: appends a layer to the empty list self.layers\n",
    "  * parameter:\n",
    "    * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* forward: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    " * parameter:\n",
    "   * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    " * return -> input = $o_i$ for layer $L_i$.\n",
    "* backward: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    " * parameter:\n",
    "   * d_Ep;\n",
    "   * learning_rate.\n",
    "\n",
    "* train_online: applies the forward and backward method to the network for a specified number of epochs **one example at a time**.\n",
    " * parameter:\n",
    "   * x_train: input matrix $X$;\n",
    "   * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "   * epochs: number of the iterations of the training algorithm;\n",
    "   * learning_rate;\n",
    "   * loss_function;\n",
    "   * loss_function_derivative.\n",
    "\n",
    "* train_minibatch: applies the forward and backward method to the network for a specified number of epochs **to batches of $l' < l$** examples.\n",
    " * parameter:\n",
    "    * x_train: input matrix $X$;\n",
    "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "    * epochs: number of the iterations of the training algorithm;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative;\n",
    "    * batch_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "\n",
    "\n",
    "    def backward(self, d_Ep, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate)\n",
    "\n",
    "\n",
    "    def train_online(self, x_train, target, epochs, learning_rate, loss_function, loss_function_derivative):\n",
    "        for epoch in range(epochs):\n",
    "          epoch_loss = 0\n",
    "\n",
    "          for x_train_row, target_row in zip(x_train, target):\n",
    "\n",
    "            x_train_row = x_train_row.reshape(1, -1)\n",
    "            target_row = target_row.reshape(1, -1)\n",
    "\n",
    "            # Forward propagation\n",
    "            predictions = self.forward(x_train_row) # predictions = output of the output layer\n",
    "\n",
    "            # Compute loss and loss gradient for backward function\n",
    "            loss = loss_function(target_row, predictions)\n",
    "            loss_gradient = loss_function_derivative(target_row, predictions)\n",
    "            epoch_loss += loss  # accumulates the losses for each example\n",
    "\n",
    "            # Backward propagation\n",
    "            self.backward(loss_gradient, learning_rate)\n",
    "\n",
    "          # computation of the average loss per epoch\n",
    "          average_epoch_loss = epoch_loss / len(x_train)\n",
    "          print(f\"ONLINE: epoch #{epoch}, Average Loss: {average_epoch_loss}\")\n",
    "\n",
    "\n",
    "    def train_minibatch(self, x_train, target, epochs, learning_rate, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_train.shape[0] # selection of the number of rows\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "          epoch_loss = 0\n",
    "\n",
    "          # the rows of the input matrix are randomized in order to have different examples in each mini-batch for each epoch\n",
    "          indices = np.arange(num_samples) # creates an array from 0 to num_samples - 1\n",
    "          np.random.shuffle(indices) # shuffling the indices\n",
    "          x_train = x_train[indices] # re-ordering of the rows according to the new indices\n",
    "          target = target[indices] # same but for the targets\n",
    "\n",
    "          # process data in batches\n",
    "          for i in range(0, num_samples, batch_size): # even if the last mini-batch does not have size equal to batch_size it is processed anyway\n",
    "            x_batch = x_train[i:i+batch_size]\n",
    "            target_batch = target[i:i+batch_size]\n",
    "\n",
    "\n",
    "           # Forward propagation\n",
    "            predictions = self.forward(x_batch) # predictions = output of the output layer\n",
    "\n",
    "            # Compute loss and loss gradient for backward function\n",
    "            loss = loss_function(target_batch, predictions)\n",
    "            loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "            epoch_loss += np.sum(loss)  # accumulates the loss for all the examples in the mini-batch for each mini-batch\n",
    "\n",
    "            # Backward propagation\n",
    "            self.backward(loss_gradient, learning_rate)\n",
    "\n",
    "          # computation of the average loss per epoch\n",
    "          average_epoch_loss = epoch_loss / num_samples\n",
    "          print(f\"MINIBATCH: epoch #{epoch}, Average Loss: {average_epoch_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "#Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT1dN_rEoKPO",
    "outputId": "b5b5c005-2a3c-4b5f-9cb7-3de066b51cfe",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      5\u001b[0m target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.rand(1000, 3)\n",
    "target = np.random.rand(1000, 2)\n",
    "\n",
    "\n",
    "layer_one1 = Layer(3, 2, linear, d_linear)\n",
    "layer_one2 = Layer(3, 2, linear, d_linear)\n",
    "layer_two1 = Layer(2, 2, linear, d_linear)\n",
    "layer_two2 = Layer(2, 2, linear, d_linear)\n",
    "\n",
    "NN1 = NeuralNetwork()\n",
    "NN1.add_layer(layer_one1)\n",
    "NN1.add_layer(layer_two1)\n",
    "NN2 = NeuralNetwork()\n",
    "NN2.add_layer(layer_one2)\n",
    "NN2.add_layer(layer_two2)\n",
    "NN1.train_minibatch(x, target, 10, 0.01, mean_squared_error, d_mean_squared_error, 3)\n",
    "NN2.train_online(x, target, 10, 0.01, mean_squared_error, d_mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
