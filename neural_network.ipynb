{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "id": "qv1wwlgWsFM8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
        "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
        "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters :**\n",
        " * input_size : $k_{i-1}$ ;\n",
        " * output_size : $k_i$ ;\n",
        " * activation_function ;\n",
        " * activation_derivative . \\\\\n",
        "\n",
        "**Constructor attributes :**\n",
        "* self.weights : $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix . \\\\\n",
        "Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$ ;\n",
        "* self.biases : $1 \\ \\textrm{x} \\ k_i$ array. Initialized to zeros;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative .\n",
        "\n",
        "**Methods :**\n",
        "* forward_layer : allows to compute the output of the layer for a given input.\n",
        " * parameter :\n",
        "   * input_array : matrix $X$ (see above for the case $L_0$ or $L_i$) .\n",
        " * attributes :\n",
        "   * self.input : input_array ;\n",
        "   * self.net : net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$) .\n",
        " * return -> output = $f(net)$, where $f$ is the activation function; $f(net)$ has the same dimensions of $net$.\n",
        "* backward_layer : computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        " * parameters :\n",
        "   * d_Ep : target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "   * learning_rate.\n",
        " * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "H8Ap9rLxLlNU",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(input_size), high=1/np.sqrt(input_size), size=(input_size, output_size))\n",
        "        self.biases = np.zeros((1, output_size))\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate):\n",
        "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
        "        self.weights += learning_rate * np.dot(self.input.T, delta) # learning rule for the weights\n",
        "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
        "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        " * data_split: splits the input data into training set, validation set and test set\n",
        "  * parameter:\n",
        "    * x_tot: total data given as input;\n",
        "    * K: number K of K-folds used in K-folds cross validation;\n",
        "    * step_cycle: number of the step in the permutation cycle of the K-fold validation process.\n",
        "  * attributes:\n",
        "    * self.x_train: training set;\n",
        "    * self.x_val: validation set;\n",
        "    * self.x_test: test set.\n",
        " * add_layer: appends a layer to the empty list self.layers\n",
        "  * parameter:\n",
        "    * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* forward: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        " * parameter:\n",
        "   * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        " * return -> input = $o_i$ for layer $L_i$.\n",
        "* backward: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        " * parameter:\n",
        "   * d_Ep;\n",
        "   * learning_rate.\n",
        "\n",
        "* train_online: applies the forward and backward method to the network for a specified number of epochs **one example at a time**.\n",
        " * parameter:\n",
        "   * x_train: input matrix $X$;\n",
        "   * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
        "   * epochs: number of the iterations of the training algorithm;\n",
        "   * learning_rate;\n",
        "   * loss_function;\n",
        "   * loss_function_derivative.\n",
        "\n",
        "* train_minibatch: applies the forward and backward method to the network for a specified number of epochs **to batches of $l' < l$** examples.\n",
        " * parameter:\n",
        "    * x_train: input matrix $X$;\n",
        "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
        "    * epochs: number of the iterations of the training algorithm;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "\n",
        "    def data_split(self, x_tot, k, step_cycle):\n",
        "        '''# randomization of the input matrix\n",
        "        indices = np.arange(num_samples) # creates an array from 0 to num_samples - 1\n",
        "        np.random.shuffle(indices) # shuffling the indices\n",
        "        x_tot = x_tot[indices] # re-ordering of the rows according to the new indices'''\n",
        "\n",
        "        # splitting of the data batch into x_train_val and x_test\n",
        "        num_samples = x_tot.shape[0]\n",
        "        x_train_val = x_tot[:int(0.8 * num_samples)] # training set and validation set make up 80% of the original data set\n",
        "        self.x_test = x_tot[int(0.8 * num_samples):] # test set makes up 20% of the original data set\n",
        "\n",
        "        # splitting of the x_train_val batch into x_train and x_val\n",
        "        if k == 1: # hold-out cross validation\n",
        "            self.x_train = x_train_val[:int(0.75 * x_train_val.shape[0])] # training set makes up 60% of the original data set\n",
        "            self.x_val = x_train_val[int(0.75 * x_train_val.shape[0]):] # validation set makes up 20% of the original data\n",
        "        else: # k-fold cross validation\n",
        "            fold_size = int(x_train_val.shape[0] / k) # number of rows per fold\n",
        "            self.x_train = np.concatenate([x_train_val[:step_cycle*fold_size], self.x_train_val[(step_cycle+1)*fold_size:]]) # training set\n",
        "            self.x_val = x_train_val[step_cycle*fold_size:(step_cycle+1)*fold_size] # validation set\n",
        "\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate)\n",
        "\n",
        "\n",
        "    def train_online(self, x_train, target, epochs, learning_rate, loss_function, loss_function_derivative):\n",
        "        for epoch in range(epochs):\n",
        "          epoch_loss = 0\n",
        "\n",
        "          for x_train_row, target_row in zip(x_train, target):\n",
        "\n",
        "            x_train_row = x_train_row.reshape(1, -1)\n",
        "            target_row = target_row.reshape(1, -1)\n",
        "\n",
        "            # Forward propagation\n",
        "            predictions = self.forward(x_train_row) # predictions = output of the output layer\n",
        "\n",
        "            # Compute loss and loss gradient for backward function\n",
        "            loss = loss_function(target_row, predictions)\n",
        "            loss_gradient = loss_function_derivative(target_row, predictions)\n",
        "            epoch_loss += loss  # accumulates the losses for each example\n",
        "\n",
        "            # Backward propagation\n",
        "            self.backward(loss_gradient, learning_rate)\n",
        "\n",
        "          # computation of the average loss per epoch\n",
        "          average_epoch_loss = epoch_loss / len(x_train)\n",
        "          print(f\"ONLINE: epoch #{epoch}, Average Loss: {average_epoch_loss}\")\n",
        "\n",
        "\n",
        "    def train_minibatch(self, x_train, target, epochs, learning_rate, loss_function, loss_function_derivative, batch_size):\n",
        "        num_samples = x_train.shape[0] # selection of the number of rows\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          epoch_loss = 0\n",
        "\n",
        "          # the rows of the input matrix are randomized in order to have different examples in each mini-batch for each epoch\n",
        "          indices = np.arange(num_samples) # creates an array from 0 to num_samples - 1\n",
        "          np.random.shuffle(indices) # shuffling the indices\n",
        "          x_train = x_train[indices] # re-ordering of the rows according to the new indices\n",
        "          target = target[indices] # same but for the targets\n",
        "\n",
        "          # process data in batches\n",
        "          for i in range(0, num_samples, batch_size): # even if the last mini-batch does not have size equal to batch_size it is processed anyway\n",
        "            x_batch = x_train[i:i+batch_size]\n",
        "            target_batch = target[i:i+batch_size]\n",
        "\n",
        "\n",
        "           # Forward propagation\n",
        "            predictions = self.forward(x_batch) # predictions = output of the output layer\n",
        "\n",
        "            # Compute loss and loss gradient for backward function\n",
        "            loss = loss_function(target_batch, predictions)\n",
        "            loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "            epoch_loss += np.sum(loss)  # accumulates the loss for all the examples in the mini-batch for each mini-batch\n",
        "\n",
        "            # Backward propagation\n",
        "            self.backward(loss_gradient, learning_rate)\n",
        "\n",
        "          # computation of the average loss per epoch\n",
        "          average_epoch_loss = epoch_loss / num_samples\n",
        "          print(f\"MINIBATCH: epoch #{epoch}, Average Loss: {average_epoch_loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "#Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.random.rand(20, 3)\n",
        "target = np.random.rand(20, 2)\n",
        "\n",
        "layer_one = Layer(3, 2, linear, d_linear)\n",
        "layer_two = Layer(2, 2, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "NN.data_split(x, 4, 1)\n",
        "print(f'validation set {NN.x_val}')\n",
        "print(f'training set {NN.x_train}')\n",
        "print(f'training + validation {NN.x_train_val}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation set [[0.03438852 0.9093204  0.25877998]\n",
            " [0.54671028 0.18485446 0.96958463]\n",
            " [0.37454012 0.95071431 0.73199394]\n",
            " [0.60754485 0.17052412 0.06505159]]\n",
            "training set [[0.19598286 0.04522729 0.32533033]\n",
            " [0.05808361 0.86617615 0.60111501]\n",
            " [0.13949386 0.29214465 0.36636184]\n",
            " [0.66252228 0.31171108 0.52006802]\n",
            " [0.43194502 0.29122914 0.61185289]\n",
            " [0.77513282 0.93949894 0.89482735]\n",
            " [0.70807258 0.02058449 0.96990985]\n",
            " [0.94888554 0.96563203 0.80839735]\n",
            " [0.18340451 0.30424224 0.52475643]\n",
            " [0.83244264 0.21233911 0.18182497]\n",
            " [0.45606998 0.78517596 0.19967378]\n",
            " [0.44015249 0.12203823 0.49517691]]\n",
            "training + validation [[0.19598286 0.04522729 0.32533033]\n",
            " [0.05808361 0.86617615 0.60111501]\n",
            " [0.13949386 0.29214465 0.36636184]\n",
            " [0.66252228 0.31171108 0.52006802]\n",
            " [0.03438852 0.9093204  0.25877998]\n",
            " [0.54671028 0.18485446 0.96958463]\n",
            " [0.37454012 0.95071431 0.73199394]\n",
            " [0.60754485 0.17052412 0.06505159]\n",
            " [0.43194502 0.29122914 0.61185289]\n",
            " [0.77513282 0.93949894 0.89482735]\n",
            " [0.70807258 0.02058449 0.96990985]\n",
            " [0.94888554 0.96563203 0.80839735]\n",
            " [0.18340451 0.30424224 0.52475643]\n",
            " [0.83244264 0.21233911 0.18182497]\n",
            " [0.45606998 0.78517596 0.19967378]\n",
            " [0.44015249 0.12203823 0.49517691]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT1dN_rEoKPO",
        "outputId": "55889d57-b879-4df4-bc0e-062e07f0338d",
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MINIBATCH: epoch #0, Average Loss: 0.1812626781214725\n",
            "MINIBATCH: epoch #1, Average Loss: 0.1697308739326982\n",
            "MINIBATCH: epoch #2, Average Loss: 0.16813324213355693\n",
            "MINIBATCH: epoch #3, Average Loss: 0.167909063796055\n",
            "MINIBATCH: epoch #4, Average Loss: 0.16769058338936446\n",
            "MINIBATCH: epoch #5, Average Loss: 0.16748801198416394\n",
            "MINIBATCH: epoch #6, Average Loss: 0.16766824920313117\n",
            "MINIBATCH: epoch #7, Average Loss: 0.1675805069710757\n",
            "MINIBATCH: epoch #8, Average Loss: 0.1689332367348612\n",
            "MINIBATCH: epoch #9, Average Loss: 0.1685914707561085\n",
            "ONLINE: epoch #0, Average Loss: 0.18308535767693287\n",
            "ONLINE: epoch #1, Average Loss: 0.170414573400587\n",
            "ONLINE: epoch #2, Average Loss: 0.16823397283664485\n",
            "ONLINE: epoch #3, Average Loss: 0.1674839462665076\n",
            "ONLINE: epoch #4, Average Loss: 0.16717077679125641\n",
            "ONLINE: epoch #5, Average Loss: 0.1670230640723701\n",
            "ONLINE: epoch #6, Average Loss: 0.1669464292220957\n",
            "ONLINE: epoch #7, Average Loss: 0.1669038083643578\n",
            "ONLINE: epoch #8, Average Loss: 0.1668794691095798\n",
            "ONLINE: epoch #9, Average Loss: 0.16686633362324652\n"
          ]
        }
      ],
      "source": [
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.random.rand(1000, 3)\n",
        "target = np.random.rand(1000, 2)\n",
        "\n",
        "\n",
        "layer_one1 = Layer(3, 2, linear, d_linear)\n",
        "layer_one2 = Layer(3, 2, linear, d_linear)\n",
        "layer_two1 = Layer(2, 2, linear, d_linear)\n",
        "layer_two2 = Layer(2, 2, linear, d_linear)\n",
        "\n",
        "NN1 = NeuralNetwork()\n",
        "NN1.add_layer(layer_one1)\n",
        "NN1.add_layer(layer_two1)\n",
        "NN2 = NeuralNetwork()\n",
        "NN2.add_layer(layer_one2)\n",
        "NN2.add_layer(layer_two2)\n",
        "NN1.train_minibatch(x, target, 10, 0.01, mean_squared_error, d_mean_squared_error, 3)\n",
        "NN2.train_online(x, target, 10, 0.01, mean_squared_error, d_mean_squared_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rNUYJgrTsFNC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}