{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zBZ8ZOoCBL"
      },
      "source": [
        "# Cose da fare\n",
        "* tocca fa adam\n",
        "* inserire documentazione per le regolarizzazioni (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
        "* inserire loss function per problemi di classificazione: BCE o altro\n",
        "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
        "* analisi training error vs validation error vs test error\n",
        "### Novelties\n",
        "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
        "* momentum -> OBBLIGATORIO\n",
        "* early stopping\n",
        "* learning rate variabile/adam\n",
        "* standarditation e normalization (FACOLTATIVO FORSE)\n",
        "# Cose da fare secondo le (!) Micheli\n",
        "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting\n",
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qv1wwlgWsFM8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n",
        "\n",
        "# np.vectorize returns an object that acts like pyfunc, but takes arrays as input\n",
        "d_ReLU = np.vectorize(d_ReLU)\n",
        "d_leaky_relu = np.vectorize(d_leaky_relu)\n",
        "ELU = np.vectorize(ELU)\n",
        "d_ELU = np.vectorize(d_ELU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return - 2 * (y_true - y_pred)\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return - (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return - (y_true - y_pred) if(np.abs(y_true-y_pred)<=delta) else - delta * np.sign(y_true-y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters:**\n",
        " * input_size: $k_{i-1}$;\n",
        " * output_size: $k_i$;\n",
        " * activation_function;\n",
        " * activation_derivative.\n",
        "\n",
        "**Constructor attributes:**\n",
        "* self.input_size = input_size;\n",
        "* self.output_size = output_size;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative;\n",
        "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
        "\n",
        "**Methods :**\n",
        "\n",
        "* **initialize_weights**: initialize weights and biases\n",
        "  * attributes:\n",
        "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
        "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
        "    \n",
        "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
        "  * parameter:\n",
        "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
        "  * attributes:\n",
        "    * self.input: input_array;\n",
        "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
        "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
        "  \n",
        "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        "  * parameters:\n",
        "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "    * learning_rate.\n",
        "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "H8Ap9rLxLlNU",
        "tags": [],
        "outputId": "4a44ac6b-e9b8-4bf8-95bc-284e250a1e36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\\n        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\\n\\n        # tikhonov and lasso implementation   \\n        if (reg_type=='tikhonov'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\\n        elif (reg_type=='lasso'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\\n        elif (reg_type=='elastic'): # lasso + tikhonov regularization\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights) \\n        \\n        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\\n        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\\n        return sum_delta_weights\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "        self.initialize_weights()\n",
        "        self.t = 1 # number of iterations for adam\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialization of the parameters of the network\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
        "        self.biases = np.zeros((1, self.output_size))\n",
        "\n",
        "        # Initialization of the parameters for Nesterov optimization\n",
        "        self.velocity_weights = np.zeros_like(self.weights) # zeros array with dim = dim(self.weights)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        # Initialization of the parameters for Adam optimization\n",
        "        self.m_weights = np.zeros_like(self.weights)\n",
        "        self.v_weights = np.zeros_like(self.weights)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        self.delta = - d_Ep * self.activation_derivative(self.net)\n",
        "        if opt_type == 'NAG':\n",
        "          weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights in order to use the Nesterov momentum,\n",
        "                                                                        # used to evaluate the gradient after the momentum is applied\n",
        "          bias_pred = self.biases + momentum * self.velocity_biases # same thing for the biases\n",
        "          net_pred = np.dot(self.input, weights_pred) + bias_pred  #  Net has been computed with respect to the predicted weights and the predicted biases\n",
        "          delta_pred = - d_Ep * self.activation_derivative(net_pred)  # Loss gradient with respect to net, minus sign due to the definition\n",
        "          grad_weights = learning_rate_w * np.dot(self.input.T, delta_pred)  # Loss gradient multiplied by the learning rate.\n",
        "                                                                          # The gradient has been computed with respect to the predicted weights and biases\n",
        "          self.velocity_weights = momentum * self.velocity_weights + grad_weights  # Delta w new before regularization\n",
        "\n",
        "          # Tikhonov, lasso, elastic regularization. To be implemented during optimization\n",
        "          if reg_type == 'tikhonov':\n",
        "              self.velocity_weights -= 2 * Lambda_t * weights_pred # learning rule of tikhonov regularization,\n",
        "                                                                   # the minus sign is due to the application of gradient descent algorithm.\n",
        "                                                                   # We use the predicted weights in the Nesterov optimization in the regularization too\n",
        "          elif reg_type == 'lasso':\n",
        "              self.velocity_weights -= Lambda_l * np.sign(weights_pred) # learning rule of lasso regularization\n",
        "          elif reg_type == 'elastic':\n",
        "              self.velocity_weights -= (2 * Lambda_t * weights_pred + Lambda_l * np.sign(weights_pred)) # lasso + tikhonov regularization\n",
        "\n",
        "\n",
        "          self.weights += self.velocity_weights  # Updating the weights\n",
        "          self.velocity_biases = momentum * self.velocity_biases + learning_rate_b * np.sum(delta_pred, axis=0, keepdims=True)\n",
        "          self.biases += self.velocity_biases # Updating the biases\n",
        "\n",
        "        elif opt_type == 'adam':\n",
        "          # In order to include the regularization term in the optimization process:\n",
        "          if reg_type == 'tikhonov':\n",
        "              reg_term = 2 * Lambda_t * self.weights # learning rule of tikhonov regularization,\n",
        "                                                                  # the minus sign is due to the application of gradient descent algorithm.\n",
        "                                                                  # We use the predicted weights in the Nesterov optimization in the regularization too\n",
        "          elif reg_type == 'lasso':\n",
        "              reg_term = Lambda_l * np.sign(self.weights) # learning rule of lasso regularization\n",
        "          elif reg_type == 'elastic':\n",
        "              reg_term = (2 * Lambda_t * self.weights + Lambda_l * np.sign(self.weights)) # lasso + tikhonov regularization\n",
        "\n",
        "\n",
        "\n",
        "          self.m_weights = beta_1 * self.m_weights + (1 - beta_1) * (- np.dot(self.input.T, self.delta) - reg_term) # np.dot(self.input.T, delta) is dLoss/dw,\n",
        "                                                                                                                    # since self.delta is defined with a minus sign\n",
        "                                                                                                                    # and the formula is with a plus sign, we put a minus sign\n",
        "                                                                                                                    # in front of np.dot(xxx)\n",
        "          self.v_weights = beta_2* self.v_weights + (1 - beta_2) * ((np.dot(self.input.T, self.delta) + reg_term)**2) # here we have a plus sign in front of (1 - beta_2) since\n",
        "                                                                                                                      # self.delta is squared\n",
        "          m_weights_hat = self.m_weights / (1 - beta_1**self.t)\n",
        "          v_weights_hat = self.v_weights / (1 - beta_2**self.t)\n",
        "\n",
        "          self.m_biases = beta_1 * self.m_biases - (1 - beta_1) * np.sum(self.delta, axis=0, keepdims=True)\n",
        "          self.v_biases = beta_2* self.v_biases + (1 - beta_2) * np.sum(self.delta**2, axis=0, keepdims=True)\n",
        "          m_biases_hat = self.m_biases / (1 - beta_1**self.t)\n",
        "          v_biases_hat = self.v_biases / (1 - beta_2**self.t)\n",
        "\n",
        "          self.weights -= learning_rate_w * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "          self.biases -= learning_rate_b * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)\n",
        "\n",
        "\n",
        "        sum_delta_weights = np.dot(self.delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights\n",
        "\n",
        "\n",
        "'''\n",
        "    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\n",
        "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
        "\n",
        "        # tikhonov and lasso implementation\n",
        "        if (reg_type=='tikhonov'):\n",
        "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\n",
        "        elif (reg_type=='lasso'):\n",
        "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\n",
        "        elif (reg_type=='elastic'): # lasso + tikhonov regularization\n",
        "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights)\n",
        "\n",
        "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
        "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
        "   * parameters:\n",
        "     * x_tot: total data given as input;\n",
        "     * target: total data labels given as input;\n",
        "     * test_split: percentile of test set with respect to the total data.\n",
        "    * return->:\n",
        "      * x_train_val: training & validation set extracted from input data;\n",
        "      * target_train_val: training & validation set labels;\n",
        "      * x_test_val: test set extracted from input data;\n",
        "      * target_test_val: test set for input data labels.\n",
        "\n",
        "     \n",
        "* **add_layer**: appends a layer to the empty list self.layers\n",
        "   * parameter:\n",
        "     * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        "  * parameter:\n",
        "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        "  * return -> input = $o_i$ for layer $L_i$.\n",
        "  \n",
        "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        "  * parameters:\n",
        "    * d_Ep;\n",
        "    * learning_rate.\n",
        "\n",
        "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
        "\n",
        "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
        "  * parameters:\n",
        "    * x_train: set of the original dataset used for training;\n",
        "    * target_train: labels corresponding to the training set;\n",
        "    * x_val: set of the original dataset used for validation;\n",
        "    * target_val: labels corresponding to the validation set;\n",
        "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
        "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
        "    * loss_function: hyperparameter;\n",
        "    * loss_function_derivative: hyperparameter;\n",
        "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
        "\n",
        "* **train_val**: actual training and validation process.\n",
        "  * parameters:\n",
        "    * x_train_val;\n",
        "    * target_train_val;\n",
        "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
        "    * epochs;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def data_split(self, x_tot, target, test_split):\n",
        "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
        "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
        "\n",
        "        x_test = x_tot[:test_size]\n",
        "        target_test = target[:test_size]\n",
        "        x_train_val = x_tot[test_size:]\n",
        "        target_train_val = target[test_size:]\n",
        "\n",
        "        return x_train_val, target_train_val, x_test, target_test\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "\n",
        "    def reinitialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.initialize_weights() # does it layer-by-layer\n",
        "\n",
        "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        train_error_epoch = np.zeros(epochs)\n",
        "        val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
        "            epoch_val_loss = 0\n",
        "\n",
        "            # shuffling training data before splitting it into batches.\n",
        "            # done in order to avoid reinforcing neurons in the same way\n",
        "            # in different epochs due to invisible patterns in the data\n",
        "            train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(train_indices)\n",
        "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
        "            target_train = target_train[train_indices]\n",
        "\n",
        "            # if batch_size=1 we get the online version,\n",
        "            # else we get mini-batch version with batches of size batch_size\n",
        "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
        "                                                             # Even if the number of examples is not divisible\n",
        "                                                             # for batch_size the last, smaller batch is processed anyway\n",
        "                Layer.t += 1 # number of iterations for adam\n",
        "                x_batch = x_train[i:i+batch_size]\n",
        "                target_batch = target_train[i:i+batch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
        "                # computing loss and gradient\n",
        "                loss = loss_function(target_batch, predictions)\n",
        "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "                epoch_tr_loss += np.sum(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(loss_gradient, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "\n",
        "            # validation\n",
        "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
        "            val_loss = loss_function(target_val, val_predictions)\n",
        "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished.\n",
        "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
        "            # Store average errors for the epoch\n",
        "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
        "            val_error_epoch[epoch] = epoch_val_loss\n",
        "\n",
        "        return train_error_epoch, val_error_epoch\n",
        "\n",
        "\n",
        "    def train_val(self, x_train_val, target_train_val, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        num_samples = x_train_val.shape[0]\n",
        "        fold_size = num_samples // K\n",
        "\n",
        "        # error storage for averaging\n",
        "        avg_train_error_epoch = np.zeros(epochs)\n",
        "        avg_val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        if K==1: # hold-out validation\n",
        "            train_indices = np.arange(0, int(0.75*num_samples)) # training set is 75% of the training & validation set\n",
        "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type\n",
        "            ) # computation of errors via train_val_setup method\n",
        "            return train_error_epoch, val_error_epoch\n",
        "\n",
        "\n",
        "        for k in range(K):\n",
        "            # creating fold indices\n",
        "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
        "                                                                        # It contains the indices of the examples used in validation set for\n",
        "                                                                        # this fold.\n",
        "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples -\n",
        "                                                                              # len(val_indices). It contains the indices of all the examples\n",
        "                                                                              # but the ones used in the validation set for this fold.\n",
        "                                                                              # It corresponds to the training set for the current fold.\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
        "\n",
        "            # re-initializing weights for each fold\n",
        "            self.reinitialize_weights()\n",
        "            Layer.t = 1\n",
        "            # training on the current fold\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type\n",
        "            )\n",
        "\n",
        "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
        "            avg_train_error_epoch += train_error_epoch\n",
        "            avg_val_error_epoch += val_error_epoch\n",
        "\n",
        "            print(f\"Fold {k+1} completed., t = {Layer.t}\")\n",
        "\n",
        "        # averaging errors across all folds\n",
        "        avg_train_error_epoch /= K\n",
        "        avg_val_error_epoch /= K\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
        "\n",
        "        return avg_train_error_epoch, avg_val_error_epoch\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "# Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "6a56620f-9e6a-40f5-a017-ef2283c6f211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 completed., t = 8001\n",
            "Fold 2 completed., t = 8001\n",
            "Fold 3 completed., t = 8001\n",
            "Fold 4 completed., t = 8001\n",
            "Fold 5 completed., t = 8001\n",
            "Epoch 1/1000, Avg Training Loss: 0.9981258823419761, Avg Validation Loss: 190.99797778508332\n",
            "Epoch 2/1000, Avg Training Loss: 0.9532236045349741, Avg Validation Loss: 181.96055202636765\n",
            "Epoch 3/1000, Avg Training Loss: 0.9093199842649391, Avg Validation Loss: 173.78438612990982\n",
            "Epoch 4/1000, Avg Training Loss: 0.8707481948342636, Avg Validation Loss: 166.7206860273152\n",
            "Epoch 5/1000, Avg Training Loss: 0.8370153440991739, Avg Validation Loss: 160.53532695245974\n",
            "Epoch 6/1000, Avg Training Loss: 0.8074369265378963, Avg Validation Loss: 155.07342169642055\n",
            "Epoch 7/1000, Avg Training Loss: 0.7816460703925909, Avg Validation Loss: 150.42591540626873\n",
            "Epoch 8/1000, Avg Training Loss: 0.7587492019961585, Avg Validation Loss: 146.07532062929516\n",
            "Epoch 9/1000, Avg Training Loss: 0.7375800248256824, Avg Validation Loss: 142.16195535082795\n",
            "Epoch 10/1000, Avg Training Loss: 0.7190388089991089, Avg Validation Loss: 138.68395494865817\n",
            "Epoch 11/1000, Avg Training Loss: 0.7021004559572707, Avg Validation Loss: 135.5458702586489\n",
            "Epoch 12/1000, Avg Training Loss: 0.6865888051105042, Avg Validation Loss: 132.627918351703\n",
            "Epoch 13/1000, Avg Training Loss: 0.6724479443625245, Avg Validation Loss: 129.91992580241828\n",
            "Epoch 14/1000, Avg Training Loss: 0.6591924562726149, Avg Validation Loss: 127.42487646724669\n",
            "Epoch 15/1000, Avg Training Loss: 0.6468937291081289, Avg Validation Loss: 125.09375474779121\n",
            "Epoch 16/1000, Avg Training Loss: 0.6353490412653414, Avg Validation Loss: 122.94653364032617\n",
            "Epoch 17/1000, Avg Training Loss: 0.6246659103826308, Avg Validation Loss: 120.84900888804982\n",
            "Epoch 18/1000, Avg Training Loss: 0.614384921338202, Avg Validation Loss: 118.90550001881871\n",
            "Epoch 19/1000, Avg Training Loss: 0.6048235878978026, Avg Validation Loss: 117.13586846048482\n",
            "Epoch 20/1000, Avg Training Loss: 0.5960484411329032, Avg Validation Loss: 115.48257458360983\n",
            "Epoch 21/1000, Avg Training Loss: 0.5877928228050502, Avg Validation Loss: 113.89995144840094\n",
            "Epoch 22/1000, Avg Training Loss: 0.5800648161537113, Avg Validation Loss: 112.46872504244357\n",
            "Epoch 23/1000, Avg Training Loss: 0.5730020083873856, Avg Validation Loss: 111.1070184505843\n",
            "Epoch 24/1000, Avg Training Loss: 0.5661479949485048, Avg Validation Loss: 109.80634781997261\n",
            "Epoch 25/1000, Avg Training Loss: 0.5596462484197303, Avg Validation Loss: 108.5704291915533\n",
            "Epoch 26/1000, Avg Training Loss: 0.5535627622193653, Avg Validation Loss: 107.41894676161601\n",
            "Epoch 27/1000, Avg Training Loss: 0.5476277331179886, Avg Validation Loss: 106.2656803678199\n",
            "Epoch 28/1000, Avg Training Loss: 0.5419648504158564, Avg Validation Loss: 105.20511795837135\n",
            "Epoch 29/1000, Avg Training Loss: 0.536728947963955, Avg Validation Loss: 104.20855901642012\n",
            "Epoch 30/1000, Avg Training Loss: 0.5317448064396495, Avg Validation Loss: 103.26317037041943\n",
            "Epoch 31/1000, Avg Training Loss: 0.5270580465234913, Avg Validation Loss: 102.38414169170686\n",
            "Epoch 32/1000, Avg Training Loss: 0.5225337911225545, Avg Validation Loss: 101.44516231205202\n",
            "Epoch 33/1000, Avg Training Loss: 0.517626911502634, Avg Validation Loss: 100.52711499641183\n",
            "Epoch 34/1000, Avg Training Loss: 0.5131495322626022, Avg Validation Loss: 99.69600526674633\n",
            "Epoch 35/1000, Avg Training Loss: 0.5090364938909225, Avg Validation Loss: 98.93456796182451\n",
            "Epoch 36/1000, Avg Training Loss: 0.5052058217576121, Avg Validation Loss: 98.20509857316202\n",
            "Epoch 37/1000, Avg Training Loss: 0.5015694114092242, Avg Validation Loss: 97.49902617887388\n",
            "Epoch 38/1000, Avg Training Loss: 0.49804621546322075, Avg Validation Loss: 96.84098368455551\n",
            "Epoch 39/1000, Avg Training Loss: 0.4947804018323004, Avg Validation Loss: 96.27399746991549\n",
            "Epoch 40/1000, Avg Training Loss: 0.4920612590304206, Avg Validation Loss: 95.72376869982752\n",
            "Epoch 41/1000, Avg Training Loss: 0.4892192208752122, Avg Validation Loss: 95.18649494795737\n",
            "Epoch 42/1000, Avg Training Loss: 0.4864152447012578, Avg Validation Loss: 94.6411235825896\n",
            "Epoch 43/1000, Avg Training Loss: 0.48363419708245964, Avg Validation Loss: 94.08172052271458\n",
            "Epoch 44/1000, Avg Training Loss: 0.4808126718059993, Avg Validation Loss: 93.55364053279514\n",
            "Epoch 45/1000, Avg Training Loss: 0.47809451067856656, Avg Validation Loss: 93.03547237108674\n",
            "Epoch 46/1000, Avg Training Loss: 0.47551762352299415, Avg Validation Loss: 92.53292378471619\n",
            "Epoch 47/1000, Avg Training Loss: 0.47297063498889313, Avg Validation Loss: 92.0499149357081\n",
            "Epoch 48/1000, Avg Training Loss: 0.4705340462878456, Avg Validation Loss: 91.58617760385482\n",
            "Epoch 49/1000, Avg Training Loss: 0.46819086216681255, Avg Validation Loss: 91.16709892794856\n",
            "Epoch 50/1000, Avg Training Loss: 0.46606585290608266, Avg Validation Loss: 90.74297732046554\n",
            "Epoch 51/1000, Avg Training Loss: 0.4639541344028808, Avg Validation Loss: 90.32574101463527\n",
            "Epoch 52/1000, Avg Training Loss: 0.4618157706132344, Avg Validation Loss: 89.93590440896588\n",
            "Epoch 53/1000, Avg Training Loss: 0.45983008927650965, Avg Validation Loss: 89.53787797750674\n",
            "Epoch 54/1000, Avg Training Loss: 0.45781338488674933, Avg Validation Loss: 89.15531451692415\n",
            "Epoch 55/1000, Avg Training Loss: 0.45585701414922747, Avg Validation Loss: 88.77705857258246\n",
            "Epoch 56/1000, Avg Training Loss: 0.45402888882583825, Avg Validation Loss: 88.44850805209163\n",
            "Epoch 57/1000, Avg Training Loss: 0.4522571473231515, Avg Validation Loss: 88.09044208866177\n",
            "Epoch 58/1000, Avg Training Loss: 0.45042471099126474, Avg Validation Loss: 87.71605979311711\n",
            "Epoch 59/1000, Avg Training Loss: 0.44857544188268933, Avg Validation Loss: 87.38537897185336\n",
            "Epoch 60/1000, Avg Training Loss: 0.44686144735514105, Avg Validation Loss: 87.04194422826757\n",
            "Epoch 61/1000, Avg Training Loss: 0.44512604581095216, Avg Validation Loss: 86.7186437972064\n",
            "Epoch 62/1000, Avg Training Loss: 0.44352970150751725, Avg Validation Loss: 86.41034711864215\n",
            "Epoch 63/1000, Avg Training Loss: 0.4418466683429428, Avg Validation Loss: 86.05665559635989\n",
            "Epoch 64/1000, Avg Training Loss: 0.43999806046851603, Avg Validation Loss: 85.71588606839168\n",
            "Epoch 65/1000, Avg Training Loss: 0.43829649379221086, Avg Validation Loss: 85.37497332672444\n",
            "Epoch 66/1000, Avg Training Loss: 0.4365555830861988, Avg Validation Loss: 85.03293509165235\n",
            "Epoch 67/1000, Avg Training Loss: 0.43485005349166317, Avg Validation Loss: 84.71396814745195\n",
            "Epoch 68/1000, Avg Training Loss: 0.4332756243142498, Avg Validation Loss: 84.4067289171281\n",
            "Epoch 69/1000, Avg Training Loss: 0.4316352929695169, Avg Validation Loss: 84.07008444925259\n",
            "Epoch 70/1000, Avg Training Loss: 0.4299175328204818, Avg Validation Loss: 83.73203544378845\n",
            "Epoch 71/1000, Avg Training Loss: 0.42824111257273484, Avg Validation Loss: 83.40394687583668\n",
            "Epoch 72/1000, Avg Training Loss: 0.42652247359902334, Avg Validation Loss: 83.06853233366141\n",
            "Epoch 73/1000, Avg Training Loss: 0.4248245100215231, Avg Validation Loss: 82.7432722092839\n",
            "Epoch 74/1000, Avg Training Loss: 0.42314590247708594, Avg Validation Loss: 82.40057282237747\n",
            "Epoch 75/1000, Avg Training Loss: 0.42138402331953656, Avg Validation Loss: 82.05984308950414\n",
            "Epoch 76/1000, Avg Training Loss: 0.41977984827307935, Avg Validation Loss: 81.77275655830417\n",
            "Epoch 77/1000, Avg Training Loss: 0.41836601691239067, Avg Validation Loss: 81.49121091986004\n",
            "Epoch 78/1000, Avg Training Loss: 0.4168311610857049, Avg Validation Loss: 81.16619459857094\n",
            "Epoch 79/1000, Avg Training Loss: 0.41509725798010705, Avg Validation Loss: 80.83101199162842\n",
            "Epoch 80/1000, Avg Training Loss: 0.41342744234707784, Avg Validation Loss: 80.50470903667659\n",
            "Epoch 81/1000, Avg Training Loss: 0.4117931318083636, Avg Validation Loss: 80.17508475413673\n",
            "Epoch 82/1000, Avg Training Loss: 0.4101407610984576, Avg Validation Loss: 79.86016219024347\n",
            "Epoch 83/1000, Avg Training Loss: 0.408572918465067, Avg Validation Loss: 79.55987852637395\n",
            "Epoch 84/1000, Avg Training Loss: 0.40705476053200695, Avg Validation Loss: 79.26519591922762\n",
            "Epoch 85/1000, Avg Training Loss: 0.4055947557591458, Avg Validation Loss: 78.97555285788955\n",
            "Epoch 86/1000, Avg Training Loss: 0.4041105552333198, Avg Validation Loss: 78.69310040828147\n",
            "Epoch 87/1000, Avg Training Loss: 0.40271916611495084, Avg Validation Loss: 78.42565020633677\n",
            "Epoch 88/1000, Avg Training Loss: 0.40134508829201754, Avg Validation Loss: 78.15423707879815\n",
            "Epoch 89/1000, Avg Training Loss: 0.39995490495814495, Avg Validation Loss: 77.87118985476621\n",
            "Epoch 90/1000, Avg Training Loss: 0.3985098477285743, Avg Validation Loss: 77.58901322268277\n",
            "Epoch 91/1000, Avg Training Loss: 0.3970953241570471, Avg Validation Loss: 77.3158005683386\n",
            "Epoch 92/1000, Avg Training Loss: 0.39570407939588187, Avg Validation Loss: 77.0471779988022\n",
            "Epoch 93/1000, Avg Training Loss: 0.394339498942281, Avg Validation Loss: 76.77691776238242\n",
            "Epoch 94/1000, Avg Training Loss: 0.3929449711795081, Avg Validation Loss: 76.48866923071097\n",
            "Epoch 95/1000, Avg Training Loss: 0.3915267874444969, Avg Validation Loss: 76.22649984967754\n",
            "Epoch 96/1000, Avg Training Loss: 0.39025918162978046, Avg Validation Loss: 75.97894104043465\n",
            "Epoch 97/1000, Avg Training Loss: 0.3889676424871458, Avg Validation Loss: 75.72479654753762\n",
            "Epoch 98/1000, Avg Training Loss: 0.38766748784199956, Avg Validation Loss: 75.46209902416899\n",
            "Epoch 99/1000, Avg Training Loss: 0.3863152010477083, Avg Validation Loss: 75.19017491532688\n",
            "Epoch 100/1000, Avg Training Loss: 0.38496536080043436, Avg Validation Loss: 74.92446673831742\n",
            "Epoch 101/1000, Avg Training Loss: 0.3836242487181142, Avg Validation Loss: 74.66691995299202\n",
            "Epoch 102/1000, Avg Training Loss: 0.3823477281939114, Avg Validation Loss: 74.41522048981159\n",
            "Epoch 103/1000, Avg Training Loss: 0.3810649437390544, Avg Validation Loss: 74.16002370680769\n",
            "Epoch 104/1000, Avg Training Loss: 0.3797602132529145, Avg Validation Loss: 73.89680336360897\n",
            "Epoch 105/1000, Avg Training Loss: 0.37843314708458164, Avg Validation Loss: 73.6348239543659\n",
            "Epoch 106/1000, Avg Training Loss: 0.3771035352345811, Avg Validation Loss: 73.36872407179405\n",
            "Epoch 107/1000, Avg Training Loss: 0.375764288665766, Avg Validation Loss: 73.11463934615192\n",
            "Epoch 108/1000, Avg Training Loss: 0.3745160640608755, Avg Validation Loss: 72.87322002093879\n",
            "Epoch 109/1000, Avg Training Loss: 0.37330180227844506, Avg Validation Loss: 72.63556098702716\n",
            "Epoch 110/1000, Avg Training Loss: 0.3721064208735075, Avg Validation Loss: 72.39606407623327\n",
            "Epoch 111/1000, Avg Training Loss: 0.37089234858494263, Avg Validation Loss: 72.16062325121223\n",
            "Epoch 112/1000, Avg Training Loss: 0.3697133767138989, Avg Validation Loss: 71.93125103155896\n",
            "Epoch 113/1000, Avg Training Loss: 0.3685600198816115, Avg Validation Loss: 71.70472230451699\n",
            "Epoch 114/1000, Avg Training Loss: 0.3674140302476886, Avg Validation Loss: 71.46874882067988\n",
            "Epoch 115/1000, Avg Training Loss: 0.36622190503000257, Avg Validation Loss: 71.23612849452432\n",
            "Epoch 116/1000, Avg Training Loss: 0.3650341820405594, Avg Validation Loss: 71.00091543839888\n",
            "Epoch 117/1000, Avg Training Loss: 0.3638615723993757, Avg Validation Loss: 70.77161550941425\n",
            "Epoch 118/1000, Avg Training Loss: 0.36271263910762747, Avg Validation Loss: 70.54573108951908\n",
            "Epoch 119/1000, Avg Training Loss: 0.36161037692669695, Avg Validation Loss: 70.33708810858404\n",
            "Epoch 120/1000, Avg Training Loss: 0.36054609202424925, Avg Validation Loss: 70.12834185158871\n",
            "Epoch 121/1000, Avg Training Loss: 0.3594715806794945, Avg Validation Loss: 69.910826761167\n",
            "Epoch 122/1000, Avg Training Loss: 0.35836420459204404, Avg Validation Loss: 69.69920436371085\n",
            "Epoch 123/1000, Avg Training Loss: 0.35732325204266585, Avg Validation Loss: 69.49872627397428\n",
            "Epoch 124/1000, Avg Training Loss: 0.3563247689643417, Avg Validation Loss: 69.29515190868669\n",
            "Epoch 125/1000, Avg Training Loss: 0.35528022507728674, Avg Validation Loss: 69.08779373694804\n",
            "Epoch 126/1000, Avg Training Loss: 0.35423948729185406, Avg Validation Loss: 68.89128598499953\n",
            "Epoch 127/1000, Avg Training Loss: 0.35325585934650555, Avg Validation Loss: 68.70010403133946\n",
            "Epoch 128/1000, Avg Training Loss: 0.35230807127467834, Avg Validation Loss: 68.50476677905213\n",
            "Epoch 129/1000, Avg Training Loss: 0.3512742470217891, Avg Validation Loss: 68.2984204969835\n",
            "Epoch 130/1000, Avg Training Loss: 0.35022739708365136, Avg Validation Loss: 68.08831124133965\n",
            "Epoch 131/1000, Avg Training Loss: 0.3491731494356534, Avg Validation Loss: 67.88916863247381\n",
            "Epoch 132/1000, Avg Training Loss: 0.34822897054483504, Avg Validation Loss: 67.71746561455606\n",
            "Epoch 133/1000, Avg Training Loss: 0.3473799958562494, Avg Validation Loss: 67.54936213395895\n",
            "Epoch 134/1000, Avg Training Loss: 0.3464819065140217, Avg Validation Loss: 67.37539743246226\n",
            "Epoch 135/1000, Avg Training Loss: 0.3456200339983285, Avg Validation Loss: 67.19747125499002\n",
            "Epoch 136/1000, Avg Training Loss: 0.34468079307881155, Avg Validation Loss: 67.01348868490305\n",
            "Epoch 137/1000, Avg Training Loss: 0.34376062279449926, Avg Validation Loss: 66.82129830102213\n",
            "Epoch 138/1000, Avg Training Loss: 0.34277449941387234, Avg Validation Loss: 66.62982263980517\n",
            "Epoch 139/1000, Avg Training Loss: 0.3418249709938812, Avg Validation Loss: 66.44191664410627\n",
            "Epoch 140/1000, Avg Training Loss: 0.34085703467327555, Avg Validation Loss: 66.25151838874186\n",
            "Epoch 141/1000, Avg Training Loss: 0.3398892580678934, Avg Validation Loss: 66.0626740590567\n",
            "Epoch 142/1000, Avg Training Loss: 0.3389452158128978, Avg Validation Loss: 65.87570321093355\n",
            "Epoch 143/1000, Avg Training Loss: 0.33800036126337035, Avg Validation Loss: 65.69765088518622\n",
            "Epoch 144/1000, Avg Training Loss: 0.33710564081900296, Avg Validation Loss: 65.5154766916024\n",
            "Epoch 145/1000, Avg Training Loss: 0.3362001987895107, Avg Validation Loss: 65.33381574892555\n",
            "Epoch 146/1000, Avg Training Loss: 0.33526626518376157, Avg Validation Loss: 65.16169941284181\n",
            "Epoch 147/1000, Avg Training Loss: 0.3344377103386227, Avg Validation Loss: 64.99694843586991\n",
            "Epoch 148/1000, Avg Training Loss: 0.3335859046821337, Avg Validation Loss: 64.83213892492023\n",
            "Epoch 149/1000, Avg Training Loss: 0.33277372886338585, Avg Validation Loss: 64.66246973250466\n",
            "Epoch 150/1000, Avg Training Loss: 0.33188731288210355, Avg Validation Loss: 64.49950194649803\n",
            "Epoch 151/1000, Avg Training Loss: 0.33107780057990266, Avg Validation Loss: 64.33396142355902\n",
            "Epoch 152/1000, Avg Training Loss: 0.33022766207081344, Avg Validation Loss: 64.16969673219455\n",
            "Epoch 153/1000, Avg Training Loss: 0.32943911665921305, Avg Validation Loss: 64.0161633415631\n",
            "Epoch 154/1000, Avg Training Loss: 0.32865562445972896, Avg Validation Loss: 63.86729418913302\n",
            "Epoch 155/1000, Avg Training Loss: 0.3278977188369372, Avg Validation Loss: 63.71683006262336\n",
            "Epoch 156/1000, Avg Training Loss: 0.327156230545909, Avg Validation Loss: 63.57493774907056\n",
            "Epoch 157/1000, Avg Training Loss: 0.32642298675097825, Avg Validation Loss: 63.424146317860895\n",
            "Epoch 158/1000, Avg Training Loss: 0.3256546759281649, Avg Validation Loss: 63.27398371200481\n",
            "Epoch 159/1000, Avg Training Loss: 0.3248905406236119, Avg Validation Loss: 63.12221349906402\n",
            "Epoch 160/1000, Avg Training Loss: 0.32411326637280935, Avg Validation Loss: 62.96893901828892\n",
            "Epoch 161/1000, Avg Training Loss: 0.3233444404159208, Avg Validation Loss: 62.821284539554725\n",
            "Epoch 162/1000, Avg Training Loss: 0.3225996565602161, Avg Validation Loss: 62.67926696056298\n",
            "Epoch 163/1000, Avg Training Loss: 0.32189169232891884, Avg Validation Loss: 62.5388059239511\n",
            "Epoch 164/1000, Avg Training Loss: 0.3211576128611909, Avg Validation Loss: 62.40103431171597\n",
            "Epoch 165/1000, Avg Training Loss: 0.3204866221940652, Avg Validation Loss: 62.26830982405786\n",
            "Epoch 166/1000, Avg Training Loss: 0.31979981091365756, Avg Validation Loss: 62.129990614889536\n",
            "Epoch 167/1000, Avg Training Loss: 0.31911916205542606, Avg Validation Loss: 61.998515886377824\n",
            "Epoch 168/1000, Avg Training Loss: 0.3184352495967548, Avg Validation Loss: 61.862733143097344\n",
            "Epoch 169/1000, Avg Training Loss: 0.31775024685338993, Avg Validation Loss: 61.731034816007934\n",
            "Epoch 170/1000, Avg Training Loss: 0.3170999375002042, Avg Validation Loss: 61.60678831542157\n",
            "Epoch 171/1000, Avg Training Loss: 0.31645687455681504, Avg Validation Loss: 61.48037711382179\n",
            "Epoch 172/1000, Avg Training Loss: 0.31579448147858624, Avg Validation Loss: 61.353286324380576\n",
            "Epoch 173/1000, Avg Training Loss: 0.31515828321600203, Avg Validation Loss: 61.22664410522921\n",
            "Epoch 174/1000, Avg Training Loss: 0.31452901210977746, Avg Validation Loss: 61.10072797558796\n",
            "Epoch 175/1000, Avg Training Loss: 0.3138838619565883, Avg Validation Loss: 60.98035928185182\n",
            "Epoch 176/1000, Avg Training Loss: 0.3132837053163592, Avg Validation Loss: 60.86145073774009\n",
            "Epoch 177/1000, Avg Training Loss: 0.31268791988601896, Avg Validation Loss: 60.75131528387996\n",
            "Epoch 178/1000, Avg Training Loss: 0.3121190612862954, Avg Validation Loss: 60.634503697814125\n",
            "Epoch 179/1000, Avg Training Loss: 0.3115203309464493, Avg Validation Loss: 60.51322651351193\n",
            "Epoch 180/1000, Avg Training Loss: 0.31089604750534766, Avg Validation Loss: 60.39606383676944\n",
            "Epoch 181/1000, Avg Training Loss: 0.31031334501900476, Avg Validation Loss: 60.28280006449268\n",
            "Epoch 182/1000, Avg Training Loss: 0.30971471208888457, Avg Validation Loss: 60.1647422302823\n",
            "Epoch 183/1000, Avg Training Loss: 0.3091036053742399, Avg Validation Loss: 60.04229877575359\n",
            "Epoch 184/1000, Avg Training Loss: 0.30848766915080456, Avg Validation Loss: 59.92226763366993\n",
            "Epoch 185/1000, Avg Training Loss: 0.30788680074394703, Avg Validation Loss: 59.800048499926135\n",
            "Epoch 186/1000, Avg Training Loss: 0.30725388353624306, Avg Validation Loss: 59.68478972945245\n",
            "Epoch 187/1000, Avg Training Loss: 0.30668958133235735, Avg Validation Loss: 59.578970682797284\n",
            "Epoch 188/1000, Avg Training Loss: 0.30615228338678124, Avg Validation Loss: 59.47302601106223\n",
            "Epoch 189/1000, Avg Training Loss: 0.3056143665383703, Avg Validation Loss: 59.36543623347379\n",
            "Epoch 190/1000, Avg Training Loss: 0.30504471895280966, Avg Validation Loss: 59.255415965967046\n",
            "Epoch 191/1000, Avg Training Loss: 0.30449742169733796, Avg Validation Loss: 59.15240869037933\n",
            "Epoch 192/1000, Avg Training Loss: 0.30397525886436005, Avg Validation Loss: 59.04867523404847\n",
            "Epoch 193/1000, Avg Training Loss: 0.3034352657768309, Avg Validation Loss: 58.94114228683462\n",
            "Epoch 194/1000, Avg Training Loss: 0.302873144502244, Avg Validation Loss: 58.834597314783224\n",
            "Epoch 195/1000, Avg Training Loss: 0.30233704673573875, Avg Validation Loss: 58.733355519022155\n",
            "Epoch 196/1000, Avg Training Loss: 0.30183119031390837, Avg Validation Loss: 58.627569363493755\n",
            "Epoch 197/1000, Avg Training Loss: 0.30128561527230346, Avg Validation Loss: 58.53151639449013\n",
            "Epoch 198/1000, Avg Training Loss: 0.30078169518520537, Avg Validation Loss: 58.427359390575205\n",
            "Epoch 199/1000, Avg Training Loss: 0.30026524099896706, Avg Validation Loss: 58.32870343616018\n",
            "Epoch 200/1000, Avg Training Loss: 0.2997704598437822, Avg Validation Loss: 58.23872016031555\n",
            "Epoch 201/1000, Avg Training Loss: 0.2993263776614822, Avg Validation Loss: 58.153406264032\n",
            "Epoch 202/1000, Avg Training Loss: 0.29887556229991674, Avg Validation Loss: 58.06544882148791\n",
            "Epoch 203/1000, Avg Training Loss: 0.29842755848867947, Avg Validation Loss: 57.97823439727736\n",
            "Epoch 204/1000, Avg Training Loss: 0.2979664731403585, Avg Validation Loss: 57.88736590393201\n",
            "Epoch 205/1000, Avg Training Loss: 0.2975046375803309, Avg Validation Loss: 57.799412532091736\n",
            "Epoch 206/1000, Avg Training Loss: 0.297041779481022, Avg Validation Loss: 57.70816047653964\n",
            "Epoch 207/1000, Avg Training Loss: 0.29657830865912815, Avg Validation Loss: 57.61717321198489\n",
            "Epoch 208/1000, Avg Training Loss: 0.2961133054767255, Avg Validation Loss: 57.5247225424459\n",
            "Epoch 209/1000, Avg Training Loss: 0.29564833675054125, Avg Validation Loss: 57.43748639812637\n",
            "Epoch 210/1000, Avg Training Loss: 0.29519830251240026, Avg Validation Loss: 57.34986043318853\n",
            "Epoch 211/1000, Avg Training Loss: 0.29474150541401184, Avg Validation Loss: 57.2629487561683\n",
            "Epoch 212/1000, Avg Training Loss: 0.2943092346324914, Avg Validation Loss: 57.18326731391048\n",
            "Epoch 213/1000, Avg Training Loss: 0.2938985398055393, Avg Validation Loss: 57.10056462471332\n",
            "Epoch 214/1000, Avg Training Loss: 0.2934655257757919, Avg Validation Loss: 57.020949763948295\n",
            "Epoch 215/1000, Avg Training Loss: 0.29305767336894256, Avg Validation Loss: 56.94247152155465\n",
            "Epoch 216/1000, Avg Training Loss: 0.29266235334073126, Avg Validation Loss: 56.867932756989106\n",
            "Epoch 217/1000, Avg Training Loss: 0.29228233212455107, Avg Validation Loss: 56.792961255004755\n",
            "Epoch 218/1000, Avg Training Loss: 0.2918789691918287, Avg Validation Loss: 56.71410737253555\n",
            "Epoch 219/1000, Avg Training Loss: 0.2914599063745891, Avg Validation Loss: 56.63048187786486\n",
            "Epoch 220/1000, Avg Training Loss: 0.2910361389112229, Avg Validation Loss: 56.553531922221914\n",
            "Epoch 221/1000, Avg Training Loss: 0.29064991464375795, Avg Validation Loss: 56.48148668379137\n",
            "Epoch 222/1000, Avg Training Loss: 0.29027734950654394, Avg Validation Loss: 56.40472480339543\n",
            "Epoch 223/1000, Avg Training Loss: 0.2898726983057043, Avg Validation Loss: 56.325878304347555\n",
            "Epoch 224/1000, Avg Training Loss: 0.28946664050612125, Avg Validation Loss: 56.24847935648728\n",
            "Epoch 225/1000, Avg Training Loss: 0.2890708932040017, Avg Validation Loss: 56.1715022575262\n",
            "Epoch 226/1000, Avg Training Loss: 0.2886812935807873, Avg Validation Loss: 56.096747182766435\n",
            "Epoch 227/1000, Avg Training Loss: 0.288288436672987, Avg Validation Loss: 56.02497712109919\n",
            "Epoch 228/1000, Avg Training Loss: 0.2879312673392484, Avg Validation Loss: 55.95790904141442\n",
            "Epoch 229/1000, Avg Training Loss: 0.2875819259249144, Avg Validation Loss: 55.89303274530768\n",
            "Epoch 230/1000, Avg Training Loss: 0.28724117737048993, Avg Validation Loss: 55.82581557911206\n",
            "Epoch 231/1000, Avg Training Loss: 0.28689521832650683, Avg Validation Loss: 55.75881083767983\n",
            "Epoch 232/1000, Avg Training Loss: 0.2865530195697589, Avg Validation Loss: 55.693944224629305\n",
            "Epoch 233/1000, Avg Training Loss: 0.28620258879688776, Avg Validation Loss: 55.62833708480957\n",
            "Epoch 234/1000, Avg Training Loss: 0.2858652001043196, Avg Validation Loss: 55.565575176574995\n",
            "Epoch 235/1000, Avg Training Loss: 0.2855467884489293, Avg Validation Loss: 55.50064669191207\n",
            "Epoch 236/1000, Avg Training Loss: 0.2852084843319201, Avg Validation Loss: 55.43687657969632\n",
            "Epoch 237/1000, Avg Training Loss: 0.2848795618350837, Avg Validation Loss: 55.37294489724942\n",
            "Epoch 238/1000, Avg Training Loss: 0.28454955493692, Avg Validation Loss: 55.31371566490091\n",
            "Epoch 239/1000, Avg Training Loss: 0.28423700611148134, Avg Validation Loss: 55.256404070977695\n",
            "Epoch 240/1000, Avg Training Loss: 0.28393001183979655, Avg Validation Loss: 55.19448462881561\n",
            "Epoch 241/1000, Avg Training Loss: 0.2836076871145864, Avg Validation Loss: 55.128359049457316\n",
            "Epoch 242/1000, Avg Training Loss: 0.2832564631036888, Avg Validation Loss: 55.061832378068914\n",
            "Epoch 243/1000, Avg Training Loss: 0.28289616364339787, Avg Validation Loss: 54.99684240170106\n",
            "Epoch 244/1000, Avg Training Loss: 0.2825778533955101, Avg Validation Loss: 54.933181633790575\n",
            "Epoch 245/1000, Avg Training Loss: 0.28225301642833767, Avg Validation Loss: 54.87147325936454\n",
            "Epoch 246/1000, Avg Training Loss: 0.28193621723434215, Avg Validation Loss: 54.811984143351076\n",
            "Epoch 247/1000, Avg Training Loss: 0.28162881206261303, Avg Validation Loss: 54.75451853509945\n",
            "Epoch 248/1000, Avg Training Loss: 0.281336142579928, Avg Validation Loss: 54.701120270499004\n",
            "Epoch 249/1000, Avg Training Loss: 0.2810589492394437, Avg Validation Loss: 54.64594999011331\n",
            "Epoch 250/1000, Avg Training Loss: 0.28075689508410095, Avg Validation Loss: 54.58922478335322\n",
            "Epoch 251/1000, Avg Training Loss: 0.28045968514541647, Avg Validation Loss: 54.53207074293388\n",
            "Epoch 252/1000, Avg Training Loss: 0.28017508649813844, Avg Validation Loss: 54.479958839091026\n",
            "Epoch 253/1000, Avg Training Loss: 0.27989994046996847, Avg Validation Loss: 54.42980671440616\n",
            "Epoch 254/1000, Avg Training Loss: 0.2796277575486351, Avg Validation Loss: 54.37571178484798\n",
            "Epoch 255/1000, Avg Training Loss: 0.27933984048343347, Avg Validation Loss: 54.322207529292186\n",
            "Epoch 256/1000, Avg Training Loss: 0.27905543306639635, Avg Validation Loss: 54.267211191204254\n",
            "Epoch 257/1000, Avg Training Loss: 0.27877738172815947, Avg Validation Loss: 54.21689306567252\n",
            "Epoch 258/1000, Avg Training Loss: 0.27851681415034973, Avg Validation Loss: 54.167287307675906\n",
            "Epoch 259/1000, Avg Training Loss: 0.27825816764383576, Avg Validation Loss: 54.11508017235732\n",
            "Epoch 260/1000, Avg Training Loss: 0.2779829760679845, Avg Validation Loss: 54.066097489019725\n",
            "Epoch 261/1000, Avg Training Loss: 0.2777212441542245, Avg Validation Loss: 54.01415654485986\n",
            "Epoch 262/1000, Avg Training Loss: 0.2774548937831523, Avg Validation Loss: 53.961962005852314\n",
            "Epoch 263/1000, Avg Training Loss: 0.2771883757280706, Avg Validation Loss: 53.91282043725056\n",
            "Epoch 264/1000, Avg Training Loss: 0.27692380188833554, Avg Validation Loss: 53.864742480385075\n",
            "Epoch 265/1000, Avg Training Loss: 0.27666622963105636, Avg Validation Loss: 53.81513781494407\n",
            "Epoch 266/1000, Avg Training Loss: 0.2764068776694762, Avg Validation Loss: 53.76708697527738\n",
            "Epoch 267/1000, Avg Training Loss: 0.27616674167385397, Avg Validation Loss: 53.72399998901287\n",
            "Epoch 268/1000, Avg Training Loss: 0.2759407714862751, Avg Validation Loss: 53.67960479276886\n",
            "Epoch 269/1000, Avg Training Loss: 0.27569704024549585, Avg Validation Loss: 53.63388903458307\n",
            "Epoch 270/1000, Avg Training Loss: 0.275453269118989, Avg Validation Loss: 53.58932046533684\n",
            "Epoch 271/1000, Avg Training Loss: 0.27522904320756225, Avg Validation Loss: 53.544122798425015\n",
            "Epoch 272/1000, Avg Training Loss: 0.27498859684832344, Avg Validation Loss: 53.50217095585853\n",
            "Epoch 273/1000, Avg Training Loss: 0.27476606890567423, Avg Validation Loss: 53.45844513977608\n",
            "Epoch 274/1000, Avg Training Loss: 0.2745289808080632, Avg Validation Loss: 53.41379033098654\n",
            "Epoch 275/1000, Avg Training Loss: 0.2742934486222575, Avg Validation Loss: 53.36945401418425\n",
            "Epoch 276/1000, Avg Training Loss: 0.2740644092877725, Avg Validation Loss: 53.3264043257335\n",
            "Epoch 277/1000, Avg Training Loss: 0.2738439738102909, Avg Validation Loss: 53.2833532479786\n",
            "Epoch 278/1000, Avg Training Loss: 0.27361420836230893, Avg Validation Loss: 53.24257355225656\n",
            "Epoch 279/1000, Avg Training Loss: 0.2733947782883007, Avg Validation Loss: 53.20122798199309\n",
            "Epoch 280/1000, Avg Training Loss: 0.27317939266622, Avg Validation Loss: 53.1605123272752\n",
            "Epoch 281/1000, Avg Training Loss: 0.2729707764943269, Avg Validation Loss: 53.123517918843504\n",
            "Epoch 282/1000, Avg Training Loss: 0.27276447378005697, Avg Validation Loss: 53.08535437729415\n",
            "Epoch 283/1000, Avg Training Loss: 0.2725641941218367, Avg Validation Loss: 53.044733906737065\n",
            "Epoch 284/1000, Avg Training Loss: 0.272358075026136, Avg Validation Loss: 53.00847493399773\n",
            "Epoch 285/1000, Avg Training Loss: 0.2721664601861663, Avg Validation Loss: 52.9718824510407\n",
            "Epoch 286/1000, Avg Training Loss: 0.27196033397038805, Avg Validation Loss: 52.93546264557467\n",
            "Epoch 287/1000, Avg Training Loss: 0.2717730322795447, Avg Validation Loss: 52.897757672400715\n",
            "Epoch 288/1000, Avg Training Loss: 0.27157278946368035, Avg Validation Loss: 52.86221858251768\n",
            "Epoch 289/1000, Avg Training Loss: 0.2713920500488208, Avg Validation Loss: 52.82961815638623\n",
            "Epoch 290/1000, Avg Training Loss: 0.27121115259812045, Avg Validation Loss: 52.79810022138146\n",
            "Epoch 291/1000, Avg Training Loss: 0.27104833812893014, Avg Validation Loss: 52.764787359320295\n",
            "Epoch 292/1000, Avg Training Loss: 0.2708677374968206, Avg Validation Loss: 52.730255390311996\n",
            "Epoch 293/1000, Avg Training Loss: 0.2706778223701692, Avg Validation Loss: 52.69476785894895\n",
            "Epoch 294/1000, Avg Training Loss: 0.2704890986984061, Avg Validation Loss: 52.661048812086975\n",
            "Epoch 295/1000, Avg Training Loss: 0.27031465807803456, Avg Validation Loss: 52.628215725371504\n",
            "Epoch 296/1000, Avg Training Loss: 0.27013277515427214, Avg Validation Loss: 52.59309575263687\n",
            "Epoch 297/1000, Avg Training Loss: 0.2699446547961165, Avg Validation Loss: 52.56102514256052\n",
            "Epoch 298/1000, Avg Training Loss: 0.2697769365767422, Avg Validation Loss: 52.527163794396486\n",
            "Epoch 299/1000, Avg Training Loss: 0.2696020251151789, Avg Validation Loss: 52.49513450772719\n",
            "Epoch 300/1000, Avg Training Loss: 0.2694310969111855, Avg Validation Loss: 52.46371750756681\n",
            "Epoch 301/1000, Avg Training Loss: 0.26926642529270844, Avg Validation Loss: 52.43453341470178\n",
            "Epoch 302/1000, Avg Training Loss: 0.2691003057581861, Avg Validation Loss: 52.40335194020669\n",
            "Epoch 303/1000, Avg Training Loss: 0.2689248350324759, Avg Validation Loss: 52.37357469516485\n",
            "Epoch 304/1000, Avg Training Loss: 0.2687690078868118, Avg Validation Loss: 52.34278275056465\n",
            "Epoch 305/1000, Avg Training Loss: 0.26860206498035455, Avg Validation Loss: 52.31384048632994\n",
            "Epoch 306/1000, Avg Training Loss: 0.2684464687702112, Avg Validation Loss: 52.28243415208662\n",
            "Epoch 307/1000, Avg Training Loss: 0.2682738595604213, Avg Validation Loss: 52.25334908150069\n",
            "Epoch 308/1000, Avg Training Loss: 0.2681175905351899, Avg Validation Loss: 52.22200532151638\n",
            "Epoch 309/1000, Avg Training Loss: 0.2679556606871293, Avg Validation Loss: 52.19479596169383\n",
            "Epoch 310/1000, Avg Training Loss: 0.26780738175323504, Avg Validation Loss: 52.166667165182886\n",
            "Epoch 311/1000, Avg Training Loss: 0.2676531936277139, Avg Validation Loss: 52.14039809072462\n",
            "Epoch 312/1000, Avg Training Loss: 0.2675168469819003, Avg Validation Loss: 52.11556997072779\n",
            "Epoch 313/1000, Avg Training Loss: 0.2673811283466741, Avg Validation Loss: 52.090619920538586\n",
            "Epoch 314/1000, Avg Training Loss: 0.26723922890480617, Avg Validation Loss: 52.064438049668276\n",
            "Epoch 315/1000, Avg Training Loss: 0.26709280195276713, Avg Validation Loss: 52.036738363143726\n",
            "Epoch 316/1000, Avg Training Loss: 0.26694695376023475, Avg Validation Loss: 52.0083783102486\n",
            "Epoch 317/1000, Avg Training Loss: 0.2667893342871348, Avg Validation Loss: 51.981503101791546\n",
            "Epoch 318/1000, Avg Training Loss: 0.26664041757392853, Avg Validation Loss: 51.956926426370586\n",
            "Epoch 319/1000, Avg Training Loss: 0.2665123182103075, Avg Validation Loss: 51.92956019695404\n",
            "Epoch 320/1000, Avg Training Loss: 0.2663627761688019, Avg Validation Loss: 51.90343832461514\n",
            "Epoch 321/1000, Avg Training Loss: 0.2662292466788323, Avg Validation Loss: 51.88027254526709\n",
            "Epoch 322/1000, Avg Training Loss: 0.26609951213693916, Avg Validation Loss: 51.85872561218703\n",
            "Epoch 323/1000, Avg Training Loss: 0.26599078016164995, Avg Validation Loss: 51.83625359710834\n",
            "Epoch 324/1000, Avg Training Loss: 0.26586495531844095, Avg Validation Loss: 51.81415691559821\n",
            "Epoch 325/1000, Avg Training Loss: 0.26573726754253457, Avg Validation Loss: 51.79215632886651\n",
            "Epoch 326/1000, Avg Training Loss: 0.2656167358400784, Avg Validation Loss: 51.771369874333246\n",
            "Epoch 327/1000, Avg Training Loss: 0.265508082169992, Avg Validation Loss: 51.75110125527942\n",
            "Epoch 328/1000, Avg Training Loss: 0.2653936802762159, Avg Validation Loss: 51.72904510180359\n",
            "Epoch 329/1000, Avg Training Loss: 0.2652675845840805, Avg Validation Loss: 51.70544029766515\n",
            "Epoch 330/1000, Avg Training Loss: 0.26513801023697553, Avg Validation Loss: 51.680244178157395\n",
            "Epoch 331/1000, Avg Training Loss: 0.2649960907271463, Avg Validation Loss: 51.655736375670145\n",
            "Epoch 332/1000, Avg Training Loss: 0.26486154270272705, Avg Validation Loss: 51.633693025268286\n",
            "Epoch 333/1000, Avg Training Loss: 0.2647485297052472, Avg Validation Loss: 51.611518050650844\n",
            "Epoch 334/1000, Avg Training Loss: 0.26462475499133725, Avg Validation Loss: 51.58840957933137\n",
            "Epoch 335/1000, Avg Training Loss: 0.2644922610713146, Avg Validation Loss: 51.56534321534715\n",
            "Epoch 336/1000, Avg Training Loss: 0.2643621207403357, Avg Validation Loss: 51.54343469033135\n",
            "Epoch 337/1000, Avg Training Loss: 0.264245005702129, Avg Validation Loss: 51.52035148487415\n",
            "Epoch 338/1000, Avg Training Loss: 0.2641157982258986, Avg Validation Loss: 51.4987342927248\n",
            "Epoch 339/1000, Avg Training Loss: 0.26400352185191883, Avg Validation Loss: 51.48019717551981\n",
            "Epoch 340/1000, Avg Training Loss: 0.2639030830552047, Avg Validation Loss: 51.461194508356\n",
            "Epoch 341/1000, Avg Training Loss: 0.2637974078653232, Avg Validation Loss: 51.441066636620775\n",
            "Epoch 342/1000, Avg Training Loss: 0.263682851330267, Avg Validation Loss: 51.421988980229315\n",
            "Epoch 343/1000, Avg Training Loss: 0.263580886325975, Avg Validation Loss: 51.40084916288403\n",
            "Epoch 344/1000, Avg Training Loss: 0.2634678143837831, Avg Validation Loss: 51.381277453568075\n",
            "Epoch 345/1000, Avg Training Loss: 0.2633586628975375, Avg Validation Loss: 51.36438068678878\n",
            "Epoch 346/1000, Avg Training Loss: 0.26326280051387163, Avg Validation Loss: 51.34607462517057\n",
            "Epoch 347/1000, Avg Training Loss: 0.2631577484638963, Avg Validation Loss: 51.32924526806637\n",
            "Epoch 348/1000, Avg Training Loss: 0.26305965491703026, Avg Validation Loss: 51.3130901079462\n",
            "Epoch 349/1000, Avg Training Loss: 0.26296514317966, Avg Validation Loss: 51.2971664819661\n",
            "Epoch 350/1000, Avg Training Loss: 0.2628830111806089, Avg Validation Loss: 51.28173612111976\n",
            "Epoch 351/1000, Avg Training Loss: 0.26279463850158413, Avg Validation Loss: 51.266529823728455\n",
            "Epoch 352/1000, Avg Training Loss: 0.262703235393241, Avg Validation Loss: 51.25080704908794\n",
            "Epoch 353/1000, Avg Training Loss: 0.26261986868364606, Avg Validation Loss: 51.23423996617434\n",
            "Epoch 354/1000, Avg Training Loss: 0.26252847826111797, Avg Validation Loss: 51.217451544647936\n",
            "Epoch 355/1000, Avg Training Loss: 0.26242599184190135, Avg Validation Loss: 51.20089440345872\n",
            "Epoch 356/1000, Avg Training Loss: 0.26233241825029535, Avg Validation Loss: 51.183935108663526\n",
            "Epoch 357/1000, Avg Training Loss: 0.26224515751770505, Avg Validation Loss: 51.16743137156569\n",
            "Epoch 358/1000, Avg Training Loss: 0.26215129431951134, Avg Validation Loss: 51.152441284314655\n",
            "Epoch 359/1000, Avg Training Loss: 0.2620634603700796, Avg Validation Loss: 51.13738553576573\n",
            "Epoch 360/1000, Avg Training Loss: 0.2619789619958231, Avg Validation Loss: 51.122720164809266\n",
            "Epoch 361/1000, Avg Training Loss: 0.2618939369040989, Avg Validation Loss: 51.10820067532116\n",
            "Epoch 362/1000, Avg Training Loss: 0.2618171625664717, Avg Validation Loss: 51.09390677594619\n",
            "Epoch 363/1000, Avg Training Loss: 0.2617334065815023, Avg Validation Loss: 51.080335171158886\n",
            "Epoch 364/1000, Avg Training Loss: 0.2616560416029695, Avg Validation Loss: 51.06672005901541\n",
            "Epoch 365/1000, Avg Training Loss: 0.26157805515800625, Avg Validation Loss: 51.05427601511066\n",
            "Epoch 366/1000, Avg Training Loss: 0.26150447377832, Avg Validation Loss: 51.04120954093591\n",
            "Epoch 367/1000, Avg Training Loss: 0.26143280552864745, Avg Validation Loss: 51.02721826726152\n",
            "Epoch 368/1000, Avg Training Loss: 0.26135361446020144, Avg Validation Loss: 51.014198002408115\n",
            "Epoch 369/1000, Avg Training Loss: 0.2612730392629606, Avg Validation Loss: 51.001034049934475\n",
            "Epoch 370/1000, Avg Training Loss: 0.26119557390414205, Avg Validation Loss: 50.98784975387479\n",
            "Epoch 371/1000, Avg Training Loss: 0.26112140308091447, Avg Validation Loss: 50.97468515942802\n",
            "Epoch 372/1000, Avg Training Loss: 0.2610405501063042, Avg Validation Loss: 50.96016303492202\n",
            "Epoch 373/1000, Avg Training Loss: 0.2609620933989643, Avg Validation Loss: 50.945997576281044\n",
            "Epoch 374/1000, Avg Training Loss: 0.26088679445450624, Avg Validation Loss: 50.93287898099679\n",
            "Epoch 375/1000, Avg Training Loss: 0.26080886059605635, Avg Validation Loss: 50.91932139265455\n",
            "Epoch 376/1000, Avg Training Loss: 0.2607333911641301, Avg Validation Loss: 50.90649466255279\n",
            "Epoch 377/1000, Avg Training Loss: 0.26066470413325704, Avg Validation Loss: 50.894804979104194\n",
            "Epoch 378/1000, Avg Training Loss: 0.2606018991904023, Avg Validation Loss: 50.88303891004773\n",
            "Epoch 379/1000, Avg Training Loss: 0.2605333778477036, Avg Validation Loss: 50.87078527911741\n",
            "Epoch 380/1000, Avg Training Loss: 0.2604541830477419, Avg Validation Loss: 50.859688534812165\n",
            "Epoch 381/1000, Avg Training Loss: 0.2603913319372118, Avg Validation Loss: 50.847537106106444\n",
            "Epoch 382/1000, Avg Training Loss: 0.2603206168250399, Avg Validation Loss: 50.8359837551009\n",
            "Epoch 383/1000, Avg Training Loss: 0.26025795577285, Avg Validation Loss: 50.82472448867007\n",
            "Epoch 384/1000, Avg Training Loss: 0.26019311756731434, Avg Validation Loss: 50.813834189414514\n",
            "Epoch 385/1000, Avg Training Loss: 0.2601254482052263, Avg Validation Loss: 50.80329465298899\n",
            "Epoch 386/1000, Avg Training Loss: 0.2600654658459951, Avg Validation Loss: 50.793105875945855\n",
            "Epoch 387/1000, Avg Training Loss: 0.26000359908089926, Avg Validation Loss: 50.78274504534404\n",
            "Epoch 388/1000, Avg Training Loss: 0.25993909974307866, Avg Validation Loss: 50.77166487686003\n",
            "Epoch 389/1000, Avg Training Loss: 0.2598867941560521, Avg Validation Loss: 50.76107338237641\n",
            "Epoch 390/1000, Avg Training Loss: 0.259823151908688, Avg Validation Loss: 50.751807381689304\n",
            "Epoch 391/1000, Avg Training Loss: 0.2597648165853777, Avg Validation Loss: 50.74130448118542\n",
            "Epoch 392/1000, Avg Training Loss: 0.25970380759368084, Avg Validation Loss: 50.73213039720241\n",
            "Epoch 393/1000, Avg Training Loss: 0.25965092104373255, Avg Validation Loss: 50.723203916937024\n",
            "Epoch 394/1000, Avg Training Loss: 0.259589345092873, Avg Validation Loss: 50.71369974061433\n",
            "Epoch 395/1000, Avg Training Loss: 0.2595355402731448, Avg Validation Loss: 50.70426847362151\n",
            "Epoch 396/1000, Avg Training Loss: 0.2594790984067183, Avg Validation Loss: 50.69545073689898\n",
            "Epoch 397/1000, Avg Training Loss: 0.2594284888387325, Avg Validation Loss: 50.686931377430454\n",
            "Epoch 398/1000, Avg Training Loss: 0.25937437604248786, Avg Validation Loss: 50.67782995934651\n",
            "Epoch 399/1000, Avg Training Loss: 0.25932272296257447, Avg Validation Loss: 50.66836750328376\n",
            "Epoch 400/1000, Avg Training Loss: 0.25926835090978556, Avg Validation Loss: 50.65861594397339\n",
            "Epoch 401/1000, Avg Training Loss: 0.25920821622042056, Avg Validation Loss: 50.65011775469319\n",
            "Epoch 402/1000, Avg Training Loss: 0.2591554598568092, Avg Validation Loss: 50.64141781904155\n",
            "Epoch 403/1000, Avg Training Loss: 0.2591025403636747, Avg Validation Loss: 50.63379642979635\n",
            "Epoch 404/1000, Avg Training Loss: 0.25906115625259063, Avg Validation Loss: 50.62632167334432\n",
            "Epoch 405/1000, Avg Training Loss: 0.2590156377722241, Avg Validation Loss: 50.619660103550665\n",
            "Epoch 406/1000, Avg Training Loss: 0.2589778921954328, Avg Validation Loss: 50.61248163279062\n",
            "Epoch 407/1000, Avg Training Loss: 0.25892727240114527, Avg Validation Loss: 50.60501272268659\n",
            "Epoch 408/1000, Avg Training Loss: 0.2588822604664977, Avg Validation Loss: 50.59772004236599\n",
            "Epoch 409/1000, Avg Training Loss: 0.2588339980102199, Avg Validation Loss: 50.59032693206727\n",
            "Epoch 410/1000, Avg Training Loss: 0.25879014306828774, Avg Validation Loss: 50.58406276110473\n",
            "Epoch 411/1000, Avg Training Loss: 0.2587508984926929, Avg Validation Loss: 50.577916748335596\n",
            "Epoch 412/1000, Avg Training Loss: 0.2587086273581397, Avg Validation Loss: 50.57106927912647\n",
            "Epoch 413/1000, Avg Training Loss: 0.25867151139718964, Avg Validation Loss: 50.564320409997705\n",
            "Epoch 414/1000, Avg Training Loss: 0.25862602954407415, Avg Validation Loss: 50.557756175976124\n",
            "Epoch 415/1000, Avg Training Loss: 0.258581310750121, Avg Validation Loss: 50.55074990667191\n",
            "Epoch 416/1000, Avg Training Loss: 0.25854265009194327, Avg Validation Loss: 50.543700058574366\n",
            "Epoch 417/1000, Avg Training Loss: 0.2584993608360937, Avg Validation Loss: 50.53705955821175\n",
            "Epoch 418/1000, Avg Training Loss: 0.2584534953838411, Avg Validation Loss: 50.530635139607654\n",
            "Epoch 419/1000, Avg Training Loss: 0.2584145164330214, Avg Validation Loss: 50.5242139049293\n",
            "Epoch 420/1000, Avg Training Loss: 0.25837319838512524, Avg Validation Loss: 50.51891762256656\n",
            "Epoch 421/1000, Avg Training Loss: 0.2583377331988711, Avg Validation Loss: 50.513234030497706\n",
            "Epoch 422/1000, Avg Training Loss: 0.25830433217882426, Avg Validation Loss: 50.507255344779985\n",
            "Epoch 423/1000, Avg Training Loss: 0.25826785580117145, Avg Validation Loss: 50.50231145529423\n",
            "Epoch 424/1000, Avg Training Loss: 0.258238197163535, Avg Validation Loss: 50.49721900441914\n",
            "Epoch 425/1000, Avg Training Loss: 0.25820402186308267, Avg Validation Loss: 50.49206217500946\n",
            "Epoch 426/1000, Avg Training Loss: 0.25816983950391703, Avg Validation Loss: 50.487168899509506\n",
            "Epoch 427/1000, Avg Training Loss: 0.2581413648249335, Avg Validation Loss: 50.48196911101476\n",
            "Epoch 428/1000, Avg Training Loss: 0.2581066331402367, Avg Validation Loss: 50.47705882013286\n",
            "Epoch 429/1000, Avg Training Loss: 0.2580746446098851, Avg Validation Loss: 50.47218032694903\n",
            "Epoch 430/1000, Avg Training Loss: 0.25804323599426515, Avg Validation Loss: 50.4674447039708\n",
            "Epoch 431/1000, Avg Training Loss: 0.2580111599711421, Avg Validation Loss: 50.462261086792324\n",
            "Epoch 432/1000, Avg Training Loss: 0.25797780980428503, Avg Validation Loss: 50.45662673686125\n",
            "Epoch 433/1000, Avg Training Loss: 0.25794277805200605, Avg Validation Loss: 50.4515062385657\n",
            "Epoch 434/1000, Avg Training Loss: 0.2579084662067192, Avg Validation Loss: 50.44694042910731\n",
            "Epoch 435/1000, Avg Training Loss: 0.2578779170903458, Avg Validation Loss: 50.44160839859646\n",
            "Epoch 436/1000, Avg Training Loss: 0.2578428917467429, Avg Validation Loss: 50.43693410587061\n",
            "Epoch 437/1000, Avg Training Loss: 0.25781652968006896, Avg Validation Loss: 50.43292670513072\n",
            "Epoch 438/1000, Avg Training Loss: 0.2577872347179833, Avg Validation Loss: 50.42725609241322\n",
            "Epoch 439/1000, Avg Training Loss: 0.2577487061856075, Avg Validation Loss: 50.42234528038483\n",
            "Epoch 440/1000, Avg Training Loss: 0.2577192490101472, Avg Validation Loss: 50.41738855232139\n",
            "Epoch 441/1000, Avg Training Loss: 0.25768478114437265, Avg Validation Loss: 50.413620734152374\n",
            "Epoch 442/1000, Avg Training Loss: 0.2576565330565972, Avg Validation Loss: 50.409505539408414\n",
            "Epoch 443/1000, Avg Training Loss: 0.25763138905199834, Avg Validation Loss: 50.406291821257774\n",
            "Epoch 444/1000, Avg Training Loss: 0.2576065298412534, Avg Validation Loss: 50.40347311819585\n",
            "Epoch 445/1000, Avg Training Loss: 0.25758173078986707, Avg Validation Loss: 50.399503771710826\n",
            "Epoch 446/1000, Avg Training Loss: 0.2575525423701771, Avg Validation Loss: 50.39609673605611\n",
            "Epoch 447/1000, Avg Training Loss: 0.25752756203013816, Avg Validation Loss: 50.392299969540844\n",
            "Epoch 448/1000, Avg Training Loss: 0.25750060930552016, Avg Validation Loss: 50.38773822953185\n",
            "Epoch 449/1000, Avg Training Loss: 0.25747485498951184, Avg Validation Loss: 50.38367465800644\n",
            "Epoch 450/1000, Avg Training Loss: 0.2574409050935643, Avg Validation Loss: 50.3799029587893\n",
            "Epoch 451/1000, Avg Training Loss: 0.2574137214605588, Avg Validation Loss: 50.376463507709076\n",
            "Epoch 452/1000, Avg Training Loss: 0.2573848365325995, Avg Validation Loss: 50.373296717743656\n",
            "Epoch 453/1000, Avg Training Loss: 0.25736108761456467, Avg Validation Loss: 50.36904677516874\n",
            "Epoch 454/1000, Avg Training Loss: 0.2573330393583309, Avg Validation Loss: 50.36540783293232\n",
            "Epoch 455/1000, Avg Training Loss: 0.2573104415034312, Avg Validation Loss: 50.361942105809696\n",
            "Epoch 456/1000, Avg Training Loss: 0.25728570233723286, Avg Validation Loss: 50.35854562261024\n",
            "Epoch 457/1000, Avg Training Loss: 0.25725815952886216, Avg Validation Loss: 50.35539114731269\n",
            "Epoch 458/1000, Avg Training Loss: 0.2572346910986787, Avg Validation Loss: 50.35248618133782\n",
            "Epoch 459/1000, Avg Training Loss: 0.25721179494626606, Avg Validation Loss: 50.34915868460483\n",
            "Epoch 460/1000, Avg Training Loss: 0.2571893026602636, Avg Validation Loss: 50.34613556651081\n",
            "Epoch 461/1000, Avg Training Loss: 0.25716745075471464, Avg Validation Loss: 50.342567127748566\n",
            "Epoch 462/1000, Avg Training Loss: 0.25714462696873, Avg Validation Loss: 50.340076477065956\n",
            "Epoch 463/1000, Avg Training Loss: 0.2571243656744161, Avg Validation Loss: 50.33669964494976\n",
            "Epoch 464/1000, Avg Training Loss: 0.25710037413034814, Avg Validation Loss: 50.33462498050843\n",
            "Epoch 465/1000, Avg Training Loss: 0.2570812798964297, Avg Validation Loss: 50.33183867859627\n",
            "Epoch 466/1000, Avg Training Loss: 0.25705220724740757, Avg Validation Loss: 50.329294124661004\n",
            "Epoch 467/1000, Avg Training Loss: 0.2570335377573853, Avg Validation Loss: 50.32633826329532\n",
            "Epoch 468/1000, Avg Training Loss: 0.2570030194424488, Avg Validation Loss: 50.32346455637753\n",
            "Epoch 469/1000, Avg Training Loss: 0.25698704385010357, Avg Validation Loss: 50.32083819138599\n",
            "Epoch 470/1000, Avg Training Loss: 0.2569636731112866, Avg Validation Loss: 50.31797488974909\n",
            "Epoch 471/1000, Avg Training Loss: 0.25694239261724033, Avg Validation Loss: 50.3158679112469\n",
            "Epoch 472/1000, Avg Training Loss: 0.2569246055647526, Avg Validation Loss: 50.31288091396384\n",
            "Epoch 473/1000, Avg Training Loss: 0.2569032201053449, Avg Validation Loss: 50.31066829116956\n",
            "Epoch 474/1000, Avg Training Loss: 0.2568850369047206, Avg Validation Loss: 50.30837709659248\n",
            "Epoch 475/1000, Avg Training Loss: 0.25686867828646703, Avg Validation Loss: 50.306220060266156\n",
            "Epoch 476/1000, Avg Training Loss: 0.25684963066362276, Avg Validation Loss: 50.30454924144565\n",
            "Epoch 477/1000, Avg Training Loss: 0.2568348514931807, Avg Validation Loss: 50.30307972770092\n",
            "Epoch 478/1000, Avg Training Loss: 0.25682054770484064, Avg Validation Loss: 50.300655584283874\n",
            "Epoch 479/1000, Avg Training Loss: 0.25680023889088516, Avg Validation Loss: 50.299204848807484\n",
            "Epoch 480/1000, Avg Training Loss: 0.25678767692259363, Avg Validation Loss: 50.29791055249755\n",
            "Epoch 481/1000, Avg Training Loss: 0.2567659910138929, Avg Validation Loss: 50.29589362392342\n",
            "Epoch 482/1000, Avg Training Loss: 0.25675204623805686, Avg Validation Loss: 50.294286909319\n",
            "Epoch 483/1000, Avg Training Loss: 0.25673687262490197, Avg Validation Loss: 50.292521596453895\n",
            "Epoch 484/1000, Avg Training Loss: 0.2567220259460334, Avg Validation Loss: 50.290717402026814\n",
            "Epoch 485/1000, Avg Training Loss: 0.2567096324604056, Avg Validation Loss: 50.289805977843876\n",
            "Epoch 486/1000, Avg Training Loss: 0.25669565910028214, Avg Validation Loss: 50.2884263958682\n",
            "Epoch 487/1000, Avg Training Loss: 0.25668222732644724, Avg Validation Loss: 50.28689481831337\n",
            "Epoch 488/1000, Avg Training Loss: 0.25666834723540005, Avg Validation Loss: 50.2856753189658\n",
            "Epoch 489/1000, Avg Training Loss: 0.25665491656820083, Avg Validation Loss: 50.28450593243383\n",
            "Epoch 490/1000, Avg Training Loss: 0.25664024611990643, Avg Validation Loss: 50.28346321908707\n",
            "Epoch 491/1000, Avg Training Loss: 0.25662752862100574, Avg Validation Loss: 50.28223348362722\n",
            "Epoch 492/1000, Avg Training Loss: 0.2566137801084569, Avg Validation Loss: 50.28089424465918\n",
            "Epoch 493/1000, Avg Training Loss: 0.25659761312167423, Avg Validation Loss: 50.279561676101835\n",
            "Epoch 494/1000, Avg Training Loss: 0.25658194453131733, Avg Validation Loss: 50.27840331564063\n",
            "Epoch 495/1000, Avg Training Loss: 0.25656736467328206, Avg Validation Loss: 50.27744655822231\n",
            "Epoch 496/1000, Avg Training Loss: 0.25655598660738105, Avg Validation Loss: 50.276619574000115\n",
            "Epoch 497/1000, Avg Training Loss: 0.2565458639112568, Avg Validation Loss: 50.27549030288892\n",
            "Epoch 498/1000, Avg Training Loss: 0.2565374566691312, Avg Validation Loss: 50.27450374923838\n",
            "Epoch 499/1000, Avg Training Loss: 0.25652500206559037, Avg Validation Loss: 50.273518391211915\n",
            "Epoch 500/1000, Avg Training Loss: 0.256511538216489, Avg Validation Loss: 50.27286982314127\n",
            "Epoch 501/1000, Avg Training Loss: 0.25650161795787296, Avg Validation Loss: 50.272171562087294\n",
            "Epoch 502/1000, Avg Training Loss: 0.2564916552498444, Avg Validation Loss: 50.2719459629552\n",
            "Epoch 503/1000, Avg Training Loss: 0.2564818640357521, Avg Validation Loss: 50.27140019228552\n",
            "Epoch 504/1000, Avg Training Loss: 0.2564693622501777, Avg Validation Loss: 50.27112437050474\n",
            "Epoch 505/1000, Avg Training Loss: 0.25646094532381586, Avg Validation Loss: 50.27003188573978\n",
            "Epoch 506/1000, Avg Training Loss: 0.2564492089577211, Avg Validation Loss: 50.26934253498395\n",
            "Epoch 507/1000, Avg Training Loss: 0.2564424303658212, Avg Validation Loss: 50.26904830715897\n",
            "Epoch 508/1000, Avg Training Loss: 0.256438765456627, Avg Validation Loss: 50.26801977016882\n",
            "Epoch 509/1000, Avg Training Loss: 0.2564280911102511, Avg Validation Loss: 50.26711911492947\n",
            "Epoch 510/1000, Avg Training Loss: 0.25642472817434464, Avg Validation Loss: 50.26655849668432\n",
            "Epoch 511/1000, Avg Training Loss: 0.2564080891964235, Avg Validation Loss: 50.26624156509192\n",
            "Epoch 512/1000, Avg Training Loss: 0.2564017009068109, Avg Validation Loss: 50.26615254287378\n",
            "Epoch 513/1000, Avg Training Loss: 0.256391723312157, Avg Validation Loss: 50.266261506795956\n",
            "Epoch 514/1000, Avg Training Loss: 0.25638102253596684, Avg Validation Loss: 50.265314317248084\n",
            "Epoch 515/1000, Avg Training Loss: 0.2563702400936826, Avg Validation Loss: 50.2649518984384\n",
            "Epoch 516/1000, Avg Training Loss: 0.25636020542559, Avg Validation Loss: 50.26451016111996\n",
            "Epoch 517/1000, Avg Training Loss: 0.25634993659594396, Avg Validation Loss: 50.26427266882207\n",
            "Epoch 518/1000, Avg Training Loss: 0.2563453402116774, Avg Validation Loss: 50.2642146826651\n",
            "Epoch 519/1000, Avg Training Loss: 0.25633866639274294, Avg Validation Loss: 50.26359084780363\n",
            "Epoch 520/1000, Avg Training Loss: 0.2563230873465454, Avg Validation Loss: 50.26339710298049\n",
            "Epoch 521/1000, Avg Training Loss: 0.2563168389576714, Avg Validation Loss: 50.26298562015744\n",
            "Epoch 522/1000, Avg Training Loss: 0.2563077637439324, Avg Validation Loss: 50.26267139715165\n",
            "Epoch 523/1000, Avg Training Loss: 0.2562999224850338, Avg Validation Loss: 50.26223203957371\n",
            "Epoch 524/1000, Avg Training Loss: 0.2562892831618868, Avg Validation Loss: 50.262031524358065\n",
            "Epoch 525/1000, Avg Training Loss: 0.25628393083902357, Avg Validation Loss: 50.262561670338755\n",
            "Epoch 526/1000, Avg Training Loss: 0.25627077035468243, Avg Validation Loss: 50.26290886705105\n",
            "Epoch 527/1000, Avg Training Loss: 0.2562655317440077, Avg Validation Loss: 50.262979447344094\n",
            "Epoch 528/1000, Avg Training Loss: 0.2562550753167999, Avg Validation Loss: 50.26305129710887\n",
            "Epoch 529/1000, Avg Training Loss: 0.25624827251491633, Avg Validation Loss: 50.26340970632769\n",
            "Epoch 530/1000, Avg Training Loss: 0.25624482617965966, Avg Validation Loss: 50.264015026257425\n",
            "Epoch 531/1000, Avg Training Loss: 0.2562365071682968, Avg Validation Loss: 50.264752880539326\n",
            "Epoch 532/1000, Avg Training Loss: 0.2562299551909987, Avg Validation Loss: 50.26584926569244\n",
            "Epoch 533/1000, Avg Training Loss: 0.25622856289015417, Avg Validation Loss: 50.2665268893904\n",
            "Epoch 534/1000, Avg Training Loss: 0.2562274445283428, Avg Validation Loss: 50.26777363972707\n",
            "Epoch 535/1000, Avg Training Loss: 0.2562193467778923, Avg Validation Loss: 50.26868916032623\n",
            "Epoch 536/1000, Avg Training Loss: 0.256215492453671, Avg Validation Loss: 50.26915436484947\n",
            "Epoch 537/1000, Avg Training Loss: 0.25620935935508, Avg Validation Loss: 50.26936091549695\n",
            "Epoch 538/1000, Avg Training Loss: 0.25620613316327784, Avg Validation Loss: 50.26986789068628\n",
            "Epoch 539/1000, Avg Training Loss: 0.25620087838088085, Avg Validation Loss: 50.270523575744264\n",
            "Epoch 540/1000, Avg Training Loss: 0.2561956755732834, Avg Validation Loss: 50.27074353837716\n",
            "Epoch 541/1000, Avg Training Loss: 0.2561918047465882, Avg Validation Loss: 50.27076038260351\n",
            "Epoch 542/1000, Avg Training Loss: 0.2561851434842385, Avg Validation Loss: 50.270911654264964\n",
            "Epoch 543/1000, Avg Training Loss: 0.25617834402890305, Avg Validation Loss: 50.27114542450886\n",
            "Epoch 544/1000, Avg Training Loss: 0.2561731710400079, Avg Validation Loss: 50.271190005484854\n",
            "Epoch 545/1000, Avg Training Loss: 0.2561692637503867, Avg Validation Loss: 50.27190198331362\n",
            "Epoch 546/1000, Avg Training Loss: 0.25616925607337715, Avg Validation Loss: 50.27194781572709\n",
            "Epoch 547/1000, Avg Training Loss: 0.256161510431079, Avg Validation Loss: 50.272939938609184\n",
            "Epoch 548/1000, Avg Training Loss: 0.2561570567291193, Avg Validation Loss: 50.27358617989428\n",
            "Epoch 549/1000, Avg Training Loss: 0.2561540977541928, Avg Validation Loss: 50.27381644811739\n",
            "Epoch 550/1000, Avg Training Loss: 0.2561503253463708, Avg Validation Loss: 50.27433320843875\n",
            "Epoch 551/1000, Avg Training Loss: 0.2561465598007208, Avg Validation Loss: 50.27477552911744\n",
            "Epoch 552/1000, Avg Training Loss: 0.2561432707974284, Avg Validation Loss: 50.27593497562584\n",
            "Epoch 553/1000, Avg Training Loss: 0.25614231113055363, Avg Validation Loss: 50.27678410842992\n",
            "Epoch 554/1000, Avg Training Loss: 0.2561383795131967, Avg Validation Loss: 50.277063047375535\n",
            "Epoch 555/1000, Avg Training Loss: 0.2561354440851478, Avg Validation Loss: 50.277942749095054\n",
            "Epoch 556/1000, Avg Training Loss: 0.25612873029310784, Avg Validation Loss: 50.27884597876155\n",
            "Epoch 557/1000, Avg Training Loss: 0.25612757049621676, Avg Validation Loss: 50.279224274790764\n",
            "Epoch 558/1000, Avg Training Loss: 0.25612317315573047, Avg Validation Loss: 50.27997188800656\n",
            "Epoch 559/1000, Avg Training Loss: 0.25612333330085235, Avg Validation Loss: 50.28062098076624\n",
            "Epoch 560/1000, Avg Training Loss: 0.256119612234336, Avg Validation Loss: 50.281055177258814\n",
            "Epoch 561/1000, Avg Training Loss: 0.2561159245192622, Avg Validation Loss: 50.28204766862184\n",
            "Epoch 562/1000, Avg Training Loss: 0.25611437998134773, Avg Validation Loss: 50.28256700989134\n",
            "Epoch 563/1000, Avg Training Loss: 0.2561115855392055, Avg Validation Loss: 50.28392857461358\n",
            "Epoch 564/1000, Avg Training Loss: 0.2561130530834194, Avg Validation Loss: 50.28503282787254\n",
            "Epoch 565/1000, Avg Training Loss: 0.2561116002661231, Avg Validation Loss: 50.285838574098946\n",
            "Epoch 566/1000, Avg Training Loss: 0.25610940032956775, Avg Validation Loss: 50.286421529474545\n",
            "Epoch 567/1000, Avg Training Loss: 0.2561059197581415, Avg Validation Loss: 50.287659222568195\n",
            "Epoch 568/1000, Avg Training Loss: 0.25610589469613537, Avg Validation Loss: 50.28859788509867\n",
            "Epoch 569/1000, Avg Training Loss: 0.2561032161455128, Avg Validation Loss: 50.289508696463976\n",
            "Epoch 570/1000, Avg Training Loss: 0.2561023259222882, Avg Validation Loss: 50.29014445449944\n",
            "Epoch 571/1000, Avg Training Loss: 0.25610200355084856, Avg Validation Loss: 50.291103932698526\n",
            "Epoch 572/1000, Avg Training Loss: 0.25609940776465584, Avg Validation Loss: 50.29234945553642\n",
            "Epoch 573/1000, Avg Training Loss: 0.25610443330345256, Avg Validation Loss: 50.2932648902779\n",
            "Epoch 574/1000, Avg Training Loss: 0.2561014301574712, Avg Validation Loss: 50.294377427016\n",
            "Epoch 575/1000, Avg Training Loss: 0.2560976772277974, Avg Validation Loss: 50.294633366975646\n",
            "Epoch 576/1000, Avg Training Loss: 0.25609720885788084, Avg Validation Loss: 50.29546366591971\n",
            "Epoch 577/1000, Avg Training Loss: 0.2560961961069676, Avg Validation Loss: 50.29646019275399\n",
            "Epoch 578/1000, Avg Training Loss: 0.25610694706674386, Avg Validation Loss: 50.29680378543682\n",
            "Epoch 579/1000, Avg Training Loss: 0.2561000047682677, Avg Validation Loss: 50.29739757592027\n",
            "Epoch 580/1000, Avg Training Loss: 0.2560986400536318, Avg Validation Loss: 50.298559483548644\n",
            "Epoch 581/1000, Avg Training Loss: 0.2561017734196452, Avg Validation Loss: 50.29921169619732\n",
            "Epoch 582/1000, Avg Training Loss: 0.2561029250663478, Avg Validation Loss: 50.29981982405042\n",
            "Epoch 583/1000, Avg Training Loss: 0.2561031275550734, Avg Validation Loss: 50.301601761495185\n",
            "Epoch 584/1000, Avg Training Loss: 0.2561030246460147, Avg Validation Loss: 50.302786466380965\n",
            "Epoch 585/1000, Avg Training Loss: 0.25610265308124536, Avg Validation Loss: 50.303821176225966\n",
            "Epoch 586/1000, Avg Training Loss: 0.2561045270294541, Avg Validation Loss: 50.305948348878786\n",
            "Epoch 587/1000, Avg Training Loss: 0.2561046488187841, Avg Validation Loss: 50.30682585399613\n",
            "Epoch 588/1000, Avg Training Loss: 0.25610234555393885, Avg Validation Loss: 50.30786629394503\n",
            "Epoch 589/1000, Avg Training Loss: 0.25610073077404755, Avg Validation Loss: 50.30890512186259\n",
            "Epoch 590/1000, Avg Training Loss: 0.25609860383942307, Avg Validation Loss: 50.31057644526392\n",
            "Epoch 591/1000, Avg Training Loss: 0.2560989584791503, Avg Validation Loss: 50.31116031714177\n",
            "Epoch 592/1000, Avg Training Loss: 0.25609506237610735, Avg Validation Loss: 50.31228974359053\n",
            "Epoch 593/1000, Avg Training Loss: 0.25609597781402677, Avg Validation Loss: 50.313511036383574\n",
            "Epoch 594/1000, Avg Training Loss: 0.256099444068917, Avg Validation Loss: 50.31430004188968\n",
            "Epoch 595/1000, Avg Training Loss: 0.25609978290368746, Avg Validation Loss: 50.31517586547322\n",
            "Epoch 596/1000, Avg Training Loss: 0.2560975334360982, Avg Validation Loss: 50.31608763360311\n",
            "Epoch 597/1000, Avg Training Loss: 0.25609553651299755, Avg Validation Loss: 50.3173064183236\n",
            "Epoch 598/1000, Avg Training Loss: 0.2560955156124652, Avg Validation Loss: 50.317932483968846\n",
            "Epoch 599/1000, Avg Training Loss: 0.25609004229101195, Avg Validation Loss: 50.31820881016209\n",
            "Epoch 600/1000, Avg Training Loss: 0.25608750879785674, Avg Validation Loss: 50.31913998388745\n",
            "Epoch 601/1000, Avg Training Loss: 0.2560867854884924, Avg Validation Loss: 50.3203153610085\n",
            "Epoch 602/1000, Avg Training Loss: 0.2560854898465576, Avg Validation Loss: 50.32094949683203\n",
            "Epoch 603/1000, Avg Training Loss: 0.2560837866341603, Avg Validation Loss: 50.32270156754307\n",
            "Epoch 604/1000, Avg Training Loss: 0.25608099022221836, Avg Validation Loss: 50.32323583982111\n",
            "Epoch 605/1000, Avg Training Loss: 0.2560790844060016, Avg Validation Loss: 50.32387202790743\n",
            "Epoch 606/1000, Avg Training Loss: 0.2560778090451085, Avg Validation Loss: 50.324452263311706\n",
            "Epoch 607/1000, Avg Training Loss: 0.25607921138538836, Avg Validation Loss: 50.32498269241309\n",
            "Epoch 608/1000, Avg Training Loss: 0.2560765027976554, Avg Validation Loss: 50.32629409032036\n",
            "Epoch 609/1000, Avg Training Loss: 0.256075681846679, Avg Validation Loss: 50.32756975445426\n",
            "Epoch 610/1000, Avg Training Loss: 0.2560762972845352, Avg Validation Loss: 50.32939780558853\n",
            "Epoch 611/1000, Avg Training Loss: 0.25607676748108993, Avg Validation Loss: 50.330094336253886\n",
            "Epoch 612/1000, Avg Training Loss: 0.25607478390451677, Avg Validation Loss: 50.330713753871365\n",
            "Epoch 613/1000, Avg Training Loss: 0.2560732025147631, Avg Validation Loss: 50.33172284574724\n",
            "Epoch 614/1000, Avg Training Loss: 0.2560721373580692, Avg Validation Loss: 50.33265453464382\n",
            "Epoch 615/1000, Avg Training Loss: 0.25607025317315163, Avg Validation Loss: 50.3335442253499\n",
            "Epoch 616/1000, Avg Training Loss: 0.2560676215954248, Avg Validation Loss: 50.334794169823084\n",
            "Epoch 617/1000, Avg Training Loss: 0.2560681274289653, Avg Validation Loss: 50.336023663449524\n",
            "Epoch 618/1000, Avg Training Loss: 0.2560664493925666, Avg Validation Loss: 50.33671384193127\n",
            "Epoch 619/1000, Avg Training Loss: 0.256066593464868, Avg Validation Loss: 50.33846713313538\n",
            "Epoch 620/1000, Avg Training Loss: 0.2560654077028519, Avg Validation Loss: 50.3392458617288\n",
            "Epoch 621/1000, Avg Training Loss: 0.2560658749215292, Avg Validation Loss: 50.3407678555291\n",
            "Epoch 622/1000, Avg Training Loss: 0.2560617440554706, Avg Validation Loss: 50.341592005336\n",
            "Epoch 623/1000, Avg Training Loss: 0.25606253292091397, Avg Validation Loss: 50.34294588147224\n",
            "Epoch 624/1000, Avg Training Loss: 0.256066477087504, Avg Validation Loss: 50.34377398854842\n",
            "Epoch 625/1000, Avg Training Loss: 0.25606527108995275, Avg Validation Loss: 50.345257758581134\n",
            "Epoch 626/1000, Avg Training Loss: 0.2560672202618115, Avg Validation Loss: 50.347333551687655\n",
            "Epoch 627/1000, Avg Training Loss: 0.25606495306325505, Avg Validation Loss: 50.34819339983368\n",
            "Epoch 628/1000, Avg Training Loss: 0.2560655933960263, Avg Validation Loss: 50.34906481313061\n",
            "Epoch 629/1000, Avg Training Loss: 0.25606521670709254, Avg Validation Loss: 50.351011168302996\n",
            "Epoch 630/1000, Avg Training Loss: 0.2560625051043429, Avg Validation Loss: 50.35219453637826\n",
            "Epoch 631/1000, Avg Training Loss: 0.2560629368310492, Avg Validation Loss: 50.353959048871246\n",
            "Epoch 632/1000, Avg Training Loss: 0.25606354770872813, Avg Validation Loss: 50.35523744641773\n",
            "Epoch 633/1000, Avg Training Loss: 0.2560620023030947, Avg Validation Loss: 50.356627311267076\n",
            "Epoch 634/1000, Avg Training Loss: 0.25606303679246034, Avg Validation Loss: 50.35778779719129\n",
            "Epoch 635/1000, Avg Training Loss: 0.25606305850595784, Avg Validation Loss: 50.35841921630141\n",
            "Epoch 636/1000, Avg Training Loss: 0.2560628061515773, Avg Validation Loss: 50.359506553813176\n",
            "Epoch 637/1000, Avg Training Loss: 0.2560633756593464, Avg Validation Loss: 50.36065904739206\n",
            "Epoch 638/1000, Avg Training Loss: 0.25606649835222395, Avg Validation Loss: 50.36168899076581\n",
            "Epoch 639/1000, Avg Training Loss: 0.25606711218896366, Avg Validation Loss: 50.36353228984697\n",
            "Epoch 640/1000, Avg Training Loss: 0.256069908298053, Avg Validation Loss: 50.36483820154865\n",
            "Epoch 641/1000, Avg Training Loss: 0.2560722862473369, Avg Validation Loss: 50.365113603923234\n",
            "Epoch 642/1000, Avg Training Loss: 0.25607227085770556, Avg Validation Loss: 50.36657237571717\n",
            "Epoch 643/1000, Avg Training Loss: 0.25607469264140326, Avg Validation Loss: 50.36843671280471\n",
            "Epoch 644/1000, Avg Training Loss: 0.2560770471720675, Avg Validation Loss: 50.369635219421696\n",
            "Epoch 645/1000, Avg Training Loss: 0.2560790098443814, Avg Validation Loss: 50.37115771536163\n",
            "Epoch 646/1000, Avg Training Loss: 0.2560827764516194, Avg Validation Loss: 50.371959846311455\n",
            "Epoch 647/1000, Avg Training Loss: 0.2560808901527714, Avg Validation Loss: 50.372876740481445\n",
            "Epoch 648/1000, Avg Training Loss: 0.2560813652189012, Avg Validation Loss: 50.37401388563258\n",
            "Epoch 649/1000, Avg Training Loss: 0.25608324949531325, Avg Validation Loss: 50.37483149869194\n",
            "Epoch 650/1000, Avg Training Loss: 0.25608362642485455, Avg Validation Loss: 50.376193373640746\n",
            "Epoch 651/1000, Avg Training Loss: 0.2560840986032471, Avg Validation Loss: 50.37673877838553\n",
            "Epoch 652/1000, Avg Training Loss: 0.2560843136257978, Avg Validation Loss: 50.37821578890649\n",
            "Epoch 653/1000, Avg Training Loss: 0.25608468761742725, Avg Validation Loss: 50.37935527128923\n",
            "Epoch 654/1000, Avg Training Loss: 0.2560830516481664, Avg Validation Loss: 50.38062596057425\n",
            "Epoch 655/1000, Avg Training Loss: 0.25607983592942796, Avg Validation Loss: 50.38158428986146\n",
            "Epoch 656/1000, Avg Training Loss: 0.25607930774645044, Avg Validation Loss: 50.38277294979322\n",
            "Epoch 657/1000, Avg Training Loss: 0.25607696541280317, Avg Validation Loss: 50.384207564911044\n",
            "Epoch 658/1000, Avg Training Loss: 0.25607886405198477, Avg Validation Loss: 50.38576596054859\n",
            "Epoch 659/1000, Avg Training Loss: 0.2560788673588877, Avg Validation Loss: 50.3867968153627\n",
            "Epoch 660/1000, Avg Training Loss: 0.2560795162486007, Avg Validation Loss: 50.387794467173265\n",
            "Epoch 661/1000, Avg Training Loss: 0.25608057133292056, Avg Validation Loss: 50.38905006116947\n",
            "Epoch 662/1000, Avg Training Loss: 0.2560827500383439, Avg Validation Loss: 50.390168511024555\n",
            "Epoch 663/1000, Avg Training Loss: 0.25608355852896114, Avg Validation Loss: 50.39138975438377\n",
            "Epoch 664/1000, Avg Training Loss: 0.25608343526297545, Avg Validation Loss: 50.392424228823614\n",
            "Epoch 665/1000, Avg Training Loss: 0.2560822070302746, Avg Validation Loss: 50.39414371917255\n",
            "Epoch 666/1000, Avg Training Loss: 0.25608300330213524, Avg Validation Loss: 50.395420446515196\n",
            "Epoch 667/1000, Avg Training Loss: 0.2560801087107367, Avg Validation Loss: 50.395941475911684\n",
            "Epoch 668/1000, Avg Training Loss: 0.25608172676248286, Avg Validation Loss: 50.397761638804084\n",
            "Epoch 669/1000, Avg Training Loss: 0.256084165239147, Avg Validation Loss: 50.39904679365751\n",
            "Epoch 670/1000, Avg Training Loss: 0.25608283507325175, Avg Validation Loss: 50.39976011447081\n",
            "Epoch 671/1000, Avg Training Loss: 0.25608255558678217, Avg Validation Loss: 50.40126577156549\n",
            "Epoch 672/1000, Avg Training Loss: 0.25608250029329893, Avg Validation Loss: 50.40239992389685\n",
            "Epoch 673/1000, Avg Training Loss: 0.2560823269028822, Avg Validation Loss: 50.40395143588193\n",
            "Epoch 674/1000, Avg Training Loss: 0.2560848013551124, Avg Validation Loss: 50.40517071624454\n",
            "Epoch 675/1000, Avg Training Loss: 0.2560832969344256, Avg Validation Loss: 50.40559664891596\n",
            "Epoch 676/1000, Avg Training Loss: 0.2560813632598614, Avg Validation Loss: 50.4067671702529\n",
            "Epoch 677/1000, Avg Training Loss: 0.2560815908041182, Avg Validation Loss: 50.40716813669298\n",
            "Epoch 678/1000, Avg Training Loss: 0.2560804777036639, Avg Validation Loss: 50.40868842221999\n",
            "Epoch 679/1000, Avg Training Loss: 0.2560797015181662, Avg Validation Loss: 50.409998152775614\n",
            "Epoch 680/1000, Avg Training Loss: 0.25608346912053315, Avg Validation Loss: 50.411337352234895\n",
            "Epoch 681/1000, Avg Training Loss: 0.25608263413158844, Avg Validation Loss: 50.41215342628648\n",
            "Epoch 682/1000, Avg Training Loss: 0.2560824238092476, Avg Validation Loss: 50.413235347955066\n",
            "Epoch 683/1000, Avg Training Loss: 0.2560800186109173, Avg Validation Loss: 50.41401988727584\n",
            "Epoch 684/1000, Avg Training Loss: 0.2560770397621599, Avg Validation Loss: 50.415486627650935\n",
            "Epoch 685/1000, Avg Training Loss: 0.25607911333146743, Avg Validation Loss: 50.41673905169695\n",
            "Epoch 686/1000, Avg Training Loss: 0.2560765268352151, Avg Validation Loss: 50.41720484437312\n",
            "Epoch 687/1000, Avg Training Loss: 0.25607495527775975, Avg Validation Loss: 50.41856285731918\n",
            "Epoch 688/1000, Avg Training Loss: 0.25606942068815486, Avg Validation Loss: 50.41970879986522\n",
            "Epoch 689/1000, Avg Training Loss: 0.256066996914192, Avg Validation Loss: 50.42084621764878\n",
            "Epoch 690/1000, Avg Training Loss: 0.2560674571932274, Avg Validation Loss: 50.42172372549311\n",
            "Epoch 691/1000, Avg Training Loss: 0.2560646439958985, Avg Validation Loss: 50.422817558500114\n",
            "Epoch 692/1000, Avg Training Loss: 0.25606732386239894, Avg Validation Loss: 50.423706673866015\n",
            "Epoch 693/1000, Avg Training Loss: 0.2560664237328828, Avg Validation Loss: 50.4242604031008\n",
            "Epoch 694/1000, Avg Training Loss: 0.2560654849223117, Avg Validation Loss: 50.4251964199611\n",
            "Epoch 695/1000, Avg Training Loss: 0.2560648976909125, Avg Validation Loss: 50.42649493128468\n",
            "Epoch 696/1000, Avg Training Loss: 0.25606530783635445, Avg Validation Loss: 50.42750196027313\n",
            "Epoch 697/1000, Avg Training Loss: 0.2560663165323136, Avg Validation Loss: 50.42785082610243\n",
            "Epoch 698/1000, Avg Training Loss: 0.25606651850754486, Avg Validation Loss: 50.42939369256386\n",
            "Epoch 699/1000, Avg Training Loss: 0.2560665952585929, Avg Validation Loss: 50.43006209031563\n",
            "Epoch 700/1000, Avg Training Loss: 0.2560697306208783, Avg Validation Loss: 50.4304633853114\n",
            "Epoch 701/1000, Avg Training Loss: 0.2560691933445712, Avg Validation Loss: 50.43135784305282\n",
            "Epoch 702/1000, Avg Training Loss: 0.2560679655390235, Avg Validation Loss: 50.43195778748066\n",
            "Epoch 703/1000, Avg Training Loss: 0.2560641116035158, Avg Validation Loss: 50.43291464889794\n",
            "Epoch 704/1000, Avg Training Loss: 0.25606181440334747, Avg Validation Loss: 50.434345224602794\n",
            "Epoch 705/1000, Avg Training Loss: 0.25605841844360205, Avg Validation Loss: 50.43550257734735\n",
            "Epoch 706/1000, Avg Training Loss: 0.25605581468797106, Avg Validation Loss: 50.43566443660447\n",
            "Epoch 707/1000, Avg Training Loss: 0.2560508707364912, Avg Validation Loss: 50.43666420161735\n",
            "Epoch 708/1000, Avg Training Loss: 0.2560474254401075, Avg Validation Loss: 50.436596662534626\n",
            "Epoch 709/1000, Avg Training Loss: 0.2560409642269941, Avg Validation Loss: 50.437166179087896\n",
            "Epoch 710/1000, Avg Training Loss: 0.2560357084100892, Avg Validation Loss: 50.437671297069855\n",
            "Epoch 711/1000, Avg Training Loss: 0.2560309548811188, Avg Validation Loss: 50.43859958587427\n",
            "Epoch 712/1000, Avg Training Loss: 0.25602817367711705, Avg Validation Loss: 50.43954603590098\n",
            "Epoch 713/1000, Avg Training Loss: 0.2560223595771246, Avg Validation Loss: 50.43995565576421\n",
            "Epoch 714/1000, Avg Training Loss: 0.2560181361085468, Avg Validation Loss: 50.44083430647863\n",
            "Epoch 715/1000, Avg Training Loss: 0.25601353141024386, Avg Validation Loss: 50.441254400371974\n",
            "Epoch 716/1000, Avg Training Loss: 0.256009004459108, Avg Validation Loss: 50.44214188783208\n",
            "Epoch 717/1000, Avg Training Loss: 0.2560050978166257, Avg Validation Loss: 50.442367689285234\n",
            "Epoch 718/1000, Avg Training Loss: 0.2559984198188308, Avg Validation Loss: 50.44306505367685\n",
            "Epoch 719/1000, Avg Training Loss: 0.25599548677971046, Avg Validation Loss: 50.44373788128054\n",
            "Epoch 720/1000, Avg Training Loss: 0.2559918366829023, Avg Validation Loss: 50.44457519892081\n",
            "Epoch 721/1000, Avg Training Loss: 0.25598721391459056, Avg Validation Loss: 50.44508760807881\n",
            "Epoch 722/1000, Avg Training Loss: 0.2559846246500998, Avg Validation Loss: 50.445439436020465\n",
            "Epoch 723/1000, Avg Training Loss: 0.25597877550544934, Avg Validation Loss: 50.44553713010028\n",
            "Epoch 724/1000, Avg Training Loss: 0.2559759905968692, Avg Validation Loss: 50.446562067759146\n",
            "Epoch 725/1000, Avg Training Loss: 0.2559696692144113, Avg Validation Loss: 50.44683324828953\n",
            "Epoch 726/1000, Avg Training Loss: 0.2559659304736599, Avg Validation Loss: 50.44692569949355\n",
            "Epoch 727/1000, Avg Training Loss: 0.2559613553526685, Avg Validation Loss: 50.44819690497539\n",
            "Epoch 728/1000, Avg Training Loss: 0.25595854282931474, Avg Validation Loss: 50.44859218796294\n",
            "Epoch 729/1000, Avg Training Loss: 0.2559525907056983, Avg Validation Loss: 50.448654409600124\n",
            "Epoch 730/1000, Avg Training Loss: 0.25594686676576717, Avg Validation Loss: 50.449392782569774\n",
            "Epoch 731/1000, Avg Training Loss: 0.25594582211166267, Avg Validation Loss: 50.44934447724735\n",
            "Epoch 732/1000, Avg Training Loss: 0.2559367354412568, Avg Validation Loss: 50.45088546487979\n",
            "Epoch 733/1000, Avg Training Loss: 0.2559324451437927, Avg Validation Loss: 50.45101058288185\n",
            "Epoch 734/1000, Avg Training Loss: 0.25593026293875987, Avg Validation Loss: 50.45138606557539\n",
            "Epoch 735/1000, Avg Training Loss: 0.25592388909408254, Avg Validation Loss: 50.45205862692542\n",
            "Epoch 736/1000, Avg Training Loss: 0.2559206093685961, Avg Validation Loss: 50.4528094587097\n",
            "Epoch 737/1000, Avg Training Loss: 0.25591654987689455, Avg Validation Loss: 50.45325498863521\n",
            "Epoch 738/1000, Avg Training Loss: 0.25591276725353135, Avg Validation Loss: 50.45363751347438\n",
            "Epoch 739/1000, Avg Training Loss: 0.25590677119721805, Avg Validation Loss: 50.453995796903\n",
            "Epoch 740/1000, Avg Training Loss: 0.25590191102140625, Avg Validation Loss: 50.45458504640473\n",
            "Epoch 741/1000, Avg Training Loss: 0.25589627108721297, Avg Validation Loss: 50.455252009104\n",
            "Epoch 742/1000, Avg Training Loss: 0.2558909909335642, Avg Validation Loss: 50.4550953643729\n",
            "Epoch 743/1000, Avg Training Loss: 0.2558867533311626, Avg Validation Loss: 50.45534078872101\n",
            "Epoch 744/1000, Avg Training Loss: 0.25587979542085376, Avg Validation Loss: 50.45549716597799\n",
            "Epoch 745/1000, Avg Training Loss: 0.25587602100764867, Avg Validation Loss: 50.45545329210445\n",
            "Epoch 746/1000, Avg Training Loss: 0.2558715411177578, Avg Validation Loss: 50.45564731567344\n",
            "Epoch 747/1000, Avg Training Loss: 0.25586801384151814, Avg Validation Loss: 50.45620448156884\n",
            "Epoch 748/1000, Avg Training Loss: 0.25586585059702643, Avg Validation Loss: 50.45700290140887\n",
            "Epoch 749/1000, Avg Training Loss: 0.2558648426087723, Avg Validation Loss: 50.45780937847284\n",
            "Epoch 750/1000, Avg Training Loss: 0.2558594122750606, Avg Validation Loss: 50.45819460989985\n",
            "Epoch 751/1000, Avg Training Loss: 0.2558564333546155, Avg Validation Loss: 50.45901351670455\n",
            "Epoch 752/1000, Avg Training Loss: 0.25585351716676774, Avg Validation Loss: 50.45926228457748\n",
            "Epoch 753/1000, Avg Training Loss: 0.2558493925431279, Avg Validation Loss: 50.45975629504137\n",
            "Epoch 754/1000, Avg Training Loss: 0.255845173855123, Avg Validation Loss: 50.46052348186866\n",
            "Epoch 755/1000, Avg Training Loss: 0.2558413497598442, Avg Validation Loss: 50.461476232322724\n",
            "Epoch 756/1000, Avg Training Loss: 0.25584073218749476, Avg Validation Loss: 50.46247590077415\n",
            "Epoch 757/1000, Avg Training Loss: 0.2558391834789171, Avg Validation Loss: 50.46297246700228\n",
            "Epoch 758/1000, Avg Training Loss: 0.25583794367545104, Avg Validation Loss: 50.46373827190297\n",
            "Epoch 759/1000, Avg Training Loss: 0.25583380804787004, Avg Validation Loss: 50.46440860820476\n",
            "Epoch 760/1000, Avg Training Loss: 0.25583155087673864, Avg Validation Loss: 50.465176498880965\n",
            "Epoch 761/1000, Avg Training Loss: 0.2558282602913031, Avg Validation Loss: 50.465521018051\n",
            "Epoch 762/1000, Avg Training Loss: 0.25582804745827065, Avg Validation Loss: 50.466218551196405\n",
            "Epoch 763/1000, Avg Training Loss: 0.25582730125329006, Avg Validation Loss: 50.466314851484356\n",
            "Epoch 764/1000, Avg Training Loss: 0.25582724805947654, Avg Validation Loss: 50.46707704551358\n",
            "Epoch 765/1000, Avg Training Loss: 0.25582698973782747, Avg Validation Loss: 50.46775407034586\n",
            "Epoch 766/1000, Avg Training Loss: 0.25582740059832404, Avg Validation Loss: 50.46827619546562\n",
            "Epoch 767/1000, Avg Training Loss: 0.2558276306066103, Avg Validation Loss: 50.46921841301437\n",
            "Epoch 768/1000, Avg Training Loss: 0.2558269421490391, Avg Validation Loss: 50.46938681826258\n",
            "Epoch 769/1000, Avg Training Loss: 0.25582390617561634, Avg Validation Loss: 50.470084145354875\n",
            "Epoch 770/1000, Avg Training Loss: 0.25582146219490254, Avg Validation Loss: 50.47125945710789\n",
            "Epoch 771/1000, Avg Training Loss: 0.2558203649498496, Avg Validation Loss: 50.47155324621512\n",
            "Epoch 772/1000, Avg Training Loss: 0.25581694142500416, Avg Validation Loss: 50.472194378119134\n",
            "Epoch 773/1000, Avg Training Loss: 0.25581568175751584, Avg Validation Loss: 50.47368614329521\n",
            "Epoch 774/1000, Avg Training Loss: 0.25581272594724164, Avg Validation Loss: 50.47447225341407\n",
            "Epoch 775/1000, Avg Training Loss: 0.2558105397734275, Avg Validation Loss: 50.47456684952556\n",
            "Epoch 776/1000, Avg Training Loss: 0.2558130331928007, Avg Validation Loss: 50.474039649846674\n",
            "Epoch 777/1000, Avg Training Loss: 0.2558060326223232, Avg Validation Loss: 50.47487145331764\n",
            "Epoch 778/1000, Avg Training Loss: 0.2558051941343172, Avg Validation Loss: 50.47575354756874\n",
            "Epoch 779/1000, Avg Training Loss: 0.2558037579651407, Avg Validation Loss: 50.47704998389773\n",
            "Epoch 780/1000, Avg Training Loss: 0.2557997848248852, Avg Validation Loss: 50.47769334392484\n",
            "Epoch 781/1000, Avg Training Loss: 0.2558005509371405, Avg Validation Loss: 50.47737582906121\n",
            "Epoch 782/1000, Avg Training Loss: 0.2557955934227251, Avg Validation Loss: 50.478177445005144\n",
            "Epoch 783/1000, Avg Training Loss: 0.2557934626325366, Avg Validation Loss: 50.478866598920575\n",
            "Epoch 784/1000, Avg Training Loss: 0.25579106493627085, Avg Validation Loss: 50.479856812535765\n",
            "Epoch 785/1000, Avg Training Loss: 0.25578996364593104, Avg Validation Loss: 50.480510731411556\n",
            "Epoch 786/1000, Avg Training Loss: 0.25578807672706305, Avg Validation Loss: 50.48175536405827\n",
            "Epoch 787/1000, Avg Training Loss: 0.2557861477097387, Avg Validation Loss: 50.4827985171132\n",
            "Epoch 788/1000, Avg Training Loss: 0.25578369313653876, Avg Validation Loss: 50.483567477782216\n",
            "Epoch 789/1000, Avg Training Loss: 0.2557814858982686, Avg Validation Loss: 50.484169178368255\n",
            "Epoch 790/1000, Avg Training Loss: 0.2557803228779495, Avg Validation Loss: 50.484582275149194\n",
            "Epoch 791/1000, Avg Training Loss: 0.25577789974229287, Avg Validation Loss: 50.485040187971904\n",
            "Epoch 792/1000, Avg Training Loss: 0.2557773022581254, Avg Validation Loss: 50.48559409412058\n",
            "Epoch 793/1000, Avg Training Loss: 0.25577734023105625, Avg Validation Loss: 50.486309429825155\n",
            "Epoch 794/1000, Avg Training Loss: 0.25577719659854503, Avg Validation Loss: 50.486537476182264\n",
            "Epoch 795/1000, Avg Training Loss: 0.25577668615991583, Avg Validation Loss: 50.48712562313913\n",
            "Epoch 796/1000, Avg Training Loss: 0.2557762299998492, Avg Validation Loss: 50.48812507445655\n",
            "Epoch 797/1000, Avg Training Loss: 0.25577570761585494, Avg Validation Loss: 50.488566466218906\n",
            "Epoch 798/1000, Avg Training Loss: 0.2557740525117857, Avg Validation Loss: 50.488985511531624\n",
            "Epoch 799/1000, Avg Training Loss: 0.2557728713763869, Avg Validation Loss: 50.49024301965164\n",
            "Epoch 800/1000, Avg Training Loss: 0.25577348598400873, Avg Validation Loss: 50.4903438426293\n",
            "Epoch 801/1000, Avg Training Loss: 0.25577052870879596, Avg Validation Loss: 50.491833199223905\n",
            "Epoch 802/1000, Avg Training Loss: 0.25576940964429073, Avg Validation Loss: 50.49251253534493\n",
            "Epoch 803/1000, Avg Training Loss: 0.2557680921493503, Avg Validation Loss: 50.49293194073032\n",
            "Epoch 804/1000, Avg Training Loss: 0.2557665763571786, Avg Validation Loss: 50.49396477664494\n",
            "Epoch 805/1000, Avg Training Loss: 0.25577053578816533, Avg Validation Loss: 50.49603067222125\n",
            "Epoch 806/1000, Avg Training Loss: 0.255765062597726, Avg Validation Loss: 50.49614746624367\n",
            "Epoch 807/1000, Avg Training Loss: 0.25576350298144424, Avg Validation Loss: 50.497147894343264\n",
            "Epoch 808/1000, Avg Training Loss: 0.2557628174927488, Avg Validation Loss: 50.497449106966286\n",
            "Epoch 809/1000, Avg Training Loss: 0.255763740409555, Avg Validation Loss: 50.498824514579084\n",
            "Epoch 810/1000, Avg Training Loss: 0.25575978563506385, Avg Validation Loss: 50.49984681932598\n",
            "Epoch 811/1000, Avg Training Loss: 0.2557579474565553, Avg Validation Loss: 50.50048785383959\n",
            "Epoch 812/1000, Avg Training Loss: 0.25575894114356673, Avg Validation Loss: 50.5014562026025\n",
            "Epoch 813/1000, Avg Training Loss: 0.25575658930056666, Avg Validation Loss: 50.50158331942546\n",
            "Epoch 814/1000, Avg Training Loss: 0.2557541754350576, Avg Validation Loss: 50.50254450963553\n",
            "Epoch 815/1000, Avg Training Loss: 0.2557556852355247, Avg Validation Loss: 50.50416971076348\n",
            "Epoch 816/1000, Avg Training Loss: 0.2557524053698556, Avg Validation Loss: 50.504718456524046\n",
            "Epoch 817/1000, Avg Training Loss: 0.25575244236032724, Avg Validation Loss: 50.50540355323474\n",
            "Epoch 818/1000, Avg Training Loss: 0.2557511094535469, Avg Validation Loss: 50.505871040442145\n",
            "Epoch 819/1000, Avg Training Loss: 0.25574932938173023, Avg Validation Loss: 50.50680763735053\n",
            "Epoch 820/1000, Avg Training Loss: 0.2557491499581297, Avg Validation Loss: 50.50812363736843\n",
            "Epoch 821/1000, Avg Training Loss: 0.25574796570099045, Avg Validation Loss: 50.50859077920093\n",
            "Epoch 822/1000, Avg Training Loss: 0.255746556751175, Avg Validation Loss: 50.50960303270007\n",
            "Epoch 823/1000, Avg Training Loss: 0.2557482421540555, Avg Validation Loss: 50.50973449395957\n",
            "Epoch 824/1000, Avg Training Loss: 0.25574613324990664, Avg Validation Loss: 50.51099282084023\n",
            "Epoch 825/1000, Avg Training Loss: 0.25574565074013306, Avg Validation Loss: 50.51203681960034\n",
            "Epoch 826/1000, Avg Training Loss: 0.2557455400915043, Avg Validation Loss: 50.51259104291236\n",
            "Epoch 827/1000, Avg Training Loss: 0.25574693528683845, Avg Validation Loss: 50.513908985502226\n",
            "Epoch 828/1000, Avg Training Loss: 0.25574753450646864, Avg Validation Loss: 50.51526800778741\n",
            "Epoch 829/1000, Avg Training Loss: 0.25574517249248335, Avg Validation Loss: 50.516565326045196\n",
            "Epoch 830/1000, Avg Training Loss: 0.2557444003768519, Avg Validation Loss: 50.51721359643521\n",
            "Epoch 831/1000, Avg Training Loss: 0.255745712064407, Avg Validation Loss: 50.51810700942181\n",
            "Epoch 832/1000, Avg Training Loss: 0.2557470631017345, Avg Validation Loss: 50.51931216005666\n",
            "Epoch 833/1000, Avg Training Loss: 0.255745072585993, Avg Validation Loss: 50.52017995603256\n",
            "Epoch 834/1000, Avg Training Loss: 0.25574489124148153, Avg Validation Loss: 50.52068674935057\n",
            "Epoch 835/1000, Avg Training Loss: 0.2557459520520888, Avg Validation Loss: 50.52180113014986\n",
            "Epoch 836/1000, Avg Training Loss: 0.2557456615053614, Avg Validation Loss: 50.521990344148776\n",
            "Epoch 837/1000, Avg Training Loss: 0.25574577008653737, Avg Validation Loss: 50.52316394500836\n",
            "Epoch 838/1000, Avg Training Loss: 0.25574548089461063, Avg Validation Loss: 50.524550149875495\n",
            "Epoch 839/1000, Avg Training Loss: 0.2557462637502752, Avg Validation Loss: 50.52509175180474\n",
            "Epoch 840/1000, Avg Training Loss: 0.25574644112231876, Avg Validation Loss: 50.52628185743402\n",
            "Epoch 841/1000, Avg Training Loss: 0.2557465319674957, Avg Validation Loss: 50.527733469895104\n",
            "Epoch 842/1000, Avg Training Loss: 0.2557474946311668, Avg Validation Loss: 50.528485521511655\n",
            "Epoch 843/1000, Avg Training Loss: 0.25574803604534774, Avg Validation Loss: 50.52843324104833\n",
            "Epoch 844/1000, Avg Training Loss: 0.2557484094577877, Avg Validation Loss: 50.52958652873869\n",
            "Epoch 845/1000, Avg Training Loss: 0.2557484657390968, Avg Validation Loss: 50.5303251127955\n",
            "Epoch 846/1000, Avg Training Loss: 0.25574924557843426, Avg Validation Loss: 50.531043154884394\n",
            "Epoch 847/1000, Avg Training Loss: 0.25575005849617233, Avg Validation Loss: 50.53200479929318\n",
            "Epoch 848/1000, Avg Training Loss: 0.2557507338358452, Avg Validation Loss: 50.533102043141525\n",
            "Epoch 849/1000, Avg Training Loss: 0.25575256644224387, Avg Validation Loss: 50.53463757858549\n",
            "Epoch 850/1000, Avg Training Loss: 0.2557545033760199, Avg Validation Loss: 50.53476939500912\n",
            "Epoch 851/1000, Avg Training Loss: 0.25575418010871964, Avg Validation Loss: 50.53594962583486\n",
            "Epoch 852/1000, Avg Training Loss: 0.25575547984035035, Avg Validation Loss: 50.53743794991051\n",
            "Epoch 853/1000, Avg Training Loss: 0.25575459930934996, Avg Validation Loss: 50.53801534041597\n",
            "Epoch 854/1000, Avg Training Loss: 0.2557566632964204, Avg Validation Loss: 50.538382225867444\n",
            "Epoch 855/1000, Avg Training Loss: 0.25576196564293174, Avg Validation Loss: 50.53844209324862\n",
            "Epoch 856/1000, Avg Training Loss: 0.25576085358833217, Avg Validation Loss: 50.53923661251531\n",
            "Epoch 857/1000, Avg Training Loss: 0.25576402661650316, Avg Validation Loss: 50.5398229885875\n",
            "Epoch 858/1000, Avg Training Loss: 0.2557643803175463, Avg Validation Loss: 50.54034279497429\n",
            "Epoch 859/1000, Avg Training Loss: 0.2557639000643157, Avg Validation Loss: 50.54176207483862\n",
            "Epoch 860/1000, Avg Training Loss: 0.2557657593580096, Avg Validation Loss: 50.54300317056659\n",
            "Epoch 861/1000, Avg Training Loss: 0.2557666174855147, Avg Validation Loss: 50.54370605867434\n",
            "Epoch 862/1000, Avg Training Loss: 0.25577095347799184, Avg Validation Loss: 50.543965193132664\n",
            "Epoch 863/1000, Avg Training Loss: 0.25577193165738615, Avg Validation Loss: 50.54495609070135\n",
            "Epoch 864/1000, Avg Training Loss: 0.25577510936133974, Avg Validation Loss: 50.54698730219518\n",
            "Epoch 865/1000, Avg Training Loss: 0.25577738965089597, Avg Validation Loss: 50.54876222844834\n",
            "Epoch 866/1000, Avg Training Loss: 0.25577561352366784, Avg Validation Loss: 50.54897325996926\n",
            "Epoch 867/1000, Avg Training Loss: 0.255777078091372, Avg Validation Loss: 50.54961758118387\n",
            "Epoch 868/1000, Avg Training Loss: 0.2557793694174464, Avg Validation Loss: 50.55002807861733\n",
            "Epoch 869/1000, Avg Training Loss: 0.25577817936683134, Avg Validation Loss: 50.550914171265156\n",
            "Epoch 870/1000, Avg Training Loss: 0.2557797800677849, Avg Validation Loss: 50.55177935941412\n",
            "Epoch 871/1000, Avg Training Loss: 0.2557817698710123, Avg Validation Loss: 50.553326263452455\n",
            "Epoch 872/1000, Avg Training Loss: 0.25578476530846017, Avg Validation Loss: 50.555220167820586\n",
            "Epoch 873/1000, Avg Training Loss: 0.2557849611554362, Avg Validation Loss: 50.55653078600655\n",
            "Epoch 874/1000, Avg Training Loss: 0.2557834725591212, Avg Validation Loss: 50.55700872878335\n",
            "Epoch 875/1000, Avg Training Loss: 0.2557867070553403, Avg Validation Loss: 50.55830341998576\n",
            "Epoch 876/1000, Avg Training Loss: 0.2557876220300022, Avg Validation Loss: 50.55841592463969\n",
            "Epoch 877/1000, Avg Training Loss: 0.25578808057983066, Avg Validation Loss: 50.55956115169554\n",
            "Epoch 878/1000, Avg Training Loss: 0.25578981280642277, Avg Validation Loss: 50.56078743612634\n",
            "Epoch 879/1000, Avg Training Loss: 0.2557920674604609, Avg Validation Loss: 50.561851104600706\n",
            "Epoch 880/1000, Avg Training Loss: 0.25579479398836447, Avg Validation Loss: 50.562247438983476\n",
            "Epoch 881/1000, Avg Training Loss: 0.25579537124250495, Avg Validation Loss: 50.5631884614985\n",
            "Epoch 882/1000, Avg Training Loss: 0.2557963266191939, Avg Validation Loss: 50.56402432846343\n",
            "Epoch 883/1000, Avg Training Loss: 0.2557986853477192, Avg Validation Loss: 50.565948074660284\n",
            "Epoch 884/1000, Avg Training Loss: 0.25579913691555506, Avg Validation Loss: 50.56620351632988\n",
            "Epoch 885/1000, Avg Training Loss: 0.25580018129194193, Avg Validation Loss: 50.56688291869801\n",
            "Epoch 886/1000, Avg Training Loss: 0.25579925700168354, Avg Validation Loss: 50.56848640578196\n",
            "Epoch 887/1000, Avg Training Loss: 0.25579939400342655, Avg Validation Loss: 50.568987070357\n",
            "Epoch 888/1000, Avg Training Loss: 0.25580096171042804, Avg Validation Loss: 50.57056404844789\n",
            "Epoch 889/1000, Avg Training Loss: 0.2558013112796039, Avg Validation Loss: 50.571219887808816\n",
            "Epoch 890/1000, Avg Training Loss: 0.25580162945148455, Avg Validation Loss: 50.57243903451601\n",
            "Epoch 891/1000, Avg Training Loss: 0.25580198387309083, Avg Validation Loss: 50.57317702935471\n",
            "Epoch 892/1000, Avg Training Loss: 0.25580440898853374, Avg Validation Loss: 50.57470399281718\n",
            "Epoch 893/1000, Avg Training Loss: 0.2558043126878453, Avg Validation Loss: 50.57585833228464\n",
            "Epoch 894/1000, Avg Training Loss: 0.255804250234071, Avg Validation Loss: 50.576779233950106\n",
            "Epoch 895/1000, Avg Training Loss: 0.25580457866374334, Avg Validation Loss: 50.57761618014932\n",
            "Epoch 896/1000, Avg Training Loss: 0.255805406816187, Avg Validation Loss: 50.578434123958885\n",
            "Epoch 897/1000, Avg Training Loss: 0.2558075035299599, Avg Validation Loss: 50.57980783269136\n",
            "Epoch 898/1000, Avg Training Loss: 0.25580822744613235, Avg Validation Loss: 50.58051735218913\n",
            "Epoch 899/1000, Avg Training Loss: 0.2558078071831297, Avg Validation Loss: 50.58103246231009\n",
            "Epoch 900/1000, Avg Training Loss: 0.2558097060519723, Avg Validation Loss: 50.58204913837855\n",
            "Epoch 901/1000, Avg Training Loss: 0.255811493679249, Avg Validation Loss: 50.58236550071608\n",
            "Epoch 902/1000, Avg Training Loss: 0.2558112690098581, Avg Validation Loss: 50.583492792893566\n",
            "Epoch 903/1000, Avg Training Loss: 0.2558151955944687, Avg Validation Loss: 50.58350242518897\n",
            "Epoch 904/1000, Avg Training Loss: 0.2558134812341243, Avg Validation Loss: 50.585229120775985\n",
            "Epoch 905/1000, Avg Training Loss: 0.25581491706005766, Avg Validation Loss: 50.58633690212016\n",
            "Epoch 906/1000, Avg Training Loss: 0.2558152038938553, Avg Validation Loss: 50.58744665538555\n",
            "Epoch 907/1000, Avg Training Loss: 0.25581709366911703, Avg Validation Loss: 50.588861447557775\n",
            "Epoch 908/1000, Avg Training Loss: 0.25581793595372315, Avg Validation Loss: 50.58942914324888\n",
            "Epoch 909/1000, Avg Training Loss: 0.25581893669791733, Avg Validation Loss: 50.59076926472947\n",
            "Epoch 910/1000, Avg Training Loss: 0.2558203630226986, Avg Validation Loss: 50.591502652741006\n",
            "Epoch 911/1000, Avg Training Loss: 0.25582109851869367, Avg Validation Loss: 50.59209402629328\n",
            "Epoch 912/1000, Avg Training Loss: 0.2558229427426774, Avg Validation Loss: 50.59335409674654\n",
            "Epoch 913/1000, Avg Training Loss: 0.2558238913722885, Avg Validation Loss: 50.59439897148837\n",
            "Epoch 914/1000, Avg Training Loss: 0.2558264761636274, Avg Validation Loss: 50.59586703396654\n",
            "Epoch 915/1000, Avg Training Loss: 0.25582750346196004, Avg Validation Loss: 50.59701312894224\n",
            "Epoch 916/1000, Avg Training Loss: 0.2558287067713722, Avg Validation Loss: 50.59752273230775\n",
            "Epoch 917/1000, Avg Training Loss: 0.2558308231123939, Avg Validation Loss: 50.598033750772245\n",
            "Epoch 918/1000, Avg Training Loss: 0.25583328396347377, Avg Validation Loss: 50.598194840160346\n",
            "Epoch 919/1000, Avg Training Loss: 0.2558349440322024, Avg Validation Loss: 50.59979511009179\n",
            "Epoch 920/1000, Avg Training Loss: 0.2558354126689413, Avg Validation Loss: 50.600156576595836\n",
            "Epoch 921/1000, Avg Training Loss: 0.2558428165647252, Avg Validation Loss: 50.60257128692429\n",
            "Epoch 922/1000, Avg Training Loss: 0.25584180706067544, Avg Validation Loss: 50.60357553923225\n",
            "Epoch 923/1000, Avg Training Loss: 0.25584088877448186, Avg Validation Loss: 50.60413921624601\n",
            "Epoch 924/1000, Avg Training Loss: 0.2558430386497429, Avg Validation Loss: 50.604376682676914\n",
            "Epoch 925/1000, Avg Training Loss: 0.255846146673981, Avg Validation Loss: 50.606028696930466\n",
            "Epoch 926/1000, Avg Training Loss: 0.2558459480497764, Avg Validation Loss: 50.606756173206904\n",
            "Epoch 927/1000, Avg Training Loss: 0.25584916900429566, Avg Validation Loss: 50.6070467132723\n",
            "Epoch 928/1000, Avg Training Loss: 0.25585079209462436, Avg Validation Loss: 50.60868315263347\n",
            "Epoch 929/1000, Avg Training Loss: 0.2558524764661753, Avg Validation Loss: 50.608699178355934\n",
            "Epoch 930/1000, Avg Training Loss: 0.25585571824316455, Avg Validation Loss: 50.61046444043723\n",
            "Epoch 931/1000, Avg Training Loss: 0.2558582658977817, Avg Validation Loss: 50.610747552008306\n",
            "Epoch 932/1000, Avg Training Loss: 0.25585749466881647, Avg Validation Loss: 50.61193321227056\n",
            "Epoch 933/1000, Avg Training Loss: 0.25586116259596364, Avg Validation Loss: 50.613719155760336\n",
            "Epoch 934/1000, Avg Training Loss: 0.255860668603641, Avg Validation Loss: 50.61400551194456\n",
            "Epoch 935/1000, Avg Training Loss: 0.2558629481112937, Avg Validation Loss: 50.61512786619636\n",
            "Epoch 936/1000, Avg Training Loss: 0.2558695822727511, Avg Validation Loss: 50.61698898578804\n",
            "Epoch 937/1000, Avg Training Loss: 0.2558662453384257, Avg Validation Loss: 50.61757450707879\n",
            "Epoch 938/1000, Avg Training Loss: 0.2558671171571432, Avg Validation Loss: 50.61823618947299\n",
            "Epoch 939/1000, Avg Training Loss: 0.25586731647129474, Avg Validation Loss: 50.61950309938187\n",
            "Epoch 940/1000, Avg Training Loss: 0.25586875306145246, Avg Validation Loss: 50.62051986230382\n",
            "Epoch 941/1000, Avg Training Loss: 0.2558706697459947, Avg Validation Loss: 50.6215594952962\n",
            "Epoch 942/1000, Avg Training Loss: 0.2558733857150913, Avg Validation Loss: 50.62276180642077\n",
            "Epoch 943/1000, Avg Training Loss: 0.2558770186863448, Avg Validation Loss: 50.62454943236507\n",
            "Epoch 944/1000, Avg Training Loss: 0.25587695374837705, Avg Validation Loss: 50.624860245166666\n",
            "Epoch 945/1000, Avg Training Loss: 0.2558788228074974, Avg Validation Loss: 50.626023395038274\n",
            "Epoch 946/1000, Avg Training Loss: 0.25588084117121396, Avg Validation Loss: 50.62636956300457\n",
            "Epoch 947/1000, Avg Training Loss: 0.2558822302089293, Avg Validation Loss: 50.62675984859717\n",
            "Epoch 948/1000, Avg Training Loss: 0.2558850936754163, Avg Validation Loss: 50.627932197728015\n",
            "Epoch 949/1000, Avg Training Loss: 0.2558869496734084, Avg Validation Loss: 50.62870392958712\n",
            "Epoch 950/1000, Avg Training Loss: 0.2558882163101558, Avg Validation Loss: 50.62944355507506\n",
            "Epoch 951/1000, Avg Training Loss: 0.2558918646682089, Avg Validation Loss: 50.63092215851809\n",
            "Epoch 952/1000, Avg Training Loss: 0.25589466948762374, Avg Validation Loss: 50.63218212049235\n",
            "Epoch 953/1000, Avg Training Loss: 0.2558973648453765, Avg Validation Loss: 50.63173905553765\n",
            "Epoch 954/1000, Avg Training Loss: 0.25589786774846757, Avg Validation Loss: 50.63244812719896\n",
            "Epoch 955/1000, Avg Training Loss: 0.255901783753957, Avg Validation Loss: 50.63306104243132\n",
            "Epoch 956/1000, Avg Training Loss: 0.25590291972878504, Avg Validation Loss: 50.63397928847162\n",
            "Epoch 957/1000, Avg Training Loss: 0.2559058262782948, Avg Validation Loss: 50.634829332699816\n",
            "Epoch 958/1000, Avg Training Loss: 0.2559082193850763, Avg Validation Loss: 50.63618195842272\n",
            "Epoch 959/1000, Avg Training Loss: 0.2559099548064999, Avg Validation Loss: 50.636321420791816\n",
            "Epoch 960/1000, Avg Training Loss: 0.2559129598617511, Avg Validation Loss: 50.63794322796629\n",
            "Epoch 961/1000, Avg Training Loss: 0.25591387071302907, Avg Validation Loss: 50.638524143769146\n",
            "Epoch 962/1000, Avg Training Loss: 0.2559163423889078, Avg Validation Loss: 50.63965973290287\n",
            "Epoch 963/1000, Avg Training Loss: 0.2559182227434339, Avg Validation Loss: 50.64035688723513\n",
            "Epoch 964/1000, Avg Training Loss: 0.2559214298206482, Avg Validation Loss: 50.641736146585245\n",
            "Epoch 965/1000, Avg Training Loss: 0.25592404916404676, Avg Validation Loss: 50.64266576102223\n",
            "Epoch 966/1000, Avg Training Loss: 0.25592551541362213, Avg Validation Loss: 50.643339413076404\n",
            "Epoch 967/1000, Avg Training Loss: 0.255928046878587, Avg Validation Loss: 50.64412056105113\n",
            "Epoch 968/1000, Avg Training Loss: 0.25593079864719753, Avg Validation Loss: 50.64503808304574\n",
            "Epoch 969/1000, Avg Training Loss: 0.2559347480220251, Avg Validation Loss: 50.646552303322196\n",
            "Epoch 970/1000, Avg Training Loss: 0.2559357474328272, Avg Validation Loss: 50.64682805413457\n",
            "Epoch 971/1000, Avg Training Loss: 0.25593931855059304, Avg Validation Loss: 50.646781411904726\n",
            "Epoch 972/1000, Avg Training Loss: 0.2559395093727294, Avg Validation Loss: 50.6476586611891\n",
            "Epoch 973/1000, Avg Training Loss: 0.2559418344267306, Avg Validation Loss: 50.648963512603814\n",
            "Epoch 974/1000, Avg Training Loss: 0.25594611813876383, Avg Validation Loss: 50.64981641122259\n",
            "Epoch 975/1000, Avg Training Loss: 0.2559467616063321, Avg Validation Loss: 50.651058022146465\n",
            "Epoch 976/1000, Avg Training Loss: 0.2559485889439531, Avg Validation Loss: 50.6522491009922\n",
            "Epoch 977/1000, Avg Training Loss: 0.25595219526146146, Avg Validation Loss: 50.65270940400252\n",
            "Epoch 978/1000, Avg Training Loss: 0.25595378733068064, Avg Validation Loss: 50.6537972384363\n",
            "Epoch 979/1000, Avg Training Loss: 0.2559573509722877, Avg Validation Loss: 50.65397966961015\n",
            "Epoch 980/1000, Avg Training Loss: 0.2559584159500494, Avg Validation Loss: 50.6548427846248\n",
            "Epoch 981/1000, Avg Training Loss: 0.25596155170056684, Avg Validation Loss: 50.656416748923384\n",
            "Epoch 982/1000, Avg Training Loss: 0.2559650817226362, Avg Validation Loss: 50.65747384177229\n",
            "Epoch 983/1000, Avg Training Loss: 0.255966590047253, Avg Validation Loss: 50.657387604904685\n",
            "Epoch 984/1000, Avg Training Loss: 0.25596723442481895, Avg Validation Loss: 50.658784192771165\n",
            "Epoch 985/1000, Avg Training Loss: 0.2559702221766512, Avg Validation Loss: 50.65995455022055\n",
            "Epoch 986/1000, Avg Training Loss: 0.2559724260131532, Avg Validation Loss: 50.66098124804246\n",
            "Epoch 987/1000, Avg Training Loss: 0.25597524327418353, Avg Validation Loss: 50.661768566350375\n",
            "Epoch 988/1000, Avg Training Loss: 0.25597849827832914, Avg Validation Loss: 50.66281207500242\n",
            "Epoch 989/1000, Avg Training Loss: 0.255979340473107, Avg Validation Loss: 50.66350347261482\n",
            "Epoch 990/1000, Avg Training Loss: 0.25598387841262377, Avg Validation Loss: 50.66502312311623\n",
            "Epoch 991/1000, Avg Training Loss: 0.2559858154215485, Avg Validation Loss: 50.66574202348261\n",
            "Epoch 992/1000, Avg Training Loss: 0.25598752259642443, Avg Validation Loss: 50.665578477005916\n",
            "Epoch 993/1000, Avg Training Loss: 0.25598879951849374, Avg Validation Loss: 50.66693056042001\n",
            "Epoch 994/1000, Avg Training Loss: 0.255991301209766, Avg Validation Loss: 50.66775181664114\n",
            "Epoch 995/1000, Avg Training Loss: 0.25599384834733974, Avg Validation Loss: 50.66855892806972\n",
            "Epoch 996/1000, Avg Training Loss: 0.25599641149334196, Avg Validation Loss: 50.66967970396554\n",
            "Epoch 997/1000, Avg Training Loss: 0.25600110300671014, Avg Validation Loss: 50.66973266747238\n",
            "Epoch 998/1000, Avg Training Loss: 0.2560022831802387, Avg Validation Loss: 50.67119351111986\n",
            "Epoch 999/1000, Avg Training Loss: 0.256005156156507, Avg Validation Loss: 50.67261065834052\n",
            "Epoch 1000/1000, Avg Training Loss: 0.256007518952959, Avg Validation Loss: 50.673106900452694\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x_tot = np.random.rand(1000, 15)\n",
        "target = np.random.rand(1000, 3)\n",
        "\n",
        "layer_one = Layer(15, 8, ELU, d_ELU)\n",
        "layer_two = Layer(8, 5, ELU, d_ELU)\n",
        "layer_out = Layer(5, 3, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "NN.add_layer(layer_one)\n",
        "NN.add_layer(layer_two)\n",
        "NN.add_layer(layer_out)\n",
        "\n",
        "# Parametri di training\n",
        "K = 5\n",
        "epochs = 1000\n",
        "learning_rate_w = 0.0001 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
        "                         # learning_rate of minibatch for number of the examples in the minibatch\n",
        "learning_rate_b = 0.0001\n",
        "batch_size = 100\n",
        "Lambda_t = 0.1\n",
        "Lambda_l = 0.1\n",
        "momentum = 0.7 # totalmente a caso\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# Cross-validation\n",
        "train_error, val_error = NN.train_val(x_tot, target, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, mean_squared_error, d_mean_squared_error, batch_size, beta_1, beta_2, epsilon, 'elastic', 'adam')\n",
        "\n",
        "# Plot degli errori\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_error, label='Training Error')\n",
        "plt.plot(val_error, label='Validation Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Error')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}