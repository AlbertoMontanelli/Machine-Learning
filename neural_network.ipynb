{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives.\n",
    "* **Hidden layers**:\n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qv1wwlgWsFM8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
    "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
    "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha):\n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjNvxQOLsFM-"
   },
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters :**\n",
    " * input_size : $k_{i-1}$ ;\n",
    " * output_size : $k_i$ ;\n",
    " * activation_function ;\n",
    " * activation_derivative . \n",
    "\n",
    "**Constructor attributes :**\n",
    "* self.input_size = input_size;\n",
    "* self.output_size = output_size;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative;\n",
    "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
    "\n",
    "**Methods :**\n",
    "\n",
    "* **initialize_weights**: initialize weights and biases\n",
    "  * attributes :\n",
    "    * self.weights : $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix . \\\n",
    "      Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$ ;\n",
    "    * self.biases : $1 \\ \\textrm{x} \\ k_i$ array. Initialized to zeros;\n",
    "* **forward_layer** : allows to compute the output of the layer for a given input.\n",
    "  * parameter :\n",
    "    * input_array : matrix $X$ (see above for the case $L_0$ or $L_i$) .\n",
    "  * attributes :\n",
    "    * self.input : input_array ;\n",
    "    * self.net : net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$) .\n",
    "  * return -> output = $f(net)$, where $f$ is the activation function; $f(net)$ has the same dimensions of $net$.\n",
    "  \n",
    "* **backward_layer** : computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    "  * parameters :\n",
    "    * d_Ep : target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "    * learning_rate.\n",
    "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H8Ap9rLxLlNU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
    "        self.biases = np.zeros((1, self.output_size))\n",
    "        \n",
    "        \n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward_layer(self, d_Ep, learning_rate, Lambda, reg_type):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "        if (reg_type=='tikhonov'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*self.weights # learning rule - tikhonov regularization\n",
    "        elif (reg_type=='lasso'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*np.sign(self.weights) # learning rule - lasso regularization\n",
    "        elif (reg_type=='elastic'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*self.weights - Lambda*np.sign(self.weights) # learning rule:\n",
    "                                                                                                                             # lasso + tikhonov\n",
    "                                                                                                                             # regularization\n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    " * **data_split**: splits the input data into training set, validation set and test set\n",
    "   * parameter:\n",
    "     * x_tot: total data given as input;\n",
    "     * target: total data labels given as input;\n",
    "     * test_split: percentile of test set with respect to the total data.\n",
    "    * return->:\n",
    "      * x_train_val: training set+validation set for input data;\n",
    "      * target_train_val: training set+validation set for input data labels;\n",
    "      * x_test_val: test set for input data;\n",
    "      * target_test_val: test set for input data labels.\n",
    "\n",
    "     \n",
    " * **add_layer**: appends a layer to the empty list self.layers\n",
    "   * parameter:\n",
    "     * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    "  * parameter:\n",
    "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    "  * return -> input = $o_i$ for layer $L_i$.\n",
    "  \n",
    "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    "  * parameter:\n",
    "    * d_Ep;\n",
    "    * learning_rate.\n",
    "\n",
    "* **train_online**: applies the forward and backward method to the network for a specified number of epochs **one example at a time**.\n",
    "  * parameter:\n",
    "    * x_train: input matrix $X$;\n",
    "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "    * epochs: number of the iterations of the training algorithm;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative.\n",
    "\n",
    "* **train_minibatch**: applies the forward and backward method to the network for a specified number of epochs **to batches of $l' < l$** examples.\n",
    "  * parameter:\n",
    "    * x_train: input matrix $X$;\n",
    "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "    * epochs: number of the iterations of the training algorithm;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative;\n",
    "    * batch_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def data_split(self, x_tot, target, test_split):\n",
    "        \n",
    "        num_samples = x_tot.shape[0] # the total number of the examples in input\n",
    "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
    "        indices = num_samples # if we don't want the randomization and the shuffle of the examples\n",
    "        \n",
    "        #indices = np.arange(num_samples)\n",
    "        #np.random.shuffle(indices)\n",
    "\n",
    "        x_tot = x_tot[indices]\n",
    "        target = target[indices]\n",
    "\n",
    "        x_test = x_tot[:test_size]\n",
    "        target_test = target[:test_size]\n",
    "        x_train_val = x_tot[test_size:]\n",
    "        target_train_val = target[test_size:]\n",
    "\n",
    "        return x_train_val, target_train_val, x_test, target_test\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, d_Ep, learning_rate, Lambda, reg_type):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate, Lambda, reg_type)\n",
    "\n",
    "    def reinitialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights()            \n",
    " \n",
    "\n",
    "    def train(self, x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size):\n",
    "        train_error_epoch = np.zeros(epochs)\n",
    "        val_error_epoch = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_tr_loss = 0\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "\n",
    "            # Shuffle training data\n",
    "            train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(train_indices)\n",
    "            x_train = x_train[train_indices]\n",
    "            target_train = target_train[train_indices]\n",
    "            \n",
    "            \n",
    "            # Mini-batch processing, if batch_size=1 we get the online version\n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                target_batch = target_train[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss and gradient\n",
    "                loss = loss_function(target_batch, predictions)\n",
    "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "                epoch_tr_loss += np.sum(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(loss_gradient, learning_rate, Lambda, reg_type)\n",
    "\n",
    "            # Validation\n",
    "            val_predictions = self.forward(x_val)\n",
    "            val_loss = loss_function(target_val, val_predictions)\n",
    "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished. \n",
    "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
    "            # Store average errors for the epoch\n",
    "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
    "            val_error_epoch[epoch] = epoch_val_loss\n",
    "\n",
    "        return train_error_epoch, val_error_epoch\n",
    "\n",
    "\n",
    "    def k_fold_cross_validation(self, x_tot, target, K, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_tot.shape[0]\n",
    "        fold_size = num_samples // K # if K=1 we get the hold-out validation\n",
    "\n",
    "        # Error storage for averaging \n",
    "        avg_train_error_epoch = np.zeros(epochs)\n",
    "        avg_val_error_epoch = np.zeros(epochs)\n",
    "        \n",
    "        if K==1: # hold-out validation\n",
    "            print(\"sono entrato in hold-out\")\n",
    "            train_indices = np.arange(0, int(0.8*num_samples))\n",
    "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices)\n",
    "            x_train, target_train = x_tot[train_indices], target[train_indices]\n",
    "            x_val, target_val = x_tot[val_indices], target[val_indices]            \n",
    "            train_error_epoch, val_error_epoch = self.train(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "            return train_error_epoch, val_error_epoch\n",
    "            \n",
    "        # ROBA FOTONICA PAZZESKA DI CHAT GPT, LEGGERE BENE\n",
    "        for k in range(K):\n",
    "            print(\"sono entrato in cross val\")\n",
    "            # Create fold indices\n",
    "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
    "                                                                        # It contains the indices of the examples used in validation set for\n",
    "                                                                        # this fold\n",
    "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples - \n",
    "                                                                              # len(val_indices). It contains the indices of all the examples\n",
    "                                                                              # but the ones used in the validation set for this fold. Thus \n",
    "                                                                              # it corresponds to the training set for this fold \n",
    "            x_train, target_train = x_tot[train_indices], target[train_indices]\n",
    "            x_val, target_val = x_tot[val_indices], target[val_indices]\n",
    "\n",
    "            # Reinitialize weights for each fold\n",
    "            self.reinitialize_weights()\n",
    "\n",
    "            # Train on the current fold\n",
    "            train_error_epoch, val_error_epoch = self.train(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "\n",
    "            # Accumulate errors for averaging, we have the errors for each epoch summed for all the folds\n",
    "            avg_train_error_epoch += train_error_epoch\n",
    "            avg_val_error_epoch += val_error_epoch\n",
    "\n",
    "            print(f\"Fold {k+1} completed.\")\n",
    "\n",
    "        # Average errors across all folds\n",
    "        avg_train_error_epoch /= K\n",
    "        avg_val_error_epoch /= K\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
    "\n",
    "        return avg_train_error_epoch, avg_val_error_epoch\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp0g8MgT3hhO",
    "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sono entrato in cross val\n",
      "Fold 1 completed.\n",
      "sono entrato in cross val\n",
      "Fold 2 completed.\n",
      "sono entrato in cross val\n",
      "Fold 3 completed.\n",
      "sono entrato in cross val\n",
      "Fold 4 completed.\n",
      "sono entrato in cross val\n",
      "Fold 5 completed.\n",
      "Epoch 1/500, Avg Training Loss: 0.5883695173910697, Avg Validation Loss: 107.70696372494837\n",
      "Epoch 2/500, Avg Training Loss: 0.4616701468660975, Avg Validation Loss: 88.08212749716364\n",
      "Epoch 3/500, Avg Training Loss: 0.3793715462325998, Avg Validation Loss: 73.77848236371625\n",
      "Epoch 4/500, Avg Training Loss: 0.31964636968937227, Avg Validation Loss: 63.34517386661585\n",
      "Epoch 5/500, Avg Training Loss: 0.2762872925922958, Avg Validation Loss: 55.73084913938694\n",
      "Epoch 6/500, Avg Training Loss: 0.24489203809877672, Avg Validation Loss: 50.16776372132825\n",
      "Epoch 7/500, Avg Training Loss: 0.22213837387092694, Avg Validation Loss: 46.10653723767463\n",
      "Epoch 8/500, Avg Training Loss: 0.20564311629506185, Avg Validation Loss: 43.12488509606135\n",
      "Epoch 9/500, Avg Training Loss: 0.19365148376079863, Avg Validation Loss: 40.936374013559835\n",
      "Epoch 10/500, Avg Training Loss: 0.18497224865016704, Avg Validation Loss: 39.31833944391664\n",
      "Epoch 11/500, Avg Training Loss: 0.17865057301430073, Avg Validation Loss: 38.12727419791507\n",
      "Epoch 12/500, Avg Training Loss: 0.17408071829658403, Avg Validation Loss: 37.25367867055121\n",
      "Epoch 13/500, Avg Training Loss: 0.1707483714483416, Avg Validation Loss: 36.597361976415215\n",
      "Epoch 14/500, Avg Training Loss: 0.16834367974295955, Avg Validation Loss: 36.11837777118008\n",
      "Epoch 15/500, Avg Training Loss: 0.166612687584319, Avg Validation Loss: 35.75592808828577\n",
      "Epoch 16/500, Avg Training Loss: 0.16537299155970928, Avg Validation Loss: 35.48873329634312\n",
      "Epoch 17/500, Avg Training Loss: 0.16444732819145194, Avg Validation Loss: 35.28600961160274\n",
      "Epoch 18/500, Avg Training Loss: 0.16379934307610136, Avg Validation Loss: 35.12901738235655\n",
      "Epoch 19/500, Avg Training Loss: 0.16330567361422624, Avg Validation Loss: 35.01176747140157\n",
      "Epoch 20/500, Avg Training Loss: 0.1629343638822045, Avg Validation Loss: 34.92129126691702\n",
      "Epoch 21/500, Avg Training Loss: 0.16267848741167804, Avg Validation Loss: 34.85315937292076\n",
      "Epoch 22/500, Avg Training Loss: 0.16249780730909452, Avg Validation Loss: 34.798455528639664\n",
      "Epoch 23/500, Avg Training Loss: 0.16237262156221405, Avg Validation Loss: 34.75639232542224\n",
      "Epoch 24/500, Avg Training Loss: 0.16225813930415323, Avg Validation Loss: 34.72635309761142\n",
      "Epoch 25/500, Avg Training Loss: 0.1622143673038723, Avg Validation Loss: 34.70033800719974\n",
      "Epoch 26/500, Avg Training Loss: 0.16215387640510417, Avg Validation Loss: 34.680581571124094\n",
      "Epoch 27/500, Avg Training Loss: 0.1621059734353244, Avg Validation Loss: 34.66415708788551\n",
      "Epoch 28/500, Avg Training Loss: 0.16208410081627875, Avg Validation Loss: 34.65088417224917\n",
      "Epoch 29/500, Avg Training Loss: 0.16207757319963664, Avg Validation Loss: 34.64076542206898\n",
      "Epoch 30/500, Avg Training Loss: 0.1620517866481905, Avg Validation Loss: 34.63079737758426\n",
      "Epoch 31/500, Avg Training Loss: 0.16204994406784856, Avg Validation Loss: 34.62494193660508\n",
      "Epoch 32/500, Avg Training Loss: 0.16203465433032327, Avg Validation Loss: 34.619817869523835\n",
      "Epoch 33/500, Avg Training Loss: 0.16206230377484526, Avg Validation Loss: 34.61700289747023\n",
      "Epoch 34/500, Avg Training Loss: 0.16203235198649132, Avg Validation Loss: 34.61249946430636\n",
      "Epoch 35/500, Avg Training Loss: 0.16203258878953875, Avg Validation Loss: 34.60778607570323\n",
      "Epoch 36/500, Avg Training Loss: 0.16201745318812447, Avg Validation Loss: 34.60416383474349\n",
      "Epoch 37/500, Avg Training Loss: 0.1620157187382563, Avg Validation Loss: 34.60232616047418\n",
      "Epoch 38/500, Avg Training Loss: 0.16201415374032968, Avg Validation Loss: 34.59830138894307\n",
      "Epoch 39/500, Avg Training Loss: 0.16204209315882828, Avg Validation Loss: 34.59650059001637\n",
      "Epoch 40/500, Avg Training Loss: 0.1620196604953104, Avg Validation Loss: 34.59521402449044\n",
      "Epoch 41/500, Avg Training Loss: 0.16201346819833115, Avg Validation Loss: 34.595368413126366\n",
      "Epoch 42/500, Avg Training Loss: 0.16201772059344716, Avg Validation Loss: 34.59314079000286\n",
      "Epoch 43/500, Avg Training Loss: 0.16201292890305166, Avg Validation Loss: 34.59144044935077\n",
      "Epoch 44/500, Avg Training Loss: 0.1620051697268118, Avg Validation Loss: 34.5920813052062\n",
      "Epoch 45/500, Avg Training Loss: 0.16202080339106106, Avg Validation Loss: 34.590947078139365\n",
      "Epoch 46/500, Avg Training Loss: 0.1620142241115218, Avg Validation Loss: 34.590990706848274\n",
      "Epoch 47/500, Avg Training Loss: 0.1619983864016626, Avg Validation Loss: 34.59011733029128\n",
      "Epoch 48/500, Avg Training Loss: 0.16201474664609633, Avg Validation Loss: 34.59106706892431\n",
      "Epoch 49/500, Avg Training Loss: 0.1620278859846809, Avg Validation Loss: 34.59147999746419\n",
      "Epoch 50/500, Avg Training Loss: 0.162005391699645, Avg Validation Loss: 34.591664095571545\n",
      "Epoch 51/500, Avg Training Loss: 0.16200669918625407, Avg Validation Loss: 34.590783581482\n",
      "Epoch 52/500, Avg Training Loss: 0.16202810817680516, Avg Validation Loss: 34.59087690279739\n",
      "Epoch 53/500, Avg Training Loss: 0.16203242257307082, Avg Validation Loss: 34.590522112285356\n",
      "Epoch 54/500, Avg Training Loss: 0.1620231086157981, Avg Validation Loss: 34.58913307249321\n",
      "Epoch 55/500, Avg Training Loss: 0.1620020740181857, Avg Validation Loss: 34.59017211784361\n",
      "Epoch 56/500, Avg Training Loss: 0.16201550437807122, Avg Validation Loss: 34.59073295723479\n",
      "Epoch 57/500, Avg Training Loss: 0.1620090214755312, Avg Validation Loss: 34.59156887214303\n",
      "Epoch 58/500, Avg Training Loss: 0.1620253033877196, Avg Validation Loss: 34.592707785285924\n",
      "Epoch 59/500, Avg Training Loss: 0.1620171913085898, Avg Validation Loss: 34.59226524824686\n",
      "Epoch 60/500, Avg Training Loss: 0.16200776992672908, Avg Validation Loss: 34.59166815898944\n",
      "Epoch 61/500, Avg Training Loss: 0.1620282204123808, Avg Validation Loss: 34.591000237315974\n",
      "Epoch 62/500, Avg Training Loss: 0.16201804647669868, Avg Validation Loss: 34.590427976890865\n",
      "Epoch 63/500, Avg Training Loss: 0.16204764618030137, Avg Validation Loss: 34.58806770975414\n",
      "Epoch 64/500, Avg Training Loss: 0.16202412163095542, Avg Validation Loss: 34.58815858490461\n",
      "Epoch 65/500, Avg Training Loss: 0.1620325092071919, Avg Validation Loss: 34.58842805368581\n",
      "Epoch 66/500, Avg Training Loss: 0.1620327324905699, Avg Validation Loss: 34.588648646209165\n",
      "Epoch 67/500, Avg Training Loss: 0.16204286070974175, Avg Validation Loss: 34.58818338897747\n",
      "Epoch 68/500, Avg Training Loss: 0.16200761179641257, Avg Validation Loss: 34.58886236067808\n",
      "Epoch 69/500, Avg Training Loss: 0.16201421256281254, Avg Validation Loss: 34.59058711467708\n",
      "Epoch 70/500, Avg Training Loss: 0.16201779009668943, Avg Validation Loss: 34.58829951855763\n",
      "Epoch 71/500, Avg Training Loss: 0.16203067355069206, Avg Validation Loss: 34.587146687689604\n",
      "Epoch 72/500, Avg Training Loss: 0.16200321895225453, Avg Validation Loss: 34.588519975330684\n",
      "Epoch 73/500, Avg Training Loss: 0.16201484748163886, Avg Validation Loss: 34.58726832023494\n",
      "Epoch 74/500, Avg Training Loss: 0.1620134894579352, Avg Validation Loss: 34.5880887611559\n",
      "Epoch 75/500, Avg Training Loss: 0.1620213516798659, Avg Validation Loss: 34.58849959088036\n",
      "Epoch 76/500, Avg Training Loss: 0.16201456974745185, Avg Validation Loss: 34.58935676538511\n",
      "Epoch 77/500, Avg Training Loss: 0.16202454122356857, Avg Validation Loss: 34.587756597130735\n",
      "Epoch 78/500, Avg Training Loss: 0.16198859083401854, Avg Validation Loss: 34.58835179640111\n",
      "Epoch 79/500, Avg Training Loss: 0.16199893042020125, Avg Validation Loss: 34.58889372088072\n",
      "Epoch 80/500, Avg Training Loss: 0.16202264016500578, Avg Validation Loss: 34.588344880196985\n",
      "Epoch 81/500, Avg Training Loss: 0.16200024509041389, Avg Validation Loss: 34.58795794550181\n",
      "Epoch 82/500, Avg Training Loss: 0.16200188124078665, Avg Validation Loss: 34.58803106135559\n",
      "Epoch 83/500, Avg Training Loss: 0.16205312381258247, Avg Validation Loss: 34.587527133552896\n",
      "Epoch 84/500, Avg Training Loss: 0.1620185925838148, Avg Validation Loss: 34.58697459206793\n",
      "Epoch 85/500, Avg Training Loss: 0.16199865637321056, Avg Validation Loss: 34.588495542695775\n",
      "Epoch 86/500, Avg Training Loss: 0.16201326170655206, Avg Validation Loss: 34.588986465416596\n",
      "Epoch 87/500, Avg Training Loss: 0.1620165720204247, Avg Validation Loss: 34.58932605822745\n",
      "Epoch 88/500, Avg Training Loss: 0.16202555026412324, Avg Validation Loss: 34.59045770708362\n",
      "Epoch 89/500, Avg Training Loss: 0.16201886301201857, Avg Validation Loss: 34.59096972829117\n",
      "Epoch 90/500, Avg Training Loss: 0.16200237550101648, Avg Validation Loss: 34.59113697072867\n",
      "Epoch 91/500, Avg Training Loss: 0.16200383218185258, Avg Validation Loss: 34.59092170923874\n",
      "Epoch 92/500, Avg Training Loss: 0.16200657081987266, Avg Validation Loss: 34.58962582911163\n",
      "Epoch 93/500, Avg Training Loss: 0.1620186862993554, Avg Validation Loss: 34.58955101262317\n",
      "Epoch 94/500, Avg Training Loss: 0.16201676902295314, Avg Validation Loss: 34.58879358398262\n",
      "Epoch 95/500, Avg Training Loss: 0.1620107209599045, Avg Validation Loss: 34.588239180686536\n",
      "Epoch 96/500, Avg Training Loss: 0.16200511039460694, Avg Validation Loss: 34.58870092437755\n",
      "Epoch 97/500, Avg Training Loss: 0.16200042957220556, Avg Validation Loss: 34.58937653030163\n",
      "Epoch 98/500, Avg Training Loss: 0.16202300035384326, Avg Validation Loss: 34.59139379339803\n",
      "Epoch 99/500, Avg Training Loss: 0.16202829325515072, Avg Validation Loss: 34.59086240709952\n",
      "Epoch 100/500, Avg Training Loss: 0.16201158581143651, Avg Validation Loss: 34.58955807541396\n",
      "Epoch 101/500, Avg Training Loss: 0.16203595763240794, Avg Validation Loss: 34.58826078274203\n",
      "Epoch 102/500, Avg Training Loss: 0.16200642312930294, Avg Validation Loss: 34.58927994548283\n",
      "Epoch 103/500, Avg Training Loss: 0.16200967423040522, Avg Validation Loss: 34.58946121011087\n",
      "Epoch 104/500, Avg Training Loss: 0.16200891696865558, Avg Validation Loss: 34.587983100489296\n",
      "Epoch 105/500, Avg Training Loss: 0.16203244987283594, Avg Validation Loss: 34.58892365542199\n",
      "Epoch 106/500, Avg Training Loss: 0.1620252710161264, Avg Validation Loss: 34.59025377361637\n",
      "Epoch 107/500, Avg Training Loss: 0.16200002137275235, Avg Validation Loss: 34.59183337923061\n",
      "Epoch 108/500, Avg Training Loss: 0.1620414782178977, Avg Validation Loss: 34.59197805334293\n",
      "Epoch 109/500, Avg Training Loss: 0.1620018041715866, Avg Validation Loss: 34.592587675484054\n",
      "Epoch 110/500, Avg Training Loss: 0.16201562714604914, Avg Validation Loss: 34.592016833847715\n",
      "Epoch 111/500, Avg Training Loss: 0.16201186858876787, Avg Validation Loss: 34.590545517081225\n",
      "Epoch 112/500, Avg Training Loss: 0.1620417390878918, Avg Validation Loss: 34.59116021647145\n",
      "Epoch 113/500, Avg Training Loss: 0.16201036659567147, Avg Validation Loss: 34.59054674016389\n",
      "Epoch 114/500, Avg Training Loss: 0.16202625244457278, Avg Validation Loss: 34.591600251788606\n",
      "Epoch 115/500, Avg Training Loss: 0.16201795641267733, Avg Validation Loss: 34.59300567512459\n",
      "Epoch 116/500, Avg Training Loss: 0.16201148816711805, Avg Validation Loss: 34.592687780871614\n",
      "Epoch 117/500, Avg Training Loss: 0.16202544131939614, Avg Validation Loss: 34.59115515632644\n",
      "Epoch 118/500, Avg Training Loss: 0.16200609870751287, Avg Validation Loss: 34.5910594575453\n",
      "Epoch 119/500, Avg Training Loss: 0.1620126516975235, Avg Validation Loss: 34.59124913242674\n",
      "Epoch 120/500, Avg Training Loss: 0.16202625473703505, Avg Validation Loss: 34.59148777854474\n",
      "Epoch 121/500, Avg Training Loss: 0.16202145189212824, Avg Validation Loss: 34.589890326415656\n",
      "Epoch 122/500, Avg Training Loss: 0.16202127729019009, Avg Validation Loss: 34.589827936732235\n",
      "Epoch 123/500, Avg Training Loss: 0.16201608727285435, Avg Validation Loss: 34.58958964238957\n",
      "Epoch 124/500, Avg Training Loss: 0.16200960501768835, Avg Validation Loss: 34.59193454062238\n",
      "Epoch 125/500, Avg Training Loss: 0.1620389518354737, Avg Validation Loss: 34.59080807062618\n",
      "Epoch 126/500, Avg Training Loss: 0.1620065661128834, Avg Validation Loss: 34.58946818162235\n",
      "Epoch 127/500, Avg Training Loss: 0.16202566710977515, Avg Validation Loss: 34.589491550528045\n",
      "Epoch 128/500, Avg Training Loss: 0.1620265735377119, Avg Validation Loss: 34.58848973240791\n",
      "Epoch 129/500, Avg Training Loss: 0.16201597798705397, Avg Validation Loss: 34.58733132553358\n",
      "Epoch 130/500, Avg Training Loss: 0.16201419093371386, Avg Validation Loss: 34.58837450726609\n",
      "Epoch 131/500, Avg Training Loss: 0.16203774075297264, Avg Validation Loss: 34.58753610580274\n",
      "Epoch 132/500, Avg Training Loss: 0.16201030519765464, Avg Validation Loss: 34.58789813137654\n",
      "Epoch 133/500, Avg Training Loss: 0.16201877021442562, Avg Validation Loss: 34.58788222994241\n",
      "Epoch 134/500, Avg Training Loss: 0.1620038034102236, Avg Validation Loss: 34.5901721774708\n",
      "Epoch 135/500, Avg Training Loss: 0.16203835644055584, Avg Validation Loss: 34.58909875378926\n",
      "Epoch 136/500, Avg Training Loss: 0.16204023026309686, Avg Validation Loss: 34.58968315528\n",
      "Epoch 137/500, Avg Training Loss: 0.1620013057380891, Avg Validation Loss: 34.58917660468883\n",
      "Epoch 138/500, Avg Training Loss: 0.16201452569101413, Avg Validation Loss: 34.58819314798343\n",
      "Epoch 139/500, Avg Training Loss: 0.16201444553532285, Avg Validation Loss: 34.5890322898964\n",
      "Epoch 140/500, Avg Training Loss: 0.16204627237594685, Avg Validation Loss: 34.58882540832521\n",
      "Epoch 141/500, Avg Training Loss: 0.16201232381399108, Avg Validation Loss: 34.587660178094644\n",
      "Epoch 142/500, Avg Training Loss: 0.16202198684846777, Avg Validation Loss: 34.587786240199385\n",
      "Epoch 143/500, Avg Training Loss: 0.16203339980085085, Avg Validation Loss: 34.58827970147595\n",
      "Epoch 144/500, Avg Training Loss: 0.16201673176825998, Avg Validation Loss: 34.58737885359658\n",
      "Epoch 145/500, Avg Training Loss: 0.16201241650407794, Avg Validation Loss: 34.58839255808492\n",
      "Epoch 146/500, Avg Training Loss: 0.16201239375080292, Avg Validation Loss: 34.589319087526945\n",
      "Epoch 147/500, Avg Training Loss: 0.1620221944108734, Avg Validation Loss: 34.589222070292436\n",
      "Epoch 148/500, Avg Training Loss: 0.16202323681063663, Avg Validation Loss: 34.58924166541662\n",
      "Epoch 149/500, Avg Training Loss: 0.1620176927725568, Avg Validation Loss: 34.5891685871794\n",
      "Epoch 150/500, Avg Training Loss: 0.1620021992747112, Avg Validation Loss: 34.58832119953363\n",
      "Epoch 151/500, Avg Training Loss: 0.16203005162174486, Avg Validation Loss: 34.589254255025736\n",
      "Epoch 152/500, Avg Training Loss: 0.162031910158559, Avg Validation Loss: 34.588048648317866\n",
      "Epoch 153/500, Avg Training Loss: 0.16201779395598787, Avg Validation Loss: 34.58799224102903\n",
      "Epoch 154/500, Avg Training Loss: 0.16200551412318934, Avg Validation Loss: 34.588476126451006\n",
      "Epoch 155/500, Avg Training Loss: 0.1620174676369256, Avg Validation Loss: 34.58886239714053\n",
      "Epoch 156/500, Avg Training Loss: 0.16202610458079028, Avg Validation Loss: 34.5879725943417\n",
      "Epoch 157/500, Avg Training Loss: 0.16201687815470345, Avg Validation Loss: 34.587840544954496\n",
      "Epoch 158/500, Avg Training Loss: 0.1620154148926266, Avg Validation Loss: 34.5883775296371\n",
      "Epoch 159/500, Avg Training Loss: 0.16203528436418785, Avg Validation Loss: 34.58816977356838\n",
      "Epoch 160/500, Avg Training Loss: 0.16202660128919516, Avg Validation Loss: 34.58805555763095\n",
      "Epoch 161/500, Avg Training Loss: 0.16201413805507095, Avg Validation Loss: 34.58668036069638\n",
      "Epoch 162/500, Avg Training Loss: 0.16201623991621286, Avg Validation Loss: 34.587156476314945\n",
      "Epoch 163/500, Avg Training Loss: 0.1620018886839597, Avg Validation Loss: 34.589458943311726\n",
      "Epoch 164/500, Avg Training Loss: 0.16203061444964026, Avg Validation Loss: 34.590526426301906\n",
      "Epoch 165/500, Avg Training Loss: 0.16200153401848266, Avg Validation Loss: 34.59066255909785\n",
      "Epoch 166/500, Avg Training Loss: 0.1620194817056257, Avg Validation Loss: 34.591099344082934\n",
      "Epoch 167/500, Avg Training Loss: 0.1620198252777762, Avg Validation Loss: 34.58922147931907\n",
      "Epoch 168/500, Avg Training Loss: 0.16201913325364092, Avg Validation Loss: 34.589894340505\n",
      "Epoch 169/500, Avg Training Loss: 0.16199774310567944, Avg Validation Loss: 34.58969519805892\n",
      "Epoch 170/500, Avg Training Loss: 0.16202319247815097, Avg Validation Loss: 34.58914971416841\n",
      "Epoch 171/500, Avg Training Loss: 0.1620184239057232, Avg Validation Loss: 34.5889200075897\n",
      "Epoch 172/500, Avg Training Loss: 0.1620287621139503, Avg Validation Loss: 34.589937167439615\n",
      "Epoch 173/500, Avg Training Loss: 0.16200470950703016, Avg Validation Loss: 34.58968776828935\n",
      "Epoch 174/500, Avg Training Loss: 0.1620067234499174, Avg Validation Loss: 34.59124664136631\n",
      "Epoch 175/500, Avg Training Loss: 0.16201272371454498, Avg Validation Loss: 34.59106148628131\n",
      "Epoch 176/500, Avg Training Loss: 0.16200444504460207, Avg Validation Loss: 34.59019876661028\n",
      "Epoch 177/500, Avg Training Loss: 0.16201296051714142, Avg Validation Loss: 34.5894532663761\n",
      "Epoch 178/500, Avg Training Loss: 0.16200110790226663, Avg Validation Loss: 34.58891969770316\n",
      "Epoch 179/500, Avg Training Loss: 0.16201819467236, Avg Validation Loss: 34.58884134986494\n",
      "Epoch 180/500, Avg Training Loss: 0.16203916959239428, Avg Validation Loss: 34.58801086233896\n",
      "Epoch 181/500, Avg Training Loss: 0.1620536238162978, Avg Validation Loss: 34.58866631288289\n",
      "Epoch 182/500, Avg Training Loss: 0.16202525345795757, Avg Validation Loss: 34.58973356046261\n",
      "Epoch 183/500, Avg Training Loss: 0.16201154640332535, Avg Validation Loss: 34.5896333310164\n",
      "Epoch 184/500, Avg Training Loss: 0.16201752944565848, Avg Validation Loss: 34.58942601454834\n",
      "Epoch 185/500, Avg Training Loss: 0.16199528751141415, Avg Validation Loss: 34.59082985678617\n",
      "Epoch 186/500, Avg Training Loss: 0.16203006014865146, Avg Validation Loss: 34.58962672074841\n",
      "Epoch 187/500, Avg Training Loss: 0.16201288913614562, Avg Validation Loss: 34.58929597839562\n",
      "Epoch 188/500, Avg Training Loss: 0.16205645264862112, Avg Validation Loss: 34.58917781342074\n",
      "Epoch 189/500, Avg Training Loss: 0.16203716455382117, Avg Validation Loss: 34.589913657559144\n",
      "Epoch 190/500, Avg Training Loss: 0.16199510972608544, Avg Validation Loss: 34.590026308082756\n",
      "Epoch 191/500, Avg Training Loss: 0.16198580977156415, Avg Validation Loss: 34.58907754849824\n",
      "Epoch 192/500, Avg Training Loss: 0.16201418122061143, Avg Validation Loss: 34.588907932754985\n",
      "Epoch 193/500, Avg Training Loss: 0.16204307630159434, Avg Validation Loss: 34.587713105437984\n",
      "Epoch 194/500, Avg Training Loss: 0.16201982600476422, Avg Validation Loss: 34.58608798784398\n",
      "Epoch 195/500, Avg Training Loss: 0.16201410734868923, Avg Validation Loss: 34.58589929452916\n",
      "Epoch 196/500, Avg Training Loss: 0.16202493180678157, Avg Validation Loss: 34.58682090047432\n",
      "Epoch 197/500, Avg Training Loss: 0.16201853102854424, Avg Validation Loss: 34.587310190635776\n",
      "Epoch 198/500, Avg Training Loss: 0.16199146459319425, Avg Validation Loss: 34.58814647925144\n",
      "Epoch 199/500, Avg Training Loss: 0.1620096410280924, Avg Validation Loss: 34.58805558407073\n",
      "Epoch 200/500, Avg Training Loss: 0.1620379977848846, Avg Validation Loss: 34.58930248152535\n",
      "Epoch 201/500, Avg Training Loss: 0.16202366508798108, Avg Validation Loss: 34.589601057893994\n",
      "Epoch 202/500, Avg Training Loss: 0.162029870287307, Avg Validation Loss: 34.58934388340272\n",
      "Epoch 203/500, Avg Training Loss: 0.16202025444339938, Avg Validation Loss: 34.58874520394007\n",
      "Epoch 204/500, Avg Training Loss: 0.1619946487148747, Avg Validation Loss: 34.58978489058496\n",
      "Epoch 205/500, Avg Training Loss: 0.1620034718261739, Avg Validation Loss: 34.58877609993731\n",
      "Epoch 206/500, Avg Training Loss: 0.16204383383331383, Avg Validation Loss: 34.5875211353298\n",
      "Epoch 207/500, Avg Training Loss: 0.16201292516484853, Avg Validation Loss: 34.587829395363485\n",
      "Epoch 208/500, Avg Training Loss: 0.16200884751490163, Avg Validation Loss: 34.5891372702123\n",
      "Epoch 209/500, Avg Training Loss: 0.162032218094447, Avg Validation Loss: 34.589503688646715\n",
      "Epoch 210/500, Avg Training Loss: 0.1620160404901565, Avg Validation Loss: 34.58884522766022\n",
      "Epoch 211/500, Avg Training Loss: 0.16201825168767034, Avg Validation Loss: 34.58849324470924\n",
      "Epoch 212/500, Avg Training Loss: 0.16203359070969156, Avg Validation Loss: 34.588243421095704\n",
      "Epoch 213/500, Avg Training Loss: 0.1620000659717325, Avg Validation Loss: 34.59074216619679\n",
      "Epoch 214/500, Avg Training Loss: 0.16202796047896537, Avg Validation Loss: 34.59030521944751\n",
      "Epoch 215/500, Avg Training Loss: 0.16204723557923953, Avg Validation Loss: 34.58792490031482\n",
      "Epoch 216/500, Avg Training Loss: 0.16204661805311343, Avg Validation Loss: 34.585638389903714\n",
      "Epoch 217/500, Avg Training Loss: 0.16201747699616167, Avg Validation Loss: 34.58557657815527\n",
      "Epoch 218/500, Avg Training Loss: 0.16198917679541744, Avg Validation Loss: 34.587578055121455\n",
      "Epoch 219/500, Avg Training Loss: 0.16201722865321005, Avg Validation Loss: 34.58587419343752\n",
      "Epoch 220/500, Avg Training Loss: 0.1620199125897158, Avg Validation Loss: 34.58509385082573\n",
      "Epoch 221/500, Avg Training Loss: 0.16202392175961478, Avg Validation Loss: 34.58497657648018\n",
      "Epoch 222/500, Avg Training Loss: 0.16199522185478496, Avg Validation Loss: 34.58854508007967\n",
      "Epoch 223/500, Avg Training Loss: 0.16200643298755524, Avg Validation Loss: 34.58839611186071\n",
      "Epoch 224/500, Avg Training Loss: 0.16201737944656242, Avg Validation Loss: 34.58883046522645\n",
      "Epoch 225/500, Avg Training Loss: 0.16200545296005395, Avg Validation Loss: 34.589144369895315\n",
      "Epoch 226/500, Avg Training Loss: 0.16201682019344882, Avg Validation Loss: 34.58787983279599\n",
      "Epoch 227/500, Avg Training Loss: 0.16202735328855947, Avg Validation Loss: 34.586431770629346\n",
      "Epoch 228/500, Avg Training Loss: 0.16200915651876988, Avg Validation Loss: 34.586848555281016\n",
      "Epoch 229/500, Avg Training Loss: 0.162014644990895, Avg Validation Loss: 34.58828085781326\n",
      "Epoch 230/500, Avg Training Loss: 0.1620250045549577, Avg Validation Loss: 34.58821671637136\n",
      "Epoch 231/500, Avg Training Loss: 0.16199215951583984, Avg Validation Loss: 34.588816046815516\n",
      "Epoch 232/500, Avg Training Loss: 0.16200736640902985, Avg Validation Loss: 34.58907851158585\n",
      "Epoch 233/500, Avg Training Loss: 0.1620207324666313, Avg Validation Loss: 34.59006580535517\n",
      "Epoch 234/500, Avg Training Loss: 0.1620125542568883, Avg Validation Loss: 34.58857381950259\n",
      "Epoch 235/500, Avg Training Loss: 0.1620291044714399, Avg Validation Loss: 34.58737726108845\n",
      "Epoch 236/500, Avg Training Loss: 0.16202815157979159, Avg Validation Loss: 34.58800499576692\n",
      "Epoch 237/500, Avg Training Loss: 0.1620000351795648, Avg Validation Loss: 34.588272024443505\n",
      "Epoch 238/500, Avg Training Loss: 0.1620215877142639, Avg Validation Loss: 34.58765131542375\n",
      "Epoch 239/500, Avg Training Loss: 0.16201972988691016, Avg Validation Loss: 34.58712551256775\n",
      "Epoch 240/500, Avg Training Loss: 0.16203003084555045, Avg Validation Loss: 34.586206867444304\n",
      "Epoch 241/500, Avg Training Loss: 0.16203848831660256, Avg Validation Loss: 34.58503928732333\n",
      "Epoch 242/500, Avg Training Loss: 0.1620252273154095, Avg Validation Loss: 34.584531211123036\n",
      "Epoch 243/500, Avg Training Loss: 0.16200120789116465, Avg Validation Loss: 34.58548254050437\n",
      "Epoch 244/500, Avg Training Loss: 0.1620184064590096, Avg Validation Loss: 34.587077703428974\n",
      "Epoch 245/500, Avg Training Loss: 0.1620270275922344, Avg Validation Loss: 34.58617554261054\n",
      "Epoch 246/500, Avg Training Loss: 0.16203701230439166, Avg Validation Loss: 34.58520038569703\n",
      "Epoch 247/500, Avg Training Loss: 0.16205355488587955, Avg Validation Loss: 34.583088808760166\n",
      "Epoch 248/500, Avg Training Loss: 0.16203264166331646, Avg Validation Loss: 34.58277130327878\n",
      "Epoch 249/500, Avg Training Loss: 0.1620184378787202, Avg Validation Loss: 34.58408950115275\n",
      "Epoch 250/500, Avg Training Loss: 0.16202907079498352, Avg Validation Loss: 34.584109432020824\n",
      "Epoch 251/500, Avg Training Loss: 0.1620309297009182, Avg Validation Loss: 34.58480805900864\n",
      "Epoch 252/500, Avg Training Loss: 0.16201624893749483, Avg Validation Loss: 34.58451247234974\n",
      "Epoch 253/500, Avg Training Loss: 0.16203399748458774, Avg Validation Loss: 34.58408728283176\n",
      "Epoch 254/500, Avg Training Loss: 0.16202513113230899, Avg Validation Loss: 34.58375515365181\n",
      "Epoch 255/500, Avg Training Loss: 0.16201965722284523, Avg Validation Loss: 34.583673737469745\n",
      "Epoch 256/500, Avg Training Loss: 0.16202568449418642, Avg Validation Loss: 34.58399071849914\n",
      "Epoch 257/500, Avg Training Loss: 0.16200490072480986, Avg Validation Loss: 34.5839131672322\n",
      "Epoch 258/500, Avg Training Loss: 0.1620114558607951, Avg Validation Loss: 34.584528108093465\n",
      "Epoch 259/500, Avg Training Loss: 0.16204837152856666, Avg Validation Loss: 34.58411717465444\n",
      "Epoch 260/500, Avg Training Loss: 0.1620229533883502, Avg Validation Loss: 34.584007082531286\n",
      "Epoch 261/500, Avg Training Loss: 0.16202370753408626, Avg Validation Loss: 34.58525744444801\n",
      "Epoch 262/500, Avg Training Loss: 0.1619872868998632, Avg Validation Loss: 34.584464508675424\n",
      "Epoch 263/500, Avg Training Loss: 0.1620511798560484, Avg Validation Loss: 34.584296052949554\n",
      "Epoch 264/500, Avg Training Loss: 0.1620573388770071, Avg Validation Loss: 34.58263972557394\n",
      "Epoch 265/500, Avg Training Loss: 0.16201983583218504, Avg Validation Loss: 34.58307630910602\n",
      "Epoch 266/500, Avg Training Loss: 0.16201076102614564, Avg Validation Loss: 34.58304844304052\n",
      "Epoch 267/500, Avg Training Loss: 0.1620380643838773, Avg Validation Loss: 34.581302095481405\n",
      "Epoch 268/500, Avg Training Loss: 0.162022124015373, Avg Validation Loss: 34.58222522607472\n",
      "Epoch 269/500, Avg Training Loss: 0.162007223536056, Avg Validation Loss: 34.5829346815458\n",
      "Epoch 270/500, Avg Training Loss: 0.1619957511770963, Avg Validation Loss: 34.584459895665184\n",
      "Epoch 271/500, Avg Training Loss: 0.16200626126070042, Avg Validation Loss: 34.58347582960394\n",
      "Epoch 272/500, Avg Training Loss: 0.16205189492025412, Avg Validation Loss: 34.58333472333801\n",
      "Epoch 273/500, Avg Training Loss: 0.16201623100618995, Avg Validation Loss: 34.584165816456945\n",
      "Epoch 274/500, Avg Training Loss: 0.1620412664182596, Avg Validation Loss: 34.5843753919862\n",
      "Epoch 275/500, Avg Training Loss: 0.16203802308800164, Avg Validation Loss: 34.58445911312957\n",
      "Epoch 276/500, Avg Training Loss: 0.16201462322320442, Avg Validation Loss: 34.58322644680963\n",
      "Epoch 277/500, Avg Training Loss: 0.1619957188503172, Avg Validation Loss: 34.584286315129134\n",
      "Epoch 278/500, Avg Training Loss: 0.16199705684825205, Avg Validation Loss: 34.58499873283153\n",
      "Epoch 279/500, Avg Training Loss: 0.16203163373256807, Avg Validation Loss: 34.58371149983033\n",
      "Epoch 280/500, Avg Training Loss: 0.16197041783888558, Avg Validation Loss: 34.584749891348515\n",
      "Epoch 281/500, Avg Training Loss: 0.16199201811033845, Avg Validation Loss: 34.58413337431797\n",
      "Epoch 282/500, Avg Training Loss: 0.16200386100039302, Avg Validation Loss: 34.585586388623575\n",
      "Epoch 283/500, Avg Training Loss: 0.1620136778894431, Avg Validation Loss: 34.5870242851441\n",
      "Epoch 284/500, Avg Training Loss: 0.1620329865155221, Avg Validation Loss: 34.58651421690239\n",
      "Epoch 285/500, Avg Training Loss: 0.1620343620127246, Avg Validation Loss: 34.583279704108335\n",
      "Epoch 286/500, Avg Training Loss: 0.16198424057864336, Avg Validation Loss: 34.584488165341874\n",
      "Epoch 287/500, Avg Training Loss: 0.1619962631040159, Avg Validation Loss: 34.585137532857985\n",
      "Epoch 288/500, Avg Training Loss: 0.1619936932152287, Avg Validation Loss: 34.586381662349325\n",
      "Epoch 289/500, Avg Training Loss: 0.16200092801409072, Avg Validation Loss: 34.5866592290652\n",
      "Epoch 290/500, Avg Training Loss: 0.16203249551710516, Avg Validation Loss: 34.58723469063433\n",
      "Epoch 291/500, Avg Training Loss: 0.1620084159656017, Avg Validation Loss: 34.586431940994636\n",
      "Epoch 292/500, Avg Training Loss: 0.16201772291334607, Avg Validation Loss: 34.58631469946444\n",
      "Epoch 293/500, Avg Training Loss: 0.16201441457197713, Avg Validation Loss: 34.58645492319265\n",
      "Epoch 294/500, Avg Training Loss: 0.1620422202528938, Avg Validation Loss: 34.586043279858146\n",
      "Epoch 295/500, Avg Training Loss: 0.16202019711858381, Avg Validation Loss: 34.58563693611538\n",
      "Epoch 296/500, Avg Training Loss: 0.16204594684114848, Avg Validation Loss: 34.585293884564685\n",
      "Epoch 297/500, Avg Training Loss: 0.16201000194996656, Avg Validation Loss: 34.58609194805159\n",
      "Epoch 298/500, Avg Training Loss: 0.16200188044962244, Avg Validation Loss: 34.58574036734299\n",
      "Epoch 299/500, Avg Training Loss: 0.16200950093439703, Avg Validation Loss: 34.58524018271096\n",
      "Epoch 300/500, Avg Training Loss: 0.16201344261330622, Avg Validation Loss: 34.58617093969364\n",
      "Epoch 301/500, Avg Training Loss: 0.16202586689346635, Avg Validation Loss: 34.586875105832576\n",
      "Epoch 302/500, Avg Training Loss: 0.16200791650816027, Avg Validation Loss: 34.586059376104416\n",
      "Epoch 303/500, Avg Training Loss: 0.16197679892268063, Avg Validation Loss: 34.587288288770736\n",
      "Epoch 304/500, Avg Training Loss: 0.1620083495734109, Avg Validation Loss: 34.58860945062811\n",
      "Epoch 305/500, Avg Training Loss: 0.16199750392703746, Avg Validation Loss: 34.589133918149635\n",
      "Epoch 306/500, Avg Training Loss: 0.16200884083018546, Avg Validation Loss: 34.58874781734765\n",
      "Epoch 307/500, Avg Training Loss: 0.16202663718670465, Avg Validation Loss: 34.5887399069578\n",
      "Epoch 308/500, Avg Training Loss: 0.16200119494406123, Avg Validation Loss: 34.5879786406947\n",
      "Epoch 309/500, Avg Training Loss: 0.16204182978846643, Avg Validation Loss: 34.58803845470991\n",
      "Epoch 310/500, Avg Training Loss: 0.16200320142069646, Avg Validation Loss: 34.589301487407866\n",
      "Epoch 311/500, Avg Training Loss: 0.16201743059418208, Avg Validation Loss: 34.58967577764387\n",
      "Epoch 312/500, Avg Training Loss: 0.16198277807917377, Avg Validation Loss: 34.588945422176664\n",
      "Epoch 313/500, Avg Training Loss: 0.16202228743820352, Avg Validation Loss: 34.59001257965197\n",
      "Epoch 314/500, Avg Training Loss: 0.16201632244097566, Avg Validation Loss: 34.59061790466083\n",
      "Epoch 315/500, Avg Training Loss: 0.16205400707512227, Avg Validation Loss: 34.58904191219631\n",
      "Epoch 316/500, Avg Training Loss: 0.1620021698848609, Avg Validation Loss: 34.5895403739753\n",
      "Epoch 317/500, Avg Training Loss: 0.1620085569606799, Avg Validation Loss: 34.590383256191004\n",
      "Epoch 318/500, Avg Training Loss: 0.16200914277350434, Avg Validation Loss: 34.59041036781225\n",
      "Epoch 319/500, Avg Training Loss: 0.1620076159542755, Avg Validation Loss: 34.59050803211855\n",
      "Epoch 320/500, Avg Training Loss: 0.16203169571095355, Avg Validation Loss: 34.58997633059185\n",
      "Epoch 321/500, Avg Training Loss: 0.16203112174071102, Avg Validation Loss: 34.58930223315994\n",
      "Epoch 322/500, Avg Training Loss: 0.16202564502031966, Avg Validation Loss: 34.58760614880307\n",
      "Epoch 323/500, Avg Training Loss: 0.16200413784979895, Avg Validation Loss: 34.586959925120574\n",
      "Epoch 324/500, Avg Training Loss: 0.16203234755075738, Avg Validation Loss: 34.58861012275716\n",
      "Epoch 325/500, Avg Training Loss: 0.16204182499035918, Avg Validation Loss: 34.58845677850462\n",
      "Epoch 326/500, Avg Training Loss: 0.1620327337235751, Avg Validation Loss: 34.588065704877\n",
      "Epoch 327/500, Avg Training Loss: 0.16204703218288913, Avg Validation Loss: 34.586788906133734\n",
      "Epoch 328/500, Avg Training Loss: 0.16202373535446665, Avg Validation Loss: 34.58551380780915\n",
      "Epoch 329/500, Avg Training Loss: 0.16199663459894556, Avg Validation Loss: 34.58662576062043\n",
      "Epoch 330/500, Avg Training Loss: 0.16201357202286684, Avg Validation Loss: 34.58739046184788\n",
      "Epoch 331/500, Avg Training Loss: 0.16201218857741378, Avg Validation Loss: 34.58692406404136\n",
      "Epoch 332/500, Avg Training Loss: 0.16201497289017705, Avg Validation Loss: 34.586187896456124\n",
      "Epoch 333/500, Avg Training Loss: 0.16206360021964264, Avg Validation Loss: 34.5845046341984\n",
      "Epoch 334/500, Avg Training Loss: 0.16202631285286231, Avg Validation Loss: 34.58301912796225\n",
      "Epoch 335/500, Avg Training Loss: 0.16202861218991452, Avg Validation Loss: 34.5832498335421\n",
      "Epoch 336/500, Avg Training Loss: 0.1620209574512859, Avg Validation Loss: 34.58280452968482\n",
      "Epoch 337/500, Avg Training Loss: 0.16203099457460635, Avg Validation Loss: 34.58405706183706\n",
      "Epoch 338/500, Avg Training Loss: 0.16201284170082933, Avg Validation Loss: 34.58374126608662\n",
      "Epoch 339/500, Avg Training Loss: 0.16202020242259565, Avg Validation Loss: 34.58532897935331\n",
      "Epoch 340/500, Avg Training Loss: 0.16201954158310652, Avg Validation Loss: 34.58631354072503\n",
      "Epoch 341/500, Avg Training Loss: 0.16200213716744422, Avg Validation Loss: 34.58643043500873\n",
      "Epoch 342/500, Avg Training Loss: 0.16200233362994956, Avg Validation Loss: 34.58457814528385\n",
      "Epoch 343/500, Avg Training Loss: 0.1620241050288087, Avg Validation Loss: 34.583129113364\n",
      "Epoch 344/500, Avg Training Loss: 0.1620357248664985, Avg Validation Loss: 34.5835855615451\n",
      "Epoch 345/500, Avg Training Loss: 0.16202617639297112, Avg Validation Loss: 34.58355286049496\n",
      "Epoch 346/500, Avg Training Loss: 0.16201019264394323, Avg Validation Loss: 34.58470410212525\n",
      "Epoch 347/500, Avg Training Loss: 0.16203790342754198, Avg Validation Loss: 34.584056498457684\n",
      "Epoch 348/500, Avg Training Loss: 0.16199263134051262, Avg Validation Loss: 34.58383593403885\n",
      "Epoch 349/500, Avg Training Loss: 0.1619998245663849, Avg Validation Loss: 34.58396389109845\n",
      "Epoch 350/500, Avg Training Loss: 0.1620290557886391, Avg Validation Loss: 34.58425120040883\n",
      "Epoch 351/500, Avg Training Loss: 0.16199484807998274, Avg Validation Loss: 34.585035507122626\n",
      "Epoch 352/500, Avg Training Loss: 0.1620222265115131, Avg Validation Loss: 34.58443891600239\n",
      "Epoch 353/500, Avg Training Loss: 0.16202503841054727, Avg Validation Loss: 34.58387066105752\n",
      "Epoch 354/500, Avg Training Loss: 0.1620011039916892, Avg Validation Loss: 34.584936093187444\n",
      "Epoch 355/500, Avg Training Loss: 0.16203334668240174, Avg Validation Loss: 34.58435657757948\n",
      "Epoch 356/500, Avg Training Loss: 0.16200640009227169, Avg Validation Loss: 34.58312258257776\n",
      "Epoch 357/500, Avg Training Loss: 0.16203147651539157, Avg Validation Loss: 34.583076872765645\n",
      "Epoch 358/500, Avg Training Loss: 0.16202520976152943, Avg Validation Loss: 34.58379742110192\n",
      "Epoch 359/500, Avg Training Loss: 0.1620133816396005, Avg Validation Loss: 34.58439120506057\n",
      "Epoch 360/500, Avg Training Loss: 0.1620430482128485, Avg Validation Loss: 34.58350797454419\n",
      "Epoch 361/500, Avg Training Loss: 0.1620123991928625, Avg Validation Loss: 34.58332385957428\n",
      "Epoch 362/500, Avg Training Loss: 0.16198437501680135, Avg Validation Loss: 34.58374953976872\n",
      "Epoch 363/500, Avg Training Loss: 0.16202265097436058, Avg Validation Loss: 34.58339701478402\n",
      "Epoch 364/500, Avg Training Loss: 0.1619990458523283, Avg Validation Loss: 34.584896961517074\n",
      "Epoch 365/500, Avg Training Loss: 0.16201027075374783, Avg Validation Loss: 34.58624121679303\n",
      "Epoch 366/500, Avg Training Loss: 0.16203227665343675, Avg Validation Loss: 34.58610609036009\n",
      "Epoch 367/500, Avg Training Loss: 0.16201450478794335, Avg Validation Loss: 34.58532788956401\n",
      "Epoch 368/500, Avg Training Loss: 0.16203193707650493, Avg Validation Loss: 34.585299853885175\n",
      "Epoch 369/500, Avg Training Loss: 0.16199625416145513, Avg Validation Loss: 34.58606710882691\n",
      "Epoch 370/500, Avg Training Loss: 0.16200915832939222, Avg Validation Loss: 34.58595158780799\n",
      "Epoch 371/500, Avg Training Loss: 0.16201091770615597, Avg Validation Loss: 34.58726799775536\n",
      "Epoch 372/500, Avg Training Loss: 0.1620029869677311, Avg Validation Loss: 34.58917020227878\n",
      "Epoch 373/500, Avg Training Loss: 0.16202584035556697, Avg Validation Loss: 34.58749097424076\n",
      "Epoch 374/500, Avg Training Loss: 0.16201566347455845, Avg Validation Loss: 34.58879234058165\n",
      "Epoch 375/500, Avg Training Loss: 0.16204218499833023, Avg Validation Loss: 34.587144446906834\n",
      "Epoch 376/500, Avg Training Loss: 0.16203330307990307, Avg Validation Loss: 34.58726618337771\n",
      "Epoch 377/500, Avg Training Loss: 0.1620002832539532, Avg Validation Loss: 34.58745198957901\n",
      "Epoch 378/500, Avg Training Loss: 0.16201970975587096, Avg Validation Loss: 34.588168640455876\n",
      "Epoch 379/500, Avg Training Loss: 0.16201872436402262, Avg Validation Loss: 34.587742403633825\n",
      "Epoch 380/500, Avg Training Loss: 0.16202825238903795, Avg Validation Loss: 34.58822862214032\n",
      "Epoch 381/500, Avg Training Loss: 0.16202728521045817, Avg Validation Loss: 34.58755366536559\n",
      "Epoch 382/500, Avg Training Loss: 0.16201467447582865, Avg Validation Loss: 34.58674343119196\n",
      "Epoch 383/500, Avg Training Loss: 0.16203027911174414, Avg Validation Loss: 34.587055014234025\n",
      "Epoch 384/500, Avg Training Loss: 0.16199929001471602, Avg Validation Loss: 34.58676325950474\n",
      "Epoch 385/500, Avg Training Loss: 0.16204973045860477, Avg Validation Loss: 34.586171041246615\n",
      "Epoch 386/500, Avg Training Loss: 0.16201985040870612, Avg Validation Loss: 34.58777109134876\n",
      "Epoch 387/500, Avg Training Loss: 0.16201542604787542, Avg Validation Loss: 34.5879480655249\n",
      "Epoch 388/500, Avg Training Loss: 0.16202265518376577, Avg Validation Loss: 34.58764778116921\n",
      "Epoch 389/500, Avg Training Loss: 0.16201388446189868, Avg Validation Loss: 34.5857811536546\n",
      "Epoch 390/500, Avg Training Loss: 0.16202149671682256, Avg Validation Loss: 34.58682150343709\n",
      "Epoch 391/500, Avg Training Loss: 0.16201399232108116, Avg Validation Loss: 34.58503731703173\n",
      "Epoch 392/500, Avg Training Loss: 0.16199877451187192, Avg Validation Loss: 34.58624922604125\n",
      "Epoch 393/500, Avg Training Loss: 0.1620574885572713, Avg Validation Loss: 34.58554188901468\n",
      "Epoch 394/500, Avg Training Loss: 0.16199969671075262, Avg Validation Loss: 34.586498075850486\n",
      "Epoch 395/500, Avg Training Loss: 0.162008131320327, Avg Validation Loss: 34.586885959597666\n",
      "Epoch 396/500, Avg Training Loss: 0.16199141515654383, Avg Validation Loss: 34.587375954685605\n",
      "Epoch 397/500, Avg Training Loss: 0.16203914861259558, Avg Validation Loss: 34.5872284664558\n",
      "Epoch 398/500, Avg Training Loss: 0.16205703427111015, Avg Validation Loss: 34.58649742361743\n",
      "Epoch 399/500, Avg Training Loss: 0.16200418144891784, Avg Validation Loss: 34.58763091002669\n",
      "Epoch 400/500, Avg Training Loss: 0.1619845174914134, Avg Validation Loss: 34.58762705527148\n",
      "Epoch 401/500, Avg Training Loss: 0.1620117673970352, Avg Validation Loss: 34.588556857517105\n",
      "Epoch 402/500, Avg Training Loss: 0.16198204644084713, Avg Validation Loss: 34.589784817758925\n",
      "Epoch 403/500, Avg Training Loss: 0.16201091958734867, Avg Validation Loss: 34.590660752764535\n",
      "Epoch 404/500, Avg Training Loss: 0.16202755416483366, Avg Validation Loss: 34.58907065898502\n",
      "Epoch 405/500, Avg Training Loss: 0.16204374705896385, Avg Validation Loss: 34.589905137149756\n",
      "Epoch 406/500, Avg Training Loss: 0.16205869265880846, Avg Validation Loss: 34.58868583798626\n",
      "Epoch 407/500, Avg Training Loss: 0.16205981553230844, Avg Validation Loss: 34.588355257365876\n",
      "Epoch 408/500, Avg Training Loss: 0.16201312700352866, Avg Validation Loss: 34.58805973930032\n",
      "Epoch 409/500, Avg Training Loss: 0.1620550551752248, Avg Validation Loss: 34.58700589211139\n",
      "Epoch 410/500, Avg Training Loss: 0.1620631675734417, Avg Validation Loss: 34.58568955052891\n",
      "Epoch 411/500, Avg Training Loss: 0.16201227453438516, Avg Validation Loss: 34.58666079775887\n",
      "Epoch 412/500, Avg Training Loss: 0.162030107058329, Avg Validation Loss: 34.58724020811562\n",
      "Epoch 413/500, Avg Training Loss: 0.16198220922322243, Avg Validation Loss: 34.58672388738134\n",
      "Epoch 414/500, Avg Training Loss: 0.1620219034939699, Avg Validation Loss: 34.585770083304936\n",
      "Epoch 415/500, Avg Training Loss: 0.16201255557721056, Avg Validation Loss: 34.58546374790241\n",
      "Epoch 416/500, Avg Training Loss: 0.1620022931083228, Avg Validation Loss: 34.585625721100996\n",
      "Epoch 417/500, Avg Training Loss: 0.16199966402568258, Avg Validation Loss: 34.58592071144456\n",
      "Epoch 418/500, Avg Training Loss: 0.1620110760792287, Avg Validation Loss: 34.58587916364861\n",
      "Epoch 419/500, Avg Training Loss: 0.161998981733063, Avg Validation Loss: 34.58461292436554\n",
      "Epoch 420/500, Avg Training Loss: 0.16203059204024836, Avg Validation Loss: 34.584939884688374\n",
      "Epoch 421/500, Avg Training Loss: 0.1620373637076049, Avg Validation Loss: 34.5851954998838\n",
      "Epoch 422/500, Avg Training Loss: 0.16205454535594582, Avg Validation Loss: 34.58588413520033\n",
      "Epoch 423/500, Avg Training Loss: 0.16200929082525095, Avg Validation Loss: 34.58532642486175\n",
      "Epoch 424/500, Avg Training Loss: 0.16203029067382083, Avg Validation Loss: 34.58511302885944\n",
      "Epoch 425/500, Avg Training Loss: 0.16197778572565594, Avg Validation Loss: 34.58631856530306\n",
      "Epoch 426/500, Avg Training Loss: 0.1620261167485386, Avg Validation Loss: 34.586076366475055\n",
      "Epoch 427/500, Avg Training Loss: 0.1620348900988358, Avg Validation Loss: 34.586075879714244\n",
      "Epoch 428/500, Avg Training Loss: 0.16201594532719496, Avg Validation Loss: 34.58527376456976\n",
      "Epoch 429/500, Avg Training Loss: 0.1620212145528402, Avg Validation Loss: 34.58489416036304\n",
      "Epoch 430/500, Avg Training Loss: 0.1620191903377011, Avg Validation Loss: 34.58370199105128\n",
      "Epoch 431/500, Avg Training Loss: 0.1620156654826578, Avg Validation Loss: 34.58442486456951\n",
      "Epoch 432/500, Avg Training Loss: 0.16198314080937412, Avg Validation Loss: 34.58480867182126\n",
      "Epoch 433/500, Avg Training Loss: 0.16201505020859489, Avg Validation Loss: 34.585631169479015\n",
      "Epoch 434/500, Avg Training Loss: 0.1619892489741705, Avg Validation Loss: 34.58603785903731\n",
      "Epoch 435/500, Avg Training Loss: 0.16201110082871775, Avg Validation Loss: 34.58510593968154\n",
      "Epoch 436/500, Avg Training Loss: 0.16201686569030213, Avg Validation Loss: 34.586186298029894\n",
      "Epoch 437/500, Avg Training Loss: 0.16202147740139852, Avg Validation Loss: 34.58686083868078\n",
      "Epoch 438/500, Avg Training Loss: 0.16203691102118023, Avg Validation Loss: 34.58628725436053\n",
      "Epoch 439/500, Avg Training Loss: 0.16198528437474605, Avg Validation Loss: 34.58591814713093\n",
      "Epoch 440/500, Avg Training Loss: 0.16201595103992233, Avg Validation Loss: 34.58512722118\n",
      "Epoch 441/500, Avg Training Loss: 0.1620045598802043, Avg Validation Loss: 34.58475370482435\n",
      "Epoch 442/500, Avg Training Loss: 0.1620572981092487, Avg Validation Loss: 34.58396555682023\n",
      "Epoch 443/500, Avg Training Loss: 0.16204557123963517, Avg Validation Loss: 34.58528885596849\n",
      "Epoch 444/500, Avg Training Loss: 0.16202505081358584, Avg Validation Loss: 34.58541919977268\n",
      "Epoch 445/500, Avg Training Loss: 0.16203677094650068, Avg Validation Loss: 34.58382278283685\n",
      "Epoch 446/500, Avg Training Loss: 0.16201050836787195, Avg Validation Loss: 34.58499058209173\n",
      "Epoch 447/500, Avg Training Loss: 0.16200925255402063, Avg Validation Loss: 34.58549411066664\n",
      "Epoch 448/500, Avg Training Loss: 0.1620150181077193, Avg Validation Loss: 34.585289518369805\n",
      "Epoch 449/500, Avg Training Loss: 0.16199758398007794, Avg Validation Loss: 34.58620322634856\n",
      "Epoch 450/500, Avg Training Loss: 0.1619742526318239, Avg Validation Loss: 34.587557457139084\n",
      "Epoch 451/500, Avg Training Loss: 0.1620001566597474, Avg Validation Loss: 34.58748153301993\n",
      "Epoch 452/500, Avg Training Loss: 0.16199604345589658, Avg Validation Loss: 34.587112012807275\n",
      "Epoch 453/500, Avg Training Loss: 0.16200830670016214, Avg Validation Loss: 34.586370779312844\n",
      "Epoch 454/500, Avg Training Loss: 0.1620169353785631, Avg Validation Loss: 34.585787502603736\n",
      "Epoch 455/500, Avg Training Loss: 0.16199359466547547, Avg Validation Loss: 34.58709753224751\n",
      "Epoch 456/500, Avg Training Loss: 0.16199958704468792, Avg Validation Loss: 34.5872413163697\n",
      "Epoch 457/500, Avg Training Loss: 0.16200950459823546, Avg Validation Loss: 34.58722268072301\n",
      "Epoch 458/500, Avg Training Loss: 0.1620166711253295, Avg Validation Loss: 34.58674490045002\n",
      "Epoch 459/500, Avg Training Loss: 0.16200596356775368, Avg Validation Loss: 34.5867699076565\n",
      "Epoch 460/500, Avg Training Loss: 0.16201616923216056, Avg Validation Loss: 34.58839500959484\n",
      "Epoch 461/500, Avg Training Loss: 0.16202715294373554, Avg Validation Loss: 34.58730396595813\n",
      "Epoch 462/500, Avg Training Loss: 0.16204829263830983, Avg Validation Loss: 34.58708117692312\n",
      "Epoch 463/500, Avg Training Loss: 0.1619829606421589, Avg Validation Loss: 34.58837385103186\n",
      "Epoch 464/500, Avg Training Loss: 0.16202584048091215, Avg Validation Loss: 34.58688806412672\n",
      "Epoch 465/500, Avg Training Loss: 0.16203219751368333, Avg Validation Loss: 34.587275501424244\n",
      "Epoch 466/500, Avg Training Loss: 0.1620034519539896, Avg Validation Loss: 34.586675598512585\n",
      "Epoch 467/500, Avg Training Loss: 0.1620202680057817, Avg Validation Loss: 34.58523756308172\n",
      "Epoch 468/500, Avg Training Loss: 0.16198465959325284, Avg Validation Loss: 34.58744362176126\n",
      "Epoch 469/500, Avg Training Loss: 0.16202273719277266, Avg Validation Loss: 34.58773050307591\n",
      "Epoch 470/500, Avg Training Loss: 0.1620045825991715, Avg Validation Loss: 34.58695622666111\n",
      "Epoch 471/500, Avg Training Loss: 0.1620353589105187, Avg Validation Loss: 34.58615336454646\n",
      "Epoch 472/500, Avg Training Loss: 0.16199402376948466, Avg Validation Loss: 34.58591309210969\n",
      "Epoch 473/500, Avg Training Loss: 0.16200998473503542, Avg Validation Loss: 34.58473696926325\n",
      "Epoch 474/500, Avg Training Loss: 0.16198942748433617, Avg Validation Loss: 34.58582177997171\n",
      "Epoch 475/500, Avg Training Loss: 0.16202775167305347, Avg Validation Loss: 34.58711713209527\n",
      "Epoch 476/500, Avg Training Loss: 0.16202181136729338, Avg Validation Loss: 34.58673330179471\n",
      "Epoch 477/500, Avg Training Loss: 0.16202237813861842, Avg Validation Loss: 34.58665628542728\n",
      "Epoch 478/500, Avg Training Loss: 0.1620328067977505, Avg Validation Loss: 34.58478083948085\n",
      "Epoch 479/500, Avg Training Loss: 0.1620205555532021, Avg Validation Loss: 34.58694563983743\n",
      "Epoch 480/500, Avg Training Loss: 0.1620123023856089, Avg Validation Loss: 34.587064129719\n",
      "Epoch 481/500, Avg Training Loss: 0.1620056661553223, Avg Validation Loss: 34.587677771141244\n",
      "Epoch 482/500, Avg Training Loss: 0.16200518478719547, Avg Validation Loss: 34.587988823587914\n",
      "Epoch 483/500, Avg Training Loss: 0.16203032399241693, Avg Validation Loss: 34.59068132202014\n",
      "Epoch 484/500, Avg Training Loss: 0.16199220515533064, Avg Validation Loss: 34.58998884782817\n",
      "Epoch 485/500, Avg Training Loss: 0.1620335392840778, Avg Validation Loss: 34.59067802227649\n",
      "Epoch 486/500, Avg Training Loss: 0.16201279520741171, Avg Validation Loss: 34.58951409452433\n",
      "Epoch 487/500, Avg Training Loss: 0.16201722296911505, Avg Validation Loss: 34.58770901011617\n",
      "Epoch 488/500, Avg Training Loss: 0.16203118116363296, Avg Validation Loss: 34.58906346853954\n",
      "Epoch 489/500, Avg Training Loss: 0.16202990888684585, Avg Validation Loss: 34.58850394494594\n",
      "Epoch 490/500, Avg Training Loss: 0.16201775159731402, Avg Validation Loss: 34.58801280321311\n",
      "Epoch 491/500, Avg Training Loss: 0.1620148184426816, Avg Validation Loss: 34.5890451904197\n",
      "Epoch 492/500, Avg Training Loss: 0.16198500309029015, Avg Validation Loss: 34.58838625078854\n",
      "Epoch 493/500, Avg Training Loss: 0.16204539621261876, Avg Validation Loss: 34.58920460079212\n",
      "Epoch 494/500, Avg Training Loss: 0.16203419770651703, Avg Validation Loss: 34.58901671997393\n",
      "Epoch 495/500, Avg Training Loss: 0.16200665643695675, Avg Validation Loss: 34.58890511831896\n",
      "Epoch 496/500, Avg Training Loss: 0.16201578694230506, Avg Validation Loss: 34.5886318931744\n",
      "Epoch 497/500, Avg Training Loss: 0.16199510615303084, Avg Validation Loss: 34.58835668082162\n",
      "Epoch 498/500, Avg Training Loss: 0.16204202559080738, Avg Validation Loss: 34.587114801775996\n",
      "Epoch 499/500, Avg Training Loss: 0.16204640426798128, Avg Validation Loss: 34.586701430162876\n",
      "Epoch 500/500, Avg Training Loss: 0.16199183579866655, Avg Validation Loss: 34.58866028919326\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IL TEST NON CONVERGE E IMPAZZISCE DOPO TOT EPOCHE\n",
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x_tot = np.random.rand(1000, 3)\n",
    "target = np.random.rand(1000, 2)\n",
    "\n",
    "layer_one = Layer(3, 2, linear, d_linear)\n",
    "layer_two = Layer(2, 2, linear, d_linear)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "NN.add_layer(layer_one)\n",
    "NN.add_layer(layer_two)\n",
    "\n",
    "# Parametri di training\n",
    "K = 5\n",
    "epochs = 500\n",
    "learning_rate = 0.0001\n",
    "Lambda = 0.1\n",
    "batch_size = 50\n",
    "\n",
    "# Cross-validation\n",
    "train_error, val_error = NN.k_fold_cross_validation(x_tot, target, K, epochs, learning_rate, Lambda, 'elastic', mean_squared_error, d_mean_squared_error, batch_size)\n",
    "\n",
    "# Plot degli errori\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_error, label='Training Error')\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
