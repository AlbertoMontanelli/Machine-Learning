{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zBZ8ZOoCBL"
      },
      "source": [
        "# Cose da fare\n",
        "* inserire lo shuffling per zig-zag gradient descent \n",
        "* (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
        "* inserire loss function per problemi di classificazione: BCE o altro\n",
        "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
        "* analisi training error vs validation error vs test error\n",
        "* analisi sulla scelta di K in base a variance/bias.\n",
        "### Novelties\n",
        "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
        "* early stopping\n",
        "* standarditation e normalization (FACOLTATIVO FORSE)\n",
        "# Cose da fare secondo le (!) Micheli\n",
        "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "qv1wwlgWsFM8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n",
        "\n",
        "# np.vectorize returns an object that acts like pyfunc, but takes arrays as input\n",
        "d_ReLU = np.vectorize(d_ReLU)\n",
        "d_leaky_relu = np.vectorize(d_leaky_relu)\n",
        "ELU = np.vectorize(ELU)\n",
        "d_ELU = np.vectorize(d_ELU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return - 2 * (y_true - y_pred)\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return - (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return - (y_true - y_pred) if(np.abs(y_true-y_pred)<=delta) else - delta * np.sign(y_true-y_pred)\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred, epsilon = 1e-7):\n",
        "    return - np.sum(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
        "\n",
        "def d_binary_cross_entropy(y_true, y_pred, epsilon = 1e-7):\n",
        "    return - (y_true / (y_pred + epsilon) - (1 - y_true) / (1 - y_pred + epsilon))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters:**\n",
        " * input_size: $k_{i-1}$;\n",
        " * output_size: $k_i$;\n",
        " * activation_function;\n",
        " * activation_derivative.\n",
        "\n",
        "**Constructor attributes:**\n",
        "* self.input_size = input_size;\n",
        "* self.output_size = output_size;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative;\n",
        "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created;\n",
        "* self.t: number of iterations for Adam.\n",
        "\n",
        "**Methods :**\n",
        "\n",
        "* **initialize_weights**: initialize weights and biases\n",
        "  * attributes:\n",
        "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
        "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
        "    * self.velocity_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Nesterov optimization;\n",
        "    * self.velocity_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Nesterov optimization;\n",
        "    * self.m_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.v_weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.m_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    * self.v_biases: $1 \\ \\textrm{x} \\ k_i$ matrix. Initialized to an array of zeros. Used in Adam optimization;\n",
        "    \n",
        "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
        "  * parameter:\n",
        "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
        "  * attributes:\n",
        "    * self.input: input_array;\n",
        "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
        "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
        "\n",
        "* **Regularization_func**: computes the regularization term using Tikhonov, Lasso or Elastic rule\n",
        "  * parameter:\n",
        "    * Lambda_t: constant used in tikhonov regularization;\n",
        "    * Lambda_l: constant used in Lasso regularization;\n",
        "    * ww: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. It is a placeholder for the kind of weights that are used in the computation of regularization: in case of NAG optimization, we use the predictions of the weights; in case of adam optimization, we use self.weights. \n",
        "    * reg_type: one of the possible string: \"tikhonov\", \"lasso\" and \"elastic\".\n",
        "  \n",
        "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        "  * parameters:\n",
        "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "    * learning_rate.\n",
        "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "H8Ap9rLxLlNU",
        "outputId": "da307f31-1bbc-4d71-826e-718b36ac131a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "        self.initialize_weights()\n",
        "        self.t = 1 \n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialization of the parameters of the network\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
        "        self.biases = np.zeros((1, self.output_size))\n",
        "\n",
        "        '''\n",
        "        # Initialization of the parameters for Nesterov optimization\n",
        "        self.velocity_weights = np.zeros_like(self.weights)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        # Initialization of the parameters for Adam optimization\n",
        "        self.m_weights = np.zeros_like(self.weights)\n",
        "        self.v_weights = np.zeros_like(self.weights)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "        '''\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "    def regularization_func(self, Lambda_t, Lambda_l, ww, reg_type):\n",
        "        if reg_type == 'tikhonov':\n",
        "            reg_term = 2 * Lambda_t * ww # learning rule of tikhonov regularization\n",
        "        elif reg_type == 'lasso':\n",
        "            reg_term = Lambda_l * np.sign(ww) # learning rule of lasso regularization\n",
        "        elif reg_type == 'elastic':\n",
        "            reg_term = (2 * Lambda_t * ww + Lambda_l * np.sign(ww)) # lasso + tikhonov regularization\n",
        "        return reg_term\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        self.delta = - d_Ep * self.activation_derivative(self.net)\n",
        "        '''\n",
        "        if opt_type == 'NAG':\n",
        "            weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights in order to use the Nesterov momentum,\n",
        "                                                                            # used to evaluate the gradient after the momentum is applied\n",
        "            bias_pred = self.biases + momentum * self.velocity_biases # same thing for the biases\n",
        "            net_pred = np.dot(self.input, weights_pred) + bias_pred  #  Net has been computed with respect to the predicted weights and the predicted biases\n",
        "            delta_pred = - d_Ep * self.activation_derivative(net_pred)  # Loss gradient with respect to net, minus sign due to the definition\n",
        "            grad_weights = learning_rate_w * np.dot(self.input.T, delta_pred)  # Loss gradient multiplied by the learning rate.\n",
        "                                                                            # The gradient has been computed with respect to the predicted weights and biases\n",
        "            \n",
        "            reg_term = self.regularization_func(Lambda_t, Lambda_l, weights_pred, reg_type)\n",
        "            self.velocity_weights = momentum * self.velocity_weights + grad_weights - reg_term  # Delta w new \n",
        "                                                                                                # the minus sign before reg_term is due to the application of gradient descent algorithm.\n",
        "            self.weights += self.velocity_weights  # Updating the weights\n",
        "            self.velocity_biases = momentum * self.velocity_biases + learning_rate_b * np.sum(delta_pred, axis=0, keepdims=True)\n",
        "            self.biases += self.velocity_biases # Updating the biases\n",
        "\n",
        "        elif opt_type == 'adam':\n",
        "            reg_term = self.regularization_func(Lambda_t, Lambda_l, self.weights, reg_type)\n",
        "            self.m_weights = beta_1 * self.m_weights + (1 - beta_1) * (- np.dot(self.input.T, self.delta) - reg_term) # np.dot(self.input.T, delta) is dLoss/dw,\n",
        "                                                                                                                        # since self.delta is defined with a minus sign\n",
        "                                                                                                                        # and the formula is with a plus sign, we put a minus sign\n",
        "                                                                                                                        # in front of np.dot(xxx)\n",
        "            self.v_weights = beta_2* self.v_weights + (1 - beta_2) * ((- np.dot(self.input.T, self.delta) - reg_term)**2) # here we have a plus sign in front of (1 - beta_2) since\n",
        "                                                                                                                        # self.delta is squared\n",
        "            m_weights_hat = self.m_weights / (1 - beta_1**self.t)\n",
        "            v_weights_hat = self.v_weights / (1 - beta_2**self.t)\n",
        "\n",
        "            self.m_biases = beta_1 * self.m_biases - (1 - beta_1) * np.sum(self.delta, axis=0, keepdims=True)\n",
        "            self.v_biases = beta_2* self.v_biases + (1 - beta_2) * np.sum(self.delta**2, axis=0, keepdims=True)\n",
        "            m_biases_hat = self.m_biases / (1 - beta_1**self.t)\n",
        "            v_biases_hat = self.v_biases / (1 - beta_2**self.t)\n",
        "\n",
        "            self.weights -= learning_rate_w * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "            self.biases -= learning_rate_b * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)\n",
        "        '''\n",
        "        \n",
        "        self.weights += (learning_rate_w * np.dot(self.input.T, self.delta))\n",
        "        self.biases += np.sum(self.delta, axis=0, keepdims=True)\n",
        "\n",
        "        sum_delta_weights = np.dot(self.delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
        "   * parameters:\n",
        "     * x_tot: total data given as input;\n",
        "     * target: total data labels given as input;\n",
        "     * test_split: percentile of test set with respect to the total data.\n",
        "    * return->:\n",
        "      * x_train_val: training & validation set extracted from input data;\n",
        "      * target_train_val: training & validation set labels;\n",
        "      * x_test_val: test set extracted from input data;\n",
        "      * target_test_val: test set for input data labels.\n",
        "\n",
        "     \n",
        "* **add_layer**: appends a layer to the empty list self.layers\n",
        "   * parameter:\n",
        "     * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        "  * parameter:\n",
        "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        "  * return -> input = $o_i$ for layer $L_i$.\n",
        "  \n",
        "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        "  * parameters:\n",
        "    * d_Ep;\n",
        "    * learning_rate.\n",
        "\n",
        "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
        "\n",
        "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
        "  * parameters:\n",
        "    * x_train: set of the original dataset used for training;\n",
        "    * target_train: labels corresponding to the training set;\n",
        "    * x_val: set of the original dataset used for validation;\n",
        "    * target_val: labels corresponding to the validation set;\n",
        "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
        "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
        "    * loss_function: hyperparameter;\n",
        "    * loss_function_derivative: hyperparameter;\n",
        "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
        "\n",
        "* **train_val**: actual training and validation process.\n",
        "  * parameters:\n",
        "    * x_train_val;\n",
        "    * target_train_val;\n",
        "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
        "    * epochs;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def data_split(self, x_tot, target, test_split):\n",
        "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
        "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
        "\n",
        "        x_test = x_tot[:test_size]\n",
        "        target_test = target[:test_size]\n",
        "        x_train_val = x_tot[test_size:]\n",
        "        target_train_val = target[test_size:]\n",
        "\n",
        "        return x_train_val, target_train_val, x_test, target_test\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "\n",
        "    def reinitialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.initialize_weights() # does it layer-by-layer\n",
        "\n",
        "    def accuracy_curve(\n",
        "            self,\n",
        "            pred,\n",
        "            target\n",
        "    ): \n",
        "        #pred = self.neural_network.forward(xx)\n",
        "        correct_classifications = 0\n",
        "        \n",
        "        for k in range(len(pred)):\n",
        "            #print(f'pred prima = {pred[k]}')\n",
        "            pred[k] = 1 if pred[k] >= 0.5 else 0\n",
        "            #print(f'pred dopo = {pred[k]}')\n",
        "            if (pred[k] == target[k]):\n",
        "                correct_classifications += 1\n",
        "\n",
        "        accuracy = correct_classifications/len(pred)\n",
        "        #print(f'corret class: {correct_classifications}')\n",
        "        #print(f'len pred: {len(pred)} ')\n",
        "        #print(f'accuracy: {accuracy}')\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        train_error_epoch = np.zeros(epochs)\n",
        "        # val_error_epoch = np.zeros(epochs)\n",
        "        accuracy_tot = np.zeros(epochs)\n",
        "        # accuracy_fold = np.array([])\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
        "            # epoch_val_loss = 0\n",
        "\n",
        "            '''\n",
        "            # shuffling training data before splitting it into batches.\n",
        "            # done in order to avoid reinforcing neurons in the same way\n",
        "            # in different epochs due to invisible patterns in the data\n",
        "            train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(train_indices)\n",
        "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
        "            target_train = target_train[train_indices]\n",
        "            '''\n",
        "\n",
        "            # if batch_size=1 we get the online version,\n",
        "            # else we get mini-batch version with batches of size batch_size\n",
        "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
        "                                                             # Even if the number of examples is not divisible\n",
        "                                                             # for batch_size the last, smaller batch is processed anyway\n",
        "\n",
        "                x_batch = x_train[i:i+batch_size]\n",
        "                target_batch = target_train[i:i+batch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
        "                # computing loss and gradient\n",
        "                loss = loss_function(target_batch, predictions)\n",
        "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "                epoch_tr_loss += np.sum(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(loss_gradient, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "                # Layer.t += 1 # number of iterations for adam\n",
        "                accuracy_epoch = self.accuracy_curve(predictions, target_batch)\n",
        "                print(f\"accuracy epoch = {accuracy_epoch}\")\n",
        "                # accuracy_fold = np.append(accuracy_fold, accuracy_epoch)\n",
        "\n",
        "            '''\n",
        "            # validation\n",
        "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
        "            val_loss = loss_function(target_val, val_predictions)\n",
        "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished.\n",
        "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
        "            # Store average errors for the epoch\n",
        "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
        "            val_error_epoch[epoch] = epoch_val_loss\n",
        "            '''\n",
        "\n",
        "\n",
        "        accuracy_tot += accuracy_epoch\n",
        "        print(f'accuracy_tot {accuracy_tot}')\n",
        "\n",
        "        return train_error_epoch\n",
        "\n",
        "\n",
        "    def train_val(self, x_train_val, target_train_val, train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type):\n",
        "        num_samples = x_train_val.shape[0]\n",
        "        fold_size = num_samples // K\n",
        "\n",
        "        # error storage for averaging\n",
        "        avg_train_error_epoch = np.zeros(epochs)\n",
        "        avg_val_error_epoch = np.zeros(epochs)\n",
        "        \n",
        "        if K==1: # hold-out validation\n",
        "            train_indices = np.arange(0, int(train_split*num_samples)) # training set is 75% of the training & validation set\n",
        "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets\n",
        "            train_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            ) # computation of errors via train_val_setup method\n",
        "            return train_error_epoch\n",
        "        for k in range(K):\n",
        "            # creating fold indices\n",
        "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
        "                                                                        # It contains the indices of the examples used in validation set for\n",
        "                                                                        # this fold.\n",
        "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples -\n",
        "                                                                              # len(val_indices). It contains the indices of all the examples\n",
        "                                                                              # but the ones used in the validation set for this fold.\n",
        "                                                                              # It corresponds to the training set for the current fold.\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
        "\n",
        "            # shuffle \n",
        "            new_train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(new_train_indices)\n",
        "            x_train = x_train[new_train_indices]\n",
        "            target_train = target_train[new_train_indices]\n",
        "\n",
        "            '''\n",
        "            new_val_indices = np.arange(x_val.shape[0])\n",
        "            np.random.shuffle(new_val_indices)\n",
        "            x_val = x_val[new_val_indices]\n",
        "            target_val = target_val[new_val_indices]\n",
        "            print(f\"x val \\n {x_val}\")\n",
        "            '''\n",
        "            \n",
        "            # re-initializing weights for each fold\n",
        "            self.reinitialize_weights()\n",
        "            Layer.t = 1\n",
        "            # training on the current fold\n",
        "            train_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, loss_function, loss_function_derivative, reg_type, opt_type\n",
        "            )\n",
        "\n",
        "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
        "            avg_train_error_epoch += train_error_epoch\n",
        "            # avg_val_error_epoch += val_error_epoch\n",
        "            \n",
        "            print(f\"Fold {k+1} completed, t = {Layer.t}\")\n",
        "\n",
        "        # averaging errors across all folds\n",
        "        avg_train_error_epoch /= K\n",
        "        # avg_val_error_epoch /= K\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}\")\n",
        "\n",
        "\n",
        "        return avg_train_error_epoch #, avg_val_error_epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "# Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "5be15b38-62a4-439a-dd86-57790daaffe2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy epoch = 0.625\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_17216\\3922403432.py:20: RuntimeWarning: invalid value encountered in log\n",
            "  return - np.sum(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "accuracy epoch = 0.375\n",
            "accuracy epoch = 0.21875\n",
            "accuracy epoch = 0.65625\n",
            "accuracy epoch = 0.7857142857142857\n",
            "[0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429 0.78571429\n",
            " 0.78571429 0.78571429]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\leona\\AppData\\Local\\Temp\\ipykernel_17216\\2265877404.py:49: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n",
            "  plt.yscale('log')\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGwCAYAAACgi8/jAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAItdJREFUeJzt3QuUVdVhP+A9PAV0RgEFCSC+ahwTicGREpsmKhGxS6JJ6iOmorZaiK/EaC1JFbWJ2JgQGzNLk5WobXWptSvaVEUiRjQaDQolPmEVl6+KiI8wAyrv8197/3tnzeU5wAx37p7vW+usmXPPnXv3mX3vPb+7H+fUFEVRBACADHWrdAEAADqKoAMAZEvQAQCyJegAANkSdACAbAk6AEC2BB0AIFs9Qhe3fv36sHjx4rDbbruFmpqaShcHAGiDeBrA5cuXhyFDhoRu3TbfbtPlg04MOcOGDat0MQCA7fDGG2+EoUOHbnZ7lw86sSWn9I+qra2tdHEAgDZobm5ODRWl4/jmdPmgU+quiiFH0AGA6rK1YScGIwMA2RJ0AIBsCToAQLa6/BgdAHaOdevWhTVr1lS6GFSJnj17hu7du+/w4wg6AHT4+U6WLFkSli1bVumiUGV23333MHjw4B06z52gA0CHKoWcvfbaK/Tt29fJWWlTOP7www/D0qVL0/ree+8dtpegA0CHdleVQs6AAQMqXRyqSJ8+fdLPGHbi62d7u7EMRgagw5TG5MSWHNhWpdfNjoztEnQA6HC6q6jU6yaLoHPSSSeFPfbYI3zlK1+pdFEAgE4ki6Bz0UUXhX/913+tdDEAgE4mi6Dz+c9/fqsX9QKAShsxYkS4/vrr23z/2bNnp+4bU/OrOOg89thj4YQTTghDhgxJlXnvvfdudJ/Gxsb04thll13C6NGjw5w5cypSVgC6hng82tJy5ZVXbtfjPv300+Hcc89t8/0/85nPhLfeeivU1dWFjjT7/wLVppZ4eoBqVvHp5R988EEYOXJkOPvss8OXvvSljbbfdddd4eKLLw433XRTCjkxCY8bNy4sXLgwTTfbVqtWrUpL68u8A0BrMVy0Pg5dccUV6bhTsuuuu5ad8yVOo+/RY+uH1D333HObytGrV690wrydZeHChaG2trbsts0da1evXp3Kt6E4Qyqe1Xhbbe/fdfoWnfHjx4fvfve7aUDxpkyfPj2cc8454ayzzgr19fUp8MTpZjfffPN2Pd+0adNSMi4tw4YN28E9AGCbTwa3em1FlvjcbRHDRWmJx4rYslFaX7BgQRouMWPGjDBq1KjQu3fv8Pjjj4eXX345fPGLXwyDBg1KQaihoSHMmjVri11X8XF//vOfp2NgPLYdeOCB4Ve/+tVmu65uvfXWdLbgmTNnhoMPPjg9z3HHHVcWzNauXRsuvPDCdL8BAwaEyy67LEycODGceOKJW93vGGpa73tcunX7/1HhzDPPTI/xve99L/XCHHTQQeHVV19N5Yth8HOf+1zqebn99tvD+vXrw9VXXx2GDh2a/j+f+tSnwoMPPtjyPJv7uyxbdLYkpsW5c+eGKVOmtNwW/+Fjx44NTz755HY9Znys2ELUukVH2AHYeT5asy7UXzGzIs/94tXjQt9e7XPo+/u///vwgx/8IOy3335p5u8bb7wRjj/++BQE4sE9TpKJQzNiK8nw4cM3+zhXXXVV+P73vx+uu+66cMMNN4TTTz89vPbaa6F///6bvH88Y3B83n/7t39Lx8Svfe1r4ZJLLmkJCv/0T/+Ufr/llltSGPrnf/7nNCzkqKOO2uF9fvjhh1OLz0MPPbTR/+KHP/xhOOyww1Joic8Z13/605+m22LjxIQJE8ILL7yQwtzm/q7LBZ133303NQfGdNxaXI+JuiQGnz/84Q+pGyymx7vvvjuMGTNmk48ZX3xxAYAdEVssvvCFL7Ssx2ASh2KU/OM//mO45557UgvN+eefv9nHiS0lp512Wvr9mmuuCT/+8Y/TWNTYUrO5Lp7Yu7H//vun9fjYsSwlMSzFL/WlnpKf/OQn4YEHHmjTPsVjaGv77LNPCicl/fr1Sy1QpS6r2DITfeMb3ygbfhKDWGxJOvXUU1vC1yOPPJJas+K425IN/67LBZ222rBpEIDOq0/P7qllpVLP3V4OP/zwsvUVK1akQcr3339/6kqKXUgfffRReP3117f4OIceemhZkIgtJqVrPG1K7OIqhZzSdaBK929qagpvv/12OOKII1q2d+/ePXWxxe6krfntb39bNot5wzEzn/zkJzc5Lqf1/yL2lCxevDgceeSRZfeJ67FRYnN/1yWDzsCBA1MFxUprLa7vzMFZALSfODajvbqPKimGktZi91Hs0omtGQcccEC6VlM8kW0chrElG4aJ+P/ZUijZ1P3bOvZoa/bdd980tqet+7y127dme/+uqgYjb0lMjTGFxj7Bklj5cX1zXVMAUAlPPPFE6oaKXUax5SN+IS917ewsceB0HN4Rp7GXrFu3LsybN2+nlSG2SMXByvH/0Vpcj5OKdraKR+rY1Ldo0aKW9VdeeSXMnz8/9XXGwVtx4HAcLR6bt2JTXOzfi2Nx4iwsAOgs4iDbX/7yl2kAcmxlufzyy9vUXdTeLrjggjTDOLYqffzjH09jdv74xz+26bpRsQts5cqVZbfFmVvbOu370ksvDVOnTk1dbHHGVRwYHY/tHTWzqlMHnWeeeaZsJHhpRlQMN3Ea3SmnnBLeeeeddA6DeNKi0hS1DQcoA0AlxdOhxHPCxZP8xaEXcTBuJc7VFp83Hi/POOOMNPwjnqAwnn8u/r41ccr4huIs5z/90z/dpjLE6e1xvNC3vvWtFJ5iS04clN16xtXOUlO0V8delYovwtjUFytkw5MkAbBjYutAbKmPYz86avowW7Z+/fo0zfzkk09OM8Fyef209fhd8RYdAKD9xHPw/PrXv04n4lu1alWaXh7Dwle/+tXQFXXqwcgAwLaJJxGMQz/imZmPPPLI8Nxzz6XTsMRWna5Iiw4AZCSe7X/DGU9dmRYdADpcFx8OSgVfN4IOAB2mNC05Xp8JtlXpdbMjVzXXdQVAh4lTmuOZdkuXKIiXL2jL+Vzo2op4hfsPP0yvm/j6acvU+M3pskEnXlQsLvGMkQB0nNIle7Z0/SbYlBhydvSST86j4zw6ADtF/GIZr7wNbRG7q7bUkuM8OgB0KvGgtSNdELA9DEYGALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJCtLht0GhsbQ319fWhoaKh0UQCADlJTFEURurDm5uZQV1cXmpqaQm1tbaWLAwC04/G7y7boAAD5E3QAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGx12aDT2NgY6uvrQ0NDQ6WLAgB0kJqiKIrQhTU3N4e6urrQ1NQUamtrK10cAKAdj99dtkUHAMifoAMAZEvQAQCyJegAANkSdACAbAk6AEC2BB0AIFuCDgCQLUEHAMiWoAMAZEvQAQCyJegAANkSdACAbAk6AEC2BB0AIFuCDgCQLUEHAMiWoAMAZEvQAQCyJegAANnqskGnsbEx1NfXh4aGhkoXBQDoIDVFURShC2tubg51dXWhqakp1NbWVro4AEA7Hr+7bIsOAJA/QQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJCtLht0GhsbQ319fWhoaKh0UQCADlJTFEURurDm5uZQV1cXmpqaQm1tbaWLAwC04/G7y7boAAD5E3QAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGSrywadxsbGUF9fHxoaGipdFACgg9QURVGELqy5uTnU1dWFpqamUFtbW+niAADtePzusi06AED+BB0AIFuCDgCQLUEHAMiWoAMAZEvQAQCyJegAANkSdACAbAk6AEC2BB0AIFuCDgCQLUEHAMiWoAMAZEvQAQCyJegAANkSdACAbAk6AEC2BB0AIFuCDgCQrW0OOmvWrAk9evQIzz//fMeUCACgUkGnZ8+eYfjw4WHdunXtVQYAgM7TdfWd73wnfPvb3w7vv/9++5cIAKCd9NieP/rJT34SFi1aFIYMGRL22Wef0K9fv7Lt8+bNa6/yAQDs3KBz4oknbv8zAgDsJDVFURShC2tubg51dXWhqakp1NbWVro4AEA7Hr+3q0WnZO7cueGll15Kvx9yyCHhsMMO25GHAwBoV9sVdJYuXRpOPfXUMHv27LD77run25YtWxaOOuqocOedd4Y999yzfUsJALCzZl1dcMEFYfny5eGFF15IM6/iEs+rE5uRLrzwwu15SACAzjFGJ/aJzZo1KzQ0NJTdPmfOnHDsscem1p1qYYwOAFSfth6/t6tFZ/369enEgRuKt8VtAACdwXYFnaOPPjpcdNFFYfHixS23vfnmm+Gb3/xmOOaYY9qzfAAAOzfoxBMGxiajESNGhP333z8t++67b7rthhtu2P7SAABUetbVsGHD0tmP4zidBQsWpNsOPvjgMHbs2PYsGwDAzg068erlffr0CfPnzw9f+MIX0gIA0Bm5ejkAkC1XLwcAsuXq5QBAtly9HADI1jYHnbVr14aamppw9tlnh6FDh3ZMqQAAKjFGp0ePHuG6665LgQcAIMszIz/66KOhmjU2Nob6+vqNrtcFAHTxi3redNNN4aqrrgqnn356GDVq1EaDkSdMmBCqhYt6AkD1aevxe7uCTrdum28IiuN3qukcO4IOAFSfth6/t2vWlSuUAwDZjdE5/vjjU3Iqufbaa8OyZcta1t9777007gUAoOqCzsyZM8OqVata1q+55pqysyPHmVgLFy5s3xICAOyMoLPhcJ7tGN4DANC5p5cDAGQXdOKMqrhseBsAQGe0TbOuYlfVmWeeGXr37p3WV65cGSZNmtRyHp3W43cAAKoq6EycOLFs/Wtf+9pG9znjjDN2vFQAADs76Nxyyy3t8ZwAADuFwcgAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQLYEHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2eqyQaexsTHU19eHhoaGShcFAOggNUVRFKELa25uDnV1daGpqSnU1tZWujgAQDsev7tsiw4AkD9BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AEAsiXoAADZEnQAgGxlEXTuu+++cNBBB4UDDzww/PznP690cQCATqJHqHJr164NF198cXjkkUdCXV1dGDVqVDjppJPCgAEDKl00AKDCqr5FZ86cOeGQQw4JH/vYx8Kuu+4axo8fH379619XulgAQCdQ8aDz2GOPhRNOOCEMGTIk1NTUhHvvvXej+zQ2NoYRI0aEXXbZJYwePTqFm5LFixenkFMSf3/zzTd3WvkBgM6r4kHngw8+CCNHjkxhZlPuuuuu1DU1derUMG/evHTfcePGhaVLl27X861atSo0NzeXLQBAnioedGJX03e/+900rmZTpk+fHs4555xw1llnhfr6+nDTTTeFvn37hptvvjltjy1BrVtw4u/xts2ZNm1aGstTWoYNG9YBewUAdAYVDzpbsnr16jB37twwduzYltu6deuW1p988sm0fsQRR4Tnn38+BZwVK1aEGTNmpBafzZkyZUpoampqWd54442dsi8AwM7XqWddvfvuu2HdunVh0KBBZbfH9QULFqTfe/ToEX74wx+Go446Kqxfvz783d/93RZnXPXu3TstAED+OnXQaasJEyakBQCgarquBg4cGLp37x7efvvtstvj+uDBgytWLgCgOnTqoNOrV690AsCHH3645bbYPRXXx4wZU9GyAQCdX8W7ruIA4kWLFrWsv/LKK2H+/Pmhf//+Yfjw4Wlq+cSJE8Phhx+eBh5ff/31aUp6nIUFANCpg84zzzyTBhKXxGATxXBz6623hlNOOSW888474YorrghLliwJn/rUp8KDDz640QBlAIAN1RRFUYQuLJ4wMJ5PJ041r62trXRxAIB2PH536jE6AAA7QtABALIl6AAA2RJ0AIBsCToAQLYEHQAgW1026DQ2Nob6+vrQ0NBQ6aIAAB2ky59HJ86/33333cMbb7zhPDoAUEXn0Rk2bFhYtmxZOp9Opz0zcqUtX748/Yz/LACg+o7jWwo6Xb5FJ14kdPHixWG33XYLNTU17Z40c24pyn0fc9+/yD5Wv9z3L7KP1a+5A/YvxpcYcoYMGRK6ddv8SJwu36IT/zlDhw7tsMePFZrji7Yr7WPu+xfZx+qX+/5F9rH61bbz/m2pJSd09cHIAED+BB0AIFuCTgfp3bt3mDp1avqZq9z3Mff9i+xj9ct9/yL7WP16V3D/uvxgZAAgX1p0AIBsCToAQLYEHQAgW4IOAJAtQacDLxo6YsSIsMsuu4TRo0eHOXPmhGo0bdq0dOHTeObovfbaK5x44olh4cKFZff5/Oc/n84q3XqZNGlSqBZXXnnlRuX/+Mc/3rJ95cqV4bzzzgsDBgwIu+66a/jyl78c3n777VAt4utww/2LS9ynaq2/xx57LJxwwgnpjKixvPfee2/Z9jjH4oorrgh777136NOnTxg7dmz4n//5n7L7vP/+++H0009PJy+L17v767/+67BixYpQDfu4Zs2acNlll4VPfvKToV+/fuk+Z5xxRjrL+9bq/tprrw3VUIdnnnnmRmU/7rjjsqnDaFPvy7hcd911VVGH09pwfGjL5+frr78e/uIv/iL07ds3Pc6ll14a1q5d227lFHQ6wF133RUuvvjiNJVu3rx5YeTIkWHcuHFh6dKlodo8+uij6UX61FNPhYceeih9wB577LHhgw8+KLvfOeecE956662W5fvf/36oJoccckhZ+R9//PGWbd/85jfDf/3Xf4W77747/T/iweRLX/pSqBZPP/102b7Feoz+8i//smrrL77+4vsqfqHYlFj+H//4x+Gmm24Kv//971MYiO/B+KFbEg+QL7zwQvp/3HfffemgdO6554Zq2McPP/wwfbZcfvnl6ecvf/nLdICZMGHCRve9+uqry+r2ggsuCNVQh1EMNq3Lfscdd5Rtr+Y6jFrvW1xuvvnmFGRiGKiGOny0DceHrX1+rlu3LoWc1atXh9/97nfhX/7lX8Ktt96avqi0mzi9nPZ1xBFHFOedd17L+rp164ohQ4YU06ZNK6rd0qVL4+kIikcffbTlts997nPFRRddVFSrqVOnFiNHjtzktmXLlhU9e/Ys7r777pbbXnrppfQ/ePLJJ4tqFOtq//33L9avX59F/cW6uOeee1rW434NHjy4uO6668rqsXfv3sUdd9yR1l988cX0d08//XTLfWbMmFHU1NQUb775ZtHZ93FT5syZk+732muvtdy2zz77FD/60Y+Kzm5T+zdx4sTii1/84mb/Jsc6jPt79NFHl91WLXW4qeNDWz4/H3jggaJbt27FkiVLWu5z4403FrW1tcWqVauK9qBFp53FVDp37tzUVN76elpx/cknnwzVrqmpKf3s379/2e233357GDhwYPjEJz4RpkyZkr5xVpPYrRGbl/fbb7/0LTE2pUaxLuO3lNb1Gbu1hg8fXpX1GV+ft912Wzj77LPLLmJb7fXX2iuvvBKWLFlSVmfxejixC7lUZ/Fn7Oo4/PDDW+4T7x/fq7EFqFrfm7FO4361Frs5YrfBYYcdlrpE2rNLoKPNnj07dWUcdNBBYfLkyeG9995r2ZZbHcbunPvvvz91v22oWuqwaYPjQ1s+P+PP2AU7aNCglvvE1td4EdDYWtceuvxFPdvbu+++m5riWldaFNcXLFgQqv1K79/4xjfCkUcemQ6IJV/96lfDPvvsk4LCs88+m8YOxGb02JxeDeIBMDaVxg/T2Cx81VVXhc9+9rPh+eefTwfMXr16bXTwiPUZt1WbOEZg2bJlafxDLvW3oVK9bOo9WNoWf8YDaGs9evRIH9DVWK+xSy7W22mnnVZ2wcQLL7wwfPrTn077FbsFYoiNr/Hp06eHzi52W8Uujn333Te8/PLL4dvf/nYYP358OjB27949uzqMXTZxrMuG3eLVUofrN3F8aMvnZ/y5qfdqaVt7EHRos9gXGw/+rcevRK37xGMyjwNAjznmmPThtP/++4fOLn54lhx66KEp+MQD/7//+7+ngaw5+cUvfpH2N4aaXOqvq4vfmE8++eQ0APvGG28s2xbHCrZ+bceDzt/+7d+mQaSd/VIDp556atnrMpY/vh5jK098feYmjs+JrclxAks11uF5mzk+dAa6rtpZbP6P3zY2HFUe1wcPHhyq1fnnn58G+z3yyCNh6NChW7xvDArRokWLQjWK3z7+5E/+JJU/1lns7omtINVen6+99lqYNWtW+Ju/+Zus669UL1t6D8afG04OiN0BcRZPNdVrKeTEuo2DQVu35myubuN+vvrqq6HaxG7l+Plael3mUofRb3/729SKurX3Zmetw/M3c3xoy+dn/Lmp92ppW3sQdNpZTNujRo0KDz/8cFmTXlwfM2ZMqDbxW2J8Ed9zzz3hN7/5TWpG3pr58+enn7FloBrF6amxNSOWP9Zlz549y+ozfiDFMTzVVp+33HJLauqPMxxyrr/4Go0fkK3rLPb3x3EbpTqLP+OHbxxDUBJf3/G9Wgp61RJy4viyGGDjGI6tiXUbx7Bs2OVTDf73f/83jdEpvS5zqMPWLa3xsybO0KqmOiy2cnxoy+dn/Pncc8+VhdZSaK+vr2+3gtLO7rzzzjTD49Zbb00zA84999xi9913LxtVXi0mT55c1NXVFbNnzy7eeuutluXDDz9M2xctWlRcffXVxTPPPFO88sorxX/+538W++23X/Hnf/7nRbX41re+lfYvlv+JJ54oxo4dWwwcODDNIIgmTZpUDB8+vPjNb36T9nPMmDFpqSZx5l/ch8suu6zs9mqtv+XLlxf//d//nZb4MTZ9+vT0e2nG0bXXXpvec3F/nn322TSbZd999y0++uijlsc47rjjisMOO6z4/e9/Xzz++OPFgQceWJx22mlFNezj6tWriwkTJhRDhw4t5s+fX/beLM1U+d3vfpdm68TtL7/8cnHbbbcVe+65Z3HGGWcUnX3/4rZLLrkkzcyJr8tZs2YVn/70p1MdrVy5Mos6LGlqair69u2bZhptqLPX4eStHB/a8vm5du3a4hOf+ERx7LHHpv188MEH0z5OmTKl3cop6HSQG264IVVur1690nTzp556qqhG8c25qeWWW25J219//fV0UOzfv38KdwcccEBx6aWXpjdvtTjllFOKvffeO9XVxz72sbQeA0BJPDh+/etfL/bYY4/0gXTSSSelN3M1mTlzZqq3hQsXlt1erfX3yCOPbPJ1Gackl6aYX3755cWgQYPSfh1zzDEb7ft7772XDoq77rprmsp61llnpQNTNexjPPhv7r0Z/y6aO3duMXr06HQg2mWXXYqDDz64uOaaa8qCQmfdv3igjAe+eMCL05PjFOtzzjlnoy+L1VyHJT/96U+LPn36pKnYG+rsdRi2cnxo6+fnq6++WowfPz79H+KXzPjlc82aNe1Wzpr/KywAQHaM0QEAsiXoAADZEnQAgGwJOgBAtgQdACBbgg4AkC1BBwDIlqADAGRL0AG6vJqamnDvvfdWuhhABxB0gIo688wzU9DYcDnuuOMqXTQgAz0qXQCAGGri1dVb6927d8XKA+RDiw5QcTHUDB48uGzZY4890rbYunPjjTeG8ePHhz59+oT99tsv/Md//EfZ3z/33HPh6KOPTtsHDBgQzj333LBixYqy+9x8883hkEMOSc+19957h/PPP79s+7vvvhtOOumk0Ldv33DggQeGX/3qVy3b/vjHP4bTTz897Lnnnuk54vYNgxnQOQk6QKd3+eWXhy9/+cvhD3/4Qwocp556anjppZfStg8++CCMGzcuBaOnn3463H333WHWrFllQSYGpfPOOy8FoBiKYog54IADyp7jqquuCieffHJ49tlnw/HHH5+e5/333295/hdffDHMmDEjPW98vIEDB+7k/wKwXdrtOugA22HixIlF9+7di379+pUt3/ve99L2+DE1adKksr8ZPXp0MXny5PT7z372s2KPPfYoVqxY0bL9/vvvL7p161YsWbIkrQ8ZMqT4zne+s9kyxOf4h3/4h5b1+FjxthkzZqT1E044oTjrrLPaec+BncEYHaDijjrqqNRK0lr//v1bfh8zZkzZtrg+f/789HtsYRk5cmTo169fy/YjjzwyrF+/PixcuDB1fS1evDgcc8wxWyzDoYce2vJ7fKza2tqwdOnStD558uTUojRv3rxw7LHHhhNPPDF85jOf2cG9BnYGQQeouBgsNuxKai9xTE1b9OzZs2w9BqQYlqI4Pui1114LDzzwQHjooYdSaIpdYT/4wQ86pMxA+zFGB+j0nnrqqY3WDz744PR7/BnH7sSxOiVPPPFE6NatWzjooIPCbrvtFkaMGBEefvjhHSpDHIg8ceLEcNttt4Xrr78+/OxnP9uhxwN2Di06QMWtWrUqLFmypOy2Hj16tAz4jQOMDz/88PBnf/Zn4fbbbw9z5swJv/jFL9K2OGh46tSpKYRceeWV4Z133gkXXHBB+Ku/+qswaNCgdJ94+6RJk8Jee+2VWmeWL1+ewlC8X1tcccUVYdSoUWnWVizrfffd1xK0gM5N0AEq7sEHH0xTvluLrTELFixomRF15513hq9//evpfnfccUeor69P2+J08JkzZ4aLLrooNDQ0pPU4nmb69OktjxVD0MqVK8OPfvSjcMkll6QA9ZWvfKXN5evVq1eYMmVKePXVV1NX2Gc/+9lUHqDzq4kjkitdCIDNiWNl7rnnnjQAGGBbGaMDAGRL0AEAsmWMDtCp6V0HdoQWHQAgW4IOAJAtQQcAyJagAwBkS9ABALIl6AAA2RJ0AIBsCToAQMjV/wMs8ZMxcBqT7QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from MonkDataProcessing import monk_data\n",
        "\n",
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x_tot = np.random.rand(256, 15)\n",
        "target = np.random.rand(256, 3)\n",
        "\n",
        "test_split = 0\n",
        "train_split = 1\n",
        "\n",
        "layer_one = Layer(17, 8, ELU, d_ELU)\n",
        "layer_two = Layer(8, 4, ELU, d_ELU)\n",
        "layer_out = Layer(4, 1, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "# Split data\n",
        "# x_train_val, target_train_val, x_test, target_test = NN.data_split(x_tot, target, test_split)\n",
        "\n",
        "NN.add_layer(layer_one)\n",
        "NN.add_layer(layer_two)\n",
        "NN.add_layer(layer_out)\n",
        "\n",
        "# Parametri di training\n",
        "K = 1\n",
        "epochs = 200\n",
        "learning_rate_w = 0.0001 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
        "                         # learning_rate of minibatch for number of the examples in the minibatch\n",
        "learning_rate_b = 0.0001\n",
        "batch_size = 32\n",
        "Lambda_t = 0.1\n",
        "Lambda_l = 0.1\n",
        "momentum = 0.9 # tipically from 0.8 and 0.99\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# Cross-validation\n",
        "train_error = NN.train_val(monk_data[\"training_set_1\"], monk_data[\"target_training_set_1\"], train_split, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, batch_size, beta_1, beta_2, epsilon, binary_cross_entropy, d_binary_cross_entropy, 'elastic', 'adam')\n",
        "\n",
        "# Plot degli errori\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_error, label='Training Error')\n",
        "# plt.plot(val_error, label='Validation Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Error')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
