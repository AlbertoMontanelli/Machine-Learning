{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives.\n",
    "* **Hidden layers**:\n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "qv1wwlgWsFM8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
    "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
    "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha):\n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "ELU = np.vectorize(ELU)\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n",
    "\n",
    "d_ELU = np.vectorize(d_ELU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjNvxQOLsFM-"
   },
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters:**\n",
    " * input_size: $k_{i-1}$;\n",
    " * output_size: $k_i$;\n",
    " * activation_function;\n",
    " * activation_derivative. \n",
    "\n",
    "**Constructor attributes:**\n",
    "* self.input_size = input_size;\n",
    "* self.output_size = output_size;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative;\n",
    "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
    "\n",
    "**Methods :**\n",
    "\n",
    "* **initialize_weights**: initialize weights and biases\n",
    "  * attributes:\n",
    "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
    "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
    "    \n",
    "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
    "  * parameter:\n",
    "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
    "  * attributes:\n",
    "    * self.input: input_array;\n",
    "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
    "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
    "  \n",
    "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    "  * parameters:\n",
    "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "    * learning_rate.\n",
    "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8Ap9rLxLlNU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.initialize_weights()\n",
    "        \n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
    "        self.biases = np.zeros((1, self.output_size))\n",
    "        \n",
    " \n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward_layer(self, d_Ep, learning_rate, Lambda, reg_type):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "        if (reg_type=='tikhonov'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*self.weights # learning rule - tikhonov regularization\n",
    "        elif (reg_type=='lasso'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*np.sign(self.weights) # learning rule - lasso regularization\n",
    "        elif (reg_type=='elastic'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda*self.weights - Lambda*np.sign(self.weights) # learning rule:\n",
    "                                                                                                                             # lasso + tikhonov\n",
    "                                                                                                                             # regularization\n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
    "   * parameters:\n",
    "     * x_tot: total data given as input;\n",
    "     * target: total data labels given as input;\n",
    "     * test_split: percentile of test set with respect to the total data.\n",
    "    * return->:\n",
    "      * x_train_val: training & validation set extracted from input data;\n",
    "      * target_train_val: training & validation set labels;\n",
    "      * x_test_val: test set extracted from input data;\n",
    "      * target_test_val: test set for input data labels.\n",
    "\n",
    "     \n",
    "* **add_layer**: appends a layer to the empty list self.layers\n",
    "   * parameter:\n",
    "     * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    "  * parameter:\n",
    "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    "  * return -> input = $o_i$ for layer $L_i$.\n",
    "  \n",
    "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    "  * parameters:\n",
    "    * d_Ep;\n",
    "    * learning_rate.\n",
    "\n",
    "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
    "\n",
    "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
    " * parameters:\n",
    "   * x_train: set of the original dataset used for training;\n",
    "   * target_train: labels corresponding to the training set;\n",
    "   * x_val: set of the original dataset used for validation;\n",
    "   * target_val: labels corresponding to the validation set;\n",
    "   * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
    "   * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
    "   * loss_function: hyperparameter;\n",
    "   * loss_function_derivative: hyperparameter;\n",
    "   * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
    "\n",
    "* **train_val**: actual training and validation process.\n",
    " * parameters:\n",
    "   * x_train_val;\n",
    "   * target_train_val;\n",
    "   * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
    "   * epochs;\n",
    "   * learning_rate;\n",
    "   * loss_function;\n",
    "   * loss_function_derivative;\n",
    "   * batch_size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "\n",
    "    def data_split(self, x_tot, target, test_split):\n",
    "        \n",
    "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
    "        test_size = int(num_samples * test_split) # the number of the examples in the test set        \n",
    "\n",
    "        x_test = x_tot[:test_size]\n",
    "        target_test = target[:test_size]\n",
    "        x_train_val = x_tot[test_size:]\n",
    "        target_train_val = target[test_size:]\n",
    "\n",
    "        return x_train_val, target_train_val, x_test, target_test\n",
    "    \n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "    \n",
    "\n",
    "    def backward(self, d_Ep, learning_rate, Lambda, reg_type):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate, Lambda, reg_type)\n",
    "\n",
    "    def reinitialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights() # does it layer-by-layer         \n",
    " \n",
    "\n",
    "    def train(self, x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size):\n",
    "        train_error_epoch = np.zeros(epochs)\n",
    "        val_error_epoch = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            \n",
    "            # shuffling training data before splitting it into batches.\n",
    "            # done in order to avoid reinforcing neurons in the same way\n",
    "            # in different epochs due to invisible patterns in the data\n",
    "            train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(train_indices)\n",
    "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
    "            target_train = target_train[train_indices]\n",
    "            \n",
    "            \n",
    "            # if batch_size=1 we get the online version, \n",
    "            # else we get mini-batch version with batches of size batch_size\n",
    "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
    "                                                             # even if the number of examples is not divisible \n",
    "                                                             # for batch_size the last, smaller batch is processed anyway\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                target_batch = target_train[i:i+batch_size]\n",
    "\n",
    "                # forward propagation\n",
    "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
    "                # computing loss and gradient\n",
    "                loss = loss_function(target_batch, predictions)\n",
    "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "                epoch_tr_loss += np.sum(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(loss_gradient, learning_rate, Lambda, reg_type)\n",
    "\n",
    "            # validation\n",
    "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
    "            val_loss = loss_function(target_val, val_predictions)\n",
    "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished. \n",
    "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
    "            # Store average errors for the epoch\n",
    "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
    "            val_error_epoch[epoch] = epoch_val_loss\n",
    "\n",
    "        return train_error_epoch, val_error_epoch\n",
    "\n",
    "\n",
    "    def train_val(self, x_train_val, target_train_val, K, epochs, learning_rate, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_train_val.shape[0]\n",
    "        fold_size = num_samples // K\n",
    "\n",
    "        # error storage for averaging\n",
    "        avg_train_error_epoch = np.zeros(epochs)\n",
    "        avg_val_error_epoch = np.zeros(epochs)\n",
    "        \n",
    "        if K==1: # hold-out validation\n",
    "            train_indices = np.arange(0, int(0.75*num_samples)) # training set is 75% of the training & validation set\n",
    "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set diffrence between the first and the second set\n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets           \n",
    "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size\n",
    "            ) # computation of errors via train_val_setup method\n",
    "            return train_error_epoch, val_error_epoch\n",
    "            \n",
    "\n",
    "        for k in range(K):\n",
    "            # creating fold indices\n",
    "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
    "                                                                        # It contains the indices of the examples used in validation set for\n",
    "                                                                        # this fold\n",
    "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples - \n",
    "                                                                              # len(val_indices). It contains the indices of all the examples\n",
    "                                                                              # but the ones used in the validation set for this fold. \n",
    "                                                                              # It corresponds to the training set for the current fold \n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
    "\n",
    "            # re-initializing weights for each fold\n",
    "            self.reinitialize_weights()\n",
    "\n",
    "            # training on the current fold\n",
    "            train_error_epoch, val_error_epoch = self.train(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda, reg_type, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "\n",
    "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
    "            avg_train_error_epoch += train_error_epoch\n",
    "            avg_val_error_epoch += val_error_epoch\n",
    "\n",
    "            print(f\"Fold {k+1} completed.\")\n",
    "\n",
    "        # averaging errors across all folds\n",
    "        avg_train_error_epoch /= K\n",
    "        avg_val_error_epoch /= K\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
    "\n",
    "        return avg_train_error_epoch, avg_val_error_epoch\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp0g8MgT3hhO",
    "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sono entrato in cross val\n",
      "Fold 1 completed.\n",
      "sono entrato in cross val\n",
      "Fold 2 completed.\n",
      "sono entrato in cross val\n",
      "Fold 3 completed.\n",
      "sono entrato in cross val\n",
      "Fold 4 completed.\n",
      "sono entrato in cross val\n",
      "Fold 5 completed.\n",
      "Epoch 1/1000, Avg Training Loss: 0.5624846742404395, Avg Validation Loss: 332.2243035092395\n",
      "Epoch 2/1000, Avg Training Loss: 0.28618572281653476, Avg Validation Loss: 264.6557067624351\n",
      "Epoch 3/1000, Avg Training Loss: 0.2567992777874338, Avg Validation Loss: 260.345102746497\n",
      "Epoch 4/1000, Avg Training Loss: 0.25433200649243054, Avg Validation Loss: 260.3554829036733\n",
      "Epoch 5/1000, Avg Training Loss: 0.2540618915502612, Avg Validation Loss: 260.35828400888346\n",
      "Epoch 6/1000, Avg Training Loss: 0.25395093350743575, Avg Validation Loss: 260.27682545658774\n",
      "Epoch 7/1000, Avg Training Loss: 0.2538542824917676, Avg Validation Loss: 260.17149873751686\n",
      "Epoch 8/1000, Avg Training Loss: 0.2537629323305733, Avg Validation Loss: 260.06370662021965\n",
      "Epoch 9/1000, Avg Training Loss: 0.2536763624094453, Avg Validation Loss: 259.9592101745979\n",
      "Epoch 10/1000, Avg Training Loss: 0.2535943269788739, Avg Validation Loss: 259.85928081951636\n",
      "Epoch 11/1000, Avg Training Loss: 0.25351653090977394, Avg Validation Loss: 259.76399791423177\n",
      "Epoch 12/1000, Avg Training Loss: 0.2534426766516187, Avg Validation Loss: 259.6731454279187\n",
      "Epoch 13/1000, Avg Training Loss: 0.2533724858321969, Avg Validation Loss: 259.58644976095917\n",
      "Epoch 14/1000, Avg Training Loss: 0.25330570102648187, Avg Validation Loss: 259.5036426559785\n",
      "Epoch 15/1000, Avg Training Loss: 0.2532420881158028, Avg Validation Loss: 259.4244730635918\n",
      "Epoch 16/1000, Avg Training Loss: 0.25318143377851127, Avg Validation Loss: 259.3487088118892\n",
      "Epoch 17/1000, Avg Training Loss: 0.2531235423426706, Avg Validation Loss: 259.27613603940017\n",
      "Epoch 18/1000, Avg Training Loss: 0.2530682345418959, Avg Validation Loss: 259.20655857332787\n",
      "Epoch 19/1000, Avg Training Loss: 0.2530153459852839, Avg Validation Loss: 259.13979606533536\n",
      "Epoch 20/1000, Avg Training Loss: 0.25296472625984695, Avg Validation Loss: 259.075681883273\n",
      "Epoch 21/1000, Avg Training Loss: 0.25291623631148596, Avg Validation Loss: 259.0140619645932\n",
      "Epoch 22/1000, Avg Training Loss: 0.2528697494056251, Avg Validation Loss: 258.9547934450028\n",
      "Epoch 23/1000, Avg Training Loss: 0.2528251469921658, Avg Validation Loss: 258.8977450207841\n",
      "Epoch 24/1000, Avg Training Loss: 0.25278231940455775, Avg Validation Loss: 258.84279607651786\n",
      "Epoch 25/1000, Avg Training Loss: 0.2527411659943438, Avg Validation Loss: 258.7898331578131\n",
      "Epoch 26/1000, Avg Training Loss: 0.2527015934227415, Avg Validation Loss: 258.7387509176731\n",
      "Epoch 27/1000, Avg Training Loss: 0.25266351411787197, Avg Validation Loss: 258.6894517887839\n",
      "Epoch 28/1000, Avg Training Loss: 0.25262684746589986, Avg Validation Loss: 258.6418442997915\n",
      "Epoch 29/1000, Avg Training Loss: 0.2525915181232563, Avg Validation Loss: 258.5958433190178\n",
      "Epoch 30/1000, Avg Training Loss: 0.2525574558491403, Avg Validation Loss: 258.5513695199295\n",
      "Epoch 31/1000, Avg Training Loss: 0.25252459520890297, Avg Validation Loss: 258.50834866478135\n",
      "Epoch 32/1000, Avg Training Loss: 0.2524928751625589, Avg Validation Loss: 258.4667112989948\n",
      "Epoch 33/1000, Avg Training Loss: 0.252462238583472, Avg Validation Loss: 258.4263918092615\n",
      "Epoch 34/1000, Avg Training Loss: 0.2524326319921071, Avg Validation Loss: 258.38732856512047\n",
      "Epoch 35/1000, Avg Training Loss: 0.25240400518361467, Avg Validation Loss: 258.34946424175416\n",
      "Epoch 36/1000, Avg Training Loss: 0.2523763112232223, Avg Validation Loss: 258.3127450442547\n",
      "Epoch 37/1000, Avg Training Loss: 0.25234950609732276, Avg Validation Loss: 258.27711979335254\n",
      "Epoch 38/1000, Avg Training Loss: 0.2523235482792428, Avg Validation Loss: 258.2425407157415\n",
      "Epoch 39/1000, Avg Training Loss: 0.25229839921735825, Avg Validation Loss: 258.2089623148746\n",
      "Epoch 40/1000, Avg Training Loss: 0.25227402183493114, Avg Validation Loss: 258.1763424353751\n",
      "Epoch 41/1000, Avg Training Loss: 0.2522503817308916, Avg Validation Loss: 258.14464097162124\n",
      "Epoch 42/1000, Avg Training Loss: 0.2522274465139408, Avg Validation Loss: 258.11381969081634\n",
      "Epoch 43/1000, Avg Training Loss: 0.25220518532266367, Avg Validation Loss: 258.08384285079205\n",
      "Epoch 44/1000, Avg Training Loss: 0.25218356912033907, Avg Validation Loss: 258.05467655535654\n",
      "Epoch 45/1000, Avg Training Loss: 0.25216257066252695, Avg Validation Loss: 258.02628857215836\n",
      "Epoch 46/1000, Avg Training Loss: 0.25214216401296713, Avg Validation Loss: 257.9986485343049\n",
      "Epoch 47/1000, Avg Training Loss: 0.2521223247704996, Avg Validation Loss: 257.9717273850785\n",
      "Epoch 48/1000, Avg Training Loss: 0.25210302963361747, Avg Validation Loss: 257.9454976675471\n",
      "Epoch 49/1000, Avg Training Loss: 0.2520842564950929, Avg Validation Loss: 257.9199334458466\n",
      "Epoch 50/1000, Avg Training Loss: 0.25206598453677337, Avg Validation Loss: 257.8950099014568\n",
      "Epoch 51/1000, Avg Training Loss: 0.2520481939171861, Avg Validation Loss: 257.8707034451566\n",
      "Epoch 52/1000, Avg Training Loss: 0.25203086594592733, Avg Validation Loss: 257.84699147418405\n",
      "Epoch 53/1000, Avg Training Loss: 0.2520139826083243, Avg Validation Loss: 257.8238526624352\n",
      "Epoch 54/1000, Avg Training Loss: 0.2519975268026499, Avg Validation Loss: 257.8012669445929\n",
      "Epoch 55/1000, Avg Training Loss: 0.25198148255545016, Avg Validation Loss: 257.7792149870675\n",
      "Epoch 56/1000, Avg Training Loss: 0.2519658345422366, Avg Validation Loss: 257.7576782878997\n",
      "Epoch 57/1000, Avg Training Loss: 0.25195056809638694, Avg Validation Loss: 257.7366392211287\n",
      "Epoch 58/1000, Avg Training Loss: 0.2519356693426022, Avg Validation Loss: 257.71608084520255\n",
      "Epoch 59/1000, Avg Training Loss: 0.2519211248710794, Avg Validation Loss: 257.695986979478\n",
      "Epoch 60/1000, Avg Training Loss: 0.25190692196775333, Avg Validation Loss: 257.6763422666569\n",
      "Epoch 61/1000, Avg Training Loss: 0.2518930487176396, Avg Validation Loss: 257.6571318073832\n",
      "Epoch 62/1000, Avg Training Loss: 0.25187949351732614, Avg Validation Loss: 257.638341563723\n",
      "Epoch 63/1000, Avg Training Loss: 0.25186624526491236, Avg Validation Loss: 257.61995809342284\n",
      "Epoch 64/1000, Avg Training Loss: 0.2518532934252405, Avg Validation Loss: 257.60196850314145\n",
      "Epoch 65/1000, Avg Training Loss: 0.2518406279910599, Avg Validation Loss: 257.58436032027805\n",
      "Epoch 66/1000, Avg Training Loss: 0.2518282392528334, Avg Validation Loss: 257.56712165247404\n",
      "Epoch 67/1000, Avg Training Loss: 0.25181611798891995, Avg Validation Loss: 257.5502411343372\n",
      "Epoch 68/1000, Avg Training Loss: 0.2518042554340607, Avg Validation Loss: 257.53370780782467\n",
      "Epoch 69/1000, Avg Training Loss: 0.251792643149377, Avg Validation Loss: 257.5175110913174\n",
      "Epoch 70/1000, Avg Training Loss: 0.25178127299526576, Avg Validation Loss: 257.5016408650725\n",
      "Epoch 71/1000, Avg Training Loss: 0.25177013720187635, Avg Validation Loss: 257.4860874618231\n",
      "Epoch 72/1000, Avg Training Loss: 0.2517592283216406, Avg Validation Loss: 257.4708415864924\n",
      "Epoch 73/1000, Avg Training Loss: 0.2517485391438166, Avg Validation Loss: 257.4558943596185\n",
      "Epoch 74/1000, Avg Training Loss: 0.2517380628777483, Avg Validation Loss: 257.4412370654644\n",
      "Epoch 75/1000, Avg Training Loss: 0.25172779289961966, Avg Validation Loss: 257.42686136252837\n",
      "Epoch 76/1000, Avg Training Loss: 0.25171772286031824, Avg Validation Loss: 257.4127592655349\n",
      "Epoch 77/1000, Avg Training Loss: 0.2517078466728206, Avg Validation Loss: 257.39892306434166\n",
      "Epoch 78/1000, Avg Training Loss: 0.2516981584855411, Avg Validation Loss: 257.38534535294616\n",
      "Epoch 79/1000, Avg Training Loss: 0.25168865266207363, Avg Validation Loss: 257.3720190514327\n",
      "Epoch 80/1000, Avg Training Loss: 0.25167932382560104, Avg Validation Loss: 257.35893727109385\n",
      "Epoch 81/1000, Avg Training Loss: 0.2516701667636274, Avg Validation Loss: 257.34609334850484\n",
      "Epoch 82/1000, Avg Training Loss: 0.25166117647716313, Avg Validation Loss: 257.33348089333055\n",
      "Epoch 83/1000, Avg Training Loss: 0.25165234818675075, Avg Validation Loss: 257.32109375845994\n",
      "Epoch 84/1000, Avg Training Loss: 0.25164367727071996, Avg Validation Loss: 257.3089260116032\n",
      "Epoch 85/1000, Avg Training Loss: 0.25163515924837393, Avg Validation Loss: 257.29697185552794\n",
      "Epoch 86/1000, Avg Training Loss: 0.2516267897668966, Avg Validation Loss: 257.28522563697476\n",
      "Epoch 87/1000, Avg Training Loss: 0.2516185646857953, Avg Validation Loss: 257.27368205333676\n",
      "Epoch 88/1000, Avg Training Loss: 0.2516104800298699, Avg Validation Loss: 257.2623359605322\n",
      "Epoch 89/1000, Avg Training Loss: 0.25160253197005605, Avg Validation Loss: 257.2511823910603\n",
      "Epoch 90/1000, Avg Training Loss: 0.25159471682453166, Avg Validation Loss: 257.2402164883645\n",
      "Epoch 91/1000, Avg Training Loss: 0.25158703095982504, Avg Validation Loss: 257.2294335576312\n",
      "Epoch 92/1000, Avg Training Loss: 0.2515794709343389, Avg Validation Loss: 257.21882910112555\n",
      "Epoch 93/1000, Avg Training Loss: 0.2515720333974967, Avg Validation Loss: 257.2083988184598\n",
      "Epoch 94/1000, Avg Training Loss: 0.2515647151124702, Avg Validation Loss: 257.1981384271734\n",
      "Epoch 95/1000, Avg Training Loss: 0.251557512916101, Avg Validation Loss: 257.18804396399776\n",
      "Epoch 96/1000, Avg Training Loss: 0.25155042387882787, Avg Validation Loss: 257.1781114012618\n",
      "Epoch 97/1000, Avg Training Loss: 0.2515434450877863, Avg Validation Loss: 257.1683368695201\n",
      "Epoch 98/1000, Avg Training Loss: 0.2515365736909575, Avg Validation Loss: 257.1587166453365\n",
      "Epoch 99/1000, Avg Training Loss: 0.25152980701111294, Avg Validation Loss: 257.149247011136\n",
      "Epoch 100/1000, Avg Training Loss: 0.25152314240531554, Avg Validation Loss: 257.1399246836485\n",
      "Epoch 101/1000, Avg Training Loss: 0.2515165773612163, Avg Validation Loss: 257.13074617960876\n",
      "Epoch 102/1000, Avg Training Loss: 0.251510109381575, Avg Validation Loss: 257.12170817351455\n",
      "Epoch 103/1000, Avg Training Loss: 0.25150373604989257, Avg Validation Loss: 257.1128075105443\n",
      "Epoch 104/1000, Avg Training Loss: 0.2514974550864728, Avg Validation Loss: 257.1040410446541\n",
      "Epoch 105/1000, Avg Training Loss: 0.251491264251696, Avg Validation Loss: 257.0954058100374\n",
      "Epoch 106/1000, Avg Training Loss: 0.2514851613728135, Avg Validation Loss: 257.086898867854\n",
      "Epoch 107/1000, Avg Training Loss: 0.2514791443464437, Avg Validation Loss: 257.0785174599082\n",
      "Epoch 108/1000, Avg Training Loss: 0.25147321114274046, Avg Validation Loss: 257.0702588995006\n",
      "Epoch 109/1000, Avg Training Loss: 0.2514673598347421, Avg Validation Loss: 257.06212052925537\n",
      "Epoch 110/1000, Avg Training Loss: 0.2514615885282745, Avg Validation Loss: 257.054099628679\n",
      "Epoch 111/1000, Avg Training Loss: 0.25145589534622603, Avg Validation Loss: 257.046193607507\n",
      "Epoch 112/1000, Avg Training Loss: 0.2514502784656953, Avg Validation Loss: 257.0384000774601\n",
      "Epoch 113/1000, Avg Training Loss: 0.2514447361424053, Avg Validation Loss: 257.0307167461725\n",
      "Epoch 114/1000, Avg Training Loss: 0.2514392667253941, Avg Validation Loss: 257.02314134072856\n",
      "Epoch 115/1000, Avg Training Loss: 0.2514338685997244, Avg Validation Loss: 257.01567152207286\n",
      "Epoch 116/1000, Avg Training Loss: 0.2514285401678919, Avg Validation Loss: 257.0083050370239\n",
      "Epoch 117/1000, Avg Training Loss: 0.25142327990675495, Avg Validation Loss: 257.00103971521776\n",
      "Epoch 118/1000, Avg Training Loss: 0.2514180862955983, Avg Validation Loss: 256.9938735277992\n",
      "Epoch 119/1000, Avg Training Loss: 0.2514129578709361, Avg Validation Loss: 256.9868044435731\n",
      "Epoch 120/1000, Avg Training Loss: 0.25140789320631685, Avg Validation Loss: 256.9798305496515\n",
      "Epoch 121/1000, Avg Training Loss: 0.2514028909585216, Avg Validation Loss: 256.9729499144992\n",
      "Epoch 122/1000, Avg Training Loss: 0.251397949818405, Avg Validation Loss: 256.96616061997736\n",
      "Epoch 123/1000, Avg Training Loss: 0.25139306847955695, Avg Validation Loss: 256.9594608519037\n",
      "Epoch 124/1000, Avg Training Loss: 0.25138824569787316, Avg Validation Loss: 256.95284887741286\n",
      "Epoch 125/1000, Avg Training Loss: 0.2513834802455506, Avg Validation Loss: 256.9463229962106\n",
      "Epoch 126/1000, Avg Training Loss: 0.2513787709478214, Avg Validation Loss: 256.93988150448536\n",
      "Epoch 127/1000, Avg Training Loss: 0.2513741166857663, Avg Validation Loss: 256.9335227940586\n",
      "Epoch 128/1000, Avg Training Loss: 0.25136951633707, Avg Validation Loss: 256.9272451845987\n",
      "Epoch 129/1000, Avg Training Loss: 0.251364968810898, Avg Validation Loss: 256.921047148277\n",
      "Epoch 130/1000, Avg Training Loss: 0.25136047303595666, Avg Validation Loss: 256.91492717897034\n",
      "Epoch 131/1000, Avg Training Loss: 0.25135602796495216, Avg Validation Loss: 256.9088837498507\n",
      "Epoch 132/1000, Avg Training Loss: 0.25135163255867327, Avg Validation Loss: 256.90291544301306\n",
      "Epoch 133/1000, Avg Training Loss: 0.25134728587392424, Avg Validation Loss: 256.89702084646024\n",
      "Epoch 134/1000, Avg Training Loss: 0.25134298695158175, Avg Validation Loss: 256.8911985432637\n",
      "Epoch 135/1000, Avg Training Loss: 0.2513387348186426, Avg Validation Loss: 256.88544724428067\n",
      "Epoch 136/1000, Avg Training Loss: 0.2513345285841892, Avg Validation Loss: 256.87976570094804\n",
      "Epoch 137/1000, Avg Training Loss: 0.25133036737245007, Avg Validation Loss: 256.8741526437428\n",
      "Epoch 138/1000, Avg Training Loss: 0.25132625030676214, Avg Validation Loss: 256.86860680060823\n",
      "Epoch 139/1000, Avg Training Loss: 0.2513221765933727, Avg Validation Loss: 256.86312711482685\n",
      "Epoch 140/1000, Avg Training Loss: 0.25131814545028686, Avg Validation Loss: 256.8577123541529\n",
      "Epoch 141/1000, Avg Training Loss: 0.25131415607276, Avg Validation Loss: 256.8523612981919\n",
      "Epoch 142/1000, Avg Training Loss: 0.2513102076800682, Avg Validation Loss: 256.84707279406973\n",
      "Epoch 143/1000, Avg Training Loss: 0.2513062995384595, Avg Validation Loss: 256.84184574904043\n",
      "Epoch 144/1000, Avg Training Loss: 0.25130243090761417, Avg Validation Loss: 256.8366791251659\n",
      "Epoch 145/1000, Avg Training Loss: 0.2512986010881285, Avg Validation Loss: 256.83157194286093\n",
      "Epoch 146/1000, Avg Training Loss: 0.2512948093782207, Avg Validation Loss: 256.826523190467\n",
      "Epoch 147/1000, Avg Training Loss: 0.25129105515105477, Avg Validation Loss: 256.82153182503964\n",
      "Epoch 148/1000, Avg Training Loss: 0.25128733771970774, Avg Validation Loss: 256.8165968244267\n",
      "Epoch 149/1000, Avg Training Loss: 0.25128365642272016, Avg Validation Loss: 256.8117172751705\n",
      "Epoch 150/1000, Avg Training Loss: 0.25128001067008043, Avg Validation Loss: 256.8068921330876\n",
      "Epoch 151/1000, Avg Training Loss: 0.2512763998185092, Avg Validation Loss: 256.8021205156774\n",
      "Epoch 152/1000, Avg Training Loss: 0.2512728232522357, Avg Validation Loss: 256.7974015169428\n",
      "Epoch 153/1000, Avg Training Loss: 0.2512692803732497, Avg Validation Loss: 256.79273422738305\n",
      "Epoch 154/1000, Avg Training Loss: 0.25126577055187965, Avg Validation Loss: 256.7881178916807\n",
      "Epoch 155/1000, Avg Training Loss: 0.2512622932838697, Avg Validation Loss: 256.7835517751083\n",
      "Epoch 156/1000, Avg Training Loss: 0.2512588480663568, Avg Validation Loss: 256.7790349188513\n",
      "Epoch 157/1000, Avg Training Loss: 0.2512554343527219, Avg Validation Loss: 256.7745664622541\n",
      "Epoch 158/1000, Avg Training Loss: 0.2512520516040847, Avg Validation Loss: 256.77014560008337\n",
      "Epoch 159/1000, Avg Training Loss: 0.2512486993406046, Avg Validation Loss: 256.7657716698937\n",
      "Epoch 160/1000, Avg Training Loss: 0.2512453770918817, Avg Validation Loss: 256.76144387580337\n",
      "Epoch 161/1000, Avg Training Loss: 0.25124208436337037, Avg Validation Loss: 256.75716141475414\n",
      "Epoch 162/1000, Avg Training Loss: 0.2512388206448872, Avg Validation Loss: 256.75292351273026\n",
      "Epoch 163/1000, Avg Training Loss: 0.25123558544086394, Avg Validation Loss: 256.7487294076725\n",
      "Epoch 164/1000, Avg Training Loss: 0.2512323782343087, Avg Validation Loss: 256.7445784823533\n",
      "Epoch 165/1000, Avg Training Loss: 0.2512291986050979, Avg Validation Loss: 256.7404701600757\n",
      "Epoch 166/1000, Avg Training Loss: 0.25122604612320043, Avg Validation Loss: 256.7364039680538\n",
      "Epoch 167/1000, Avg Training Loss: 0.25122292044085615, Avg Validation Loss: 256.73237917579075\n",
      "Epoch 168/1000, Avg Training Loss: 0.25121982116177355, Avg Validation Loss: 256.72839491839\n",
      "Epoch 169/1000, Avg Training Loss: 0.2512167478090078, Avg Validation Loss: 256.72445069138473\n",
      "Epoch 170/1000, Avg Training Loss: 0.2512136999841927, Avg Validation Loss: 256.7205458959561\n",
      "Epoch 171/1000, Avg Training Loss: 0.25121067730156754, Avg Validation Loss: 256.71667997517466\n",
      "Epoch 172/1000, Avg Training Loss: 0.25120767941958166, Avg Validation Loss: 256.71285236471\n",
      "Epoch 173/1000, Avg Training Loss: 0.2512047059813745, Avg Validation Loss: 256.7090623210873\n",
      "Epoch 174/1000, Avg Training Loss: 0.2512017564945628, Avg Validation Loss: 256.70530921609407\n",
      "Epoch 175/1000, Avg Training Loss: 0.25119883061034864, Avg Validation Loss: 256.701592585742\n",
      "Epoch 176/1000, Avg Training Loss: 0.2511959279784649, Avg Validation Loss: 256.69791196577967\n",
      "Epoch 177/1000, Avg Training Loss: 0.2511930482425727, Avg Validation Loss: 256.6942668527044\n",
      "Epoch 178/1000, Avg Training Loss: 0.251190191050133, Avg Validation Loss: 256.69065657338723\n",
      "Epoch 179/1000, Avg Training Loss: 0.25118735600503916, Avg Validation Loss: 256.6870805992278\n",
      "Epoch 180/1000, Avg Training Loss: 0.2511845428120128, Avg Validation Loss: 256.68353847251853\n",
      "Epoch 181/1000, Avg Training Loss: 0.25118175117219066, Avg Validation Loss: 256.6800296832266\n",
      "Epoch 182/1000, Avg Training Loss: 0.2511789807527892, Avg Validation Loss: 256.6765537388866\n",
      "Epoch 183/1000, Avg Training Loss: 0.251176231207754, Avg Validation Loss: 256.6731102339487\n",
      "Epoch 184/1000, Avg Training Loss: 0.2511735022030333, Avg Validation Loss: 256.66969883922667\n",
      "Epoch 185/1000, Avg Training Loss: 0.25117079350087745, Avg Validation Loss: 256.66631906111127\n",
      "Epoch 186/1000, Avg Training Loss: 0.25116810491300384, Avg Validation Loss: 256.66297040291033\n",
      "Epoch 187/1000, Avg Training Loss: 0.2511654361347171, Avg Validation Loss: 256.6596522890263\n",
      "Epoch 188/1000, Avg Training Loss: 0.25116278690834676, Avg Validation Loss: 256.65636423243586\n",
      "Epoch 189/1000, Avg Training Loss: 0.25116015690785704, Avg Validation Loss: 256.65310580258557\n",
      "Epoch 190/1000, Avg Training Loss: 0.2511575458771464, Avg Validation Loss: 256.6498765661113\n",
      "Epoch 191/1000, Avg Training Loss: 0.25115495353140277, Avg Validation Loss: 256.6466761574194\n",
      "Epoch 192/1000, Avg Training Loss: 0.25115237961606834, Avg Validation Loss: 256.6435042360956\n",
      "Epoch 193/1000, Avg Training Loss: 0.25114982393301916, Avg Validation Loss: 256.6403604113633\n",
      "Epoch 194/1000, Avg Training Loss: 0.2511472862008383, Avg Validation Loss: 256.6372443836419\n",
      "Epoch 195/1000, Avg Training Loss: 0.2511447662199822, Avg Validation Loss: 256.6341556614734\n",
      "Epoch 196/1000, Avg Training Loss: 0.2511422637586598, Avg Validation Loss: 256.6310938207244\n",
      "Epoch 197/1000, Avg Training Loss: 0.2511397786350504, Avg Validation Loss: 256.6280584601618\n",
      "Epoch 198/1000, Avg Training Loss: 0.2511373105638866, Avg Validation Loss: 256.6250493007084\n",
      "Epoch 199/1000, Avg Training Loss: 0.25113485928849966, Avg Validation Loss: 256.62206610516773\n",
      "Epoch 200/1000, Avg Training Loss: 0.2511324247197106, Avg Validation Loss: 256.6191083806482\n",
      "Epoch 201/1000, Avg Training Loss: 0.25113000661819945, Avg Validation Loss: 256.6161758457641\n",
      "Epoch 202/1000, Avg Training Loss: 0.25112760476362506, Avg Validation Loss: 256.61326809216126\n",
      "Epoch 203/1000, Avg Training Loss: 0.25112521895111, Avg Validation Loss: 256.61038473109977\n",
      "Epoch 204/1000, Avg Training Loss: 0.2511228489789224, Avg Validation Loss: 256.60752547568046\n",
      "Epoch 205/1000, Avg Training Loss: 0.2511204946376836, Avg Validation Loss: 256.604690008133\n",
      "Epoch 206/1000, Avg Training Loss: 0.2511181557084512, Avg Validation Loss: 256.6018780495999\n",
      "Epoch 207/1000, Avg Training Loss: 0.2511158320608106, Avg Validation Loss: 256.59908925456926\n",
      "Epoch 208/1000, Avg Training Loss: 0.25111352350535476, Avg Validation Loss: 256.5963232839607\n",
      "Epoch 209/1000, Avg Training Loss: 0.25111122988468876, Avg Validation Loss: 256.59357979050407\n",
      "Epoch 210/1000, Avg Training Loss: 0.25110895096437696, Avg Validation Loss: 256.59085859811483\n",
      "Epoch 211/1000, Avg Training Loss: 0.25110668660103375, Avg Validation Loss: 256.58815939211155\n",
      "Epoch 212/1000, Avg Training Loss: 0.2511044366263936, Avg Validation Loss: 256.58548191710986\n",
      "Epoch 213/1000, Avg Training Loss: 0.2511022008676229, Avg Validation Loss: 256.58282588415835\n",
      "Epoch 214/1000, Avg Training Loss: 0.25109997919791666, Avg Validation Loss: 256.58019100455067\n",
      "Epoch 215/1000, Avg Training Loss: 0.2510977714176292, Avg Validation Loss: 256.5775768462354\n",
      "Epoch 216/1000, Avg Training Loss: 0.2510955772864293, Avg Validation Loss: 256.57498322197273\n",
      "Epoch 217/1000, Avg Training Loss: 0.2510933966675702, Avg Validation Loss: 256.5724099823026\n",
      "Epoch 218/1000, Avg Training Loss: 0.2510912293972972, Avg Validation Loss: 256.56985697330225\n",
      "Epoch 219/1000, Avg Training Loss: 0.25108907534194586, Avg Validation Loss: 256.56732386251036\n",
      "Epoch 220/1000, Avg Training Loss: 0.25108693435126617, Avg Validation Loss: 256.5648103999408\n",
      "Epoch 221/1000, Avg Training Loss: 0.25108480628070234, Avg Validation Loss: 256.5623163370626\n",
      "Epoch 222/1000, Avg Training Loss: 0.2510826909712258, Avg Validation Loss: 256.5598413628494\n",
      "Epoch 223/1000, Avg Training Loss: 0.2510805882933171, Avg Validation Loss: 256.5573853562363\n",
      "Epoch 224/1000, Avg Training Loss: 0.2510784980939656, Avg Validation Loss: 256.5549481495977\n",
      "Epoch 225/1000, Avg Training Loss: 0.251076420275, Avg Validation Loss: 256.55252942083047\n",
      "Epoch 226/1000, Avg Training Loss: 0.25107435467248357, Avg Validation Loss: 256.55012889637777\n",
      "Epoch 227/1000, Avg Training Loss: 0.25107230117908, Avg Validation Loss: 256.54774628133475\n",
      "Epoch 228/1000, Avg Training Loss: 0.25107025968788665, Avg Validation Loss: 256.5453813484636\n",
      "Epoch 229/1000, Avg Training Loss: 0.2510682300409904, Avg Validation Loss: 256.54303406455205\n",
      "Epoch 230/1000, Avg Training Loss: 0.25106621212711433, Avg Validation Loss: 256.54070411026044\n",
      "Epoch 231/1000, Avg Training Loss: 0.25106420582014105, Avg Validation Loss: 256.5383912511199\n",
      "Epoch 232/1000, Avg Training Loss: 0.25106221093606806, Avg Validation Loss: 256.5360953218981\n",
      "Epoch 233/1000, Avg Training Loss: 0.2510602273333703, Avg Validation Loss: 256.5338161587056\n",
      "Epoch 234/1000, Avg Training Loss: 0.25105825501022416, Avg Validation Loss: 256.53155354791204\n",
      "Epoch 235/1000, Avg Training Loss: 0.25105629382516725, Avg Validation Loss: 256.5293074096435\n",
      "Epoch 236/1000, Avg Training Loss: 0.25105434368417495, Avg Validation Loss: 256.5270773971466\n",
      "Epoch 237/1000, Avg Training Loss: 0.2510524044470199, Avg Validation Loss: 256.5248632541716\n",
      "Epoch 238/1000, Avg Training Loss: 0.2510504759908375, Avg Validation Loss: 256.5226647544997\n",
      "Epoch 239/1000, Avg Training Loss: 0.2510485581849596, Avg Validation Loss: 256.52048175002216\n",
      "Epoch 240/1000, Avg Training Loss: 0.2510466508947154, Avg Validation Loss: 256.51831412641263\n",
      "Epoch 241/1000, Avg Training Loss: 0.25104475406301835, Avg Validation Loss: 256.5161616698742\n",
      "Epoch 242/1000, Avg Training Loss: 0.2510428675433275, Avg Validation Loss: 256.51402415280575\n",
      "Epoch 243/1000, Avg Training Loss: 0.2510409911998742, Avg Validation Loss: 256.5119014023236\n",
      "Epoch 244/1000, Avg Training Loss: 0.25103912494065467, Avg Validation Loss: 256.5097932969959\n",
      "Epoch 245/1000, Avg Training Loss: 0.2510372686500645, Avg Validation Loss: 256.50769960690775\n",
      "Epoch 246/1000, Avg Training Loss: 0.2510354222159243, Avg Validation Loss: 256.5056201515119\n",
      "Epoch 247/1000, Avg Training Loss: 0.25103358550940946, Avg Validation Loss: 256.50355487705565\n",
      "Epoch 248/1000, Avg Training Loss: 0.2510317584702735, Avg Validation Loss: 256.50150362289014\n",
      "Epoch 249/1000, Avg Training Loss: 0.2510299410659681, Avg Validation Loss: 256.4994661208481\n",
      "Epoch 250/1000, Avg Training Loss: 0.2510281331282395, Avg Validation Loss: 256.49744232004275\n",
      "Epoch 251/1000, Avg Training Loss: 0.2510263345440444, Avg Validation Loss: 256.4954320762857\n",
      "Epoch 252/1000, Avg Training Loss: 0.25102454525690926, Avg Validation Loss: 256.49343520089906\n",
      "Epoch 253/1000, Avg Training Loss: 0.2510227651494349, Avg Validation Loss: 256.491451720679\n",
      "Epoch 254/1000, Avg Training Loss: 0.2510209941989089, Avg Validation Loss: 256.4894814924591\n",
      "Epoch 255/1000, Avg Training Loss: 0.25101923234731605, Avg Validation Loss: 256.48752423646016\n",
      "Epoch 256/1000, Avg Training Loss: 0.2510174794422446, Avg Validation Loss: 256.48557978989356\n",
      "Epoch 257/1000, Avg Training Loss: 0.2510157354139952, Avg Validation Loss: 256.4836481240202\n",
      "Epoch 258/1000, Avg Training Loss: 0.25101400018167946, Avg Validation Loss: 256.48172915971503\n",
      "Epoch 259/1000, Avg Training Loss: 0.2510122736873973, Avg Validation Loss: 256.4798226923907\n",
      "Epoch 260/1000, Avg Training Loss: 0.2510105558141924, Avg Validation Loss: 256.47792868856084\n",
      "Epoch 261/1000, Avg Training Loss: 0.25100884655359373, Avg Validation Loss: 256.4760470049373\n",
      "Epoch 262/1000, Avg Training Loss: 0.2510071458298771, Avg Validation Loss: 256.47417744657326\n",
      "Epoch 263/1000, Avg Training Loss: 0.2510054535533391, Avg Validation Loss: 256.4723198558275\n",
      "Epoch 264/1000, Avg Training Loss: 0.2510037696352676, Avg Validation Loss: 256.470474073766\n",
      "Epoch 265/1000, Avg Training Loss: 0.25100209399058815, Avg Validation Loss: 256.46864000229505\n",
      "Epoch 266/1000, Avg Training Loss: 0.25100042656163246, Avg Validation Loss: 256.4668176174911\n",
      "Epoch 267/1000, Avg Training Loss: 0.2509987673149823, Avg Validation Loss: 256.46500676754977\n",
      "Epoch 268/1000, Avg Training Loss: 0.25099711621270565, Avg Validation Loss: 256.4632072111685\n",
      "Epoch 269/1000, Avg Training Loss: 0.2509954730943553, Avg Validation Loss: 256.4614190679699\n",
      "Epoch 270/1000, Avg Training Loss: 0.25099383796500174, Avg Validation Loss: 256.4596421713836\n",
      "Epoch 271/1000, Avg Training Loss: 0.2509922107432525, Avg Validation Loss: 256.4578764013594\n",
      "Epoch 272/1000, Avg Training Loss: 0.25099059138180196, Avg Validation Loss: 256.4561216083699\n",
      "Epoch 273/1000, Avg Training Loss: 0.2509889798037958, Avg Validation Loss: 256.45437767070325\n",
      "Epoch 274/1000, Avg Training Loss: 0.25098737594895015, Avg Validation Loss: 256.4526445356905\n",
      "Epoch 275/1000, Avg Training Loss: 0.250985779791175, Avg Validation Loss: 256.4509220529431\n",
      "Epoch 276/1000, Avg Training Loss: 0.2509841912681966, Avg Validation Loss: 256.4492100019629\n",
      "Epoch 277/1000, Avg Training Loss: 0.2509826102504674, Avg Validation Loss: 256.447508265092\n",
      "Epoch 278/1000, Avg Training Loss: 0.25098103669774596, Avg Validation Loss: 256.4458167260792\n",
      "Epoch 279/1000, Avg Training Loss: 0.2509794705347298, Avg Validation Loss: 256.4441353458544\n",
      "Epoch 280/1000, Avg Training Loss: 0.25097791171895456, Avg Validation Loss: 256.44246395987454\n",
      "Epoch 281/1000, Avg Training Loss: 0.2509763601940414, Avg Validation Loss: 256.44080249202574\n",
      "Epoch 282/1000, Avg Training Loss: 0.2509748158681052, Avg Validation Loss: 256.4391508945267\n",
      "Epoch 283/1000, Avg Training Loss: 0.25097327870642777, Avg Validation Loss: 256.4375090524674\n",
      "Epoch 284/1000, Avg Training Loss: 0.2509717486350191, Avg Validation Loss: 256.43587689677685\n",
      "Epoch 285/1000, Avg Training Loss: 0.2509702256182419, Avg Validation Loss: 256.4342542701283\n",
      "Epoch 286/1000, Avg Training Loss: 0.250968709635173, Avg Validation Loss: 256.43264106834084\n",
      "Epoch 287/1000, Avg Training Loss: 0.2509672006021342, Avg Validation Loss: 256.43103725197875\n",
      "Epoch 288/1000, Avg Training Loss: 0.2509656985068388, Avg Validation Loss: 256.42944268197215\n",
      "Epoch 289/1000, Avg Training Loss: 0.25096420320079593, Avg Validation Loss: 256.42785727142933\n",
      "Epoch 290/1000, Avg Training Loss: 0.25096271467993114, Avg Validation Loss: 256.42628104251696\n",
      "Epoch 291/1000, Avg Training Loss: 0.2509612329594439, Avg Validation Loss: 256.42471392802224\n",
      "Epoch 292/1000, Avg Training Loss: 0.2509597579629996, Avg Validation Loss: 256.4231558056968\n",
      "Epoch 293/1000, Avg Training Loss: 0.2509582896877274, Avg Validation Loss: 256.4216065038982\n",
      "Epoch 294/1000, Avg Training Loss: 0.2509568279559716, Avg Validation Loss: 256.42006606769894\n",
      "Epoch 295/1000, Avg Training Loss: 0.2509553727686952, Avg Validation Loss: 256.4185344884065\n",
      "Epoch 296/1000, Avg Training Loss: 0.25095392408004674, Avg Validation Loss: 256.41701171632366\n",
      "Epoch 297/1000, Avg Training Loss: 0.2509524818565748, Avg Validation Loss: 256.415497605623\n",
      "Epoch 298/1000, Avg Training Loss: 0.2509510460586872, Avg Validation Loss: 256.4139920294489\n",
      "Epoch 299/1000, Avg Training Loss: 0.2509496166447519, Avg Validation Loss: 256.41249491341125\n",
      "Epoch 300/1000, Avg Training Loss: 0.2509481935527307, Avg Validation Loss: 256.4110061867067\n",
      "Epoch 301/1000, Avg Training Loss: 0.25094677679331134, Avg Validation Loss: 256.40952573599566\n",
      "Epoch 302/1000, Avg Training Loss: 0.25094536629856695, Avg Validation Loss: 256.4080535139757\n",
      "Epoch 303/1000, Avg Training Loss: 0.25094396198223196, Avg Validation Loss: 256.4065894609838\n",
      "Epoch 304/1000, Avg Training Loss: 0.25094256379999025, Avg Validation Loss: 256.40513349253945\n",
      "Epoch 305/1000, Avg Training Loss: 0.25094117172082975, Avg Validation Loss: 256.4036854968134\n",
      "Epoch 306/1000, Avg Training Loss: 0.25093978569334413, Avg Validation Loss: 256.4022453622233\n",
      "Epoch 307/1000, Avg Training Loss: 0.25093840566734715, Avg Validation Loss: 256.40081296489177\n",
      "Epoch 308/1000, Avg Training Loss: 0.2509370315742917, Avg Validation Loss: 256.39938822550215\n",
      "Epoch 309/1000, Avg Training Loss: 0.25093566337772316, Avg Validation Loss: 256.3979712064888\n",
      "Epoch 310/1000, Avg Training Loss: 0.2509343010706119, Avg Validation Loss: 256.3965618529463\n",
      "Epoch 311/1000, Avg Training Loss: 0.25093294462655324, Avg Validation Loss: 256.3951600553254\n",
      "Epoch 312/1000, Avg Training Loss: 0.2509315940251497, Avg Validation Loss: 256.3937657897701\n",
      "Epoch 313/1000, Avg Training Loss: 0.250930249250008, Avg Validation Loss: 256.392379057139\n",
      "Epoch 314/1000, Avg Training Loss: 0.25092891020029856, Avg Validation Loss: 256.3909997399736\n",
      "Epoch 315/1000, Avg Training Loss: 0.2509275768181938, Avg Validation Loss: 256.3896277232372\n",
      "Epoch 316/1000, Avg Training Loss: 0.2509262490946534, Avg Validation Loss: 256.38826295570186\n",
      "Epoch 317/1000, Avg Training Loss: 0.2509249269402642, Avg Validation Loss: 256.38690543059977\n",
      "Epoch 318/1000, Avg Training Loss: 0.25092361034673333, Avg Validation Loss: 256.38555505945754\n",
      "Epoch 319/1000, Avg Training Loss: 0.2509222992646593, Avg Validation Loss: 256.3842117893711\n",
      "Epoch 320/1000, Avg Training Loss: 0.2509209936129344, Avg Validation Loss: 256.38287564644804\n",
      "Epoch 321/1000, Avg Training Loss: 0.25091969350733084, Avg Validation Loss: 256.38154645859504\n",
      "Epoch 322/1000, Avg Training Loss: 0.2509183988527997, Avg Validation Loss: 256.3802242510601\n",
      "Epoch 323/1000, Avg Training Loss: 0.2509171095865802, Avg Validation Loss: 256.3789091339398\n",
      "Epoch 324/1000, Avg Training Loss: 0.25091582570885895, Avg Validation Loss: 256.37760097597055\n",
      "Epoch 325/1000, Avg Training Loss: 0.2509145471930468, Avg Validation Loss: 256.37629975833556\n",
      "Epoch 326/1000, Avg Training Loss: 0.2509132740151494, Avg Validation Loss: 256.3750053122587\n",
      "Epoch 327/1000, Avg Training Loss: 0.2509120061324568, Avg Validation Loss: 256.3737176634258\n",
      "Epoch 328/1000, Avg Training Loss: 0.25091074353734416, Avg Validation Loss: 256.3724367688879\n",
      "Epoch 329/1000, Avg Training Loss: 0.2509094862112572, Avg Validation Loss: 256.3711625072027\n",
      "Epoch 330/1000, Avg Training Loss: 0.25090823408265955, Avg Validation Loss: 256.36989492242947\n",
      "Epoch 331/1000, Avg Training Loss: 0.2509069872059171, Avg Validation Loss: 256.3686338419881\n",
      "Epoch 332/1000, Avg Training Loss: 0.25090574549228256, Avg Validation Loss: 256.36737911573306\n",
      "Epoch 333/1000, Avg Training Loss: 0.2509045088832449, Avg Validation Loss: 256.3661307775026\n",
      "Epoch 334/1000, Avg Training Loss: 0.2509032774175313, Avg Validation Loss: 256.36488880373287\n",
      "Epoch 335/1000, Avg Training Loss: 0.250902051044402, Avg Validation Loss: 256.363653110286\n",
      "Epoch 336/1000, Avg Training Loss: 0.25090082972695327, Avg Validation Loss: 256.36242370822436\n",
      "Epoch 337/1000, Avg Training Loss: 0.25089961346811, Avg Validation Loss: 256.36120042463375\n",
      "Epoch 338/1000, Avg Training Loss: 0.25089840220787485, Avg Validation Loss: 256.3599832852688\n",
      "Epoch 339/1000, Avg Training Loss: 0.25089719587256604, Avg Validation Loss: 256.35877220583626\n",
      "Epoch 340/1000, Avg Training Loss: 0.2508959944317657, Avg Validation Loss: 256.3575671423004\n",
      "Epoch 341/1000, Avg Training Loss: 0.25089479786940083, Avg Validation Loss: 256.35636807967614\n",
      "Epoch 342/1000, Avg Training Loss: 0.2508936060992865, Avg Validation Loss: 256.35517498377516\n",
      "Epoch 343/1000, Avg Training Loss: 0.2508924191409099, Avg Validation Loss: 256.3539877965028\n",
      "Epoch 344/1000, Avg Training Loss: 0.2508912369531683, Avg Validation Loss: 256.35280656438556\n",
      "Epoch 345/1000, Avg Training Loss: 0.25089005953323124, Avg Validation Loss: 256.35163122235247\n",
      "Epoch 346/1000, Avg Training Loss: 0.2508888868726451, Avg Validation Loss: 256.35046166532976\n",
      "Epoch 347/1000, Avg Training Loss: 0.2508877189282925, Avg Validation Loss: 256.3492978396766\n",
      "Epoch 348/1000, Avg Training Loss: 0.2508865556480998, Avg Validation Loss: 256.34813963588647\n",
      "Epoch 349/1000, Avg Training Loss: 0.25088539695269146, Avg Validation Loss: 256.3469870678048\n",
      "Epoch 350/1000, Avg Training Loss: 0.2508842428000842, Avg Validation Loss: 256.3458401746952\n",
      "Epoch 351/1000, Avg Training Loss: 0.25088309315656215, Avg Validation Loss: 256.3446988293624\n",
      "Epoch 352/1000, Avg Training Loss: 0.2508819480629567, Avg Validation Loss: 256.3435630594028\n",
      "Epoch 353/1000, Avg Training Loss: 0.250880807496968, Avg Validation Loss: 256.34243282322313\n",
      "Epoch 354/1000, Avg Training Loss: 0.2508796714128204, Avg Validation Loss: 256.34130812774094\n",
      "Epoch 355/1000, Avg Training Loss: 0.25087853986261976, Avg Validation Loss: 256.3401889081562\n",
      "Epoch 356/1000, Avg Training Loss: 0.2508774127849455, Avg Validation Loss: 256.33907509290134\n",
      "Epoch 357/1000, Avg Training Loss: 0.25087629015084084, Avg Validation Loss: 256.3379666134369\n",
      "Epoch 358/1000, Avg Training Loss: 0.25087517194663483, Avg Validation Loss: 256.33686343086623\n",
      "Epoch 359/1000, Avg Training Loss: 0.25087405814589064, Avg Validation Loss: 256.33576552579785\n",
      "Epoch 360/1000, Avg Training Loss: 0.25087294871812227, Avg Validation Loss: 256.33467284040836\n",
      "Epoch 361/1000, Avg Training Loss: 0.2508718436420413, Avg Validation Loss: 256.33358525748145\n",
      "Epoch 362/1000, Avg Training Loss: 0.2508707428951871, Avg Validation Loss: 256.3325027805218\n",
      "Epoch 363/1000, Avg Training Loss: 0.250869646434847, Avg Validation Loss: 256.33142541410353\n",
      "Epoch 364/1000, Avg Training Loss: 0.250868554226504, Avg Validation Loss: 256.3303531994572\n",
      "Epoch 365/1000, Avg Training Loss: 0.2508674662896192, Avg Validation Loss: 256.3292861279398\n",
      "Epoch 366/1000, Avg Training Loss: 0.25086638262076927, Avg Validation Loss: 256.32822399131953\n",
      "Epoch 367/1000, Avg Training Loss: 0.250865303155311, Avg Validation Loss: 256.3271667976554\n",
      "Epoch 368/1000, Avg Training Loss: 0.2508642278548554, Avg Validation Loss: 256.3261144929304\n",
      "Epoch 369/1000, Avg Training Loss: 0.2508631567156304, Avg Validation Loss: 256.3250669814935\n",
      "Epoch 370/1000, Avg Training Loss: 0.2508620896731049, Avg Validation Loss: 256.3240243589539\n",
      "Epoch 371/1000, Avg Training Loss: 0.25086102674744015, Avg Validation Loss: 256.32298649292517\n",
      "Epoch 372/1000, Avg Training Loss: 0.25085996787820164, Avg Validation Loss: 256.32195339441\n",
      "Epoch 373/1000, Avg Training Loss: 0.2508589130317762, Avg Validation Loss: 256.3209250747782\n",
      "Epoch 374/1000, Avg Training Loss: 0.2508578622188072, Avg Validation Loss: 256.31990156169104\n",
      "Epoch 375/1000, Avg Training Loss: 0.25085681539533566, Avg Validation Loss: 256.31888284394887\n",
      "Epoch 376/1000, Avg Training Loss: 0.2508557725758806, Avg Validation Loss: 256.31786880816185\n",
      "Epoch 377/1000, Avg Training Loss: 0.25085473376742257, Avg Validation Loss: 256.3168593315879\n",
      "Epoch 378/1000, Avg Training Loss: 0.2508536988947689, Avg Validation Loss: 256.31585447877444\n",
      "Epoch 379/1000, Avg Training Loss: 0.2508526679467622, Avg Validation Loss: 256.31485423567625\n",
      "Epoch 380/1000, Avg Training Loss: 0.2508516409142616, Avg Validation Loss: 256.313858605163\n",
      "Epoch 381/1000, Avg Training Loss: 0.250850617778609, Avg Validation Loss: 256.31286754450247\n",
      "Epoch 382/1000, Avg Training Loss: 0.2508495984943548, Avg Validation Loss: 256.31188100694266\n",
      "Epoch 383/1000, Avg Training Loss: 0.250848583035737, Avg Validation Loss: 256.3108989396787\n",
      "Epoch 384/1000, Avg Training Loss: 0.2508475714079686, Avg Validation Loss: 256.30992127352914\n",
      "Epoch 385/1000, Avg Training Loss: 0.25084656354171775, Avg Validation Loss: 256.30894803251505\n",
      "Epoch 386/1000, Avg Training Loss: 0.2508455594290685, Avg Validation Loss: 256.3079791884268\n",
      "Epoch 387/1000, Avg Training Loss: 0.2508445590408666, Avg Validation Loss: 256.30701469928124\n",
      "Epoch 388/1000, Avg Training Loss: 0.2508435623775008, Avg Validation Loss: 256.3060545765776\n",
      "Epoch 389/1000, Avg Training Loss: 0.2508425694147678, Avg Validation Loss: 256.3050988528388\n",
      "Epoch 390/1000, Avg Training Loss: 0.2508415801462128, Avg Validation Loss: 256.3041475166639\n",
      "Epoch 391/1000, Avg Training Loss: 0.25084059458156904, Avg Validation Loss: 256.3032004663092\n",
      "Epoch 392/1000, Avg Training Loss: 0.25083961266102917, Avg Validation Loss: 256.3022578059678\n",
      "Epoch 393/1000, Avg Training Loss: 0.2508386344022454, Avg Validation Loss: 256.3013194267981\n",
      "Epoch 394/1000, Avg Training Loss: 0.25083765968589344, Avg Validation Loss: 256.3003853502304\n",
      "Epoch 395/1000, Avg Training Loss: 0.25083668862790753, Avg Validation Loss: 256.2994555292285\n",
      "Epoch 396/1000, Avg Training Loss: 0.2508357212153728, Avg Validation Loss: 256.29852984064325\n",
      "Epoch 397/1000, Avg Training Loss: 0.25083475731848975, Avg Validation Loss: 256.297608280993\n",
      "Epoch 398/1000, Avg Training Loss: 0.2508337969842443, Avg Validation Loss: 256.2966908406329\n",
      "Epoch 399/1000, Avg Training Loss: 0.25083284018980906, Avg Validation Loss: 256.2957774240175\n",
      "Epoch 400/1000, Avg Training Loss: 0.25083188695352204, Avg Validation Loss: 256.29486803138985\n",
      "Epoch 401/1000, Avg Training Loss: 0.25083093726951633, Avg Validation Loss: 256.2939625377569\n",
      "Epoch 402/1000, Avg Training Loss: 0.2508299910559897, Avg Validation Loss: 256.29306106048125\n",
      "Epoch 403/1000, Avg Training Loss: 0.2508290483152542, Avg Validation Loss: 256.2921635930827\n",
      "Epoch 404/1000, Avg Training Loss: 0.25082810904691305, Avg Validation Loss: 256.29127007929094\n",
      "Epoch 405/1000, Avg Training Loss: 0.2508271732136539, Avg Validation Loss: 256.29038047298354\n",
      "Epoch 406/1000, Avg Training Loss: 0.250826240763378, Avg Validation Loss: 256.28949478125\n",
      "Epoch 407/1000, Avg Training Loss: 0.25082531169907685, Avg Validation Loss: 256.2886129720338\n",
      "Epoch 408/1000, Avg Training Loss: 0.25082438598850776, Avg Validation Loss: 256.28773504689514\n",
      "Epoch 409/1000, Avg Training Loss: 0.25082346366056196, Avg Validation Loss: 256.2868609315679\n",
      "Epoch 410/1000, Avg Training Loss: 0.25082254467296405, Avg Validation Loss: 256.28599054503377\n",
      "Epoch 411/1000, Avg Training Loss: 0.25082162893148013, Avg Validation Loss: 256.2851240153134\n",
      "Epoch 412/1000, Avg Training Loss: 0.25082071645229886, Avg Validation Loss: 256.28426141683957\n",
      "Epoch 413/1000, Avg Training Loss: 0.2508198073094597, Avg Validation Loss: 256.28340264681\n",
      "Epoch 414/1000, Avg Training Loss: 0.2508189014391503, Avg Validation Loss: 256.2825477216645\n",
      "Epoch 415/1000, Avg Training Loss: 0.2508179988737798, Avg Validation Loss: 256.28169656752704\n",
      "Epoch 416/1000, Avg Training Loss: 0.25081709956124576, Avg Validation Loss: 256.2808491482866\n",
      "Epoch 417/1000, Avg Training Loss: 0.2508162034601786, Avg Validation Loss: 256.28000537036434\n",
      "Epoch 418/1000, Avg Training Loss: 0.25081531057366296, Avg Validation Loss: 256.2791651440526\n",
      "Epoch 419/1000, Avg Training Loss: 0.2508144208827515, Avg Validation Loss: 256.2783285089963\n",
      "Epoch 420/1000, Avg Training Loss: 0.25081353438919574, Avg Validation Loss: 256.2774954322844\n",
      "Epoch 421/1000, Avg Training Loss: 0.25081265108844697, Avg Validation Loss: 256.2766657778121\n",
      "Epoch 422/1000, Avg Training Loss: 0.25081177094113327, Avg Validation Loss: 256.27583960674906\n",
      "Epoch 423/1000, Avg Training Loss: 0.25081089391415673, Avg Validation Loss: 256.27501698429313\n",
      "Epoch 424/1000, Avg Training Loss: 0.2508100200001548, Avg Validation Loss: 256.2741977703711\n",
      "Epoch 425/1000, Avg Training Loss: 0.25080914917048197, Avg Validation Loss: 256.2733820587755\n",
      "Epoch 426/1000, Avg Training Loss: 0.2508082814592383, Avg Validation Loss: 256.27256990504793\n",
      "Epoch 427/1000, Avg Training Loss: 0.25080741686234503, Avg Validation Loss: 256.27176124444213\n",
      "Epoch 428/1000, Avg Training Loss: 0.25080655536604735, Avg Validation Loss: 256.2709560500428\n",
      "Epoch 429/1000, Avg Training Loss: 0.2508056969774783, Avg Validation Loss: 256.2701542785352\n",
      "Epoch 430/1000, Avg Training Loss: 0.2508048416356884, Avg Validation Loss: 256.269355942052\n",
      "Epoch 431/1000, Avg Training Loss: 0.2508039893604533, Avg Validation Loss: 256.2685609951116\n",
      "Epoch 432/1000, Avg Training Loss: 0.25080314013216504, Avg Validation Loss: 256.26776933863863\n",
      "Epoch 433/1000, Avg Training Loss: 0.2508022938890155, Avg Validation Loss: 256.2669811357451\n",
      "Epoch 434/1000, Avg Training Loss: 0.25080145062991926, Avg Validation Loss: 256.26619647997154\n",
      "Epoch 435/1000, Avg Training Loss: 0.2508006104350848, Avg Validation Loss: 256.26541522373066\n",
      "Epoch 436/1000, Avg Training Loss: 0.2507997732701332, Avg Validation Loss: 256.26463734758266\n",
      "Epoch 437/1000, Avg Training Loss: 0.2507989391033984, Avg Validation Loss: 256.2638627859066\n",
      "Epoch 438/1000, Avg Training Loss: 0.25079810792014484, Avg Validation Loss: 256.2630915000461\n",
      "Epoch 439/1000, Avg Training Loss: 0.25079727969418886, Avg Validation Loss: 256.2623234796819\n",
      "Epoch 440/1000, Avg Training Loss: 0.25079645441916654, Avg Validation Loss: 256.26155864576106\n",
      "Epoch 441/1000, Avg Training Loss: 0.2507956320664467, Avg Validation Loss: 256.2607970462408\n",
      "Epoch 442/1000, Avg Training Loss: 0.2507948125815732, Avg Validation Loss: 256.26003865518726\n",
      "Epoch 443/1000, Avg Training Loss: 0.2507939959892571, Avg Validation Loss: 256.2592833610355\n",
      "Epoch 444/1000, Avg Training Loss: 0.2507931822718226, Avg Validation Loss: 256.25853128079655\n",
      "Epoch 445/1000, Avg Training Loss: 0.25079237145905764, Avg Validation Loss: 256.25778240120377\n",
      "Epoch 446/1000, Avg Training Loss: 0.2507915635083409, Avg Validation Loss: 256.257036689524\n",
      "Epoch 447/1000, Avg Training Loss: 0.25079075847432275, Avg Validation Loss: 256.2562941708057\n",
      "Epoch 448/1000, Avg Training Loss: 0.250789956303936, Avg Validation Loss: 256.25555473401255\n",
      "Epoch 449/1000, Avg Training Loss: 0.2507891569579527, Avg Validation Loss: 256.2548184204949\n",
      "Epoch 450/1000, Avg Training Loss: 0.25078836043215336, Avg Validation Loss: 256.25408519463826\n",
      "Epoch 451/1000, Avg Training Loss: 0.25078756673613933, Avg Validation Loss: 256.25335502515384\n",
      "Epoch 452/1000, Avg Training Loss: 0.2507867758703516, Avg Validation Loss: 256.2526278261921\n",
      "Epoch 453/1000, Avg Training Loss: 0.25078598778301175, Avg Validation Loss: 256.2519036553393\n",
      "Epoch 454/1000, Avg Training Loss: 0.25078520241917246, Avg Validation Loss: 256.25118256528316\n",
      "Epoch 455/1000, Avg Training Loss: 0.2507844198508023, Avg Validation Loss: 256.2504644020738\n",
      "Epoch 456/1000, Avg Training Loss: 0.25078363998538244, Avg Validation Loss: 256.2497492220531\n",
      "Epoch 457/1000, Avg Training Loss: 0.25078286278864476, Avg Validation Loss: 256.2490371126306\n",
      "Epoch 458/1000, Avg Training Loss: 0.2507820883056086, Avg Validation Loss: 256.248328077976\n",
      "Epoch 459/1000, Avg Training Loss: 0.2507813165419117, Avg Validation Loss: 256.2476222359012\n",
      "Epoch 460/1000, Avg Training Loss: 0.2507805475121419, Avg Validation Loss: 256.2469196490015\n",
      "Epoch 461/1000, Avg Training Loss: 0.2507797812727879, Avg Validation Loss: 256.2462201102244\n",
      "Epoch 462/1000, Avg Training Loss: 0.2507790177491963, Avg Validation Loss: 256.24552349963096\n",
      "Epoch 463/1000, Avg Training Loss: 0.250778256924624, Avg Validation Loss: 256.24482974639074\n",
      "Epoch 464/1000, Avg Training Loss: 0.2507774987851703, Avg Validation Loss: 256.2441388387339\n",
      "Epoch 465/1000, Avg Training Loss: 0.25077674333624767, Avg Validation Loss: 256.2434507428822\n",
      "Epoch 466/1000, Avg Training Loss: 0.25077599057061867, Avg Validation Loss: 256.2427655345313\n",
      "Epoch 467/1000, Avg Training Loss: 0.2507752404928162, Avg Validation Loss: 256.2420832141416\n",
      "Epoch 468/1000, Avg Training Loss: 0.2507744930891206, Avg Validation Loss: 256.2414035928772\n",
      "Epoch 469/1000, Avg Training Loss: 0.25077374828485377, Avg Validation Loss: 256.240726689692\n",
      "Epoch 470/1000, Avg Training Loss: 0.25077300605855846, Avg Validation Loss: 256.24005251941935\n",
      "Epoch 471/1000, Avg Training Loss: 0.2507722664449329, Avg Validation Loss: 256.2393810321147\n",
      "Epoch 472/1000, Avg Training Loss: 0.2507715293807492, Avg Validation Loss: 256.2387123812139\n",
      "Epoch 473/1000, Avg Training Loss: 0.2507707948907963, Avg Validation Loss: 256.2380465619577\n",
      "Epoch 474/1000, Avg Training Loss: 0.25077006300433924, Avg Validation Loss: 256.23738342075643\n",
      "Epoch 475/1000, Avg Training Loss: 0.2507693336105921, Avg Validation Loss: 256.2367230978881\n",
      "Epoch 476/1000, Avg Training Loss: 0.2507686067578718, Avg Validation Loss: 256.2360654978737\n",
      "Epoch 477/1000, Avg Training Loss: 0.250767882449899, Avg Validation Loss: 256.2354105245994\n",
      "Epoch 478/1000, Avg Training Loss: 0.2507671606293033, Avg Validation Loss: 256.2347582932297\n",
      "Epoch 479/1000, Avg Training Loss: 0.2507664413154497, Avg Validation Loss: 256.2341087953235\n",
      "Epoch 480/1000, Avg Training Loss: 0.2507657245154251, Avg Validation Loss: 256.23346200628856\n",
      "Epoch 481/1000, Avg Training Loss: 0.25076501022543024, Avg Validation Loss: 256.2328178603633\n",
      "Epoch 482/1000, Avg Training Loss: 0.25076429839472475, Avg Validation Loss: 256.23217634440596\n",
      "Epoch 483/1000, Avg Training Loss: 0.2507635890109312, Avg Validation Loss: 256.2315375064586\n",
      "Epoch 484/1000, Avg Training Loss: 0.2507628820775946, Avg Validation Loss: 256.2309012548671\n",
      "Epoch 485/1000, Avg Training Loss: 0.25076217756553454, Avg Validation Loss: 256.2302676502255\n",
      "Epoch 486/1000, Avg Training Loss: 0.2507614754936532, Avg Validation Loss: 256.22963680639685\n",
      "Epoch 487/1000, Avg Training Loss: 0.2507607758353775, Avg Validation Loss: 256.22900868299575\n",
      "Epoch 488/1000, Avg Training Loss: 0.25076007861026334, Avg Validation Loss: 256.22838323541214\n",
      "Epoch 489/1000, Avg Training Loss: 0.2507593837830106, Avg Validation Loss: 256.22776038572664\n",
      "Epoch 490/1000, Avg Training Loss: 0.2507586913323393, Avg Validation Loss: 256.2271400802256\n",
      "Epoch 491/1000, Avg Training Loss: 0.25075800120840086, Avg Validation Loss: 256.22652230144695\n",
      "Epoch 492/1000, Avg Training Loss: 0.25075731338775986, Avg Validation Loss: 256.2259070219841\n",
      "Epoch 493/1000, Avg Training Loss: 0.2507566278661024, Avg Validation Loss: 256.22529430635063\n",
      "Epoch 494/1000, Avg Training Loss: 0.2507559446430625, Avg Validation Loss: 256.2246841314845\n",
      "Epoch 495/1000, Avg Training Loss: 0.2507552637318134, Avg Validation Loss: 256.2240764510999\n",
      "Epoch 496/1000, Avg Training Loss: 0.25075458506696224, Avg Validation Loss: 256.2234713384382\n",
      "Epoch 497/1000, Avg Training Loss: 0.25075390865698727, Avg Validation Loss: 256.2228688243175\n",
      "Epoch 498/1000, Avg Training Loss: 0.2507532344939858, Avg Validation Loss: 256.2222689335921\n",
      "Epoch 499/1000, Avg Training Loss: 0.25075256259540885, Avg Validation Loss: 256.22167169986733\n",
      "Epoch 500/1000, Avg Training Loss: 0.25075189297652234, Avg Validation Loss: 256.2210770359738\n",
      "Epoch 501/1000, Avg Training Loss: 0.250751225612256, Avg Validation Loss: 256.22048489127513\n",
      "Epoch 502/1000, Avg Training Loss: 0.2507505604406972, Avg Validation Loss: 256.21989529199016\n",
      "Epoch 503/1000, Avg Training Loss: 0.2507498975110119, Avg Validation Loss: 256.2193082757897\n",
      "Epoch 504/1000, Avg Training Loss: 0.25074923682679234, Avg Validation Loss: 256.2187237546016\n",
      "Epoch 505/1000, Avg Training Loss: 0.2507485783210012, Avg Validation Loss: 256.21814179930664\n",
      "Epoch 506/1000, Avg Training Loss: 0.2507479220338294, Avg Validation Loss: 256.2175623841777\n",
      "Epoch 507/1000, Avg Training Loss: 0.2507472679277445, Avg Validation Loss: 256.2169855154066\n",
      "Epoch 508/1000, Avg Training Loss: 0.25074661598399, Avg Validation Loss: 256.21641123185253\n",
      "Epoch 509/1000, Avg Training Loss: 0.25074596623552947, Avg Validation Loss: 256.2158394961996\n",
      "Epoch 510/1000, Avg Training Loss: 0.2507453186527657, Avg Validation Loss: 256.21527022992746\n",
      "Epoch 511/1000, Avg Training Loss: 0.25074467318934895, Avg Validation Loss: 256.21470349931934\n",
      "Epoch 512/1000, Avg Training Loss: 0.250744029806358, Avg Validation Loss: 256.2141393878003\n",
      "Epoch 513/1000, Avg Training Loss: 0.2507433885740004, Avg Validation Loss: 256.21357779979564\n",
      "Epoch 514/1000, Avg Training Loss: 0.2507427495070861, Avg Validation Loss: 256.2130186282396\n",
      "Epoch 515/1000, Avg Training Loss: 0.2507421125203739, Avg Validation Loss: 256.2124619846805\n",
      "Epoch 516/1000, Avg Training Loss: 0.2507414775938473, Avg Validation Loss: 256.21190791450783\n",
      "Epoch 517/1000, Avg Training Loss: 0.25074084480013825, Avg Validation Loss: 256.2113562945608\n",
      "Epoch 518/1000, Avg Training Loss: 0.2507402140875539, Avg Validation Loss: 256.210807106758\n",
      "Epoch 519/1000, Avg Training Loss: 0.250739585444422, Avg Validation Loss: 256.21026033073247\n",
      "Epoch 520/1000, Avg Training Loss: 0.2507389588796437, Avg Validation Loss: 256.20971592970386\n",
      "Epoch 521/1000, Avg Training Loss: 0.25073833441614984, Avg Validation Loss: 256.2091737782523\n",
      "Epoch 522/1000, Avg Training Loss: 0.2507377119933942, Avg Validation Loss: 256.2086339233075\n",
      "Epoch 523/1000, Avg Training Loss: 0.2507370915581517, Avg Validation Loss: 256.2080964895994\n",
      "Epoch 524/1000, Avg Training Loss: 0.25073647316345016, Avg Validation Loss: 256.2075613376658\n",
      "Epoch 525/1000, Avg Training Loss: 0.25073585673842824, Avg Validation Loss: 256.2070284927398\n",
      "Epoch 526/1000, Avg Training Loss: 0.2507352422884278, Avg Validation Loss: 256.2064980058957\n",
      "Epoch 527/1000, Avg Training Loss: 0.25073462984763983, Avg Validation Loss: 256.2059698521605\n",
      "Epoch 528/1000, Avg Training Loss: 0.25073401942451806, Avg Validation Loss: 256.2054439140385\n",
      "Epoch 529/1000, Avg Training Loss: 0.2507334109731929, Avg Validation Loss: 256.20492021756854\n",
      "Epoch 530/1000, Avg Training Loss: 0.25073280447295093, Avg Validation Loss: 256.20439885086836\n",
      "Epoch 531/1000, Avg Training Loss: 0.2507321999284311, Avg Validation Loss: 256.2038798066285\n",
      "Epoch 532/1000, Avg Training Loss: 0.2507315973750191, Avg Validation Loss: 256.20336297091063\n",
      "Epoch 533/1000, Avg Training Loss: 0.25073099675929045, Avg Validation Loss: 256.20284830284305\n",
      "Epoch 534/1000, Avg Training Loss: 0.25073039805516373, Avg Validation Loss: 256.2023359033527\n",
      "Epoch 535/1000, Avg Training Loss: 0.25072980126111516, Avg Validation Loss: 256.20182579537214\n",
      "Epoch 536/1000, Avg Training Loss: 0.2507292063915127, Avg Validation Loss: 256.201318063739\n",
      "Epoch 537/1000, Avg Training Loss: 0.2507286134080663, Avg Validation Loss: 256.20081272773496\n",
      "Epoch 538/1000, Avg Training Loss: 0.25072802234979363, Avg Validation Loss: 256.20030964900684\n",
      "Epoch 539/1000, Avg Training Loss: 0.25072743321842317, Avg Validation Loss: 256.19980879046165\n",
      "Epoch 540/1000, Avg Training Loss: 0.2507268459775765, Avg Validation Loss: 256.1993102451934\n",
      "Epoch 541/1000, Avg Training Loss: 0.2507262606611183, Avg Validation Loss: 256.1988139710123\n",
      "Epoch 542/1000, Avg Training Loss: 0.25072567725787853, Avg Validation Loss: 256.19831992059085\n",
      "Epoch 543/1000, Avg Training Loss: 0.2507250957308982, Avg Validation Loss: 256.1978280912967\n",
      "Epoch 544/1000, Avg Training Loss: 0.25072451607492674, Avg Validation Loss: 256.19733839388255\n",
      "Epoch 545/1000, Avg Training Loss: 0.25072393830078965, Avg Validation Loss: 256.1968508365438\n",
      "Epoch 546/1000, Avg Training Loss: 0.25072336235937415, Avg Validation Loss: 256.19636541390025\n",
      "Epoch 547/1000, Avg Training Loss: 0.2507227882557726, Avg Validation Loss: 256.19588206879143\n",
      "Epoch 548/1000, Avg Training Loss: 0.25072221601237227, Avg Validation Loss: 256.19540071743324\n",
      "Epoch 549/1000, Avg Training Loss: 0.25072164556946375, Avg Validation Loss: 256.19492138998686\n",
      "Epoch 550/1000, Avg Training Loss: 0.25072107690608963, Avg Validation Loss: 256.1944441754979\n",
      "Epoch 551/1000, Avg Training Loss: 0.25072051002908996, Avg Validation Loss: 256.193969029895\n",
      "Epoch 552/1000, Avg Training Loss: 0.2507199449033359, Avg Validation Loss: 256.1934960216448\n",
      "Epoch 553/1000, Avg Training Loss: 0.25071938156165674, Avg Validation Loss: 256.19302508725434\n",
      "Epoch 554/1000, Avg Training Loss: 0.2507188199690752, Avg Validation Loss: 256.1925561986103\n",
      "Epoch 555/1000, Avg Training Loss: 0.2507182601191079, Avg Validation Loss: 256.19208939987607\n",
      "Epoch 556/1000, Avg Training Loss: 0.2507177020104471, Avg Validation Loss: 256.1916246902847\n",
      "Epoch 557/1000, Avg Training Loss: 0.25071714559778063, Avg Validation Loss: 256.1911621895711\n",
      "Epoch 558/1000, Avg Training Loss: 0.2507165908844179, Avg Validation Loss: 256.1907019201839\n",
      "Epoch 559/1000, Avg Training Loss: 0.2507160378863473, Avg Validation Loss: 256.1902438247557\n",
      "Epoch 560/1000, Avg Training Loss: 0.25071548665222443, Avg Validation Loss: 256.1897877831128\n",
      "Epoch 561/1000, Avg Training Loss: 0.2507149371189904, Avg Validation Loss: 256.18933376140706\n",
      "Epoch 562/1000, Avg Training Loss: 0.25071438926347256, Avg Validation Loss: 256.1888817431435\n",
      "Epoch 563/1000, Avg Training Loss: 0.2507138430588314, Avg Validation Loss: 256.1884317383967\n",
      "Epoch 564/1000, Avg Training Loss: 0.2507132984902571, Avg Validation Loss: 256.18798379643636\n",
      "Epoch 565/1000, Avg Training Loss: 0.25071275557604966, Avg Validation Loss: 256.18753789324404\n",
      "Epoch 566/1000, Avg Training Loss: 0.25071221433595364, Avg Validation Loss: 256.1870939726549\n",
      "Epoch 567/1000, Avg Training Loss: 0.2507116747169791, Avg Validation Loss: 256.1866521228378\n",
      "Epoch 568/1000, Avg Training Loss: 0.2507111367029459, Avg Validation Loss: 256.1862124395768\n",
      "Epoch 569/1000, Avg Training Loss: 0.2507106003298502, Avg Validation Loss: 256.1857748534301\n",
      "Epoch 570/1000, Avg Training Loss: 0.2507100656002395, Avg Validation Loss: 256.18533931470216\n",
      "Epoch 571/1000, Avg Training Loss: 0.250709532496666, Avg Validation Loss: 256.1849057636817\n",
      "Epoch 572/1000, Avg Training Loss: 0.2507090009958937, Avg Validation Loss: 256.18447421870496\n",
      "Epoch 573/1000, Avg Training Loss: 0.25070847113004663, Avg Validation Loss: 256.18404461127153\n",
      "Epoch 574/1000, Avg Training Loss: 0.2507079428193853, Avg Validation Loss: 256.1836169774547\n",
      "Epoch 575/1000, Avg Training Loss: 0.2507074160616478, Avg Validation Loss: 256.18319137331525\n",
      "Epoch 576/1000, Avg Training Loss: 0.2507068909058944, Avg Validation Loss: 256.18276770789805\n",
      "Epoch 577/1000, Avg Training Loss: 0.2507063673320512, Avg Validation Loss: 256.1823460172694\n",
      "Epoch 578/1000, Avg Training Loss: 0.2507058453410383, Avg Validation Loss: 256.18192625930425\n",
      "Epoch 579/1000, Avg Training Loss: 0.250705324892182, Avg Validation Loss: 256.1815085190193\n",
      "Epoch 580/1000, Avg Training Loss: 0.2507048059990062, Avg Validation Loss: 256.18109279368866\n",
      "Epoch 581/1000, Avg Training Loss: 0.2507042886898226, Avg Validation Loss: 256.18067897680226\n",
      "Epoch 582/1000, Avg Training Loss: 0.2507037729325055, Avg Validation Loss: 256.1802670396438\n",
      "Epoch 583/1000, Avg Training Loss: 0.2507032586848821, Avg Validation Loss: 256.1798570702524\n",
      "Epoch 584/1000, Avg Training Loss: 0.25070274595655984, Avg Validation Loss: 256.1794491177733\n",
      "Epoch 585/1000, Avg Training Loss: 0.2507022347565467, Avg Validation Loss: 256.1790430702281\n",
      "Epoch 586/1000, Avg Training Loss: 0.2507017250442515, Avg Validation Loss: 256.1786389390901\n",
      "Epoch 587/1000, Avg Training Loss: 0.250701216834063, Avg Validation Loss: 256.17823671754536\n",
      "Epoch 588/1000, Avg Training Loss: 0.25070071014437, Avg Validation Loss: 256.1778362490404\n",
      "Epoch 589/1000, Avg Training Loss: 0.2507002048933577, Avg Validation Loss: 256.17743761954705\n",
      "Epoch 590/1000, Avg Training Loss: 0.25069970108844786, Avg Validation Loss: 256.17704087272114\n",
      "Epoch 591/1000, Avg Training Loss: 0.25069919873274754, Avg Validation Loss: 256.17664603052515\n",
      "Epoch 592/1000, Avg Training Loss: 0.25069869785669263, Avg Validation Loss: 256.1762530189251\n",
      "Epoch 593/1000, Avg Training Loss: 0.2506981983913303, Avg Validation Loss: 256.1758618487099\n",
      "Epoch 594/1000, Avg Training Loss: 0.25069770036012007, Avg Validation Loss: 256.1754725419445\n",
      "Epoch 595/1000, Avg Training Loss: 0.25069720376388654, Avg Validation Loss: 256.1750850751264\n",
      "Epoch 596/1000, Avg Training Loss: 0.2506967085699172, Avg Validation Loss: 256.1746994768105\n",
      "Epoch 597/1000, Avg Training Loss: 0.25069621478920506, Avg Validation Loss: 256.1743157185317\n",
      "Epoch 598/1000, Avg Training Loss: 0.25069572240542803, Avg Validation Loss: 256.17393379604766\n",
      "Epoch 599/1000, Avg Training Loss: 0.25069523140435845, Avg Validation Loss: 256.17355364540435\n",
      "Epoch 600/1000, Avg Training Loss: 0.25069474176440354, Avg Validation Loss: 256.1731753288797\n",
      "Epoch 601/1000, Avg Training Loss: 0.2506942534766607, Avg Validation Loss: 256.1727989023923\n",
      "Epoch 602/1000, Avg Training Loss: 0.25069376656702647, Avg Validation Loss: 256.1724242868522\n",
      "Epoch 603/1000, Avg Training Loss: 0.25069328101040655, Avg Validation Loss: 256.172051480157\n",
      "Epoch 604/1000, Avg Training Loss: 0.2506927968046257, Avg Validation Loss: 256.17168048378335\n",
      "Epoch 605/1000, Avg Training Loss: 0.25069231396238356, Avg Validation Loss: 256.17131131521626\n",
      "Epoch 606/1000, Avg Training Loss: 0.25069183246360716, Avg Validation Loss: 256.17094399347786\n",
      "Epoch 607/1000, Avg Training Loss: 0.2506913523139904, Avg Validation Loss: 256.1705785053108\n",
      "Epoch 608/1000, Avg Training Loss: 0.25069087352403224, Avg Validation Loss: 256.1702147514993\n",
      "Epoch 609/1000, Avg Training Loss: 0.25069039605921833, Avg Validation Loss: 256.1698527096819\n",
      "Epoch 610/1000, Avg Training Loss: 0.25068991992261763, Avg Validation Loss: 256.1694922945908\n",
      "Epoch 611/1000, Avg Training Loss: 0.25068944504176044, Avg Validation Loss: 256.1691336186507\n",
      "Epoch 612/1000, Avg Training Loss: 0.2506889714476673, Avg Validation Loss: 256.1687766997984\n",
      "Epoch 613/1000, Avg Training Loss: 0.2506884991104022, Avg Validation Loss: 256.16842159408293\n",
      "Epoch 614/1000, Avg Training Loss: 0.25068802805529183, Avg Validation Loss: 256.168068262577\n",
      "Epoch 615/1000, Avg Training Loss: 0.25068755827600775, Avg Validation Loss: 256.1677166700941\n",
      "Epoch 616/1000, Avg Training Loss: 0.25068708977752396, Avg Validation Loss: 256.1673667332844\n",
      "Epoch 617/1000, Avg Training Loss: 0.2506866225028435, Avg Validation Loss: 256.16701857131625\n",
      "Epoch 618/1000, Avg Training Loss: 0.25068615646402426, Avg Validation Loss: 256.16667222557595\n",
      "Epoch 619/1000, Avg Training Loss: 0.25068569164798016, Avg Validation Loss: 256.1663277020648\n",
      "Epoch 620/1000, Avg Training Loss: 0.2506852280619467, Avg Validation Loss: 256.1659849536719\n",
      "Epoch 621/1000, Avg Training Loss: 0.250684765686453, Avg Validation Loss: 256.1656439563309\n",
      "Epoch 622/1000, Avg Training Loss: 0.2506843045096216, Avg Validation Loss: 256.165304801208\n",
      "Epoch 623/1000, Avg Training Loss: 0.2506838445503548, Avg Validation Loss: 256.16496748178594\n",
      "Epoch 624/1000, Avg Training Loss: 0.25068338579380883, Avg Validation Loss: 256.16463200421305\n",
      "Epoch 625/1000, Avg Training Loss: 0.2506829282783156, Avg Validation Loss: 256.16429832892595\n",
      "Epoch 626/1000, Avg Training Loss: 0.2506824719567591, Avg Validation Loss: 256.1639664113078\n",
      "Epoch 627/1000, Avg Training Loss: 0.25068201681211777, Avg Validation Loss: 256.1636362695996\n",
      "Epoch 628/1000, Avg Training Loss: 0.2506815628444301, Avg Validation Loss: 256.1633079381379\n",
      "Epoch 629/1000, Avg Training Loss: 0.25068111008095567, Avg Validation Loss: 256.1629814017643\n",
      "Epoch 630/1000, Avg Training Loss: 0.2506806584905015, Avg Validation Loss: 256.1626566202193\n",
      "Epoch 631/1000, Avg Training Loss: 0.2506802080681802, Avg Validation Loss: 256.16233362620346\n",
      "Epoch 632/1000, Avg Training Loss: 0.2506797587939429, Avg Validation Loss: 256.16201244748703\n",
      "Epoch 633/1000, Avg Training Loss: 0.25067931070221494, Avg Validation Loss: 256.16169303874176\n",
      "Epoch 634/1000, Avg Training Loss: 0.25067886379077137, Avg Validation Loss: 256.1613753430269\n",
      "Epoch 635/1000, Avg Training Loss: 0.2506784180392125, Avg Validation Loss: 256.1610593427223\n",
      "Epoch 636/1000, Avg Training Loss: 0.2506779734262516, Avg Validation Loss: 256.16074505086885\n",
      "Epoch 637/1000, Avg Training Loss: 0.25067752997083786, Avg Validation Loss: 256.1604324266258\n",
      "Epoch 638/1000, Avg Training Loss: 0.2506770876525788, Avg Validation Loss: 256.1601214431254\n",
      "Epoch 639/1000, Avg Training Loss: 0.2506766464632581, Avg Validation Loss: 256.15981212832105\n",
      "Epoch 640/1000, Avg Training Loss: 0.25067620640701743, Avg Validation Loss: 256.1595044483056\n",
      "Epoch 641/1000, Avg Training Loss: 0.2506757674775249, Avg Validation Loss: 256.159198411118\n",
      "Epoch 642/1000, Avg Training Loss: 0.2506753296647712, Avg Validation Loss: 256.1588940497368\n",
      "Epoch 643/1000, Avg Training Loss: 0.25067489297036394, Avg Validation Loss: 256.1585912862464\n",
      "Epoch 644/1000, Avg Training Loss: 0.25067445736303795, Avg Validation Loss: 256.158290178236\n",
      "Epoch 645/1000, Avg Training Loss: 0.2506740228343717, Avg Validation Loss: 256.1579907408894\n",
      "Epoch 646/1000, Avg Training Loss: 0.2506735894102428, Avg Validation Loss: 256.1576930370031\n",
      "Epoch 647/1000, Avg Training Loss: 0.2506731570963553, Avg Validation Loss: 256.1573969999125\n",
      "Epoch 648/1000, Avg Training Loss: 0.2506727258582066, Avg Validation Loss: 256.15710258509944\n",
      "Epoch 649/1000, Avg Training Loss: 0.2506722956783598, Avg Validation Loss: 256.15680981315893\n",
      "Epoch 650/1000, Avg Training Loss: 0.25067186658482576, Avg Validation Loss: 256.1565186468677\n",
      "Epoch 651/1000, Avg Training Loss: 0.25067143860374724, Avg Validation Loss: 256.15622904953904\n",
      "Epoch 652/1000, Avg Training Loss: 0.2506710116762257, Avg Validation Loss: 256.15594107029375\n",
      "Epoch 653/1000, Avg Training Loss: 0.2506705858245387, Avg Validation Loss: 256.1556546849815\n",
      "Epoch 654/1000, Avg Training Loss: 0.2506701610344955, Avg Validation Loss: 256.15536988739456\n",
      "Epoch 655/1000, Avg Training Loss: 0.25066973730413794, Avg Validation Loss: 256.1550867040749\n",
      "Epoch 656/1000, Avg Training Loss: 0.25066931461878655, Avg Validation Loss: 256.15480513362957\n",
      "Epoch 657/1000, Avg Training Loss: 0.2506688929839993, Avg Validation Loss: 256.15452512370973\n",
      "Epoch 658/1000, Avg Training Loss: 0.2506684723900892, Avg Validation Loss: 256.15424667338704\n",
      "Epoch 659/1000, Avg Training Loss: 0.2506680528282918, Avg Validation Loss: 256.1539698594037\n",
      "Epoch 660/1000, Avg Training Loss: 0.2506676343041978, Avg Validation Loss: 256.1536946300771\n",
      "Epoch 661/1000, Avg Training Loss: 0.250667216784575, Avg Validation Loss: 256.1534210050475\n",
      "Epoch 662/1000, Avg Training Loss: 0.25066680027969307, Avg Validation Loss: 256.1531489095587\n",
      "Epoch 663/1000, Avg Training Loss: 0.2506663847657562, Avg Validation Loss: 256.1528783942404\n",
      "Epoch 664/1000, Avg Training Loss: 0.25066597023470805, Avg Validation Loss: 256.1526095410975\n",
      "Epoch 665/1000, Avg Training Loss: 0.25066555670753365, Avg Validation Loss: 256.15234232381397\n",
      "Epoch 666/1000, Avg Training Loss: 0.2506651442022113, Avg Validation Loss: 256.1520766185315\n",
      "Epoch 667/1000, Avg Training Loss: 0.2506647326891013, Avg Validation Loss: 256.15181250826254\n",
      "Epoch 668/1000, Avg Training Loss: 0.25066432214580253, Avg Validation Loss: 256.1515500241271\n",
      "Epoch 669/1000, Avg Training Loss: 0.2506639126145744, Avg Validation Loss: 256.15128917713406\n",
      "Epoch 670/1000, Avg Training Loss: 0.2506635040770117, Avg Validation Loss: 256.15102989923923\n",
      "Epoch 671/1000, Avg Training Loss: 0.2506630965209238, Avg Validation Loss: 256.150772132326\n",
      "Epoch 672/1000, Avg Training Loss: 0.25066268993645563, Avg Validation Loss: 256.1505158130656\n",
      "Epoch 673/1000, Avg Training Loss: 0.25066228429781556, Avg Validation Loss: 256.15026099467866\n",
      "Epoch 674/1000, Avg Training Loss: 0.2506618795909764, Avg Validation Loss: 256.1500077878386\n",
      "Epoch 675/1000, Avg Training Loss: 0.2506614758283569, Avg Validation Loss: 256.14975610907857\n",
      "Epoch 676/1000, Avg Training Loss: 0.2506610730089005, Avg Validation Loss: 256.1495059471514\n",
      "Epoch 677/1000, Avg Training Loss: 0.2506606711295229, Avg Validation Loss: 256.1492572835398\n",
      "Epoch 678/1000, Avg Training Loss: 0.25066027017362824, Avg Validation Loss: 256.1490101404504\n",
      "Epoch 679/1000, Avg Training Loss: 0.2506598701403587, Avg Validation Loss: 256.1487645095771\n",
      "Epoch 680/1000, Avg Training Loss: 0.25065947100712177, Avg Validation Loss: 256.1485203307765\n",
      "Epoch 681/1000, Avg Training Loss: 0.2506590727422911, Avg Validation Loss: 256.14827761278696\n",
      "Epoch 682/1000, Avg Training Loss: 0.250658675371355, Avg Validation Loss: 256.1480363859611\n",
      "Epoch 683/1000, Avg Training Loss: 0.25065827887640707, Avg Validation Loss: 256.1477966515061\n",
      "Epoch 684/1000, Avg Training Loss: 0.25065788328573285, Avg Validation Loss: 256.14755841946453\n",
      "Epoch 685/1000, Avg Training Loss: 0.2506574885514803, Avg Validation Loss: 256.14732176243984\n",
      "Epoch 686/1000, Avg Training Loss: 0.250657094713319, Avg Validation Loss: 256.1470866324304\n",
      "Epoch 687/1000, Avg Training Loss: 0.2506567017830191, Avg Validation Loss: 256.14685296079966\n",
      "Epoch 688/1000, Avg Training Loss: 0.25065630975024417, Avg Validation Loss: 256.1466208069701\n",
      "Epoch 689/1000, Avg Training Loss: 0.25065591860071207, Avg Validation Loss: 256.1463902019906\n",
      "Epoch 690/1000, Avg Training Loss: 0.2506555283195372, Avg Validation Loss: 256.1461611771364\n",
      "Epoch 691/1000, Avg Training Loss: 0.25065513892268027, Avg Validation Loss: 256.1459336228263\n",
      "Epoch 692/1000, Avg Training Loss: 0.2506547504329905, Avg Validation Loss: 256.1457075082069\n",
      "Epoch 693/1000, Avg Training Loss: 0.2506543628206225, Avg Validation Loss: 256.1454828425857\n",
      "Epoch 694/1000, Avg Training Loss: 0.25065397608077655, Avg Validation Loss: 256.1452596778132\n",
      "Epoch 695/1000, Avg Training Loss: 0.250653590172714, Avg Validation Loss: 256.14503815370153\n",
      "Epoch 696/1000, Avg Training Loss: 0.25065320518083956, Avg Validation Loss: 256.1448180754454\n",
      "Epoch 697/1000, Avg Training Loss: 0.2506528210791966, Avg Validation Loss: 256.14459939389616\n",
      "Epoch 698/1000, Avg Training Loss: 0.25065243780775837, Avg Validation Loss: 256.14438216631464\n",
      "Epoch 699/1000, Avg Training Loss: 0.25065205539171675, Avg Validation Loss: 256.1441663786999\n",
      "Epoch 700/1000, Avg Training Loss: 0.25065167382842773, Avg Validation Loss: 256.1439519393886\n",
      "Epoch 701/1000, Avg Training Loss: 0.25065129303248923, Avg Validation Loss: 256.14373905952766\n",
      "Epoch 702/1000, Avg Training Loss: 0.25065091311134596, Avg Validation Loss: 256.1435275704329\n",
      "Epoch 703/1000, Avg Training Loss: 0.25065053399426335, Avg Validation Loss: 256.14331748825134\n",
      "Epoch 704/1000, Avg Training Loss: 0.2506501557041253, Avg Validation Loss: 256.1431088557646\n",
      "Epoch 705/1000, Avg Training Loss: 0.25064977824859636, Avg Validation Loss: 256.1429016611378\n",
      "Epoch 706/1000, Avg Training Loss: 0.25064940163678373, Avg Validation Loss: 256.142695874442\n",
      "Epoch 707/1000, Avg Training Loss: 0.2506490258603654, Avg Validation Loss: 256.1424914864165\n",
      "Epoch 708/1000, Avg Training Loss: 0.25064865090032484, Avg Validation Loss: 256.1422885174513\n",
      "Epoch 709/1000, Avg Training Loss: 0.2506482767421579, Avg Validation Loss: 256.14208704786256\n",
      "Epoch 710/1000, Avg Training Loss: 0.2506479034142358, Avg Validation Loss: 256.1418870356292\n",
      "Epoch 711/1000, Avg Training Loss: 0.25064753088271025, Avg Validation Loss: 256.14168848284174\n",
      "Epoch 712/1000, Avg Training Loss: 0.25064715917843805, Avg Validation Loss: 256.14149128324465\n",
      "Epoch 713/1000, Avg Training Loss: 0.25064678828759895, Avg Validation Loss: 256.1412954904366\n",
      "Epoch 714/1000, Avg Training Loss: 0.25064641818452904, Avg Validation Loss: 256.1411011050573\n",
      "Epoch 715/1000, Avg Training Loss: 0.25064604887932385, Avg Validation Loss: 256.14090816018006\n",
      "Epoch 716/1000, Avg Training Loss: 0.2506456803765452, Avg Validation Loss: 256.14071662814877\n",
      "Epoch 717/1000, Avg Training Loss: 0.25064531268939755, Avg Validation Loss: 256.1405264238223\n",
      "Epoch 718/1000, Avg Training Loss: 0.2506449457994411, Avg Validation Loss: 256.1403375558236\n",
      "Epoch 719/1000, Avg Training Loss: 0.250644579724744, Avg Validation Loss: 256.1401499553382\n",
      "Epoch 720/1000, Avg Training Loss: 0.25064421442730267, Avg Validation Loss: 256.1399636435068\n",
      "Epoch 721/1000, Avg Training Loss: 0.25064384991664374, Avg Validation Loss: 256.13977854794683\n",
      "Epoch 722/1000, Avg Training Loss: 0.2506434861753036, Avg Validation Loss: 256.1395946464068\n",
      "Epoch 723/1000, Avg Training Loss: 0.2506431231804059, Avg Validation Loss: 256.1394119792872\n",
      "Epoch 724/1000, Avg Training Loss: 0.25064276093392, Avg Validation Loss: 256.13923058391254\n",
      "Epoch 725/1000, Avg Training Loss: 0.25064239944180566, Avg Validation Loss: 256.1390504093258\n",
      "Epoch 726/1000, Avg Training Loss: 0.25064203871139423, Avg Validation Loss: 256.1388714778332\n",
      "Epoch 727/1000, Avg Training Loss: 0.2506416787297015, Avg Validation Loss: 256.13869377506563\n",
      "Epoch 728/1000, Avg Training Loss: 0.25064131949084184, Avg Validation Loss: 256.1385172862651\n",
      "Epoch 729/1000, Avg Training Loss: 0.25064096097706906, Avg Validation Loss: 256.13834204014745\n",
      "Epoch 730/1000, Avg Training Loss: 0.2506406031949352, Avg Validation Loss: 256.13816805400023\n",
      "Epoch 731/1000, Avg Training Loss: 0.25064024616467595, Avg Validation Loss: 256.13799526857827\n",
      "Epoch 732/1000, Avg Training Loss: 0.2506398898596716, Avg Validation Loss: 256.1378236951572\n",
      "Epoch 733/1000, Avg Training Loss: 0.2506395342584924, Avg Validation Loss: 256.1376534054954\n",
      "Epoch 734/1000, Avg Training Loss: 0.2506391793928258, Avg Validation Loss: 256.1374843771238\n",
      "Epoch 735/1000, Avg Training Loss: 0.25063882525241515, Avg Validation Loss: 256.13731659979294\n",
      "Epoch 736/1000, Avg Training Loss: 0.250638471824375, Avg Validation Loss: 256.1371500721506\n",
      "Epoch 737/1000, Avg Training Loss: 0.2506381191087479, Avg Validation Loss: 256.13698476265864\n",
      "Epoch 738/1000, Avg Training Loss: 0.2506377671277305, Avg Validation Loss: 256.13682059309326\n",
      "Epoch 739/1000, Avg Training Loss: 0.2506374158188408, Avg Validation Loss: 256.13665762792897\n",
      "Epoch 740/1000, Avg Training Loss: 0.25063706521872825, Avg Validation Loss: 256.13649583043656\n",
      "Epoch 741/1000, Avg Training Loss: 0.2506367153339741, Avg Validation Loss: 256.1363352085866\n",
      "Epoch 742/1000, Avg Training Loss: 0.2506363661240772, Avg Validation Loss: 256.1361757646246\n",
      "Epoch 743/1000, Avg Training Loss: 0.25063601760704207, Avg Validation Loss: 256.13601744969367\n",
      "Epoch 744/1000, Avg Training Loss: 0.25063566976131757, Avg Validation Loss: 256.13586030156705\n",
      "Epoch 745/1000, Avg Training Loss: 0.2506353225831907, Avg Validation Loss: 256.13570432782956\n",
      "Epoch 746/1000, Avg Training Loss: 0.2506349760523849, Avg Validation Loss: 256.13554957620795\n",
      "Epoch 747/1000, Avg Training Loss: 0.2506346301842758, Avg Validation Loss: 256.13539605437495\n",
      "Epoch 748/1000, Avg Training Loss: 0.25063428497417417, Avg Validation Loss: 256.13524372485\n",
      "Epoch 749/1000, Avg Training Loss: 0.250633940420649, Avg Validation Loss: 256.1350925877749\n",
      "Epoch 750/1000, Avg Training Loss: 0.2506335965317794, Avg Validation Loss: 256.13494261670303\n",
      "Epoch 751/1000, Avg Training Loss: 0.25063325328139313, Avg Validation Loss: 256.1347937853999\n",
      "Epoch 752/1000, Avg Training Loss: 0.25063291066635596, Avg Validation Loss: 256.13464612184436\n",
      "Epoch 753/1000, Avg Training Loss: 0.250632568702523, Avg Validation Loss: 256.1344996395527\n",
      "Epoch 754/1000, Avg Training Loss: 0.2506322273772413, Avg Validation Loss: 256.1343543989938\n",
      "Epoch 755/1000, Avg Training Loss: 0.2506318866528811, Avg Validation Loss: 256.1342104482068\n",
      "Epoch 756/1000, Avg Training Loss: 0.25063154659470843, Avg Validation Loss: 256.1340676972093\n",
      "Epoch 757/1000, Avg Training Loss: 0.25063120719496623, Avg Validation Loss: 256.13392611160117\n",
      "Epoch 758/1000, Avg Training Loss: 0.25063086843178906, Avg Validation Loss: 256.1337856284987\n",
      "Epoch 759/1000, Avg Training Loss: 0.25063053029306154, Avg Validation Loss: 256.1336462990486\n",
      "Epoch 760/1000, Avg Training Loss: 0.2506301927860331, Avg Validation Loss: 256.1335081015342\n",
      "Epoch 761/1000, Avg Training Loss: 0.25062985589589076, Avg Validation Loss: 256.1333710589922\n",
      "Epoch 762/1000, Avg Training Loss: 0.2506295196234286, Avg Validation Loss: 256.13323516785357\n",
      "Epoch 763/1000, Avg Training Loss: 0.2506291839685321, Avg Validation Loss: 256.1331003482637\n",
      "Epoch 764/1000, Avg Training Loss: 0.25062884890964043, Avg Validation Loss: 256.13296660945633\n",
      "Epoch 765/1000, Avg Training Loss: 0.2506285144495036, Avg Validation Loss: 256.132833996778\n",
      "Epoch 766/1000, Avg Training Loss: 0.25062818059373754, Avg Validation Loss: 256.13270250799116\n",
      "Epoch 767/1000, Avg Training Loss: 0.25062784735331683, Avg Validation Loss: 256.1325721610169\n",
      "Epoch 768/1000, Avg Training Loss: 0.250627514720893, Avg Validation Loss: 256.13244291453503\n",
      "Epoch 769/1000, Avg Training Loss: 0.2506271826878718, Avg Validation Loss: 256.13231478249907\n",
      "Epoch 770/1000, Avg Training Loss: 0.25062685124801426, Avg Validation Loss: 256.1321877652706\n",
      "Epoch 771/1000, Avg Training Loss: 0.2506265204114399, Avg Validation Loss: 256.13206181429996\n",
      "Epoch 772/1000, Avg Training Loss: 0.250626190165362, Avg Validation Loss: 256.1319368843823\n",
      "Epoch 773/1000, Avg Training Loss: 0.2506258604734444, Avg Validation Loss: 256.1318129931235\n",
      "Epoch 774/1000, Avg Training Loss: 0.25062553132798465, Avg Validation Loss: 256.1316901787776\n",
      "Epoch 775/1000, Avg Training Loss: 0.2506252027517005, Avg Validation Loss: 256.13156843354614\n",
      "Epoch 776/1000, Avg Training Loss: 0.2506248747101421, Avg Validation Loss: 256.13144779492916\n",
      "Epoch 777/1000, Avg Training Loss: 0.250624547243434, Avg Validation Loss: 256.13132821209376\n",
      "Epoch 778/1000, Avg Training Loss: 0.2506242203448909, Avg Validation Loss: 256.13120967626634\n",
      "Epoch 779/1000, Avg Training Loss: 0.25062389400463353, Avg Validation Loss: 256.13109214361356\n",
      "Epoch 780/1000, Avg Training Loss: 0.2506235682158343, Avg Validation Loss: 256.13097563477277\n",
      "Epoch 781/1000, Avg Training Loss: 0.2506232429630055, Avg Validation Loss: 256.1308602031852\n",
      "Epoch 782/1000, Avg Training Loss: 0.25062291825065147, Avg Validation Loss: 256.13074585847073\n",
      "Epoch 783/1000, Avg Training Loss: 0.25062259407891746, Avg Validation Loss: 256.1306326119573\n",
      "Epoch 784/1000, Avg Training Loss: 0.25062227044943036, Avg Validation Loss: 256.13052049621785\n",
      "Epoch 785/1000, Avg Training Loss: 0.25062194735021825, Avg Validation Loss: 256.1304094689623\n",
      "Epoch 786/1000, Avg Training Loss: 0.2506216247922286, Avg Validation Loss: 256.1302995040402\n",
      "Epoch 787/1000, Avg Training Loss: 0.2506213027550609, Avg Validation Loss: 256.1301906732729\n",
      "Epoch 788/1000, Avg Training Loss: 0.2506209812414463, Avg Validation Loss: 256.1300829968629\n",
      "Epoch 789/1000, Avg Training Loss: 0.2506206602881074, Avg Validation Loss: 256.1299763981108\n",
      "Epoch 790/1000, Avg Training Loss: 0.2506203398849854, Avg Validation Loss: 256.1298709005937\n",
      "Epoch 791/1000, Avg Training Loss: 0.25062002004345657, Avg Validation Loss: 256.1297663974176\n",
      "Epoch 792/1000, Avg Training Loss: 0.25061970077013535, Avg Validation Loss: 256.12966279454224\n",
      "Epoch 793/1000, Avg Training Loss: 0.2506193820258264, Avg Validation Loss: 256.1295601632994\n",
      "Epoch 794/1000, Avg Training Loss: 0.25061906381771526, Avg Validation Loss: 256.12945851607793\n",
      "Epoch 795/1000, Avg Training Loss: 0.2506187461229803, Avg Validation Loss: 256.12935791624136\n",
      "Epoch 796/1000, Avg Training Loss: 0.250618428988239, Avg Validation Loss: 256.1292583288092\n",
      "Epoch 797/1000, Avg Training Loss: 0.2506181123879286, Avg Validation Loss: 256.12915975922135\n",
      "Epoch 798/1000, Avg Training Loss: 0.25061779629641445, Avg Validation Loss: 256.12906225386973\n",
      "Epoch 799/1000, Avg Training Loss: 0.25061748077152823, Avg Validation Loss: 256.12896573500024\n",
      "Epoch 800/1000, Avg Training Loss: 0.25061716576666004, Avg Validation Loss: 256.12887018045285\n",
      "Epoch 801/1000, Avg Training Loss: 0.2506168512982578, Avg Validation Loss: 256.1287755498799\n",
      "Epoch 802/1000, Avg Training Loss: 0.2506165373348591, Avg Validation Loss: 256.12868187670927\n",
      "Epoch 803/1000, Avg Training Loss: 0.2506162238671513, Avg Validation Loss: 256.12858919517066\n",
      "Epoch 804/1000, Avg Training Loss: 0.2506159109030714, Avg Validation Loss: 256.1284975137535\n",
      "Epoch 805/1000, Avg Training Loss: 0.2506155984329279, Avg Validation Loss: 256.1284068209109\n",
      "Epoch 806/1000, Avg Training Loss: 0.25061528646117043, Avg Validation Loss: 256.1283171287913\n",
      "Epoch 807/1000, Avg Training Loss: 0.25061497502849683, Avg Validation Loss: 256.128228393113\n",
      "Epoch 808/1000, Avg Training Loss: 0.2506146640998124, Avg Validation Loss: 256.1281405894113\n",
      "Epoch 809/1000, Avg Training Loss: 0.2506143536400289, Avg Validation Loss: 256.12805381254174\n",
      "Epoch 810/1000, Avg Training Loss: 0.2506140436393672, Avg Validation Loss: 256.1279681565967\n",
      "Epoch 811/1000, Avg Training Loss: 0.25061373415705124, Avg Validation Loss: 256.12788350446715\n",
      "Epoch 812/1000, Avg Training Loss: 0.25061342516609175, Avg Validation Loss: 256.1277998505577\n",
      "Epoch 813/1000, Avg Training Loss: 0.25061311667445824, Avg Validation Loss: 256.12771719180245\n",
      "Epoch 814/1000, Avg Training Loss: 0.2506128086820095, Avg Validation Loss: 256.1276355200233\n",
      "Epoch 815/1000, Avg Training Loss: 0.25061250118193507, Avg Validation Loss: 256.1275548412474\n",
      "Epoch 816/1000, Avg Training Loss: 0.25061219416338854, Avg Validation Loss: 256.127475146369\n",
      "Epoch 817/1000, Avg Training Loss: 0.2506118876322298, Avg Validation Loss: 256.12739643321254\n",
      "Epoch 818/1000, Avg Training Loss: 0.25061158157863134, Avg Validation Loss: 256.1273187675826\n",
      "Epoch 819/1000, Avg Training Loss: 0.2506112760076038, Avg Validation Loss: 256.12724215621733\n",
      "Epoch 820/1000, Avg Training Loss: 0.2506109709354116, Avg Validation Loss: 256.12716655682516\n",
      "Epoch 821/1000, Avg Training Loss: 0.25061066634773443, Avg Validation Loss: 256.1270919330183\n",
      "Epoch 822/1000, Avg Training Loss: 0.2506103622393526, Avg Validation Loss: 256.1270182923946\n",
      "Epoch 823/1000, Avg Training Loss: 0.2506100585952734, Avg Validation Loss: 256.12694562749715\n",
      "Epoch 824/1000, Avg Training Loss: 0.25060975540798575, Avg Validation Loss: 256.1268739362374\n",
      "Epoch 825/1000, Avg Training Loss: 0.25060945269654555, Avg Validation Loss: 256.1268032225739\n",
      "Epoch 826/1000, Avg Training Loss: 0.2506091504576363, Avg Validation Loss: 256.126733508721\n",
      "Epoch 827/1000, Avg Training Loss: 0.25060884870703903, Avg Validation Loss: 256.126664721084\n",
      "Epoch 828/1000, Avg Training Loss: 0.2506085474196091, Avg Validation Loss: 256.12659690795266\n",
      "Epoch 829/1000, Avg Training Loss: 0.2506082466107819, Avg Validation Loss: 256.1265300360521\n",
      "Epoch 830/1000, Avg Training Loss: 0.250607946278413, Avg Validation Loss: 256.1264641026504\n",
      "Epoch 831/1000, Avg Training Loss: 0.25060764640676125, Avg Validation Loss: 256.12639912696926\n",
      "Epoch 832/1000, Avg Training Loss: 0.2506073470122928, Avg Validation Loss: 256.1263350941576\n",
      "Epoch 833/1000, Avg Training Loss: 0.2506070481140506, Avg Validation Loss: 256.1262719517099\n",
      "Epoch 834/1000, Avg Training Loss: 0.25060674969004915, Avg Validation Loss: 256.1262097424234\n",
      "Epoch 835/1000, Avg Training Loss: 0.25060645174041324, Avg Validation Loss: 256.1261484719871\n",
      "Epoch 836/1000, Avg Training Loss: 0.25060615425694704, Avg Validation Loss: 256.12608814021246\n",
      "Epoch 837/1000, Avg Training Loss: 0.25060585724269274, Avg Validation Loss: 256.12602876235076\n",
      "Epoch 838/1000, Avg Training Loss: 0.25060556069529216, Avg Validation Loss: 256.1259703152044\n",
      "Epoch 839/1000, Avg Training Loss: 0.2506052646209539, Avg Validation Loss: 256.1259127595995\n",
      "Epoch 840/1000, Avg Training Loss: 0.2506049690104336, Avg Validation Loss: 256.1258560647821\n",
      "Epoch 841/1000, Avg Training Loss: 0.25060467385551033, Avg Validation Loss: 256.1258002717174\n",
      "Epoch 842/1000, Avg Training Loss: 0.25060437915217826, Avg Validation Loss: 256.1257453876118\n",
      "Epoch 843/1000, Avg Training Loss: 0.25060408489947406, Avg Validation Loss: 256.12569145799785\n",
      "Epoch 844/1000, Avg Training Loss: 0.2506037911130805, Avg Validation Loss: 256.1256384371901\n",
      "Epoch 845/1000, Avg Training Loss: 0.2506034977790355, Avg Validation Loss: 256.1255862996478\n",
      "Epoch 846/1000, Avg Training Loss: 0.2506032048675636, Avg Validation Loss: 256.1255351075921\n",
      "Epoch 847/1000, Avg Training Loss: 0.250602912415117, Avg Validation Loss: 256.12548481666545\n",
      "Epoch 848/1000, Avg Training Loss: 0.2506026204286654, Avg Validation Loss: 256.1254354550978\n",
      "Epoch 849/1000, Avg Training Loss: 0.2506023289189779, Avg Validation Loss: 256.1253870752934\n",
      "Epoch 850/1000, Avg Training Loss: 0.25060203786441476, Avg Validation Loss: 256.1253396007309\n",
      "Epoch 851/1000, Avg Training Loss: 0.25060174727158213, Avg Validation Loss: 256.1252929564172\n",
      "Epoch 852/1000, Avg Training Loss: 0.250601457119397, Avg Validation Loss: 256.1252471650569\n",
      "Epoch 853/1000, Avg Training Loss: 0.25060116741139093, Avg Validation Loss: 256.1252021532067\n",
      "Epoch 854/1000, Avg Training Loss: 0.2506008781492418, Avg Validation Loss: 256.12515801868426\n",
      "Epoch 855/1000, Avg Training Loss: 0.2506005893524399, Avg Validation Loss: 256.1251147247606\n",
      "Epoch 856/1000, Avg Training Loss: 0.25060030097570224, Avg Validation Loss: 256.1250722688871\n",
      "Epoch 857/1000, Avg Training Loss: 0.25060001303974566, Avg Validation Loss: 256.12503057291224\n",
      "Epoch 858/1000, Avg Training Loss: 0.2505997255166923, Avg Validation Loss: 256.12498969655996\n",
      "Epoch 859/1000, Avg Training Loss: 0.25059943841295984, Avg Validation Loss: 256.124949632062\n",
      "Epoch 860/1000, Avg Training Loss: 0.25059915173908326, Avg Validation Loss: 256.12491036159133\n",
      "Epoch 861/1000, Avg Training Loss: 0.25059886542691917, Avg Validation Loss: 256.12487197243007\n",
      "Epoch 862/1000, Avg Training Loss: 0.25059857954295267, Avg Validation Loss: 256.1248344194013\n",
      "Epoch 863/1000, Avg Training Loss: 0.2505982940710576, Avg Validation Loss: 256.1247976856148\n",
      "Epoch 864/1000, Avg Training Loss: 0.2505980090050401, Avg Validation Loss: 256.12476179635564\n",
      "Epoch 865/1000, Avg Training Loss: 0.2505977243353276, Avg Validation Loss: 256.1247268080445\n",
      "Epoch 866/1000, Avg Training Loss: 0.25059744008594176, Avg Validation Loss: 256.12469265308187\n",
      "Epoch 867/1000, Avg Training Loss: 0.25059715624842943, Avg Validation Loss: 256.1246592444395\n",
      "Epoch 868/1000, Avg Training Loss: 0.25059687282309007, Avg Validation Loss: 256.1246265854679\n",
      "Epoch 869/1000, Avg Training Loss: 0.2505965897874209, Avg Validation Loss: 256.124594675613\n",
      "Epoch 870/1000, Avg Training Loss: 0.2505963071387802, Avg Validation Loss: 256.12456353522236\n",
      "Epoch 871/1000, Avg Training Loss: 0.2505960248847937, Avg Validation Loss: 256.1245331497689\n",
      "Epoch 872/1000, Avg Training Loss: 0.25059574301754206, Avg Validation Loss: 256.1245035333286\n",
      "Epoch 873/1000, Avg Training Loss: 0.25059546154128104, Avg Validation Loss: 256.1244746930066\n",
      "Epoch 874/1000, Avg Training Loss: 0.2505951804553784, Avg Validation Loss: 256.12444664718055\n",
      "Epoch 875/1000, Avg Training Loss: 0.25059489976046984, Avg Validation Loss: 256.12441940016686\n",
      "Epoch 876/1000, Avg Training Loss: 0.2505946194467644, Avg Validation Loss: 256.12439294531225\n",
      "Epoch 877/1000, Avg Training Loss: 0.2505943395101875, Avg Validation Loss: 256.1243672824053\n",
      "Epoch 878/1000, Avg Training Loss: 0.25059405993240924, Avg Validation Loss: 256.1243423777704\n",
      "Epoch 879/1000, Avg Training Loss: 0.25059378071371236, Avg Validation Loss: 256.12431824122933\n",
      "Epoch 880/1000, Avg Training Loss: 0.2505935018568462, Avg Validation Loss: 256.12429488915393\n",
      "Epoch 881/1000, Avg Training Loss: 0.2505932233410657, Avg Validation Loss: 256.12427235621703\n",
      "Epoch 882/1000, Avg Training Loss: 0.25059294519044395, Avg Validation Loss: 256.12425059570444\n",
      "Epoch 883/1000, Avg Training Loss: 0.2505926674002154, Avg Validation Loss: 256.12422946817026\n",
      "Epoch 884/1000, Avg Training Loss: 0.2505923899506925, Avg Validation Loss: 256.12420901528253\n",
      "Epoch 885/1000, Avg Training Loss: 0.25059211283011634, Avg Validation Loss: 256.12418931382854\n",
      "Epoch 886/1000, Avg Training Loss: 0.25059183606062657, Avg Validation Loss: 256.1241703562222\n",
      "Epoch 887/1000, Avg Training Loss: 0.2505915596419002, Avg Validation Loss: 256.12415214242685\n",
      "Epoch 888/1000, Avg Training Loss: 0.25059128356428134, Avg Validation Loss: 256.1241346988055\n",
      "Epoch 889/1000, Avg Training Loss: 0.25059100782149074, Avg Validation Loss: 256.1241180608644\n",
      "Epoch 890/1000, Avg Training Loss: 0.2505907324124219, Avg Validation Loss: 256.1241022016993\n",
      "Epoch 891/1000, Avg Training Loss: 0.25059045732903357, Avg Validation Loss: 256.12408714569796\n",
      "Epoch 892/1000, Avg Training Loss: 0.25059018258466037, Avg Validation Loss: 256.1240728844394\n",
      "Epoch 893/1000, Avg Training Loss: 0.2505899081792333, Avg Validation Loss: 256.12405939676006\n",
      "Epoch 894/1000, Avg Training Loss: 0.2505896341216646, Avg Validation Loss: 256.1240466285773\n",
      "Epoch 895/1000, Avg Training Loss: 0.2505893603920271, Avg Validation Loss: 256.1240345940512\n",
      "Epoch 896/1000, Avg Training Loss: 0.2505890869943574, Avg Validation Loss: 256.1240233053568\n",
      "Epoch 897/1000, Avg Training Loss: 0.25058881392309185, Avg Validation Loss: 256.1240128095589\n",
      "Epoch 898/1000, Avg Training Loss: 0.25058854119227436, Avg Validation Loss: 256.1240030626701\n",
      "Epoch 899/1000, Avg Training Loss: 0.25058826877877033, Avg Validation Loss: 256.1239940722395\n",
      "Epoch 900/1000, Avg Training Loss: 0.25058799668121606, Avg Validation Loss: 256.12398581223715\n",
      "Epoch 901/1000, Avg Training Loss: 0.25058772491264114, Avg Validation Loss: 256.12397827974314\n",
      "Epoch 902/1000, Avg Training Loss: 0.2505874534559917, Avg Validation Loss: 256.1239715173317\n",
      "Epoch 903/1000, Avg Training Loss: 0.25058718231973065, Avg Validation Loss: 256.12396564697883\n",
      "Epoch 904/1000, Avg Training Loss: 0.2505869115105293, Avg Validation Loss: 256.1239605423486\n",
      "Epoch 905/1000, Avg Training Loss: 0.25058664101908706, Avg Validation Loss: 256.12395617600106\n",
      "Epoch 906/1000, Avg Training Loss: 0.2505863708786997, Avg Validation Loss: 256.1239524292132\n",
      "Epoch 907/1000, Avg Training Loss: 0.2505861010620574, Avg Validation Loss: 256.1239493337003\n",
      "Epoch 908/1000, Avg Training Loss: 0.250585831546862, Avg Validation Loss: 256.12394701098594\n",
      "Epoch 909/1000, Avg Training Loss: 0.2505855623615879, Avg Validation Loss: 256.12394543284097\n",
      "Epoch 910/1000, Avg Training Loss: 0.25058529349230024, Avg Validation Loss: 256.1239445960408\n",
      "Epoch 911/1000, Avg Training Loss: 0.2505850249397602, Avg Validation Loss: 256.1239444984678\n",
      "Epoch 912/1000, Avg Training Loss: 0.2505847566990612, Avg Validation Loss: 256.12394515492\n",
      "Epoch 913/1000, Avg Training Loss: 0.250584488779218, Avg Validation Loss: 256.12394654951174\n",
      "Epoch 914/1000, Avg Training Loss: 0.2505842211797321, Avg Validation Loss: 256.1239486635425\n",
      "Epoch 915/1000, Avg Training Loss: 0.25058395388985594, Avg Validation Loss: 256.1239514530565\n",
      "Epoch 916/1000, Avg Training Loss: 0.25058368690959004, Avg Validation Loss: 256.12395491278244\n",
      "Epoch 917/1000, Avg Training Loss: 0.2505834202309165, Avg Validation Loss: 256.1239590990492\n",
      "Epoch 918/1000, Avg Training Loss: 0.25058315387219765, Avg Validation Loss: 256.1239640213738\n",
      "Epoch 919/1000, Avg Training Loss: 0.2505828878403498, Avg Validation Loss: 256.1239696180925\n",
      "Epoch 920/1000, Avg Training Loss: 0.25058262211830784, Avg Validation Loss: 256.1239758729686\n",
      "Epoch 921/1000, Avg Training Loss: 0.25058235669996964, Avg Validation Loss: 256.1239827809518\n",
      "Epoch 922/1000, Avg Training Loss: 0.250582091583602, Avg Validation Loss: 256.12399035358726\n",
      "Epoch 923/1000, Avg Training Loss: 0.25058182676637303, Avg Validation Loss: 256.12399859676754\n",
      "Epoch 924/1000, Avg Training Loss: 0.25058156225314165, Avg Validation Loss: 256.12400751099256\n",
      "Epoch 925/1000, Avg Training Loss: 0.25058129804083085, Avg Validation Loss: 256.12401710570276\n",
      "Epoch 926/1000, Avg Training Loss: 0.2505810341229611, Avg Validation Loss: 256.12402737403283\n",
      "Epoch 927/1000, Avg Training Loss: 0.25058077050034283, Avg Validation Loss: 256.12403829829776\n",
      "Epoch 928/1000, Avg Training Loss: 0.2505805071763192, Avg Validation Loss: 256.1240498877606\n",
      "Epoch 929/1000, Avg Training Loss: 0.2505802441438233, Avg Validation Loss: 256.124062195875\n",
      "Epoch 930/1000, Avg Training Loss: 0.2505799814108151, Avg Validation Loss: 256.12407521481487\n",
      "Epoch 931/1000, Avg Training Loss: 0.2505797189892536, Avg Validation Loss: 256.1240888897659\n",
      "Epoch 932/1000, Avg Training Loss: 0.2505794568614037, Avg Validation Loss: 256.12410323106633\n",
      "Epoch 933/1000, Avg Training Loss: 0.25057919502070375, Avg Validation Loss: 256.12411825432486\n",
      "Epoch 934/1000, Avg Training Loss: 0.2505789334717606, Avg Validation Loss: 256.1241339679983\n",
      "Epoch 935/1000, Avg Training Loss: 0.2505786722255775, Avg Validation Loss: 256.1241503358093\n",
      "Epoch 936/1000, Avg Training Loss: 0.25057841127498653, Avg Validation Loss: 256.1241674166992\n",
      "Epoch 937/1000, Avg Training Loss: 0.2505781506265942, Avg Validation Loss: 256.1241851824532\n",
      "Epoch 938/1000, Avg Training Loss: 0.2505778902754228, Avg Validation Loss: 256.12420355775146\n",
      "Epoch 939/1000, Avg Training Loss: 0.250577630204744, Avg Validation Loss: 256.12422255277704\n",
      "Epoch 940/1000, Avg Training Loss: 0.2505773704271701, Avg Validation Loss: 256.1242421500097\n",
      "Epoch 941/1000, Avg Training Loss: 0.25057711093238727, Avg Validation Loss: 256.124262330083\n",
      "Epoch 942/1000, Avg Training Loss: 0.2505768517190747, Avg Validation Loss: 256.12428311705924\n",
      "Epoch 943/1000, Avg Training Loss: 0.25057659279872907, Avg Validation Loss: 256.124304551837\n",
      "Epoch 944/1000, Avg Training Loss: 0.25057633417555064, Avg Validation Loss: 256.12432661824755\n",
      "Epoch 945/1000, Avg Training Loss: 0.25057607584282965, Avg Validation Loss: 256.12434929989\n",
      "Epoch 946/1000, Avg Training Loss: 0.2505758178003147, Avg Validation Loss: 256.12437259630497\n",
      "Epoch 947/1000, Avg Training Loss: 0.2505755600476498, Avg Validation Loss: 256.1243964940337\n",
      "Epoch 948/1000, Avg Training Loss: 0.2505753025814832, Avg Validation Loss: 256.12442098218105\n",
      "Epoch 949/1000, Avg Training Loss: 0.2505750454005882, Avg Validation Loss: 256.12444605619055\n",
      "Epoch 950/1000, Avg Training Loss: 0.25057478849894804, Avg Validation Loss: 256.124471730256\n",
      "Epoch 951/1000, Avg Training Loss: 0.25057453188721374, Avg Validation Loss: 256.1244979734249\n",
      "Epoch 952/1000, Avg Training Loss: 0.2505742755712845, Avg Validation Loss: 256.1245247289135\n",
      "Epoch 953/1000, Avg Training Loss: 0.25057401952836467, Avg Validation Loss: 256.12455203861873\n",
      "Epoch 954/1000, Avg Training Loss: 0.2505737637568849, Avg Validation Loss: 256.1245799315483\n",
      "Epoch 955/1000, Avg Training Loss: 0.250573508269213, Avg Validation Loss: 256.1246083739467\n",
      "Epoch 956/1000, Avg Training Loss: 0.2505732530568624, Avg Validation Loss: 256.1246374094748\n",
      "Epoch 957/1000, Avg Training Loss: 0.25057299812152317, Avg Validation Loss: 256.1246670580765\n",
      "Epoch 958/1000, Avg Training Loss: 0.25057274346946873, Avg Validation Loss: 256.1246972909654\n",
      "Epoch 959/1000, Avg Training Loss: 0.2505724890994274, Avg Validation Loss: 256.1247280556412\n",
      "Epoch 960/1000, Avg Training Loss: 0.25057223500179804, Avg Validation Loss: 256.1247593399026\n",
      "Epoch 961/1000, Avg Training Loss: 0.2505719811656868, Avg Validation Loss: 256.12479117631676\n",
      "Epoch 962/1000, Avg Training Loss: 0.25057172759314206, Avg Validation Loss: 256.1248235506974\n",
      "Epoch 963/1000, Avg Training Loss: 0.2505714742879257, Avg Validation Loss: 256.1248564697903\n",
      "Epoch 964/1000, Avg Training Loss: 0.25057122125420844, Avg Validation Loss: 256.1248899536152\n",
      "Epoch 965/1000, Avg Training Loss: 0.25057096848499816, Avg Validation Loss: 256.1249239969055\n",
      "Epoch 966/1000, Avg Training Loss: 0.2505707159762986, Avg Validation Loss: 256.1249586151332\n",
      "Epoch 967/1000, Avg Training Loss: 0.2505704637275804, Avg Validation Loss: 256.1249938024555\n",
      "Epoch 968/1000, Avg Training Loss: 0.25057021173856747, Avg Validation Loss: 256.1250295972792\n",
      "Epoch 969/1000, Avg Training Loss: 0.25056996002296267, Avg Validation Loss: 256.1250659464933\n",
      "Epoch 970/1000, Avg Training Loss: 0.250569708574812, Avg Validation Loss: 256.1251028365381\n",
      "Epoch 971/1000, Avg Training Loss: 0.2505694573736248, Avg Validation Loss: 256.12514030409284\n",
      "Epoch 972/1000, Avg Training Loss: 0.2505692064289438, Avg Validation Loss: 256.1251783473052\n",
      "Epoch 973/1000, Avg Training Loss: 0.250568955748852, Avg Validation Loss: 256.12521697387547\n",
      "Epoch 974/1000, Avg Training Loss: 0.25056870534173553, Avg Validation Loss: 256.1252562350296\n",
      "Epoch 975/1000, Avg Training Loss: 0.25056845521089927, Avg Validation Loss: 256.12529604712375\n",
      "Epoch 976/1000, Avg Training Loss: 0.250568205349682, Avg Validation Loss: 256.12533641182824\n",
      "Epoch 977/1000, Avg Training Loss: 0.2505679557594632, Avg Validation Loss: 256.1253772977708\n",
      "Epoch 978/1000, Avg Training Loss: 0.2505677064347188, Avg Validation Loss: 256.12541870145105\n",
      "Epoch 979/1000, Avg Training Loss: 0.25056745737275526, Avg Validation Loss: 256.12546062732423\n",
      "Epoch 980/1000, Avg Training Loss: 0.25056720856927917, Avg Validation Loss: 256.1255030788693\n",
      "Epoch 981/1000, Avg Training Loss: 0.2505669600347121, Avg Validation Loss: 256.1255460166135\n",
      "Epoch 982/1000, Avg Training Loss: 0.25056671175514367, Avg Validation Loss: 256.1255894642052\n",
      "Epoch 983/1000, Avg Training Loss: 0.25056646373208363, Avg Validation Loss: 256.12563340244026\n",
      "Epoch 984/1000, Avg Training Loss: 0.25056621596258344, Avg Validation Loss: 256.1256778302608\n",
      "Epoch 985/1000, Avg Training Loss: 0.2505659684412441, Avg Validation Loss: 256.12572276455865\n",
      "Epoch 986/1000, Avg Training Loss: 0.25056572116638015, Avg Validation Loss: 256.1257681555423\n",
      "Epoch 987/1000, Avg Training Loss: 0.25056547412580166, Avg Validation Loss: 256.1258140373509\n",
      "Epoch 988/1000, Avg Training Loss: 0.25056522731062014, Avg Validation Loss: 256.12586043905185\n",
      "Epoch 989/1000, Avg Training Loss: 0.25056498073510886, Avg Validation Loss: 256.1259073518979\n",
      "Epoch 990/1000, Avg Training Loss: 0.2505647344048086, Avg Validation Loss: 256.12595479345055\n",
      "Epoch 991/1000, Avg Training Loss: 0.2505644883204397, Avg Validation Loss: 256.1260027508922\n",
      "Epoch 992/1000, Avg Training Loss: 0.25056424248103176, Avg Validation Loss: 256.12605122480943\n",
      "Epoch 993/1000, Avg Training Loss: 0.25056399689033626, Avg Validation Loss: 256.1261001749043\n",
      "Epoch 994/1000, Avg Training Loss: 0.25056375153044114, Avg Validation Loss: 256.1261496645999\n",
      "Epoch 995/1000, Avg Training Loss: 0.25056350641636926, Avg Validation Loss: 256.1261996882524\n",
      "Epoch 996/1000, Avg Training Loss: 0.25056326154875064, Avg Validation Loss: 256.1262502348295\n",
      "Epoch 997/1000, Avg Training Loss: 0.25056301694100375, Avg Validation Loss: 256.1263012673166\n",
      "Epoch 998/1000, Avg Training Loss: 0.25056277257021137, Avg Validation Loss: 256.1263528125746\n",
      "Epoch 999/1000, Avg Training Loss: 0.2505625284276947, Avg Validation Loss: 256.12640490086926\n",
      "Epoch 1000/1000, Avg Training Loss: 0.2505622845213256, Avg Validation Loss: 256.12645754950165\n"
      "Epoch 1/500, Avg Training Loss: 0.5883695173910697, Avg Validation Loss: 107.70696372494837\n",
      "Epoch 2/500, Avg Training Loss: 0.4616701468660975, Avg Validation Loss: 88.08212749716364\n",
      "Epoch 3/500, Avg Training Loss: 0.3793715462325998, Avg Validation Loss: 73.77848236371625\n",
      "Epoch 4/500, Avg Training Loss: 0.31964636968937227, Avg Validation Loss: 63.34517386661585\n",
      "Epoch 5/500, Avg Training Loss: 0.2762872925922958, Avg Validation Loss: 55.73084913938694\n",
      "Epoch 6/500, Avg Training Loss: 0.24489203809877672, Avg Validation Loss: 50.16776372132825\n",
      "Epoch 7/500, Avg Training Loss: 0.22213837387092694, Avg Validation Loss: 46.10653723767463\n",
      "Epoch 8/500, Avg Training Loss: 0.20564311629506185, Avg Validation Loss: 43.12488509606135\n",
      "Epoch 9/500, Avg Training Loss: 0.19365148376079863, Avg Validation Loss: 40.936374013559835\n",
      "Epoch 10/500, Avg Training Loss: 0.18497224865016704, Avg Validation Loss: 39.31833944391664\n",
      "Epoch 11/500, Avg Training Loss: 0.17865057301430073, Avg Validation Loss: 38.12727419791507\n",
      "Epoch 12/500, Avg Training Loss: 0.17408071829658403, Avg Validation Loss: 37.25367867055121\n",
      "Epoch 13/500, Avg Training Loss: 0.1707483714483416, Avg Validation Loss: 36.597361976415215\n",
      "Epoch 14/500, Avg Training Loss: 0.16834367974295955, Avg Validation Loss: 36.11837777118008\n",
      "Epoch 15/500, Avg Training Loss: 0.166612687584319, Avg Validation Loss: 35.75592808828577\n",
      "Epoch 16/500, Avg Training Loss: 0.16537299155970928, Avg Validation Loss: 35.48873329634312\n",
      "Epoch 17/500, Avg Training Loss: 0.16444732819145194, Avg Validation Loss: 35.28600961160274\n",
      "Epoch 18/500, Avg Training Loss: 0.16379934307610136, Avg Validation Loss: 35.12901738235655\n",
      "Epoch 19/500, Avg Training Loss: 0.16330567361422624, Avg Validation Loss: 35.01176747140157\n",
      "Epoch 20/500, Avg Training Loss: 0.1629343638822045, Avg Validation Loss: 34.92129126691702\n",
      "Epoch 21/500, Avg Training Loss: 0.16267848741167804, Avg Validation Loss: 34.85315937292076\n",
      "Epoch 22/500, Avg Training Loss: 0.16249780730909452, Avg Validation Loss: 34.798455528639664\n",
      "Epoch 23/500, Avg Training Loss: 0.16237262156221405, Avg Validation Loss: 34.75639232542224\n",
      "Epoch 24/500, Avg Training Loss: 0.16225813930415323, Avg Validation Loss: 34.72635309761142\n",
      "Epoch 25/500, Avg Training Loss: 0.1622143673038723, Avg Validation Loss: 34.70033800719974\n",
      "Epoch 26/500, Avg Training Loss: 0.16215387640510417, Avg Validation Loss: 34.680581571124094\n",
      "Epoch 27/500, Avg Training Loss: 0.1621059734353244, Avg Validation Loss: 34.66415708788551\n",
      "Epoch 28/500, Avg Training Loss: 0.16208410081627875, Avg Validation Loss: 34.65088417224917\n",
      "Epoch 29/500, Avg Training Loss: 0.16207757319963664, Avg Validation Loss: 34.64076542206898\n",
      "Epoch 30/500, Avg Training Loss: 0.1620517866481905, Avg Validation Loss: 34.63079737758426\n",
      "Epoch 31/500, Avg Training Loss: 0.16204994406784856, Avg Validation Loss: 34.62494193660508\n",
      "Epoch 32/500, Avg Training Loss: 0.16203465433032327, Avg Validation Loss: 34.619817869523835\n",
      "Epoch 33/500, Avg Training Loss: 0.16206230377484526, Avg Validation Loss: 34.61700289747023\n",
      "Epoch 34/500, Avg Training Loss: 0.16203235198649132, Avg Validation Loss: 34.61249946430636\n",
      "Epoch 35/500, Avg Training Loss: 0.16203258878953875, Avg Validation Loss: 34.60778607570323\n",
      "Epoch 36/500, Avg Training Loss: 0.16201745318812447, Avg Validation Loss: 34.60416383474349\n",
      "Epoch 37/500, Avg Training Loss: 0.1620157187382563, Avg Validation Loss: 34.60232616047418\n",
      "Epoch 38/500, Avg Training Loss: 0.16201415374032968, Avg Validation Loss: 34.59830138894307\n",
      "Epoch 39/500, Avg Training Loss: 0.16204209315882828, Avg Validation Loss: 34.59650059001637\n",
      "Epoch 40/500, Avg Training Loss: 0.1620196604953104, Avg Validation Loss: 34.59521402449044\n",
      "Epoch 41/500, Avg Training Loss: 0.16201346819833115, Avg Validation Loss: 34.595368413126366\n",
      "Epoch 42/500, Avg Training Loss: 0.16201772059344716, Avg Validation Loss: 34.59314079000286\n",
      "Epoch 43/500, Avg Training Loss: 0.16201292890305166, Avg Validation Loss: 34.59144044935077\n",
      "Epoch 44/500, Avg Training Loss: 0.1620051697268118, Avg Validation Loss: 34.5920813052062\n",
      "Epoch 45/500, Avg Training Loss: 0.16202080339106106, Avg Validation Loss: 34.590947078139365\n",
      "Epoch 46/500, Avg Training Loss: 0.1620142241115218, Avg Validation Loss: 34.590990706848274\n",
      "Epoch 47/500, Avg Training Loss: 0.1619983864016626, Avg Validation Loss: 34.59011733029128\n",
      "Epoch 48/500, Avg Training Loss: 0.16201474664609633, Avg Validation Loss: 34.59106706892431\n",
      "Epoch 49/500, Avg Training Loss: 0.1620278859846809, Avg Validation Loss: 34.59147999746419\n",
      "Epoch 50/500, Avg Training Loss: 0.162005391699645, Avg Validation Loss: 34.591664095571545\n",
      "Epoch 51/500, Avg Training Loss: 0.16200669918625407, Avg Validation Loss: 34.590783581482\n",
      "Epoch 52/500, Avg Training Loss: 0.16202810817680516, Avg Validation Loss: 34.59087690279739\n",
      "Epoch 53/500, Avg Training Loss: 0.16203242257307082, Avg Validation Loss: 34.590522112285356\n",
      "Epoch 54/500, Avg Training Loss: 0.1620231086157981, Avg Validation Loss: 34.58913307249321\n",
      "Epoch 55/500, Avg Training Loss: 0.1620020740181857, Avg Validation Loss: 34.59017211784361\n",
      "Epoch 56/500, Avg Training Loss: 0.16201550437807122, Avg Validation Loss: 34.59073295723479\n",
      "Epoch 57/500, Avg Training Loss: 0.1620090214755312, Avg Validation Loss: 34.59156887214303\n",
      "Epoch 58/500, Avg Training Loss: 0.1620253033877196, Avg Validation Loss: 34.592707785285924\n",
      "Epoch 59/500, Avg Training Loss: 0.1620171913085898, Avg Validation Loss: 34.59226524824686\n",
      "Epoch 60/500, Avg Training Loss: 0.16200776992672908, Avg Validation Loss: 34.59166815898944\n",
      "Epoch 61/500, Avg Training Loss: 0.1620282204123808, Avg Validation Loss: 34.591000237315974\n",
      "Epoch 62/500, Avg Training Loss: 0.16201804647669868, Avg Validation Loss: 34.590427976890865\n",
      "Epoch 63/500, Avg Training Loss: 0.16204764618030137, Avg Validation Loss: 34.58806770975414\n",
      "Epoch 64/500, Avg Training Loss: 0.16202412163095542, Avg Validation Loss: 34.58815858490461\n",
      "Epoch 65/500, Avg Training Loss: 0.1620325092071919, Avg Validation Loss: 34.58842805368581\n",
      "Epoch 66/500, Avg Training Loss: 0.1620327324905699, Avg Validation Loss: 34.588648646209165\n",
      "Epoch 67/500, Avg Training Loss: 0.16204286070974175, Avg Validation Loss: 34.58818338897747\n",
      "Epoch 68/500, Avg Training Loss: 0.16200761179641257, Avg Validation Loss: 34.58886236067808\n",
      "Epoch 69/500, Avg Training Loss: 0.16201421256281254, Avg Validation Loss: 34.59058711467708\n",
      "Epoch 70/500, Avg Training Loss: 0.16201779009668943, Avg Validation Loss: 34.58829951855763\n",
      "Epoch 71/500, Avg Training Loss: 0.16203067355069206, Avg Validation Loss: 34.587146687689604\n",
      "Epoch 72/500, Avg Training Loss: 0.16200321895225453, Avg Validation Loss: 34.588519975330684\n",
      "Epoch 73/500, Avg Training Loss: 0.16201484748163886, Avg Validation Loss: 34.58726832023494\n",
      "Epoch 74/500, Avg Training Loss: 0.1620134894579352, Avg Validation Loss: 34.5880887611559\n",
      "Epoch 75/500, Avg Training Loss: 0.1620213516798659, Avg Validation Loss: 34.58849959088036\n",
      "Epoch 76/500, Avg Training Loss: 0.16201456974745185, Avg Validation Loss: 34.58935676538511\n",
      "Epoch 77/500, Avg Training Loss: 0.16202454122356857, Avg Validation Loss: 34.587756597130735\n",
      "Epoch 78/500, Avg Training Loss: 0.16198859083401854, Avg Validation Loss: 34.58835179640111\n",
      "Epoch 79/500, Avg Training Loss: 0.16199893042020125, Avg Validation Loss: 34.58889372088072\n",
      "Epoch 80/500, Avg Training Loss: 0.16202264016500578, Avg Validation Loss: 34.588344880196985\n",
      "Epoch 81/500, Avg Training Loss: 0.16200024509041389, Avg Validation Loss: 34.58795794550181\n",
      "Epoch 82/500, Avg Training Loss: 0.16200188124078665, Avg Validation Loss: 34.58803106135559\n",
      "Epoch 83/500, Avg Training Loss: 0.16205312381258247, Avg Validation Loss: 34.587527133552896\n",
      "Epoch 84/500, Avg Training Loss: 0.1620185925838148, Avg Validation Loss: 34.58697459206793\n",
      "Epoch 85/500, Avg Training Loss: 0.16199865637321056, Avg Validation Loss: 34.588495542695775\n",
      "Epoch 86/500, Avg Training Loss: 0.16201326170655206, Avg Validation Loss: 34.588986465416596\n",
      "Epoch 87/500, Avg Training Loss: 0.1620165720204247, Avg Validation Loss: 34.58932605822745\n",
      "Epoch 88/500, Avg Training Loss: 0.16202555026412324, Avg Validation Loss: 34.59045770708362\n",
      "Epoch 89/500, Avg Training Loss: 0.16201886301201857, Avg Validation Loss: 34.59096972829117\n",
      "Epoch 90/500, Avg Training Loss: 0.16200237550101648, Avg Validation Loss: 34.59113697072867\n",
      "Epoch 91/500, Avg Training Loss: 0.16200383218185258, Avg Validation Loss: 34.59092170923874\n",
      "Epoch 92/500, Avg Training Loss: 0.16200657081987266, Avg Validation Loss: 34.58962582911163\n",
      "Epoch 93/500, Avg Training Loss: 0.1620186862993554, Avg Validation Loss: 34.58955101262317\n",
      "Epoch 94/500, Avg Training Loss: 0.16201676902295314, Avg Validation Loss: 34.58879358398262\n",
      "Epoch 95/500, Avg Training Loss: 0.1620107209599045, Avg Validation Loss: 34.588239180686536\n",
      "Epoch 96/500, Avg Training Loss: 0.16200511039460694, Avg Validation Loss: 34.58870092437755\n",
      "Epoch 97/500, Avg Training Loss: 0.16200042957220556, Avg Validation Loss: 34.58937653030163\n",
      "Epoch 98/500, Avg Training Loss: 0.16202300035384326, Avg Validation Loss: 34.59139379339803\n",
      "Epoch 99/500, Avg Training Loss: 0.16202829325515072, Avg Validation Loss: 34.59086240709952\n",
      "Epoch 100/500, Avg Training Loss: 0.16201158581143651, Avg Validation Loss: 34.58955807541396\n",
      "Epoch 101/500, Avg Training Loss: 0.16203595763240794, Avg Validation Loss: 34.58826078274203\n",
      "Epoch 102/500, Avg Training Loss: 0.16200642312930294, Avg Validation Loss: 34.58927994548283\n",
      "Epoch 103/500, Avg Training Loss: 0.16200967423040522, Avg Validation Loss: 34.58946121011087\n",
      "Epoch 104/500, Avg Training Loss: 0.16200891696865558, Avg Validation Loss: 34.587983100489296\n",
      "Epoch 105/500, Avg Training Loss: 0.16203244987283594, Avg Validation Loss: 34.58892365542199\n",
      "Epoch 106/500, Avg Training Loss: 0.1620252710161264, Avg Validation Loss: 34.59025377361637\n",
      "Epoch 107/500, Avg Training Loss: 0.16200002137275235, Avg Validation Loss: 34.59183337923061\n",
      "Epoch 108/500, Avg Training Loss: 0.1620414782178977, Avg Validation Loss: 34.59197805334293\n",
      "Epoch 109/500, Avg Training Loss: 0.1620018041715866, Avg Validation Loss: 34.592587675484054\n",
      "Epoch 110/500, Avg Training Loss: 0.16201562714604914, Avg Validation Loss: 34.592016833847715\n",
      "Epoch 111/500, Avg Training Loss: 0.16201186858876787, Avg Validation Loss: 34.590545517081225\n",
      "Epoch 112/500, Avg Training Loss: 0.1620417390878918, Avg Validation Loss: 34.59116021647145\n",
      "Epoch 113/500, Avg Training Loss: 0.16201036659567147, Avg Validation Loss: 34.59054674016389\n",
      "Epoch 114/500, Avg Training Loss: 0.16202625244457278, Avg Validation Loss: 34.591600251788606\n",
      "Epoch 115/500, Avg Training Loss: 0.16201795641267733, Avg Validation Loss: 34.59300567512459\n",
      "Epoch 116/500, Avg Training Loss: 0.16201148816711805, Avg Validation Loss: 34.592687780871614\n",
      "Epoch 117/500, Avg Training Loss: 0.16202544131939614, Avg Validation Loss: 34.59115515632644\n",
      "Epoch 118/500, Avg Training Loss: 0.16200609870751287, Avg Validation Loss: 34.5910594575453\n",
      "Epoch 119/500, Avg Training Loss: 0.1620126516975235, Avg Validation Loss: 34.59124913242674\n",
      "Epoch 120/500, Avg Training Loss: 0.16202625473703505, Avg Validation Loss: 34.59148777854474\n",
      "Epoch 121/500, Avg Training Loss: 0.16202145189212824, Avg Validation Loss: 34.589890326415656\n",
      "Epoch 122/500, Avg Training Loss: 0.16202127729019009, Avg Validation Loss: 34.589827936732235\n",
      "Epoch 123/500, Avg Training Loss: 0.16201608727285435, Avg Validation Loss: 34.58958964238957\n",
      "Epoch 124/500, Avg Training Loss: 0.16200960501768835, Avg Validation Loss: 34.59193454062238\n",
      "Epoch 125/500, Avg Training Loss: 0.1620389518354737, Avg Validation Loss: 34.59080807062618\n",
      "Epoch 126/500, Avg Training Loss: 0.1620065661128834, Avg Validation Loss: 34.58946818162235\n",
      "Epoch 127/500, Avg Training Loss: 0.16202566710977515, Avg Validation Loss: 34.589491550528045\n",
      "Epoch 128/500, Avg Training Loss: 0.1620265735377119, Avg Validation Loss: 34.58848973240791\n",
      "Epoch 129/500, Avg Training Loss: 0.16201597798705397, Avg Validation Loss: 34.58733132553358\n",
      "Epoch 130/500, Avg Training Loss: 0.16201419093371386, Avg Validation Loss: 34.58837450726609\n",
      "Epoch 131/500, Avg Training Loss: 0.16203774075297264, Avg Validation Loss: 34.58753610580274\n",
      "Epoch 132/500, Avg Training Loss: 0.16201030519765464, Avg Validation Loss: 34.58789813137654\n",
      "Epoch 133/500, Avg Training Loss: 0.16201877021442562, Avg Validation Loss: 34.58788222994241\n",
      "Epoch 134/500, Avg Training Loss: 0.1620038034102236, Avg Validation Loss: 34.5901721774708\n",
      "Epoch 135/500, Avg Training Loss: 0.16203835644055584, Avg Validation Loss: 34.58909875378926\n",
      "Epoch 136/500, Avg Training Loss: 0.16204023026309686, Avg Validation Loss: 34.58968315528\n",
      "Epoch 137/500, Avg Training Loss: 0.1620013057380891, Avg Validation Loss: 34.58917660468883\n",
      "Epoch 138/500, Avg Training Loss: 0.16201452569101413, Avg Validation Loss: 34.58819314798343\n",
      "Epoch 139/500, Avg Training Loss: 0.16201444553532285, Avg Validation Loss: 34.5890322898964\n",
      "Epoch 140/500, Avg Training Loss: 0.16204627237594685, Avg Validation Loss: 34.58882540832521\n",
      "Epoch 141/500, Avg Training Loss: 0.16201232381399108, Avg Validation Loss: 34.587660178094644\n",
      "Epoch 142/500, Avg Training Loss: 0.16202198684846777, Avg Validation Loss: 34.587786240199385\n",
      "Epoch 143/500, Avg Training Loss: 0.16203339980085085, Avg Validation Loss: 34.58827970147595\n",
      "Epoch 144/500, Avg Training Loss: 0.16201673176825998, Avg Validation Loss: 34.58737885359658\n",
      "Epoch 145/500, Avg Training Loss: 0.16201241650407794, Avg Validation Loss: 34.58839255808492\n",
      "Epoch 146/500, Avg Training Loss: 0.16201239375080292, Avg Validation Loss: 34.589319087526945\n",
      "Epoch 147/500, Avg Training Loss: 0.1620221944108734, Avg Validation Loss: 34.589222070292436\n",
      "Epoch 148/500, Avg Training Loss: 0.16202323681063663, Avg Validation Loss: 34.58924166541662\n",
      "Epoch 149/500, Avg Training Loss: 0.1620176927725568, Avg Validation Loss: 34.5891685871794\n",
      "Epoch 150/500, Avg Training Loss: 0.1620021992747112, Avg Validation Loss: 34.58832119953363\n",
      "Epoch 151/500, Avg Training Loss: 0.16203005162174486, Avg Validation Loss: 34.589254255025736\n",
      "Epoch 152/500, Avg Training Loss: 0.162031910158559, Avg Validation Loss: 34.588048648317866\n",
      "Epoch 153/500, Avg Training Loss: 0.16201779395598787, Avg Validation Loss: 34.58799224102903\n",
      "Epoch 154/500, Avg Training Loss: 0.16200551412318934, Avg Validation Loss: 34.588476126451006\n",
      "Epoch 155/500, Avg Training Loss: 0.1620174676369256, Avg Validation Loss: 34.58886239714053\n",
      "Epoch 156/500, Avg Training Loss: 0.16202610458079028, Avg Validation Loss: 34.5879725943417\n",
      "Epoch 157/500, Avg Training Loss: 0.16201687815470345, Avg Validation Loss: 34.587840544954496\n",
      "Epoch 158/500, Avg Training Loss: 0.1620154148926266, Avg Validation Loss: 34.5883775296371\n",
      "Epoch 159/500, Avg Training Loss: 0.16203528436418785, Avg Validation Loss: 34.58816977356838\n",
      "Epoch 160/500, Avg Training Loss: 0.16202660128919516, Avg Validation Loss: 34.58805555763095\n",
      "Epoch 161/500, Avg Training Loss: 0.16201413805507095, Avg Validation Loss: 34.58668036069638\n",
      "Epoch 162/500, Avg Training Loss: 0.16201623991621286, Avg Validation Loss: 34.587156476314945\n",
      "Epoch 163/500, Avg Training Loss: 0.1620018886839597, Avg Validation Loss: 34.589458943311726\n",
      "Epoch 164/500, Avg Training Loss: 0.16203061444964026, Avg Validation Loss: 34.590526426301906\n",
      "Epoch 165/500, Avg Training Loss: 0.16200153401848266, Avg Validation Loss: 34.59066255909785\n",
      "Epoch 166/500, Avg Training Loss: 0.1620194817056257, Avg Validation Loss: 34.591099344082934\n",
      "Epoch 167/500, Avg Training Loss: 0.1620198252777762, Avg Validation Loss: 34.58922147931907\n",
      "Epoch 168/500, Avg Training Loss: 0.16201913325364092, Avg Validation Loss: 34.589894340505\n",
      "Epoch 169/500, Avg Training Loss: 0.16199774310567944, Avg Validation Loss: 34.58969519805892\n",
      "Epoch 170/500, Avg Training Loss: 0.16202319247815097, Avg Validation Loss: 34.58914971416841\n",
      "Epoch 171/500, Avg Training Loss: 0.1620184239057232, Avg Validation Loss: 34.5889200075897\n",
      "Epoch 172/500, Avg Training Loss: 0.1620287621139503, Avg Validation Loss: 34.589937167439615\n",
      "Epoch 173/500, Avg Training Loss: 0.16200470950703016, Avg Validation Loss: 34.58968776828935\n",
      "Epoch 174/500, Avg Training Loss: 0.1620067234499174, Avg Validation Loss: 34.59124664136631\n",
      "Epoch 175/500, Avg Training Loss: 0.16201272371454498, Avg Validation Loss: 34.59106148628131\n",
      "Epoch 176/500, Avg Training Loss: 0.16200444504460207, Avg Validation Loss: 34.59019876661028\n",
      "Epoch 177/500, Avg Training Loss: 0.16201296051714142, Avg Validation Loss: 34.5894532663761\n",
      "Epoch 178/500, Avg Training Loss: 0.16200110790226663, Avg Validation Loss: 34.58891969770316\n",
      "Epoch 179/500, Avg Training Loss: 0.16201819467236, Avg Validation Loss: 34.58884134986494\n",
      "Epoch 180/500, Avg Training Loss: 0.16203916959239428, Avg Validation Loss: 34.58801086233896\n",
      "Epoch 181/500, Avg Training Loss: 0.1620536238162978, Avg Validation Loss: 34.58866631288289\n",
      "Epoch 182/500, Avg Training Loss: 0.16202525345795757, Avg Validation Loss: 34.58973356046261\n",
      "Epoch 183/500, Avg Training Loss: 0.16201154640332535, Avg Validation Loss: 34.5896333310164\n",
      "Epoch 184/500, Avg Training Loss: 0.16201752944565848, Avg Validation Loss: 34.58942601454834\n",
      "Epoch 185/500, Avg Training Loss: 0.16199528751141415, Avg Validation Loss: 34.59082985678617\n",
      "Epoch 186/500, Avg Training Loss: 0.16203006014865146, Avg Validation Loss: 34.58962672074841\n",
      "Epoch 187/500, Avg Training Loss: 0.16201288913614562, Avg Validation Loss: 34.58929597839562\n",
      "Epoch 188/500, Avg Training Loss: 0.16205645264862112, Avg Validation Loss: 34.58917781342074\n",
      "Epoch 189/500, Avg Training Loss: 0.16203716455382117, Avg Validation Loss: 34.589913657559144\n",
      "Epoch 190/500, Avg Training Loss: 0.16199510972608544, Avg Validation Loss: 34.590026308082756\n",
      "Epoch 191/500, Avg Training Loss: 0.16198580977156415, Avg Validation Loss: 34.58907754849824\n",
      "Epoch 192/500, Avg Training Loss: 0.16201418122061143, Avg Validation Loss: 34.588907932754985\n",
      "Epoch 193/500, Avg Training Loss: 0.16204307630159434, Avg Validation Loss: 34.587713105437984\n",
      "Epoch 194/500, Avg Training Loss: 0.16201982600476422, Avg Validation Loss: 34.58608798784398\n",
      "Epoch 195/500, Avg Training Loss: 0.16201410734868923, Avg Validation Loss: 34.58589929452916\n",
      "Epoch 196/500, Avg Training Loss: 0.16202493180678157, Avg Validation Loss: 34.58682090047432\n",
      "Epoch 197/500, Avg Training Loss: 0.16201853102854424, Avg Validation Loss: 34.587310190635776\n",
      "Epoch 198/500, Avg Training Loss: 0.16199146459319425, Avg Validation Loss: 34.58814647925144\n",
      "Epoch 199/500, Avg Training Loss: 0.1620096410280924, Avg Validation Loss: 34.58805558407073\n",
      "Epoch 200/500, Avg Training Loss: 0.1620379977848846, Avg Validation Loss: 34.58930248152535\n",
      "Epoch 201/500, Avg Training Loss: 0.16202366508798108, Avg Validation Loss: 34.589601057893994\n",
      "Epoch 202/500, Avg Training Loss: 0.162029870287307, Avg Validation Loss: 34.58934388340272\n",
      "Epoch 203/500, Avg Training Loss: 0.16202025444339938, Avg Validation Loss: 34.58874520394007\n",
      "Epoch 204/500, Avg Training Loss: 0.1619946487148747, Avg Validation Loss: 34.58978489058496\n",
      "Epoch 205/500, Avg Training Loss: 0.1620034718261739, Avg Validation Loss: 34.58877609993731\n",
      "Epoch 206/500, Avg Training Loss: 0.16204383383331383, Avg Validation Loss: 34.5875211353298\n",
      "Epoch 207/500, Avg Training Loss: 0.16201292516484853, Avg Validation Loss: 34.587829395363485\n",
      "Epoch 208/500, Avg Training Loss: 0.16200884751490163, Avg Validation Loss: 34.5891372702123\n",
      "Epoch 209/500, Avg Training Loss: 0.162032218094447, Avg Validation Loss: 34.589503688646715\n",
      "Epoch 210/500, Avg Training Loss: 0.1620160404901565, Avg Validation Loss: 34.58884522766022\n",
      "Epoch 211/500, Avg Training Loss: 0.16201825168767034, Avg Validation Loss: 34.58849324470924\n",
      "Epoch 212/500, Avg Training Loss: 0.16203359070969156, Avg Validation Loss: 34.588243421095704\n",
      "Epoch 213/500, Avg Training Loss: 0.1620000659717325, Avg Validation Loss: 34.59074216619679\n",
      "Epoch 214/500, Avg Training Loss: 0.16202796047896537, Avg Validation Loss: 34.59030521944751\n",
      "Epoch 215/500, Avg Training Loss: 0.16204723557923953, Avg Validation Loss: 34.58792490031482\n",
      "Epoch 216/500, Avg Training Loss: 0.16204661805311343, Avg Validation Loss: 34.585638389903714\n",
      "Epoch 217/500, Avg Training Loss: 0.16201747699616167, Avg Validation Loss: 34.58557657815527\n",
      "Epoch 218/500, Avg Training Loss: 0.16198917679541744, Avg Validation Loss: 34.587578055121455\n",
      "Epoch 219/500, Avg Training Loss: 0.16201722865321005, Avg Validation Loss: 34.58587419343752\n",
      "Epoch 220/500, Avg Training Loss: 0.1620199125897158, Avg Validation Loss: 34.58509385082573\n",
      "Epoch 221/500, Avg Training Loss: 0.16202392175961478, Avg Validation Loss: 34.58497657648018\n",
      "Epoch 222/500, Avg Training Loss: 0.16199522185478496, Avg Validation Loss: 34.58854508007967\n",
      "Epoch 223/500, Avg Training Loss: 0.16200643298755524, Avg Validation Loss: 34.58839611186071\n",
      "Epoch 224/500, Avg Training Loss: 0.16201737944656242, Avg Validation Loss: 34.58883046522645\n",
      "Epoch 225/500, Avg Training Loss: 0.16200545296005395, Avg Validation Loss: 34.589144369895315\n",
      "Epoch 226/500, Avg Training Loss: 0.16201682019344882, Avg Validation Loss: 34.58787983279599\n",
      "Epoch 227/500, Avg Training Loss: 0.16202735328855947, Avg Validation Loss: 34.586431770629346\n",
      "Epoch 228/500, Avg Training Loss: 0.16200915651876988, Avg Validation Loss: 34.586848555281016\n",
      "Epoch 229/500, Avg Training Loss: 0.162014644990895, Avg Validation Loss: 34.58828085781326\n",
      "Epoch 230/500, Avg Training Loss: 0.1620250045549577, Avg Validation Loss: 34.58821671637136\n",
      "Epoch 231/500, Avg Training Loss: 0.16199215951583984, Avg Validation Loss: 34.588816046815516\n",
      "Epoch 232/500, Avg Training Loss: 0.16200736640902985, Avg Validation Loss: 34.58907851158585\n",
      "Epoch 233/500, Avg Training Loss: 0.1620207324666313, Avg Validation Loss: 34.59006580535517\n",
      "Epoch 234/500, Avg Training Loss: 0.1620125542568883, Avg Validation Loss: 34.58857381950259\n",
      "Epoch 235/500, Avg Training Loss: 0.1620291044714399, Avg Validation Loss: 34.58737726108845\n",
      "Epoch 236/500, Avg Training Loss: 0.16202815157979159, Avg Validation Loss: 34.58800499576692\n",
      "Epoch 237/500, Avg Training Loss: 0.1620000351795648, Avg Validation Loss: 34.588272024443505\n",
      "Epoch 238/500, Avg Training Loss: 0.1620215877142639, Avg Validation Loss: 34.58765131542375\n",
      "Epoch 239/500, Avg Training Loss: 0.16201972988691016, Avg Validation Loss: 34.58712551256775\n",
      "Epoch 240/500, Avg Training Loss: 0.16203003084555045, Avg Validation Loss: 34.586206867444304\n",
      "Epoch 241/500, Avg Training Loss: 0.16203848831660256, Avg Validation Loss: 34.58503928732333\n",
      "Epoch 242/500, Avg Training Loss: 0.1620252273154095, Avg Validation Loss: 34.584531211123036\n",
      "Epoch 243/500, Avg Training Loss: 0.16200120789116465, Avg Validation Loss: 34.58548254050437\n",
      "Epoch 244/500, Avg Training Loss: 0.1620184064590096, Avg Validation Loss: 34.587077703428974\n",
      "Epoch 245/500, Avg Training Loss: 0.1620270275922344, Avg Validation Loss: 34.58617554261054\n",
      "Epoch 246/500, Avg Training Loss: 0.16203701230439166, Avg Validation Loss: 34.58520038569703\n",
      "Epoch 247/500, Avg Training Loss: 0.16205355488587955, Avg Validation Loss: 34.583088808760166\n",
      "Epoch 248/500, Avg Training Loss: 0.16203264166331646, Avg Validation Loss: 34.58277130327878\n",
      "Epoch 249/500, Avg Training Loss: 0.1620184378787202, Avg Validation Loss: 34.58408950115275\n",
      "Epoch 250/500, Avg Training Loss: 0.16202907079498352, Avg Validation Loss: 34.584109432020824\n",
      "Epoch 251/500, Avg Training Loss: 0.1620309297009182, Avg Validation Loss: 34.58480805900864\n",
      "Epoch 252/500, Avg Training Loss: 0.16201624893749483, Avg Validation Loss: 34.58451247234974\n",
      "Epoch 253/500, Avg Training Loss: 0.16203399748458774, Avg Validation Loss: 34.58408728283176\n",
      "Epoch 254/500, Avg Training Loss: 0.16202513113230899, Avg Validation Loss: 34.58375515365181\n",
      "Epoch 255/500, Avg Training Loss: 0.16201965722284523, Avg Validation Loss: 34.583673737469745\n",
      "Epoch 256/500, Avg Training Loss: 0.16202568449418642, Avg Validation Loss: 34.58399071849914\n",
      "Epoch 257/500, Avg Training Loss: 0.16200490072480986, Avg Validation Loss: 34.5839131672322\n",
      "Epoch 258/500, Avg Training Loss: 0.1620114558607951, Avg Validation Loss: 34.584528108093465\n",
      "Epoch 259/500, Avg Training Loss: 0.16204837152856666, Avg Validation Loss: 34.58411717465444\n",
      "Epoch 260/500, Avg Training Loss: 0.1620229533883502, Avg Validation Loss: 34.584007082531286\n",
      "Epoch 261/500, Avg Training Loss: 0.16202370753408626, Avg Validation Loss: 34.58525744444801\n",
      "Epoch 262/500, Avg Training Loss: 0.1619872868998632, Avg Validation Loss: 34.584464508675424\n",
      "Epoch 263/500, Avg Training Loss: 0.1620511798560484, Avg Validation Loss: 34.584296052949554\n",
      "Epoch 264/500, Avg Training Loss: 0.1620573388770071, Avg Validation Loss: 34.58263972557394\n",
      "Epoch 265/500, Avg Training Loss: 0.16201983583218504, Avg Validation Loss: 34.58307630910602\n",
      "Epoch 266/500, Avg Training Loss: 0.16201076102614564, Avg Validation Loss: 34.58304844304052\n",
      "Epoch 267/500, Avg Training Loss: 0.1620380643838773, Avg Validation Loss: 34.581302095481405\n",
      "Epoch 268/500, Avg Training Loss: 0.162022124015373, Avg Validation Loss: 34.58222522607472\n",
      "Epoch 269/500, Avg Training Loss: 0.162007223536056, Avg Validation Loss: 34.5829346815458\n",
      "Epoch 270/500, Avg Training Loss: 0.1619957511770963, Avg Validation Loss: 34.584459895665184\n",
      "Epoch 271/500, Avg Training Loss: 0.16200626126070042, Avg Validation Loss: 34.58347582960394\n",
      "Epoch 272/500, Avg Training Loss: 0.16205189492025412, Avg Validation Loss: 34.58333472333801\n",
      "Epoch 273/500, Avg Training Loss: 0.16201623100618995, Avg Validation Loss: 34.584165816456945\n",
      "Epoch 274/500, Avg Training Loss: 0.1620412664182596, Avg Validation Loss: 34.5843753919862\n",
      "Epoch 275/500, Avg Training Loss: 0.16203802308800164, Avg Validation Loss: 34.58445911312957\n",
      "Epoch 276/500, Avg Training Loss: 0.16201462322320442, Avg Validation Loss: 34.58322644680963\n",
      "Epoch 277/500, Avg Training Loss: 0.1619957188503172, Avg Validation Loss: 34.584286315129134\n",
      "Epoch 278/500, Avg Training Loss: 0.16199705684825205, Avg Validation Loss: 34.58499873283153\n",
      "Epoch 279/500, Avg Training Loss: 0.16203163373256807, Avg Validation Loss: 34.58371149983033\n",
      "Epoch 280/500, Avg Training Loss: 0.16197041783888558, Avg Validation Loss: 34.584749891348515\n",
      "Epoch 281/500, Avg Training Loss: 0.16199201811033845, Avg Validation Loss: 34.58413337431797\n",
      "Epoch 282/500, Avg Training Loss: 0.16200386100039302, Avg Validation Loss: 34.585586388623575\n",
      "Epoch 283/500, Avg Training Loss: 0.1620136778894431, Avg Validation Loss: 34.5870242851441\n",
      "Epoch 284/500, Avg Training Loss: 0.1620329865155221, Avg Validation Loss: 34.58651421690239\n",
      "Epoch 285/500, Avg Training Loss: 0.1620343620127246, Avg Validation Loss: 34.583279704108335\n",
      "Epoch 286/500, Avg Training Loss: 0.16198424057864336, Avg Validation Loss: 34.584488165341874\n",
      "Epoch 287/500, Avg Training Loss: 0.1619962631040159, Avg Validation Loss: 34.585137532857985\n",
      "Epoch 288/500, Avg Training Loss: 0.1619936932152287, Avg Validation Loss: 34.586381662349325\n",
      "Epoch 289/500, Avg Training Loss: 0.16200092801409072, Avg Validation Loss: 34.5866592290652\n",
      "Epoch 290/500, Avg Training Loss: 0.16203249551710516, Avg Validation Loss: 34.58723469063433\n",
      "Epoch 291/500, Avg Training Loss: 0.1620084159656017, Avg Validation Loss: 34.586431940994636\n",
      "Epoch 292/500, Avg Training Loss: 0.16201772291334607, Avg Validation Loss: 34.58631469946444\n",
      "Epoch 293/500, Avg Training Loss: 0.16201441457197713, Avg Validation Loss: 34.58645492319265\n",
      "Epoch 294/500, Avg Training Loss: 0.1620422202528938, Avg Validation Loss: 34.586043279858146\n",
      "Epoch 295/500, Avg Training Loss: 0.16202019711858381, Avg Validation Loss: 34.58563693611538\n",
      "Epoch 296/500, Avg Training Loss: 0.16204594684114848, Avg Validation Loss: 34.585293884564685\n",
      "Epoch 297/500, Avg Training Loss: 0.16201000194996656, Avg Validation Loss: 34.58609194805159\n",
      "Epoch 298/500, Avg Training Loss: 0.16200188044962244, Avg Validation Loss: 34.58574036734299\n",
      "Epoch 299/500, Avg Training Loss: 0.16200950093439703, Avg Validation Loss: 34.58524018271096\n",
      "Epoch 300/500, Avg Training Loss: 0.16201344261330622, Avg Validation Loss: 34.58617093969364\n",
      "Epoch 301/500, Avg Training Loss: 0.16202586689346635, Avg Validation Loss: 34.586875105832576\n",
      "Epoch 302/500, Avg Training Loss: 0.16200791650816027, Avg Validation Loss: 34.586059376104416\n",
      "Epoch 303/500, Avg Training Loss: 0.16197679892268063, Avg Validation Loss: 34.587288288770736\n",
      "Epoch 304/500, Avg Training Loss: 0.1620083495734109, Avg Validation Loss: 34.58860945062811\n",
      "Epoch 305/500, Avg Training Loss: 0.16199750392703746, Avg Validation Loss: 34.589133918149635\n",
      "Epoch 306/500, Avg Training Loss: 0.16200884083018546, Avg Validation Loss: 34.58874781734765\n",
      "Epoch 307/500, Avg Training Loss: 0.16202663718670465, Avg Validation Loss: 34.5887399069578\n",
      "Epoch 308/500, Avg Training Loss: 0.16200119494406123, Avg Validation Loss: 34.5879786406947\n",
      "Epoch 309/500, Avg Training Loss: 0.16204182978846643, Avg Validation Loss: 34.58803845470991\n",
      "Epoch 310/500, Avg Training Loss: 0.16200320142069646, Avg Validation Loss: 34.589301487407866\n",
      "Epoch 311/500, Avg Training Loss: 0.16201743059418208, Avg Validation Loss: 34.58967577764387\n",
      "Epoch 312/500, Avg Training Loss: 0.16198277807917377, Avg Validation Loss: 34.588945422176664\n",
      "Epoch 313/500, Avg Training Loss: 0.16202228743820352, Avg Validation Loss: 34.59001257965197\n",
      "Epoch 314/500, Avg Training Loss: 0.16201632244097566, Avg Validation Loss: 34.59061790466083\n",
      "Epoch 315/500, Avg Training Loss: 0.16205400707512227, Avg Validation Loss: 34.58904191219631\n",
      "Epoch 316/500, Avg Training Loss: 0.1620021698848609, Avg Validation Loss: 34.5895403739753\n",
      "Epoch 317/500, Avg Training Loss: 0.1620085569606799, Avg Validation Loss: 34.590383256191004\n",
      "Epoch 318/500, Avg Training Loss: 0.16200914277350434, Avg Validation Loss: 34.59041036781225\n",
      "Epoch 319/500, Avg Training Loss: 0.1620076159542755, Avg Validation Loss: 34.59050803211855\n",
      "Epoch 320/500, Avg Training Loss: 0.16203169571095355, Avg Validation Loss: 34.58997633059185\n",
      "Epoch 321/500, Avg Training Loss: 0.16203112174071102, Avg Validation Loss: 34.58930223315994\n",
      "Epoch 322/500, Avg Training Loss: 0.16202564502031966, Avg Validation Loss: 34.58760614880307\n",
      "Epoch 323/500, Avg Training Loss: 0.16200413784979895, Avg Validation Loss: 34.586959925120574\n",
      "Epoch 324/500, Avg Training Loss: 0.16203234755075738, Avg Validation Loss: 34.58861012275716\n",
      "Epoch 325/500, Avg Training Loss: 0.16204182499035918, Avg Validation Loss: 34.58845677850462\n",
      "Epoch 326/500, Avg Training Loss: 0.1620327337235751, Avg Validation Loss: 34.588065704877\n",
      "Epoch 327/500, Avg Training Loss: 0.16204703218288913, Avg Validation Loss: 34.586788906133734\n",
      "Epoch 328/500, Avg Training Loss: 0.16202373535446665, Avg Validation Loss: 34.58551380780915\n",
      "Epoch 329/500, Avg Training Loss: 0.16199663459894556, Avg Validation Loss: 34.58662576062043\n",
      "Epoch 330/500, Avg Training Loss: 0.16201357202286684, Avg Validation Loss: 34.58739046184788\n",
      "Epoch 331/500, Avg Training Loss: 0.16201218857741378, Avg Validation Loss: 34.58692406404136\n",
      "Epoch 332/500, Avg Training Loss: 0.16201497289017705, Avg Validation Loss: 34.586187896456124\n",
      "Epoch 333/500, Avg Training Loss: 0.16206360021964264, Avg Validation Loss: 34.5845046341984\n",
      "Epoch 334/500, Avg Training Loss: 0.16202631285286231, Avg Validation Loss: 34.58301912796225\n",
      "Epoch 335/500, Avg Training Loss: 0.16202861218991452, Avg Validation Loss: 34.5832498335421\n",
      "Epoch 336/500, Avg Training Loss: 0.1620209574512859, Avg Validation Loss: 34.58280452968482\n",
      "Epoch 337/500, Avg Training Loss: 0.16203099457460635, Avg Validation Loss: 34.58405706183706\n",
      "Epoch 338/500, Avg Training Loss: 0.16201284170082933, Avg Validation Loss: 34.58374126608662\n",
      "Epoch 339/500, Avg Training Loss: 0.16202020242259565, Avg Validation Loss: 34.58532897935331\n",
      "Epoch 340/500, Avg Training Loss: 0.16201954158310652, Avg Validation Loss: 34.58631354072503\n",
      "Epoch 341/500, Avg Training Loss: 0.16200213716744422, Avg Validation Loss: 34.58643043500873\n",
      "Epoch 342/500, Avg Training Loss: 0.16200233362994956, Avg Validation Loss: 34.58457814528385\n",
      "Epoch 343/500, Avg Training Loss: 0.1620241050288087, Avg Validation Loss: 34.583129113364\n",
      "Epoch 344/500, Avg Training Loss: 0.1620357248664985, Avg Validation Loss: 34.5835855615451\n",
      "Epoch 345/500, Avg Training Loss: 0.16202617639297112, Avg Validation Loss: 34.58355286049496\n",
      "Epoch 346/500, Avg Training Loss: 0.16201019264394323, Avg Validation Loss: 34.58470410212525\n",
      "Epoch 347/500, Avg Training Loss: 0.16203790342754198, Avg Validation Loss: 34.584056498457684\n",
      "Epoch 348/500, Avg Training Loss: 0.16199263134051262, Avg Validation Loss: 34.58383593403885\n",
      "Epoch 349/500, Avg Training Loss: 0.1619998245663849, Avg Validation Loss: 34.58396389109845\n",
      "Epoch 350/500, Avg Training Loss: 0.1620290557886391, Avg Validation Loss: 34.58425120040883\n",
      "Epoch 351/500, Avg Training Loss: 0.16199484807998274, Avg Validation Loss: 34.585035507122626\n",
      "Epoch 352/500, Avg Training Loss: 0.1620222265115131, Avg Validation Loss: 34.58443891600239\n",
      "Epoch 353/500, Avg Training Loss: 0.16202503841054727, Avg Validation Loss: 34.58387066105752\n",
      "Epoch 354/500, Avg Training Loss: 0.1620011039916892, Avg Validation Loss: 34.584936093187444\n",
      "Epoch 355/500, Avg Training Loss: 0.16203334668240174, Avg Validation Loss: 34.58435657757948\n",
      "Epoch 356/500, Avg Training Loss: 0.16200640009227169, Avg Validation Loss: 34.58312258257776\n",
      "Epoch 357/500, Avg Training Loss: 0.16203147651539157, Avg Validation Loss: 34.583076872765645\n",
      "Epoch 358/500, Avg Training Loss: 0.16202520976152943, Avg Validation Loss: 34.58379742110192\n",
      "Epoch 359/500, Avg Training Loss: 0.1620133816396005, Avg Validation Loss: 34.58439120506057\n",
      "Epoch 360/500, Avg Training Loss: 0.1620430482128485, Avg Validation Loss: 34.58350797454419\n",
      "Epoch 361/500, Avg Training Loss: 0.1620123991928625, Avg Validation Loss: 34.58332385957428\n",
      "Epoch 362/500, Avg Training Loss: 0.16198437501680135, Avg Validation Loss: 34.58374953976872\n",
      "Epoch 363/500, Avg Training Loss: 0.16202265097436058, Avg Validation Loss: 34.58339701478402\n",
      "Epoch 364/500, Avg Training Loss: 0.1619990458523283, Avg Validation Loss: 34.584896961517074\n",
      "Epoch 365/500, Avg Training Loss: 0.16201027075374783, Avg Validation Loss: 34.58624121679303\n",
      "Epoch 366/500, Avg Training Loss: 0.16203227665343675, Avg Validation Loss: 34.58610609036009\n",
      "Epoch 367/500, Avg Training Loss: 0.16201450478794335, Avg Validation Loss: 34.58532788956401\n",
      "Epoch 368/500, Avg Training Loss: 0.16203193707650493, Avg Validation Loss: 34.585299853885175\n",
      "Epoch 369/500, Avg Training Loss: 0.16199625416145513, Avg Validation Loss: 34.58606710882691\n",
      "Epoch 370/500, Avg Training Loss: 0.16200915832939222, Avg Validation Loss: 34.58595158780799\n",
      "Epoch 371/500, Avg Training Loss: 0.16201091770615597, Avg Validation Loss: 34.58726799775536\n",
      "Epoch 372/500, Avg Training Loss: 0.1620029869677311, Avg Validation Loss: 34.58917020227878\n",
      "Epoch 373/500, Avg Training Loss: 0.16202584035556697, Avg Validation Loss: 34.58749097424076\n",
      "Epoch 374/500, Avg Training Loss: 0.16201566347455845, Avg Validation Loss: 34.58879234058165\n",
      "Epoch 375/500, Avg Training Loss: 0.16204218499833023, Avg Validation Loss: 34.587144446906834\n",
      "Epoch 376/500, Avg Training Loss: 0.16203330307990307, Avg Validation Loss: 34.58726618337771\n",
      "Epoch 377/500, Avg Training Loss: 0.1620002832539532, Avg Validation Loss: 34.58745198957901\n",
      "Epoch 378/500, Avg Training Loss: 0.16201970975587096, Avg Validation Loss: 34.588168640455876\n",
      "Epoch 379/500, Avg Training Loss: 0.16201872436402262, Avg Validation Loss: 34.587742403633825\n",
      "Epoch 380/500, Avg Training Loss: 0.16202825238903795, Avg Validation Loss: 34.58822862214032\n",
      "Epoch 381/500, Avg Training Loss: 0.16202728521045817, Avg Validation Loss: 34.58755366536559\n",
      "Epoch 382/500, Avg Training Loss: 0.16201467447582865, Avg Validation Loss: 34.58674343119196\n",
      "Epoch 383/500, Avg Training Loss: 0.16203027911174414, Avg Validation Loss: 34.587055014234025\n",
      "Epoch 384/500, Avg Training Loss: 0.16199929001471602, Avg Validation Loss: 34.58676325950474\n",
      "Epoch 385/500, Avg Training Loss: 0.16204973045860477, Avg Validation Loss: 34.586171041246615\n",
      "Epoch 386/500, Avg Training Loss: 0.16201985040870612, Avg Validation Loss: 34.58777109134876\n",
      "Epoch 387/500, Avg Training Loss: 0.16201542604787542, Avg Validation Loss: 34.5879480655249\n",
      "Epoch 388/500, Avg Training Loss: 0.16202265518376577, Avg Validation Loss: 34.58764778116921\n",
      "Epoch 389/500, Avg Training Loss: 0.16201388446189868, Avg Validation Loss: 34.5857811536546\n",
      "Epoch 390/500, Avg Training Loss: 0.16202149671682256, Avg Validation Loss: 34.58682150343709\n",
      "Epoch 391/500, Avg Training Loss: 0.16201399232108116, Avg Validation Loss: 34.58503731703173\n",
      "Epoch 392/500, Avg Training Loss: 0.16199877451187192, Avg Validation Loss: 34.58624922604125\n",
      "Epoch 393/500, Avg Training Loss: 0.1620574885572713, Avg Validation Loss: 34.58554188901468\n",
      "Epoch 394/500, Avg Training Loss: 0.16199969671075262, Avg Validation Loss: 34.586498075850486\n",
      "Epoch 395/500, Avg Training Loss: 0.162008131320327, Avg Validation Loss: 34.586885959597666\n",
      "Epoch 396/500, Avg Training Loss: 0.16199141515654383, Avg Validation Loss: 34.587375954685605\n",
      "Epoch 397/500, Avg Training Loss: 0.16203914861259558, Avg Validation Loss: 34.5872284664558\n",
      "Epoch 398/500, Avg Training Loss: 0.16205703427111015, Avg Validation Loss: 34.58649742361743\n",
      "Epoch 399/500, Avg Training Loss: 0.16200418144891784, Avg Validation Loss: 34.58763091002669\n",
      "Epoch 400/500, Avg Training Loss: 0.1619845174914134, Avg Validation Loss: 34.58762705527148\n",
      "Epoch 401/500, Avg Training Loss: 0.1620117673970352, Avg Validation Loss: 34.588556857517105\n",
      "Epoch 402/500, Avg Training Loss: 0.16198204644084713, Avg Validation Loss: 34.589784817758925\n",
      "Epoch 403/500, Avg Training Loss: 0.16201091958734867, Avg Validation Loss: 34.590660752764535\n",
      "Epoch 404/500, Avg Training Loss: 0.16202755416483366, Avg Validation Loss: 34.58907065898502\n",
      "Epoch 405/500, Avg Training Loss: 0.16204374705896385, Avg Validation Loss: 34.589905137149756\n",
      "Epoch 406/500, Avg Training Loss: 0.16205869265880846, Avg Validation Loss: 34.58868583798626\n",
      "Epoch 407/500, Avg Training Loss: 0.16205981553230844, Avg Validation Loss: 34.588355257365876\n",
      "Epoch 408/500, Avg Training Loss: 0.16201312700352866, Avg Validation Loss: 34.58805973930032\n",
      "Epoch 409/500, Avg Training Loss: 0.1620550551752248, Avg Validation Loss: 34.58700589211139\n",
      "Epoch 410/500, Avg Training Loss: 0.1620631675734417, Avg Validation Loss: 34.58568955052891\n",
      "Epoch 411/500, Avg Training Loss: 0.16201227453438516, Avg Validation Loss: 34.58666079775887\n",
      "Epoch 412/500, Avg Training Loss: 0.162030107058329, Avg Validation Loss: 34.58724020811562\n",
      "Epoch 413/500, Avg Training Loss: 0.16198220922322243, Avg Validation Loss: 34.58672388738134\n",
      "Epoch 414/500, Avg Training Loss: 0.1620219034939699, Avg Validation Loss: 34.585770083304936\n",
      "Epoch 415/500, Avg Training Loss: 0.16201255557721056, Avg Validation Loss: 34.58546374790241\n",
      "Epoch 416/500, Avg Training Loss: 0.1620022931083228, Avg Validation Loss: 34.585625721100996\n",
      "Epoch 417/500, Avg Training Loss: 0.16199966402568258, Avg Validation Loss: 34.58592071144456\n",
      "Epoch 418/500, Avg Training Loss: 0.1620110760792287, Avg Validation Loss: 34.58587916364861\n",
      "Epoch 419/500, Avg Training Loss: 0.161998981733063, Avg Validation Loss: 34.58461292436554\n",
      "Epoch 420/500, Avg Training Loss: 0.16203059204024836, Avg Validation Loss: 34.584939884688374\n",
      "Epoch 421/500, Avg Training Loss: 0.1620373637076049, Avg Validation Loss: 34.5851954998838\n",
      "Epoch 422/500, Avg Training Loss: 0.16205454535594582, Avg Validation Loss: 34.58588413520033\n",
      "Epoch 423/500, Avg Training Loss: 0.16200929082525095, Avg Validation Loss: 34.58532642486175\n",
      "Epoch 424/500, Avg Training Loss: 0.16203029067382083, Avg Validation Loss: 34.58511302885944\n",
      "Epoch 425/500, Avg Training Loss: 0.16197778572565594, Avg Validation Loss: 34.58631856530306\n",
      "Epoch 426/500, Avg Training Loss: 0.1620261167485386, Avg Validation Loss: 34.586076366475055\n",
      "Epoch 427/500, Avg Training Loss: 0.1620348900988358, Avg Validation Loss: 34.586075879714244\n",
      "Epoch 428/500, Avg Training Loss: 0.16201594532719496, Avg Validation Loss: 34.58527376456976\n",
      "Epoch 429/500, Avg Training Loss: 0.1620212145528402, Avg Validation Loss: 34.58489416036304\n",
      "Epoch 430/500, Avg Training Loss: 0.1620191903377011, Avg Validation Loss: 34.58370199105128\n",
      "Epoch 431/500, Avg Training Loss: 0.1620156654826578, Avg Validation Loss: 34.58442486456951\n",
      "Epoch 432/500, Avg Training Loss: 0.16198314080937412, Avg Validation Loss: 34.58480867182126\n",
      "Epoch 433/500, Avg Training Loss: 0.16201505020859489, Avg Validation Loss: 34.585631169479015\n",
      "Epoch 434/500, Avg Training Loss: 0.1619892489741705, Avg Validation Loss: 34.58603785903731\n",
      "Epoch 435/500, Avg Training Loss: 0.16201110082871775, Avg Validation Loss: 34.58510593968154\n",
      "Epoch 436/500, Avg Training Loss: 0.16201686569030213, Avg Validation Loss: 34.586186298029894\n",
      "Epoch 437/500, Avg Training Loss: 0.16202147740139852, Avg Validation Loss: 34.58686083868078\n",
      "Epoch 438/500, Avg Training Loss: 0.16203691102118023, Avg Validation Loss: 34.58628725436053\n",
      "Epoch 439/500, Avg Training Loss: 0.16198528437474605, Avg Validation Loss: 34.58591814713093\n",
      "Epoch 440/500, Avg Training Loss: 0.16201595103992233, Avg Validation Loss: 34.58512722118\n",
      "Epoch 441/500, Avg Training Loss: 0.1620045598802043, Avg Validation Loss: 34.58475370482435\n",
      "Epoch 442/500, Avg Training Loss: 0.1620572981092487, Avg Validation Loss: 34.58396555682023\n",
      "Epoch 443/500, Avg Training Loss: 0.16204557123963517, Avg Validation Loss: 34.58528885596849\n",
      "Epoch 444/500, Avg Training Loss: 0.16202505081358584, Avg Validation Loss: 34.58541919977268\n",
      "Epoch 445/500, Avg Training Loss: 0.16203677094650068, Avg Validation Loss: 34.58382278283685\n",
      "Epoch 446/500, Avg Training Loss: 0.16201050836787195, Avg Validation Loss: 34.58499058209173\n",
      "Epoch 447/500, Avg Training Loss: 0.16200925255402063, Avg Validation Loss: 34.58549411066664\n",
      "Epoch 448/500, Avg Training Loss: 0.1620150181077193, Avg Validation Loss: 34.585289518369805\n",
      "Epoch 449/500, Avg Training Loss: 0.16199758398007794, Avg Validation Loss: 34.58620322634856\n",
      "Epoch 450/500, Avg Training Loss: 0.1619742526318239, Avg Validation Loss: 34.587557457139084\n",
      "Epoch 451/500, Avg Training Loss: 0.1620001566597474, Avg Validation Loss: 34.58748153301993\n",
      "Epoch 452/500, Avg Training Loss: 0.16199604345589658, Avg Validation Loss: 34.587112012807275\n",
      "Epoch 453/500, Avg Training Loss: 0.16200830670016214, Avg Validation Loss: 34.586370779312844\n",
      "Epoch 454/500, Avg Training Loss: 0.1620169353785631, Avg Validation Loss: 34.585787502603736\n",
      "Epoch 455/500, Avg Training Loss: 0.16199359466547547, Avg Validation Loss: 34.58709753224751\n",
      "Epoch 456/500, Avg Training Loss: 0.16199958704468792, Avg Validation Loss: 34.5872413163697\n",
      "Epoch 457/500, Avg Training Loss: 0.16200950459823546, Avg Validation Loss: 34.58722268072301\n",
      "Epoch 458/500, Avg Training Loss: 0.1620166711253295, Avg Validation Loss: 34.58674490045002\n",
      "Epoch 459/500, Avg Training Loss: 0.16200596356775368, Avg Validation Loss: 34.5867699076565\n",
      "Epoch 460/500, Avg Training Loss: 0.16201616923216056, Avg Validation Loss: 34.58839500959484\n",
      "Epoch 461/500, Avg Training Loss: 0.16202715294373554, Avg Validation Loss: 34.58730396595813\n",
      "Epoch 462/500, Avg Training Loss: 0.16204829263830983, Avg Validation Loss: 34.58708117692312\n",
      "Epoch 463/500, Avg Training Loss: 0.1619829606421589, Avg Validation Loss: 34.58837385103186\n",
      "Epoch 464/500, Avg Training Loss: 0.16202584048091215, Avg Validation Loss: 34.58688806412672\n",
      "Epoch 465/500, Avg Training Loss: 0.16203219751368333, Avg Validation Loss: 34.587275501424244\n",
      "Epoch 466/500, Avg Training Loss: 0.1620034519539896, Avg Validation Loss: 34.586675598512585\n",
      "Epoch 467/500, Avg Training Loss: 0.1620202680057817, Avg Validation Loss: 34.58523756308172\n",
      "Epoch 468/500, Avg Training Loss: 0.16198465959325284, Avg Validation Loss: 34.58744362176126\n",
      "Epoch 469/500, Avg Training Loss: 0.16202273719277266, Avg Validation Loss: 34.58773050307591\n",
      "Epoch 470/500, Avg Training Loss: 0.1620045825991715, Avg Validation Loss: 34.58695622666111\n",
      "Epoch 471/500, Avg Training Loss: 0.1620353589105187, Avg Validation Loss: 34.58615336454646\n",
      "Epoch 472/500, Avg Training Loss: 0.16199402376948466, Avg Validation Loss: 34.58591309210969\n",
      "Epoch 473/500, Avg Training Loss: 0.16200998473503542, Avg Validation Loss: 34.58473696926325\n",
      "Epoch 474/500, Avg Training Loss: 0.16198942748433617, Avg Validation Loss: 34.58582177997171\n",
      "Epoch 475/500, Avg Training Loss: 0.16202775167305347, Avg Validation Loss: 34.58711713209527\n",
      "Epoch 476/500, Avg Training Loss: 0.16202181136729338, Avg Validation Loss: 34.58673330179471\n",
      "Epoch 477/500, Avg Training Loss: 0.16202237813861842, Avg Validation Loss: 34.58665628542728\n",
      "Epoch 478/500, Avg Training Loss: 0.1620328067977505, Avg Validation Loss: 34.58478083948085\n",
      "Epoch 479/500, Avg Training Loss: 0.1620205555532021, Avg Validation Loss: 34.58694563983743\n",
      "Epoch 480/500, Avg Training Loss: 0.1620123023856089, Avg Validation Loss: 34.587064129719\n",
      "Epoch 481/500, Avg Training Loss: 0.1620056661553223, Avg Validation Loss: 34.587677771141244\n",
      "Epoch 482/500, Avg Training Loss: 0.16200518478719547, Avg Validation Loss: 34.587988823587914\n",
      "Epoch 483/500, Avg Training Loss: 0.16203032399241693, Avg Validation Loss: 34.59068132202014\n",
      "Epoch 484/500, Avg Training Loss: 0.16199220515533064, Avg Validation Loss: 34.58998884782817\n",
      "Epoch 485/500, Avg Training Loss: 0.1620335392840778, Avg Validation Loss: 34.59067802227649\n",
      "Epoch 486/500, Avg Training Loss: 0.16201279520741171, Avg Validation Loss: 34.58951409452433\n",
      "Epoch 487/500, Avg Training Loss: 0.16201722296911505, Avg Validation Loss: 34.58770901011617\n",
      "Epoch 488/500, Avg Training Loss: 0.16203118116363296, Avg Validation Loss: 34.58906346853954\n",
      "Epoch 489/500, Avg Training Loss: 0.16202990888684585, Avg Validation Loss: 34.58850394494594\n",
      "Epoch 490/500, Avg Training Loss: 0.16201775159731402, Avg Validation Loss: 34.58801280321311\n",
      "Epoch 491/500, Avg Training Loss: 0.1620148184426816, Avg Validation Loss: 34.5890451904197\n",
      "Epoch 492/500, Avg Training Loss: 0.16198500309029015, Avg Validation Loss: 34.58838625078854\n",
      "Epoch 493/500, Avg Training Loss: 0.16204539621261876, Avg Validation Loss: 34.58920460079212\n",
      "Epoch 494/500, Avg Training Loss: 0.16203419770651703, Avg Validation Loss: 34.58901671997393\n",
      "Epoch 495/500, Avg Training Loss: 0.16200665643695675, Avg Validation Loss: 34.58890511831896\n",
      "Epoch 496/500, Avg Training Loss: 0.16201578694230506, Avg Validation Loss: 34.5886318931744\n",
      "Epoch 497/500, Avg Training Loss: 0.16199510615303084, Avg Validation Loss: 34.58835668082162\n",
      "Epoch 498/500, Avg Training Loss: 0.16204202559080738, Avg Validation Loss: 34.587114801775996\n",
      "Epoch 499/500, Avg Training Loss: 0.16204640426798128, Avg Validation Loss: 34.586701430162876\n",
      "Epoch 500/500, Avg Training Loss: 0.16199183579866655, Avg Validation Loss: 34.58866028919326\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGwCAYAAABPSaTdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4IElEQVR4nO3de3wU1f3/8ffmtrmQhHDLJiVAkKjcBEmUEqwEuYnWSrFfL6CC4IVrSVFRRAWtJEoFUVEsfhWoSqEU8au1AkEFUeQngii3UtGIVEijgEkgsIHk/P6IGVnCJYHszjK8no/HPHb3zOzsZ4/RfXvmzIzLGGMEAADgUCF2FwAAAOBPhB0AAOBohB0AAOBohB0AAOBohB0AAOBohB0AAOBohB0AAOBoYXYXEAwqKiq0a9cuxcbGyuVy2V0OAACoAWOMSkpKlJycrJCQE4/fEHYk7dq1SykpKXaXAQAATsPOnTvVtGnTE64n7EiKjY2VVNlZcXFxNlcDAABqori4WCkpKdbv+IkQdiTr0FVcXBxhBwCAs8yppqAwQRkAADgaYQcAADgaYQcAADgac3YAALVWUVGhsrIyu8uAw4WHhys0NPSM90PYAQDUSllZmfLz81VRUWF3KTgH1K9fXx6P54yug0fYAQDUmDFGu3fvVmhoqFJSUk56ITfgTBhjVFpaqsLCQklSUlLSae+LsAMAqLEjR46otLRUycnJio6OtrscOFxUVJQkqbCwUE2aNDntQ1pEcgBAjZWXl0uSIiIibK4E54qqUH348OHT3gdhBwBQa9xHEIFSF39rhB0AAOBohB0AAOBohB0AAE5DVlaWsrOza7z9N998I5fLpQ0bNvitJhwfYcefSvdK+3ZIB3+0uxIAOGe5XK6TLoMHDz6t/b7++uv64x//WOPtU1JStHv3brVr1+60Pq+mqkLV8ZY1a9b49bODFaee+9PySdL6uVL3B6Vu99pdDQCck3bv3m09X7BggR5++GFt27bNaqs6vbnK4cOHFR4efsr9NmjQoFZ1hIaGyuPx1Oo9Z2L58uVq27atT1vDhg2Pu+2JvnNN+6Ku3ucvjOz4U8hP1wOoOGJvHQDgJ8YYlZYdsWUxxtSoRo/HYy3x8fFyuVzW60OHDql+/fr629/+pqysLEVGRurVV1/Vnj17dNNNN6lp06aKjo5W+/bt9de//tVnv8cexmrRooVycnI0ZMgQxcbGqlmzZpo1a5a1/tjDWCtWrJDL5dK7776rjIwMRUdHKzMz0yeISdJjjz2mJk2aKDY2Vrfffrvuv/9+dezY8ZTfu2HDhj7f3ePxWAFk0qRJ6tixo15++WW1bNlSbrdbxhi5XC698MILuvbaaxUTE6PHHntMkjRz5kydd955ioiI0AUXXKBXXnnF57NO9L5gwciOP4X81L2m3N46AMBPDh4uV5uHl9ry2Vse7aPoiLr5Gbvvvvs0depUzZ49W263W4cOHVJ6erruu+8+xcXF6e2339Ytt9yili1bqnPnzifcz9SpU/XHP/5RDzzwgP7+979r+PDhuvzyy3XhhRee8D0TJkzQ1KlT1bhxYw0bNkxDhgzRRx99JEl67bXXNHnyZD3//PPq2rWr5s+fr6lTpyo1NfWMv/P27dv1t7/9TYsWLfK5WN/EiROVm5urp556SqGhoVq8eLHGjBmj6dOnq2fPnvrHP/6h2267TU2bNlX37t1P+L5gQtjxp6qww8gOAAS17Oxs9e/f36ftnnvusZ6PHj1aS5Ys0cKFC08adq666iqNGDFCUmWAeuqpp7RixYqThp3JkyerW7dukqT7779fV199tQ4dOqTIyEg9++yzGjp0qG677TZJ0sMPP6xly5Zp//79p/xOmZmZ1W7nUVRUZAWRsrIyvfLKK2rcuLHPNgMGDNCQIUN8Xg8ePNj6XmPHjtWaNWv05JNP+oSdY98XTAg7/uT66Y+MsAPAoaLCQ7Xl0T62fXZdycjI8HldXl6uxx9/XAsWLNB3330nr9crr9ermJiYk+7noosusp5XHS6rurdTTd5Tdf+nwsJCNWvWTNu2bbNCRpVLL71U77333im/04IFC9S6dWuftqNHXJo3b14t6EjV+2Lr1q268847fdq6du2qp59++qTvCyaEHX+yRna4MzAAZ3K5XHV2KMlOx4aYqVOn6qmnntL06dPVvn17xcTEKDs7W2VlZSfdz7GTcl0u1ynvDn/0e6quFnz0e469gnBN5yqlpKSoVatWJ1x/ouB2vPbj1XBs26mCoJ2YoOxPHMYCgLPSqlWrdO211+rmm29Whw4d1LJlS3355ZcBr+OCCy7QJ5984tP26aefBrSG1q1b68MPP/RpW716dbVRo2B29sfxYMbZWABwVmrVqpUWLVqk1atXKyEhQdOmTVNBQUHAf+BHjx6tO+64QxkZGcrMzNSCBQv0xRdfqGXLlqd87549e1RQUODTVr9+fUVGRtaqhnvvvVfXX3+9OnXqpB49euitt97S66+/ruXLl9dqP3aydWTngw8+0DXXXKPk5GS5XC698cYbPuuNMZo0aZKSk5MVFRWlrKwsbd682Wcbr9er0aNHq1GjRoqJidFvfvMb/ec//wngtzgJzsYCgLPSQw89pE6dOqlPnz7KysqSx+NRv379Al7HwIEDNX78eN1zzz3q1KmT8vPzNXjw4BoFlp49eyopKclnOfZ3tib69eunp59+Wn/605/Utm1b/fnPf9bs2bOVlZVV+y9kE5ep6cE/P3jnnXf00UcfqVOnTrruuuu0ePFinz+mJ554QpMnT9acOXN0/vnn67HHHtMHH3ygbdu2KTY2VpI0fPhwvfXWW5ozZ44aNmyou+++W3v37tW6detqfOpbcXGx4uPjVVRUpLi4uLr7gh88Kb33R+nim6Vrn6u7/QKATQ4dOqT8/HylpqbWeoQAdaNXr17yeDzVrnXjVCf7m6vp77eth7H69u2rvn37HnedMUbTp0/XhAkTrNMB586dq8TERM2bN0933XWXioqK9NJLL+mVV15Rz549JUmvvvqqUlJStHz5cvXpY88ZAhZrzg4jOwCA2istLdULL7ygPn36KDQ0VH/961+1fPly5eXl2V3aWSVoJyjn5+eroKBAvXv3ttrcbre6deum1atXS5LWrVunw4cP+2yTnJysdu3aWdscj9frVXFxsc/iF4QdAMAZcLlc+uc//6lf/epXSk9P11tvvaVFixZZ/4OPmgnaCcpVk6oSExN92hMTE7Vjxw5rm4iICCUkJFTb5thJWUfLzc3VI488UscVHwcTlAEAZyAqKuqsmggcrIJ2ZKdKTc7tP9apthk/fryKioqsZefOnXVSazWceg4AgO2CNuxU3Rn22BGawsJCa7TH4/GorKxM+/btO+E2x+N2uxUXF+ez+EXVyI7hooIAANglaMNOamqqPB6PzySssrIyrVy5UpmZmZKk9PR0hYeH+2yze/dubdq0ydrGVi4OYwEAYDdb5+zs379f27dvt17n5+drw4YNatCggZo1a6bs7Gzl5OQoLS1NaWlpysnJUXR0tAYMGCBJio+P19ChQ3X33XerYcOGatCgge655x61b98+OCZvcRgLAADb2Rp2Pv30U587po4dO1aSNGjQIM2ZM0fjxo3TwYMHNWLECO3bt0+dO3fWsmXLrGvsSNJTTz2lsLAwXX/99Tp48KB69OihOXPmBMft5TkbCwAA29l6GCsrK0vGmGrLnDlzJFVOTp40aZJ2796tQ4cOaeXKlWrXrp3PPiIjI/Xss89qz549Ki0t1VtvvaWUlBQbvs1xcDYWADhGVlaWsrOzrdctWrTQ9OnTT/qe490d4HTU1X7OVUE7Z8cRrLDDyA4A2OWaa6454dSGjz/+WC6XS+vXr6/1fteuXas777zzTMvzMWnSJHXs2LFa++7du094Ed66MmfOHLlcrmqLE66UHbTX2XEE7o0FALYbOnSo+vfvrx07dqh58+Y+615++WV17NhRnTp1qvV+GzduXFclnlLVGcr+FhcXp23btvm0nexSLmVlZYqIiPBpM8aovLxcYWG1ixin+76aYGTHnzgbCwBs9+tf/1pNmjSxpkhUKS0t1YIFCzR06FDt2bNHN910k5o2baro6Gi1b99ef/3rX0+632MPY3355Ze6/PLLFRkZqTZt2hz3lg733Xefzj//fEVHR6tly5Z66KGHdPjwYUmVIyuPPPKIPv/8c2tU5ehpHUcfxtq4caOuuOIKRUVFqWHDhrrzzju1f/9+a/3gwYPVr18/Pfnkk0pKSlLDhg01cuRI67NOxOVyyePx+CxHX8olKytLo0aN0tixY9WoUSP16tVLK1askMvl0tKlS5WRkSG3261Vq1bJ6/Xq97//vZo0aaLIyEhddtllWrt2rbWvE73PHxjZ8SfOxgLgdMZIh0vt+ezwaOkUF5mVpLCwMN16662aM2eOHn74YWukYuHChSorK9PAgQNVWlqq9PR03XfffYqLi9Pbb7+tW265RS1btlTnzp1P+RkVFRXq37+/GjVqpDVr1qi4uNhnfk+V2NhYzZkzR8nJydq4caPuuOMOxcbGaty4cbrhhhu0adMmLVmyxLpqcnx8fLV9lJaW6sorr9Qvf/lLrV27VoWFhbr99ts1atQon0D3/vvvKykpSe+//762b9+uG264QR07dtQdd9xxyu9zMnPnztXw4cP10UcfyRhjXQ9v3LhxevLJJ9WyZUvVr19f48aN06JFizR37lw1b95cU6ZMUZ8+fbR9+3Y1aNDA2t+x7/MHwo4/WXN2uKggAIc6XCrlJNvz2Q/skiJiarTpkCFD9Kc//UkrVqywzgJ++eWX1b9/fyUkJCghIUH33HOPtf3o0aO1ZMkSLVy4sEZhZ/ny5dq6dau++eYbNW3aVJKUk5NTbZ7Ngw8+aD1v0aKF7r77bi1YsEDjxo1TVFSU6tWrp7CwsJMetnrttdd08OBB/eUvf1FMTOX3nzFjhq655ho98cQT1khMQkKCZsyYodDQUF144YW6+uqr9e6775407BQVFalevXo+bZmZmVq2bJn1ulWrVpoyZYr1uirsPProo+rVq5ck6cCBA5o5c6bmzJlj9cGLL76ovLw8vfTSS7r33nut9x/9Pn8h7PgTZ2MBQFC48MILlZmZqZdfflndu3fXV199pVWrVlk/4uXl5Xr88ce1YMECfffdd/J6vfJ6vVaYOJWtW7eqWbNmVtCRpC5dulTb7u9//7umT5+u7du3a//+/Tpy5Eitr+K/detWdejQwae2rl27qqKiQtu2bbPCTtu2bX0uw5KUlKSNGzeedN+xsbHVJmtHRUX5vM7IyDjue49u/+qrr3T48GF17drVagsPD9ell16qrVu31mh/dYmw408cxgLgdOHRlSMsdn12LQwdOlSjRo3Sc889p9mzZ6t58+bq0aOHJGnq1Kl66qmnNH36dLVv314xMTHKzs5WWVlZjfZtjKnWduzE3jVr1ujGG2/UI488oj59+ig+Pl7z58/X1KlTa/U9Tnb/x6Pbw8PDq62rOMWRhpCQELVq1eqk25woAB7dXtUfNbm/ZU0D5ZlggrI/cTYWAKdzuSoPJdmx1GC+ztGuv/56hYaGat68eZo7d65uu+0264d31apVuvbaa3XzzTerQ4cOatmypb788ssa77tNmzb69ttvtWvXz8Hv448/9tnmo48+UvPmzTVhwgRlZGQoLS1NO3bs8NkmIiJC5eUn/81o06aNNmzYoAMHDvjsOyQkROeff36Na/anVq1aKSIiQh9++KHVdvjwYX366adq3bp1wOsh7PgTZ2MBQNCoV6+ebrjhBj3wwAPatWuXBg8ebK1r1aqV8vLytHr1am3dulV33XVXtRtRn0zPnj11wQUX6NZbb9Xnn3+uVatWacKECT7btGrVSt9++63mz5+vr776Ss8884wWL17ss02LFi2sWyf98MMP8nq91T5r4MCBioyM1KBBg7Rp0ya9//77Gj16tG655ZaT3gS7JqomHB+7nGpE6FgxMTEaPny47r33Xi1ZskRbtmzRHXfcodLSUg0dOvSMajwdhB1/4qKCABBUhg4dqn379qlnz55q1qyZ1f7QQw+pU6dO6tOnj7KysuTxeNSvX78a7zckJESLFy+W1+vVpZdeqttvv12TJ0/22ebaa6/VH/7wB40aNUodO3bU6tWr9dBDD/lsc9111+nKK69U9+7d1bhx4+Oe/h4dHa2lS5dq7969uuSSS/S73/1OPXr00IwZM2rXGcdRXFyspKSkakthYWGt9/X444/ruuuu0y233KJOnTpp+/btWrp0qRISEs64ztpymeMdaDzHFBcXKz4+XkVFRbWeKHZSu7+Q/vwrqZ5HumfbqbcHgCB36NAh5efnKzU11RFX1kXwO9nfXE1/vxnZ8SfOxgIAwHaEHX9igjIAALYj7PiTdeo5YQcAALsQdvzJ9VP3chgLAADbEHb8iZEdAA7FuS0IlLr4WyPs+BNXUAbgMFW3H6jplYWBM1VaWnmj2WOvCF0b3C7Cn6rOxjLllXcGruXVPgEg2ISFhSk6Olrff/+9wsPDFRLC/zPDP4wxKi0tVWFhoerXr+9zn6/aIuz4U8hR3Wsqfr6iMgCcpVwul5KSkpSfn1/tVgeAP9SvX/+kd4GvCcKOP4UcFW4qjvi+BoCzVEREhNLS0jiUBb8LDw8/oxGdKoQdf3IdE3bktq0UAKhLISEhXEEZZw0OtvrT0YexOCMLAABbEHb8ySfscEYWAAB2IOz4k8+cHUZ2AACwA2HHn1yun6+izP2xAACwBWHH37iwIAAAtiLs+FvVGVmEHQAAbEHY8TfujwUAgK0IO/5WNUmZsAMAgC0IO/4WwmEsAADsRNjxt6rDWJyNBQCALQg7/sbZWAAA2Iqw428u5uwAAGAnwo6/MUEZAABbEXb8jcNYAADYirDjb5yNBQCArQg7/sbZWAAA2Iqw42/M2QEAwFaEHX/j3lgAANiKsONv3BsLAABbEXb8jbOxAACwFWHH3zgbCwAAWxF2/K0q7JgKe+sAAOAcRdjxNw5jAQBgK8KOv3E2FgAAtiLs+BsjOwAA2Iqw42+hP4WdcsIOAAB2IOz4W0h45WPFYXvrAADgHEXY8bfQn8JOOWEHAAA7EHb8zZqzQ9gBAMAOhB1/s0Z2mLMDAIAdCDv+FhpR+VheZm8dAACcowg7/sZhLAAAbEXY8TcOYwEAYKugDjtHjhzRgw8+qNTUVEVFRally5Z69NFHVVHx832mjDGaNGmSkpOTFRUVpaysLG3evNnGqo/BqecAANgqqMPOE088oRdeeEEzZszQ1q1bNWXKFP3pT3/Ss88+a20zZcoUTZs2TTNmzNDatWvl8XjUq1cvlZSU2Fj5UTj1HAAAWwV12Pn444917bXX6uqrr1aLFi30u9/9Tr1799ann34qqXJUZ/r06ZowYYL69++vdu3aae7cuSotLdW8efNsrv4n3C4CAABbBXXYueyyy/Tuu+/q3//+tyTp888/14cffqirrrpKkpSfn6+CggL17t3beo/b7Va3bt20evXqE+7X6/WquLjYZ/EbRnYAALBVmN0FnMx9992noqIiXXjhhQoNDVV5ebkmT56sm266SZJUUFAgSUpMTPR5X2Jionbs2HHC/ebm5uqRRx7xX+FHY84OAAC2CuqRnQULFujVV1/VvHnztH79es2dO1dPPvmk5s6d67Ody+XyeW2MqdZ2tPHjx6uoqMhadu7c6Zf6JR11I1DCDgAAdgjqkZ17771X999/v2688UZJUvv27bVjxw7l5uZq0KBB8ng8kipHeJKSkqz3FRYWVhvtOZrb7Zbb7fZv8VWskR3m7AAAYIegHtkpLS1VSIhviaGhodap56mpqfJ4PMrLy7PWl5WVaeXKlcrMzAxorSfEnB0AAGwV1CM711xzjSZPnqxmzZqpbdu2+uyzzzRt2jQNGTJEUuXhq+zsbOXk5CgtLU1paWnKyclRdHS0BgwYYHP1P2HODgAAtgrqsPPss8/qoYce0ogRI1RYWKjk5GTdddddevjhh61txo0bp4MHD2rEiBHat2+fOnfurGXLlik2NtbGyo9izdnhMBYAAHZwGWOM3UXYrbi4WPHx8SoqKlJcXFzd7nzrP6QFA6Wml0i3L6/bfQMAcA6r6e93UM/ZcQTm7AAAYCvCjr+FcjYWAAB2Iuz4WwgjOwAA2Imw42+hnI0FAICdCDv+Zo3scBgLAAA7EHb8rerUc0Z2AACwBWHH35izAwCArQg7/sap5wAA2Iqw428hHMYCAMBOhB1/Y2QHAABbEXb87egbgXJnDgAAAo6w429VIzuSVFFuXx0AAJyjCDv+FnLUjeWZtwMAQMARdvzt6JEd5u0AABBwhB1/Czn6MBZXUQYAINAIO/4WEvrzc0Z2AAAIOMKOv7lcUmhE5XPm7AAAEHCEnUDglhEAANiGsBMI1s1AmbMDAECgEXYCgZEdAABsQ9gJhNCjrqIMAAACirATCNbIDoexAAAINMJOIIRy53MAAOxC2AkE5uwAAGAbwk4gMGcHAADbEHYCoepmoIzsAAAQcISdQKi6gnJ5mb11AABwDiLsBEKYu/LxiNfeOgAAOAcRdgKBkR0AAGxD2AkERnYAALANYScQGNkBAMA2hJ1AqBrZIewAABBwhJ1AqBrZ4TAWAAABR9gJBEZ2AACwDWEnEEKZoAwAgF0IO4EQxgRlAADsQtgJBEZ2AACwDWEnEKyRHcIOAACBRtgJBGtkh8NYAAAEGmEnEKyzsRjZAQAg0Ag7gWBdZ4eRHQAAAo2wEwiM7AAAYBvCTiAwsgMAgG0IO4HAyA4AALYh7AQC19kBAMA2hJ1A4ArKAADYhrATCNz1HAAA2xB2AiGUkR0AAOxC2AmEMObsAABgF8JOIFRNUGZkBwCAgCPsBEIYc3YAALALYScQQo+6zo4x9tYCAMA5hrATCFUjO5JUfti+OgAAOAcRdgKhamRH4irKAAAEGGEnEMKOCjvcHwsAgIAK+rDz3Xff6eabb1bDhg0VHR2tjh07at26ddZ6Y4wmTZqk5ORkRUVFKSsrS5s3b7ax4uMICZVcoZXPOSMLAICACuqws2/fPnXt2lXh4eF65513tGXLFk2dOlX169e3tpkyZYqmTZumGTNmaO3atfJ4POrVq5dKSkrsK/x4uBkoAAC2CLO7gJN54oknlJKSotmzZ1ttLVq0sJ4bYzR9+nRNmDBB/fv3lyTNnTtXiYmJmjdvnu66667j7tfr9crr/Tl0FBcX++cLHC00QjpcymEsAAACLKhHdt58801lZGTof/7nf9SkSRNdfPHFevHFF631+fn5KigoUO/eva02t9utbt26afXq1Sfcb25uruLj460lJSXFr99DEiM7AADYJKjDztdff62ZM2cqLS1NS5cu1bBhw/T73/9ef/nLXyRJBQUFkqTExESf9yUmJlrrjmf8+PEqKiqylp07d/rvS1SpCjuHD/n/swAAgCWoD2NVVFQoIyNDOTk5kqSLL75Ymzdv1syZM3Xrrbda27lcLp/3GWOqtR3N7XbL7XafcL1fhEdXPh45GNjPBQDgHBfUIztJSUlq06aNT1vr1q317bffSpI8Ho8kVRvFKSwsrDbaY7vwqMrHw4QdAAACKajDTteuXbVt2zaftn//+99q3ry5JCk1NVUej0d5eXnW+rKyMq1cuVKZmZkBrfWUwgg7AADYIagPY/3hD39QZmamcnJydP311+uTTz7RrFmzNGvWLEmVh6+ys7OVk5OjtLQ0paWlKScnR9HR0RowYIDN1R+DkR0AAGwR1GHnkksu0eLFizV+/Hg9+uijSk1N1fTp0zVw4EBrm3HjxungwYMaMWKE9u3bp86dO2vZsmWKjY21sfLjqAo7zNkBACCgXMZwG+7i4mLFx8erqKhIcXFx/vmQRbdLGxdKfXKkLiP98xkAAJxDavr7HdRzdhzFOoxVam8dAACcYwg7gVJ16jnX2QEAIKAIO4ESFln5yARlAAACirATKFxUEAAAWxB2AiWckR0AAOxA2AkUa84OE5QBAAikWoedI0eOKCwsTJs2bfJHPc5lnY3FBGUAAAKp1mEnLCxMzZs3V3l5uT/qcS5rgjIjOwAABNJpHcZ68MEHNX78eO3du7eu63Eu6zAWc3YAAAik07pdxDPPPKPt27crOTlZzZs3V0xMjM/69evX10lxjmLdLoLDWAAABNJphZ1+/frVcRnnAK6gDACALU4r7EycOLGu63A+JigDAGCLM7rr+bp167R161a5XC61adNGF198cV3V5Tyceg4AgC1OK+wUFhbqxhtv1IoVK1S/fn0ZY1RUVKTu3btr/vz5aty4cV3XefbjdhEAANjitM7GGj16tIqLi7V582bt3btX+/bt06ZNm1RcXKzf//73dV2jM1SN7JR7pYoKe2sBAOAcclojO0uWLNHy5cvVunVrq61NmzZ67rnn1Lt37zorzlGq5uxIlffHiog58bYAAKDOnNbITkVFhcLDw6u1h4eHq4JRi+OrOowlcSgLAIAAOq2wc8UVV2jMmDHatWuX1fbdd9/pD3/4g3r06FFnxTlKSIgU9tPoTtkBe2sBAOAcclphZ8aMGSopKVGLFi103nnnqVWrVkpNTVVJSYmeffbZuq7ROdz1Kh/L9ttbBwAA55DTmrOTkpKi9evXKy8vT//6179kjFGbNm3Us2fPuq7PWSLqSQe+l7wldlcCAMA5o9Zh58iRI4qMjNSGDRvUq1cv9erVyx91OZM7tvLRy8gOAACBwl3PA6kq7JQxsgMAQKBw1/NAivhpzg6HsQAACBjueh5IVROUOYwFAEDAcNfzQLIOYxF2AAAIlNOaoCxJQ4YMUUpKSp0X5GjWYaxie+sAAOAccloTlJ988kkmKJ8OzsYCACDgTmuCco8ePbRixYo6LuUcEMFFBQEACLTTmrPTt29fjR8/Xps2bVJ6enq1Ccq/+c1v6qQ4x2FkBwCAgDutsDN8+HBJ0rRp06qtc7lcHOI6ETenngMAEGinFXa4s/lpiuCiggAABFqt5uxcddVVKioqsl5PnjxZP/74o/V6z549atOmTZ0V5zhcZwcAgICrVdhZunSpvF6v9fqJJ57wuYrykSNHtG3btrqrzmm4gjIAAAFXq7BjjDnpa5wCFxUEACDgTuvUc5wmd1zl4+FSqfyIvbUAAHCOqFXYcblccrlc1dpQQ5HxPz8/9KNtZQAAcC6p1dlYxhgNHjxYbrdbknTo0CENGzbMus7O0fN5cByhYZWjO95i6eA+KaaR3RUBAOB4tQo7gwYN8nl98803V9vm1ltvPbOKnC4q4eewAwAA/K5WYWf27Nn+quPcEZUg/biDsAMAQIAwQTnQohIqH0v3nnw7AABQJwg7gRbdoPKRkR0AAAKCsBNoVSM7hB0AAAKCsBNohB0AAAKKsBNoURzGAgAgkAg7gWaN7DBBGQCAQCDsBBqHsQAACCjCTqBxNhYAAAFF2Am06IaVjwd+sLcOAADOEYSdQKuXWPlYtl/y7re3FgAAzgGEnUBz15Mi6lU+3/9fe2sBAOAcQNixQ9XoTslue+sAAOAcQNixQ6yn8rGkwN46AAA4BxB27FAVdjiMBQCA3xF27FCvamSHw1gAAPjbWRV2cnNz5XK5lJ2dbbUZYzRp0iQlJycrKipKWVlZ2rx5s31F1kRs1ZwdRnYAAPC3sybsrF27VrNmzdJFF13k0z5lyhRNmzZNM2bM0Nq1a+XxeNSrVy+VlJTYVGkNVI3s7GfODgAA/nZWhJ39+/dr4MCBevHFF5WQkGC1G2M0ffp0TZgwQf3791e7du00d+5clZaWat68eSfcn9frVXFxsc8SUHFJlY9F3wX2cwEAOAedFWFn5MiRuvrqq9WzZ0+f9vz8fBUUFKh3795Wm9vtVrdu3bR69eoT7i83N1fx8fHWkpKS4rfajyuhReXjj99KFeWB/WwAAM4xQR925s+fr/Xr1ys3N7fauoKCysNAiYmJPu2JiYnWuuMZP368ioqKrGXnzp11W/SpxP1CCgmXKg5LxYzuAADgT2F2F3AyO3fu1JgxY7Rs2TJFRkaecDuXy+Xz2hhTre1obrdbbre7zuqstZBQKaG5tGe7tDdfqt/MvloAAHC4oB7ZWbdunQoLC5Wenq6wsDCFhYVp5cqVeuaZZxQWFmaN6Bw7ilNYWFhttCfoJKRWPu7Lt7cOAAAcLqjDTo8ePbRx40Zt2LDBWjIyMjRw4EBt2LBBLVu2lMfjUV5envWesrIyrVy5UpmZmTZWXgMNfgo7ewk7AAD4U1AfxoqNjVW7du182mJiYtSwYUOrPTs7Wzk5OUpLS1NaWppycnIUHR2tAQMG2FFyzVVNUt77ta1lAADgdEEddmpi3LhxOnjwoEaMGKF9+/apc+fOWrZsmWJjY+0u7eQaXVD5+P2/7K0DAACHcxljjN1F2K24uFjx8fEqKipSXFxcYD605L/S1PMluaQHvpMiYgLzuQAAOERNf7+Des6Oo8UmSjFNJBmpcKvd1QAA4FiEHTt52lc+Fnxhbx0AADgYYcdOVWFn1wZbywAAwMkIO3ZKubTycceJb20BAADODGHHTs27Sq4Qac+XUvEuu6sBAMCRCDt2iqovJXWsfJ7/gZ2VAADgWIQdu7XMqnz81z9sLQMAAKci7Nit3XWVj/9eKh380dZSAABwIsKO3TztpCZtpPIyaeNCu6sBAMBxCDvBIP22ysePnpaOlNlbCwAADkPYCQadbpXqeaSindLHz9pdDQAAjkLYCQbhkVLPSZXP38+VvvnQ1nIAAHASwk6w6HCj1KafVHFYeu166fMFEvdoBQDgjBF2goXLJf32z9J5PaTDB6TFd0p//pW0eoZUsJG5PAAAnKYwuwvAUcIjpQF/kz6cJq2aVhlyCjZWrnOFSjGNpOhGUlSCFBYhhbql0HApzF15JWaL66cHl+/rE7b56fsAjsa/OECtdLhJatHVlo8m7ASb0DCp2zgpY6i0ZbG0+Q1p9xeSt0ja/9/KBQCAs80v0gk7OEZMQ+mS2ysXY6SSAulAoXTge+lQkVR+uPLaPEe8lY/W/J6fHo99fco2ADXHvzdArf2ik20fTdg5G7hcUlxS5QIAAGqFCcoAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRCDsAAMDRgjrs5Obm6pJLLlFsbKyaNGmifv36adu2bT7bGGM0adIkJScnKyoqSllZWdq8ebNNFQMAgGAT1GFn5cqVGjlypNasWaO8vDwdOXJEvXv31oEDB6xtpkyZomnTpmnGjBlau3atPB6PevXqpZKSEhsrBwAAwcJljDF2F1FT33//vZo0aaKVK1fq8ssvlzFGycnJys7O1n333SdJ8nq9SkxM1BNPPKG77rrruPvxer3yer3W6+LiYqWkpKioqEhxcXEB+S4AAODMFBcXKz4+/pS/30E9snOsoqIiSVKDBg0kSfn5+SooKFDv3r2tbdxut7p166bVq1efcD+5ubmKj4+3lpSUFP8WDgAAbHPWhB1jjMaOHavLLrtM7dq1kyQVFBRIkhITE322TUxMtNYdz/jx41VUVGQtO3fu9F/hAADAVmF2F1BTo0aN0hdffKEPP/yw2jqXy+Xz2hhTre1obrdbbre7zmsEAADB56wY2Rk9erTefPNNvf/++2ratKnV7vF4JKnaKE5hYWG10R4AAHBuCuqwY4zRqFGj9Prrr+u9995Tamqqz/rU1FR5PB7l5eVZbWVlZVq5cqUyMzMDXS4AAAhCQX0Ya+TIkZo3b57+7//+T7GxsdYITnx8vKKiouRyuZSdna2cnBylpaUpLS1NOTk5io6O1oABA2yuHgAABIOgDjszZ86UJGVlZfm0z549W4MHD5YkjRs3TgcPHtSIESO0b98+de7cWcuWLVNsbGyAqwUAAMHorLrOjr/U9Dx9AAAQPBx5nR0AAIDaIuwAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHI+wAAABHc0zYef7555WamqrIyEilp6dr1apVdpekT7/Zq4fe2KTthfvtLgUAgHNWmN0F1IUFCxYoOztbzz//vLp27ao///nP6tu3r7Zs2aJmzZrZVtf/rsrXks0FemXNDqU2ilF68wSlJETrFwlRSogOV1R4qCIjQhUZFqrQEJdcLinEJUkuhbikEFdVm0uS9NMDAABnnfrREarntid2uIwxxpZPrkOdO3dWp06dNHPmTKutdevW6tevn3Jzc6tt7/V65fV6rdfFxcVKSUlRUVGR4uLi6qyu1V/9oJc/zNd7/ypUxVnfywAAnL6c37bXgM51OwBRXFys+Pj4U/5+n/UjO2VlZVq3bp3uv/9+n/bevXtr9erVx31Pbm6uHnnkEb/XlnleI2We10jFhw7r/329V1t3F2vXjwf13Y8HVXzwsA4eLldpWbkOHa6QMUYVxshIqqiofJTRz21nfyYFLPw5A+eeUBsnzpz1YeeHH35QeXm5EhMTfdoTExNVUFBw3PeMHz9eY8eOtV5Xjez4S1xkuHq1SVSvNomn3hgAANSpsz7sVHEdM6HFGFOtrYrb7Zbb7Q5EWQAAwGZn/dlYjRo1UmhoaLVRnMLCwmqjPQAA4Nxz1oediIgIpaenKy8vz6c9Ly9PmZmZNlUFAACChSMOY40dO1a33HKLMjIy1KVLF82aNUvffvuthg0bZndpAADAZo4IOzfccIP27NmjRx99VLt371a7du30z3/+U82bN7e7NAAAYDNHXGfnTNX0PH0AABA8avr7fdbP2QEAADgZwg4AAHA0wg4AAHA0wg4AAHA0wg4AAHA0wg4AAHA0wg4AAHA0wg4AAHA0R1xB+UxVXVexuLjY5koAAEBNVf1un+r6yIQdSSUlJZKklJQUmysBAAC1VVJSovj4+BOu53YRkioqKrRr1y7FxsbK5XLV2X6Li4uVkpKinTt3chsKP6OvA4N+Dhz6OjDo58DwVz8bY1RSUqLk5GSFhJx4Zg4jO5JCQkLUtGlTv+0/Li6Of4kChL4ODPo5cOjrwKCfA8Mf/XyyEZ0qTFAGAACORtgBAACORtjxI7fbrYkTJ8rtdttdiuPR14FBPwcOfR0Y9HNg2N3PTFAGAACOxsgOAABwNMIOAABwNMIOAABwNMIOAABwNMKOHz3//PNKTU1VZGSk0tPTtWrVKrtLOqt88MEHuuaaa5ScnCyXy6U33njDZ70xRpMmTVJycrKioqKUlZWlzZs3+2zj9Xo1evRoNWrUSDExMfrNb36j//znPwH8FsEvNzdXl1xyiWJjY9WkSRP169dP27Zt89mGvj5zM2fO1EUXXWRdVK1Lly565513rPX0sX/k5ubK5XIpOzvbaqOv68akSZPkcrl8Fo/HY60Pqn428Iv58+eb8PBw8+KLL5otW7aYMWPGmJiYGLNjxw67Sztr/POf/zQTJkwwixYtMpLM4sWLfdY//vjjJjY21ixatMhs3LjR3HDDDSYpKckUFxdb2wwbNsz84he/MHl5eWb9+vWme/fupkOHDubIkSMB/jbBq0+fPmb27Nlm06ZNZsOGDebqq682zZo1M/v377e2oa/P3Jtvvmnefvtts23bNrNt2zbzwAMPmPDwcLNp0yZjDH3sD5988olp0aKFueiii8yYMWOsdvq6bkycONG0bdvW7N6921oKCwut9cHUz4QdP7n00kvNsGHDfNouvPBCc//999tU0dnt2LBTUVFhPB6Pefzxx622Q4cOmfj4ePPCCy8YY4z58ccfTXh4uJk/f761zXfffWdCQkLMkiVLAlb72aawsNBIMitXrjTG0Nf+lJCQYP73f/+XPvaDkpISk5aWZvLy8ky3bt2ssENf152JEyeaDh06HHddsPUzh7H8oKysTOvWrVPv3r192nv37q3Vq1fbVJWz5Ofnq6CgwKeP3W63unXrZvXxunXrdPjwYZ9tkpOT1a5dO/45nERRUZEkqUGDBpLoa38oLy/X/PnzdeDAAXXp0oU+9oORI0fq6quvVs+ePX3a6eu69eWXXyo5OVmpqam68cYb9fXXX0sKvn7mRqB+8MMPP6i8vFyJiYk+7YmJiSooKLCpKmep6sfj9fGOHTusbSIiIpSQkFBtG/45HJ8xRmPHjtVll12mdu3aSaKv69LGjRvVpUsXHTp0SPXq1dPixYvVpk0b6z/s9HHdmD9/vtavX6+1a9dWW8ffc93p3Lmz/vKXv+j888/Xf//7Xz322GPKzMzU5s2bg66fCTt+5HK5fF4bY6q14cycTh/zz+HERo0apS+++EIffvhhtXX09Zm74IILtGHDBv34449atGiRBg0apJUrV1rr6eMzt3PnTo0ZM0bLli1TZGTkCbejr89c3759reft27dXly5ddN5552nu3Ln65S9/KSl4+pnDWH7QqFEjhYaGVkumhYWF1VIuTk/VjP+T9bHH41FZWZn27dt3wm3ws9GjR+vNN9/U+++/r6ZNm1rt9HXdiYiIUKtWrZSRkaHc3Fx16NBBTz/9NH1ch9atW6fCwkKlp6crLCxMYWFhWrlypZ555hmFhYVZfUVf172YmBi1b99eX375ZdD9TRN2/CAiIkLp6enKy8vzac/Ly1NmZqZNVTlLamqqPB6PTx+XlZVp5cqVVh+np6crPDzcZ5vdu3dr06ZN/HM4ijFGo0aN0uuvv6733ntPqampPuvpa/8xxsjr9dLHdahHjx7auHGjNmzYYC0ZGRkaOHCgNmzYoJYtW9LXfuL1erV161YlJSUF3990nU53hqXq1POXXnrJbNmyxWRnZ5uYmBjzzTff2F3aWaOkpMR89tln5rPPPjOSzLRp08xnn31mnb7/+OOPm/j4ePP666+bjRs3mptuuum4pzU2bdrULF++3Kxfv95cccUVnD56jOHDh5v4+HizYsUKn1NIS0tLrW3o6zM3fvx488EHH5j8/HzzxRdfmAceeMCEhISYZcuWGWPoY386+mwsY+jrunL33XebFStWmK+//tqsWbPG/PrXvzaxsbHW71ww9TNhx4+ee+4507x5cxMREWE6depkncqLmnn//feNpGrLoEGDjDGVpzZOnDjReDwe43a7zeWXX242btzos4+DBw+aUaNGmQYNGpioqCjz61//2nz77bc2fJvgdbw+lmRmz55tbUNfn7khQ4ZY/z1o3Lix6dGjhxV0jKGP/enYsENf142q6+aEh4eb5ORk079/f7N582ZrfTD1s8sYY+p2rAgAACB4MGcHAAA4GmEHAAA4GmEHAAA4GmEHAAA4GmEHAAA4GmEHAAA4GmEHAAA4GmEHAAA4GmEHAFR5d+Y33njD7jIA+AFhB4DtBg8eLJfLVW258sor7S4NgAOE2V0AAEjSlVdeqdmzZ/u0ud1um6oB4CSM7AAICm63Wx6Px2dJSEiQVHmIaebMmerbt6+ioqKUmpqqhQsX+rx/48aNuuKKKxQVFaWGDRvqzjvv1P79+322efnll9W2bVu53W4lJSVp1KhRPut/+OEH/fa3v1V0dLTS0tL05ptvWuv27dungQMHqnHjxoqKilJaWlq1cAYgOBF2AJwVHnroIV133XX6/PPPdfPNN+umm27S1q1bJUmlpaW68sorlZCQoLVr12rhwoVavny5T5iZOXOmRo4cqTvvvFMbN27Um2++qVatWvl8xiOPPKLrr79eX3zxha666ioNHDhQe/futT5/y5Yteuedd7R161bNnDlTjRo1ClwHADh9dX4fdQCopUGDBpnQ0FATExPjszz66KPGGGMkmWHDhvm8p3Pnzmb48OHGGGNmzZplEhISzP79+631b7/9tgkJCTEFBQXGGGOSk5PNhAkTTliDJPPggw9ar/fv329cLpd55513jDHGXHPNNea2226rmy8MIKCYswMgKHTv3l0zZ870aWvQoIH1vEuXLj7runTpog0bNkiStm7dqg4dOigmJsZa37VrV1VUVGjbtm1yuVzatWuXevTocdIaLrroIut5TEyMYmNjVVhYKEkaPny4rrvuOq1fv169e/dWv379lJmZeVrfFUBgEXYABIWYmJhqh5VOxeVySZKMMdbz420TFRVVo/2Fh4dXe29FRYUkqW/fvtqxY4fefvttLV++XD169NDIkSP15JNP1qpmAIHHnB0AZ4U1a9ZUe33hhRdKktq0aaMNGzbowIED1vqPPvpIISEhOv/88xUbG6sWLVro3XffPaMaGjdurMGDB+vVV1/V9OnTNWvWrDPaH4DAYGQHQFDwer0qKCjwaQsLC7MmAS9cuFAZGRm67LLL9Nprr+mTTz7RSy+9JEkaOHCgJk6cqEGDBmnSpEn6/vvvNXr0aN1yyy1KTEyUJE2aNEnDhg1TkyZN1LdvX5WUlOijjz7S6NGja1Tfww8/rPT0dLVt21Zer1f/+Mc/1Lp16zrsAQD+QtgBEBSWLFmipKQkn7YLLrhA//rXvyRVnik1f/58jRgxQh6PR6+99pratGkjSYqOjtbSpUs1ZswYXXLJJYqOjtZ1112nadOmWfsaNGiQDh06pKeeekr33HOPGjVqpN/97nc1ri8iIkLjx4/XN998o6ioKP3qV7/S/Pnz6+CbA/A3lzHG2F0EAJyMy+XS4sWL1a9fP7tLAXAWYs4OAABwNMIOAABwNObsAAh6HG0HcCYY2QEAAI5G2AEAAI5G2AEAAI5G2AEAAI5G2AEAAI5G2AEAAI5G2AEAAI5G2AEAAI72/wEr0/nSQr9MVwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x_tot = np.random.rand(5000, 15)\n",
    "target = np.random.rand(5000, 3)\n",
    "\n",
    "layer_one = Layer(15, 8, ELU, d_ELU)\n",
    "layer_two = Layer(8, 5, ELU, d_ELU)\n",
    "layer_out = Layer(5, 3, linear, d_linear)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "NN.add_layer(layer_one)\n",
    "NN.add_layer(layer_two)\n",
    "NN.add_layer(layer_out)\n",
    "\n",
    "# Parametri di training\n",
    "K = 5\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "Lambda = 0.1\n",
    "\n",
    "# Cross-validation\n",
    "train_error, val_error = NN.k_fold_cross_validation(x_tot, target, K, epochs, learning_rate, Lambda, 'elastic', mean_squared_error, d_mean_squared_error, batch_size)\n",
    "\n",
    "# Plot degli errori\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_error, label='Training Error')\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
