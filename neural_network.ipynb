{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives.\n",
    "* **Hidden layers**:\n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "qv1wwlgWsFM8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
    "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
    "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha):\n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "ELU = np.vectorize(ELU)\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n",
    "\n",
    "d_ELU = np.vectorize(d_ELU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjNvxQOLsFM-"
   },
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters :**\n",
    " * input_size : $k_{i-1}$ ;\n",
    " * output_size : $k_i$ ;\n",
    " * activation_function ;\n",
    " * activation_derivative . \n",
    "\n",
    "**Constructor attributes :**\n",
    "* self.input_size = input_size;\n",
    "* self.output_size = output_size;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative;\n",
    "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
    "\n",
    "**Methods :**\n",
    "\n",
    "* **initialize_weights**: initialize weights and biases\n",
    "  * attributes :\n",
    "    * self.weights : $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix . \\\n",
    "      Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$ ;\n",
    "    * self.biases : $1 \\ \\textrm{x} \\ k_i$ array. Initialized to zeros;\n",
    "* **forward_layer** : allows to compute the output of the layer for a given input.\n",
    "  * parameter :\n",
    "    * input_array : matrix $X$ (see above for the case $L_0$ or $L_i$) .\n",
    "  * attributes :\n",
    "    * self.input : input_array ;\n",
    "    * self.net : net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$) .\n",
    "  * return -> output = $f(net)$, where $f$ is the activation function; $f(net)$ has the same dimensions of $net$.\n",
    "  \n",
    "* **backward_layer** : computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    "  * parameters :\n",
    "    * d_Ep : target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "    * learning_rate.\n",
    "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "H8Ap9rLxLlNU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
    "        self.biases = np.zeros((1, self.output_size))\n",
    "        \n",
    "        \n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward_layer(self, d_Ep, learning_rate):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "        self.weights += learning_rate * np.dot(self.input.T, delta) # learning rule for the weights\n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    " * **data_split**: splits the input data into training set, validation set and test set\n",
    "   * parameter:\n",
    "     * x_tot: total data given as input;\n",
    "     * target: total data labels given as input;\n",
    "     * test_split: percentile of test set with respect to the total data.\n",
    "    * return->:\n",
    "      * x_train_val: training set+validation set for input data;\n",
    "      * target_train_val: training set+validation set for input data labels;\n",
    "      * x_test_val: test set for input data;\n",
    "      * target_test_val: test set for input data labels.\n",
    "\n",
    "     \n",
    " * **add_layer**: appends a layer to the empty list self.layers\n",
    "   * parameter:\n",
    "     * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    "  * parameter:\n",
    "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    "  * return -> input = $o_i$ for layer $L_i$.\n",
    "  \n",
    "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    "  * parameter:\n",
    "    * d_Ep;\n",
    "    * learning_rate.\n",
    "\n",
    "* **train_online**: applies the forward and backward method to the network for a specified number of epochs **one example at a time**.\n",
    "  * parameter:\n",
    "    * x_train: input matrix $X$;\n",
    "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "    * epochs: number of the iterations of the training algorithm;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative.\n",
    "\n",
    "* **train_minibatch**: applies the forward and backward method to the network for a specified number of epochs **to batches of $l' < l$** examples.\n",
    "  * parameter:\n",
    "    * x_train: input matrix $X$;\n",
    "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "    * epochs: number of the iterations of the training algorithm;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative;\n",
    "    * batch_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def data_split(self, x_tot, target, test_split):\n",
    "        \n",
    "        num_samples = x_tot.shape[0] # the total number of the examples in input\n",
    "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
    "        indices = num_samples # if we don't want the randomization and the shuffle of the examples\n",
    "        \n",
    "        #indices = np.arange(num_samples)\n",
    "        #np.random.shuffle(indices)\n",
    "\n",
    "        x_tot = x_tot[indices]\n",
    "        target = target[indices]\n",
    "\n",
    "        x_test = x_tot[:test_size]\n",
    "        target_test = target[:test_size]\n",
    "        x_train_val = x_tot[test_size:]\n",
    "        target_train_val = target[test_size:]\n",
    "\n",
    "        return x_train_val, target_train_val, x_test, target_test\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, d_Ep, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate)\n",
    "\n",
    "    def reinitialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights()            \n",
    " \n",
    "\n",
    "    def train(self, x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size):\n",
    "        train_error_epoch = np.zeros(epochs)\n",
    "        val_error_epoch = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_tr_loss = 0\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            \"\"\"\n",
    "            # Shuffle training data\n",
    "            train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(train_indices)\n",
    "            x_train = x_train[train_indices]\n",
    "            target_train = target_train[train_indices]\n",
    "            \"\"\"\n",
    "            \n",
    "            # Mini-batch processing, if batch_size=1 we get the online version\n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                target_batch = target_train[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss and gradient\n",
    "                loss = loss_function(target_batch, predictions)\n",
    "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "                epoch_tr_loss += np.sum(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(loss_gradient, learning_rate)\n",
    "\n",
    "            # Validation\n",
    "            val_predictions = self.forward(x_val)\n",
    "            val_loss = loss_function(target_val, val_predictions)\n",
    "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished. \n",
    "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
    "            # Store average errors for the epoch\n",
    "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
    "            val_error_epoch[epoch] = epoch_val_loss\n",
    "\n",
    "        return train_error_epoch, val_error_epoch\n",
    "\n",
    "\n",
    "    def k_fold_cross_validation(self, x_tot, target, K, epochs, learning_rate, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_tot.shape[0]\n",
    "        fold_size = num_samples // K # if K=1 we get the hold-out validation\n",
    "\n",
    "        # Error storage for averaging\n",
    "        avg_train_error_epoch = np.zeros(epochs)\n",
    "        avg_val_error_epoch = np.zeros(epochs)\n",
    "        \n",
    "        if K==1: # hold-out validation\n",
    "            print(\"sono entrato in hold-out\")\n",
    "            train_indices = np.arange(0, int(0.8*num_samples))\n",
    "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices)\n",
    "            x_train, target_train = x_tot[train_indices], target[train_indices]\n",
    "            x_val, target_val = x_tot[val_indices], target[val_indices]            \n",
    "            train_error_epoch, val_error_epoch = self.train(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "            return train_error_epoch, val_error_epoch\n",
    "            \n",
    "        # ROBA FOTONICA PAZZESKA DI CHAT GPT, LEGGERE BENE\n",
    "        for k in range(K):\n",
    "            print(\"sono entrato in cross val\")\n",
    "            # Create fold indices\n",
    "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
    "                                                                        # It contains the indices of the examples used in validation set for\n",
    "                                                                        # this fold\n",
    "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples - \n",
    "                                                                              # len(val_indices). It contains the indices of all the examples\n",
    "                                                                              # but the ones used in the validation set for this fold. Thus \n",
    "                                                                              # it corresponds to the training set for this fold \n",
    "            x_train, target_train = x_tot[train_indices], target[train_indices]\n",
    "            x_val, target_val = x_tot[val_indices], target[val_indices]\n",
    "\n",
    "            # Reinitialize weights for each fold\n",
    "            self.reinitialize_weights()\n",
    "\n",
    "            # Train on the current fold\n",
    "            train_error_epoch, val_error_epoch = self.train(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "\n",
    "            # Accumulate errors for averaging, we have the errors for each epoch summed for all the folds\n",
    "            avg_train_error_epoch += train_error_epoch\n",
    "            avg_val_error_epoch += val_error_epoch\n",
    "\n",
    "            print(f\"Fold {k+1} completed.\")\n",
    "\n",
    "        # Average errors across all folds\n",
    "        avg_train_error_epoch /= K\n",
    "        avg_val_error_epoch /= K\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
    "\n",
    "        return avg_train_error_epoch, avg_val_error_epoch\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp0g8MgT3hhO",
    "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sono entrato in cross val\n",
      "Fold 1 completed.\n",
      "sono entrato in cross val\n",
      "Fold 2 completed.\n",
      "sono entrato in cross val\n",
      "Fold 3 completed.\n",
      "sono entrato in cross val\n",
      "Fold 4 completed.\n",
      "sono entrato in cross val\n",
      "Fold 5 completed.\n",
      "Epoch 1/1000, Avg Training Loss: 0.5624846742404395, Avg Validation Loss: 332.2243035092395\n",
      "Epoch 2/1000, Avg Training Loss: 0.28618572281653476, Avg Validation Loss: 264.6557067624351\n",
      "Epoch 3/1000, Avg Training Loss: 0.2567992777874338, Avg Validation Loss: 260.345102746497\n",
      "Epoch 4/1000, Avg Training Loss: 0.25433200649243054, Avg Validation Loss: 260.3554829036733\n",
      "Epoch 5/1000, Avg Training Loss: 0.2540618915502612, Avg Validation Loss: 260.35828400888346\n",
      "Epoch 6/1000, Avg Training Loss: 0.25395093350743575, Avg Validation Loss: 260.27682545658774\n",
      "Epoch 7/1000, Avg Training Loss: 0.2538542824917676, Avg Validation Loss: 260.17149873751686\n",
      "Epoch 8/1000, Avg Training Loss: 0.2537629323305733, Avg Validation Loss: 260.06370662021965\n",
      "Epoch 9/1000, Avg Training Loss: 0.2536763624094453, Avg Validation Loss: 259.9592101745979\n",
      "Epoch 10/1000, Avg Training Loss: 0.2535943269788739, Avg Validation Loss: 259.85928081951636\n",
      "Epoch 11/1000, Avg Training Loss: 0.25351653090977394, Avg Validation Loss: 259.76399791423177\n",
      "Epoch 12/1000, Avg Training Loss: 0.2534426766516187, Avg Validation Loss: 259.6731454279187\n",
      "Epoch 13/1000, Avg Training Loss: 0.2533724858321969, Avg Validation Loss: 259.58644976095917\n",
      "Epoch 14/1000, Avg Training Loss: 0.25330570102648187, Avg Validation Loss: 259.5036426559785\n",
      "Epoch 15/1000, Avg Training Loss: 0.2532420881158028, Avg Validation Loss: 259.4244730635918\n",
      "Epoch 16/1000, Avg Training Loss: 0.25318143377851127, Avg Validation Loss: 259.3487088118892\n",
      "Epoch 17/1000, Avg Training Loss: 0.2531235423426706, Avg Validation Loss: 259.27613603940017\n",
      "Epoch 18/1000, Avg Training Loss: 0.2530682345418959, Avg Validation Loss: 259.20655857332787\n",
      "Epoch 19/1000, Avg Training Loss: 0.2530153459852839, Avg Validation Loss: 259.13979606533536\n",
      "Epoch 20/1000, Avg Training Loss: 0.25296472625984695, Avg Validation Loss: 259.075681883273\n",
      "Epoch 21/1000, Avg Training Loss: 0.25291623631148596, Avg Validation Loss: 259.0140619645932\n",
      "Epoch 22/1000, Avg Training Loss: 0.2528697494056251, Avg Validation Loss: 258.9547934450028\n",
      "Epoch 23/1000, Avg Training Loss: 0.2528251469921658, Avg Validation Loss: 258.8977450207841\n",
      "Epoch 24/1000, Avg Training Loss: 0.25278231940455775, Avg Validation Loss: 258.84279607651786\n",
      "Epoch 25/1000, Avg Training Loss: 0.2527411659943438, Avg Validation Loss: 258.7898331578131\n",
      "Epoch 26/1000, Avg Training Loss: 0.2527015934227415, Avg Validation Loss: 258.7387509176731\n",
      "Epoch 27/1000, Avg Training Loss: 0.25266351411787197, Avg Validation Loss: 258.6894517887839\n",
      "Epoch 28/1000, Avg Training Loss: 0.25262684746589986, Avg Validation Loss: 258.6418442997915\n",
      "Epoch 29/1000, Avg Training Loss: 0.2525915181232563, Avg Validation Loss: 258.5958433190178\n",
      "Epoch 30/1000, Avg Training Loss: 0.2525574558491403, Avg Validation Loss: 258.5513695199295\n",
      "Epoch 31/1000, Avg Training Loss: 0.25252459520890297, Avg Validation Loss: 258.50834866478135\n",
      "Epoch 32/1000, Avg Training Loss: 0.2524928751625589, Avg Validation Loss: 258.4667112989948\n",
      "Epoch 33/1000, Avg Training Loss: 0.252462238583472, Avg Validation Loss: 258.4263918092615\n",
      "Epoch 34/1000, Avg Training Loss: 0.2524326319921071, Avg Validation Loss: 258.38732856512047\n",
      "Epoch 35/1000, Avg Training Loss: 0.25240400518361467, Avg Validation Loss: 258.34946424175416\n",
      "Epoch 36/1000, Avg Training Loss: 0.2523763112232223, Avg Validation Loss: 258.3127450442547\n",
      "Epoch 37/1000, Avg Training Loss: 0.25234950609732276, Avg Validation Loss: 258.27711979335254\n",
      "Epoch 38/1000, Avg Training Loss: 0.2523235482792428, Avg Validation Loss: 258.2425407157415\n",
      "Epoch 39/1000, Avg Training Loss: 0.25229839921735825, Avg Validation Loss: 258.2089623148746\n",
      "Epoch 40/1000, Avg Training Loss: 0.25227402183493114, Avg Validation Loss: 258.1763424353751\n",
      "Epoch 41/1000, Avg Training Loss: 0.2522503817308916, Avg Validation Loss: 258.14464097162124\n",
      "Epoch 42/1000, Avg Training Loss: 0.2522274465139408, Avg Validation Loss: 258.11381969081634\n",
      "Epoch 43/1000, Avg Training Loss: 0.25220518532266367, Avg Validation Loss: 258.08384285079205\n",
      "Epoch 44/1000, Avg Training Loss: 0.25218356912033907, Avg Validation Loss: 258.05467655535654\n",
      "Epoch 45/1000, Avg Training Loss: 0.25216257066252695, Avg Validation Loss: 258.02628857215836\n",
      "Epoch 46/1000, Avg Training Loss: 0.25214216401296713, Avg Validation Loss: 257.9986485343049\n",
      "Epoch 47/1000, Avg Training Loss: 0.2521223247704996, Avg Validation Loss: 257.9717273850785\n",
      "Epoch 48/1000, Avg Training Loss: 0.25210302963361747, Avg Validation Loss: 257.9454976675471\n",
      "Epoch 49/1000, Avg Training Loss: 0.2520842564950929, Avg Validation Loss: 257.9199334458466\n",
      "Epoch 50/1000, Avg Training Loss: 0.25206598453677337, Avg Validation Loss: 257.8950099014568\n",
      "Epoch 51/1000, Avg Training Loss: 0.2520481939171861, Avg Validation Loss: 257.8707034451566\n",
      "Epoch 52/1000, Avg Training Loss: 0.25203086594592733, Avg Validation Loss: 257.84699147418405\n",
      "Epoch 53/1000, Avg Training Loss: 0.2520139826083243, Avg Validation Loss: 257.8238526624352\n",
      "Epoch 54/1000, Avg Training Loss: 0.2519975268026499, Avg Validation Loss: 257.8012669445929\n",
      "Epoch 55/1000, Avg Training Loss: 0.25198148255545016, Avg Validation Loss: 257.7792149870675\n",
      "Epoch 56/1000, Avg Training Loss: 0.2519658345422366, Avg Validation Loss: 257.7576782878997\n",
      "Epoch 57/1000, Avg Training Loss: 0.25195056809638694, Avg Validation Loss: 257.7366392211287\n",
      "Epoch 58/1000, Avg Training Loss: 0.2519356693426022, Avg Validation Loss: 257.71608084520255\n",
      "Epoch 59/1000, Avg Training Loss: 0.2519211248710794, Avg Validation Loss: 257.695986979478\n",
      "Epoch 60/1000, Avg Training Loss: 0.25190692196775333, Avg Validation Loss: 257.6763422666569\n",
      "Epoch 61/1000, Avg Training Loss: 0.2518930487176396, Avg Validation Loss: 257.6571318073832\n",
      "Epoch 62/1000, Avg Training Loss: 0.25187949351732614, Avg Validation Loss: 257.638341563723\n",
      "Epoch 63/1000, Avg Training Loss: 0.25186624526491236, Avg Validation Loss: 257.61995809342284\n",
      "Epoch 64/1000, Avg Training Loss: 0.2518532934252405, Avg Validation Loss: 257.60196850314145\n",
      "Epoch 65/1000, Avg Training Loss: 0.2518406279910599, Avg Validation Loss: 257.58436032027805\n",
      "Epoch 66/1000, Avg Training Loss: 0.2518282392528334, Avg Validation Loss: 257.56712165247404\n",
      "Epoch 67/1000, Avg Training Loss: 0.25181611798891995, Avg Validation Loss: 257.5502411343372\n",
      "Epoch 68/1000, Avg Training Loss: 0.2518042554340607, Avg Validation Loss: 257.53370780782467\n",
      "Epoch 69/1000, Avg Training Loss: 0.251792643149377, Avg Validation Loss: 257.5175110913174\n",
      "Epoch 70/1000, Avg Training Loss: 0.25178127299526576, Avg Validation Loss: 257.5016408650725\n",
      "Epoch 71/1000, Avg Training Loss: 0.25177013720187635, Avg Validation Loss: 257.4860874618231\n",
      "Epoch 72/1000, Avg Training Loss: 0.2517592283216406, Avg Validation Loss: 257.4708415864924\n",
      "Epoch 73/1000, Avg Training Loss: 0.2517485391438166, Avg Validation Loss: 257.4558943596185\n",
      "Epoch 74/1000, Avg Training Loss: 0.2517380628777483, Avg Validation Loss: 257.4412370654644\n",
      "Epoch 75/1000, Avg Training Loss: 0.25172779289961966, Avg Validation Loss: 257.42686136252837\n",
      "Epoch 76/1000, Avg Training Loss: 0.25171772286031824, Avg Validation Loss: 257.4127592655349\n",
      "Epoch 77/1000, Avg Training Loss: 0.2517078466728206, Avg Validation Loss: 257.39892306434166\n",
      "Epoch 78/1000, Avg Training Loss: 0.2516981584855411, Avg Validation Loss: 257.38534535294616\n",
      "Epoch 79/1000, Avg Training Loss: 0.25168865266207363, Avg Validation Loss: 257.3720190514327\n",
      "Epoch 80/1000, Avg Training Loss: 0.25167932382560104, Avg Validation Loss: 257.35893727109385\n",
      "Epoch 81/1000, Avg Training Loss: 0.2516701667636274, Avg Validation Loss: 257.34609334850484\n",
      "Epoch 82/1000, Avg Training Loss: 0.25166117647716313, Avg Validation Loss: 257.33348089333055\n",
      "Epoch 83/1000, Avg Training Loss: 0.25165234818675075, Avg Validation Loss: 257.32109375845994\n",
      "Epoch 84/1000, Avg Training Loss: 0.25164367727071996, Avg Validation Loss: 257.3089260116032\n",
      "Epoch 85/1000, Avg Training Loss: 0.25163515924837393, Avg Validation Loss: 257.29697185552794\n",
      "Epoch 86/1000, Avg Training Loss: 0.2516267897668966, Avg Validation Loss: 257.28522563697476\n",
      "Epoch 87/1000, Avg Training Loss: 0.2516185646857953, Avg Validation Loss: 257.27368205333676\n",
      "Epoch 88/1000, Avg Training Loss: 0.2516104800298699, Avg Validation Loss: 257.2623359605322\n",
      "Epoch 89/1000, Avg Training Loss: 0.25160253197005605, Avg Validation Loss: 257.2511823910603\n",
      "Epoch 90/1000, Avg Training Loss: 0.25159471682453166, Avg Validation Loss: 257.2402164883645\n",
      "Epoch 91/1000, Avg Training Loss: 0.25158703095982504, Avg Validation Loss: 257.2294335576312\n",
      "Epoch 92/1000, Avg Training Loss: 0.2515794709343389, Avg Validation Loss: 257.21882910112555\n",
      "Epoch 93/1000, Avg Training Loss: 0.2515720333974967, Avg Validation Loss: 257.2083988184598\n",
      "Epoch 94/1000, Avg Training Loss: 0.2515647151124702, Avg Validation Loss: 257.1981384271734\n",
      "Epoch 95/1000, Avg Training Loss: 0.251557512916101, Avg Validation Loss: 257.18804396399776\n",
      "Epoch 96/1000, Avg Training Loss: 0.25155042387882787, Avg Validation Loss: 257.1781114012618\n",
      "Epoch 97/1000, Avg Training Loss: 0.2515434450877863, Avg Validation Loss: 257.1683368695201\n",
      "Epoch 98/1000, Avg Training Loss: 0.2515365736909575, Avg Validation Loss: 257.1587166453365\n",
      "Epoch 99/1000, Avg Training Loss: 0.25152980701111294, Avg Validation Loss: 257.149247011136\n",
      "Epoch 100/1000, Avg Training Loss: 0.25152314240531554, Avg Validation Loss: 257.1399246836485\n",
      "Epoch 101/1000, Avg Training Loss: 0.2515165773612163, Avg Validation Loss: 257.13074617960876\n",
      "Epoch 102/1000, Avg Training Loss: 0.251510109381575, Avg Validation Loss: 257.12170817351455\n",
      "Epoch 103/1000, Avg Training Loss: 0.25150373604989257, Avg Validation Loss: 257.1128075105443\n",
      "Epoch 104/1000, Avg Training Loss: 0.2514974550864728, Avg Validation Loss: 257.1040410446541\n",
      "Epoch 105/1000, Avg Training Loss: 0.251491264251696, Avg Validation Loss: 257.0954058100374\n",
      "Epoch 106/1000, Avg Training Loss: 0.2514851613728135, Avg Validation Loss: 257.086898867854\n",
      "Epoch 107/1000, Avg Training Loss: 0.2514791443464437, Avg Validation Loss: 257.0785174599082\n",
      "Epoch 108/1000, Avg Training Loss: 0.25147321114274046, Avg Validation Loss: 257.0702588995006\n",
      "Epoch 109/1000, Avg Training Loss: 0.2514673598347421, Avg Validation Loss: 257.06212052925537\n",
      "Epoch 110/1000, Avg Training Loss: 0.2514615885282745, Avg Validation Loss: 257.054099628679\n",
      "Epoch 111/1000, Avg Training Loss: 0.25145589534622603, Avg Validation Loss: 257.046193607507\n",
      "Epoch 112/1000, Avg Training Loss: 0.2514502784656953, Avg Validation Loss: 257.0384000774601\n",
      "Epoch 113/1000, Avg Training Loss: 0.2514447361424053, Avg Validation Loss: 257.0307167461725\n",
      "Epoch 114/1000, Avg Training Loss: 0.2514392667253941, Avg Validation Loss: 257.02314134072856\n",
      "Epoch 115/1000, Avg Training Loss: 0.2514338685997244, Avg Validation Loss: 257.01567152207286\n",
      "Epoch 116/1000, Avg Training Loss: 0.2514285401678919, Avg Validation Loss: 257.0083050370239\n",
      "Epoch 117/1000, Avg Training Loss: 0.25142327990675495, Avg Validation Loss: 257.00103971521776\n",
      "Epoch 118/1000, Avg Training Loss: 0.2514180862955983, Avg Validation Loss: 256.9938735277992\n",
      "Epoch 119/1000, Avg Training Loss: 0.2514129578709361, Avg Validation Loss: 256.9868044435731\n",
      "Epoch 120/1000, Avg Training Loss: 0.25140789320631685, Avg Validation Loss: 256.9798305496515\n",
      "Epoch 121/1000, Avg Training Loss: 0.2514028909585216, Avg Validation Loss: 256.9729499144992\n",
      "Epoch 122/1000, Avg Training Loss: 0.251397949818405, Avg Validation Loss: 256.96616061997736\n",
      "Epoch 123/1000, Avg Training Loss: 0.25139306847955695, Avg Validation Loss: 256.9594608519037\n",
      "Epoch 124/1000, Avg Training Loss: 0.25138824569787316, Avg Validation Loss: 256.95284887741286\n",
      "Epoch 125/1000, Avg Training Loss: 0.2513834802455506, Avg Validation Loss: 256.9463229962106\n",
      "Epoch 126/1000, Avg Training Loss: 0.2513787709478214, Avg Validation Loss: 256.93988150448536\n",
      "Epoch 127/1000, Avg Training Loss: 0.2513741166857663, Avg Validation Loss: 256.9335227940586\n",
      "Epoch 128/1000, Avg Training Loss: 0.25136951633707, Avg Validation Loss: 256.9272451845987\n",
      "Epoch 129/1000, Avg Training Loss: 0.251364968810898, Avg Validation Loss: 256.921047148277\n",
      "Epoch 130/1000, Avg Training Loss: 0.25136047303595666, Avg Validation Loss: 256.91492717897034\n",
      "Epoch 131/1000, Avg Training Loss: 0.25135602796495216, Avg Validation Loss: 256.9088837498507\n",
      "Epoch 132/1000, Avg Training Loss: 0.25135163255867327, Avg Validation Loss: 256.90291544301306\n",
      "Epoch 133/1000, Avg Training Loss: 0.25134728587392424, Avg Validation Loss: 256.89702084646024\n",
      "Epoch 134/1000, Avg Training Loss: 0.25134298695158175, Avg Validation Loss: 256.8911985432637\n",
      "Epoch 135/1000, Avg Training Loss: 0.2513387348186426, Avg Validation Loss: 256.88544724428067\n",
      "Epoch 136/1000, Avg Training Loss: 0.2513345285841892, Avg Validation Loss: 256.87976570094804\n",
      "Epoch 137/1000, Avg Training Loss: 0.25133036737245007, Avg Validation Loss: 256.8741526437428\n",
      "Epoch 138/1000, Avg Training Loss: 0.25132625030676214, Avg Validation Loss: 256.86860680060823\n",
      "Epoch 139/1000, Avg Training Loss: 0.2513221765933727, Avg Validation Loss: 256.86312711482685\n",
      "Epoch 140/1000, Avg Training Loss: 0.25131814545028686, Avg Validation Loss: 256.8577123541529\n",
      "Epoch 141/1000, Avg Training Loss: 0.25131415607276, Avg Validation Loss: 256.8523612981919\n",
      "Epoch 142/1000, Avg Training Loss: 0.2513102076800682, Avg Validation Loss: 256.84707279406973\n",
      "Epoch 143/1000, Avg Training Loss: 0.2513062995384595, Avg Validation Loss: 256.84184574904043\n",
      "Epoch 144/1000, Avg Training Loss: 0.25130243090761417, Avg Validation Loss: 256.8366791251659\n",
      "Epoch 145/1000, Avg Training Loss: 0.2512986010881285, Avg Validation Loss: 256.83157194286093\n",
      "Epoch 146/1000, Avg Training Loss: 0.2512948093782207, Avg Validation Loss: 256.826523190467\n",
      "Epoch 147/1000, Avg Training Loss: 0.25129105515105477, Avg Validation Loss: 256.82153182503964\n",
      "Epoch 148/1000, Avg Training Loss: 0.25128733771970774, Avg Validation Loss: 256.8165968244267\n",
      "Epoch 149/1000, Avg Training Loss: 0.25128365642272016, Avg Validation Loss: 256.8117172751705\n",
      "Epoch 150/1000, Avg Training Loss: 0.25128001067008043, Avg Validation Loss: 256.8068921330876\n",
      "Epoch 151/1000, Avg Training Loss: 0.2512763998185092, Avg Validation Loss: 256.8021205156774\n",
      "Epoch 152/1000, Avg Training Loss: 0.2512728232522357, Avg Validation Loss: 256.7974015169428\n",
      "Epoch 153/1000, Avg Training Loss: 0.2512692803732497, Avg Validation Loss: 256.79273422738305\n",
      "Epoch 154/1000, Avg Training Loss: 0.25126577055187965, Avg Validation Loss: 256.7881178916807\n",
      "Epoch 155/1000, Avg Training Loss: 0.2512622932838697, Avg Validation Loss: 256.7835517751083\n",
      "Epoch 156/1000, Avg Training Loss: 0.2512588480663568, Avg Validation Loss: 256.7790349188513\n",
      "Epoch 157/1000, Avg Training Loss: 0.2512554343527219, Avg Validation Loss: 256.7745664622541\n",
      "Epoch 158/1000, Avg Training Loss: 0.2512520516040847, Avg Validation Loss: 256.77014560008337\n",
      "Epoch 159/1000, Avg Training Loss: 0.2512486993406046, Avg Validation Loss: 256.7657716698937\n",
      "Epoch 160/1000, Avg Training Loss: 0.2512453770918817, Avg Validation Loss: 256.76144387580337\n",
      "Epoch 161/1000, Avg Training Loss: 0.25124208436337037, Avg Validation Loss: 256.75716141475414\n",
      "Epoch 162/1000, Avg Training Loss: 0.2512388206448872, Avg Validation Loss: 256.75292351273026\n",
      "Epoch 163/1000, Avg Training Loss: 0.25123558544086394, Avg Validation Loss: 256.7487294076725\n",
      "Epoch 164/1000, Avg Training Loss: 0.2512323782343087, Avg Validation Loss: 256.7445784823533\n",
      "Epoch 165/1000, Avg Training Loss: 0.2512291986050979, Avg Validation Loss: 256.7404701600757\n",
      "Epoch 166/1000, Avg Training Loss: 0.25122604612320043, Avg Validation Loss: 256.7364039680538\n",
      "Epoch 167/1000, Avg Training Loss: 0.25122292044085615, Avg Validation Loss: 256.73237917579075\n",
      "Epoch 168/1000, Avg Training Loss: 0.25121982116177355, Avg Validation Loss: 256.72839491839\n",
      "Epoch 169/1000, Avg Training Loss: 0.2512167478090078, Avg Validation Loss: 256.72445069138473\n",
      "Epoch 170/1000, Avg Training Loss: 0.2512136999841927, Avg Validation Loss: 256.7205458959561\n",
      "Epoch 171/1000, Avg Training Loss: 0.25121067730156754, Avg Validation Loss: 256.71667997517466\n",
      "Epoch 172/1000, Avg Training Loss: 0.25120767941958166, Avg Validation Loss: 256.71285236471\n",
      "Epoch 173/1000, Avg Training Loss: 0.2512047059813745, Avg Validation Loss: 256.7090623210873\n",
      "Epoch 174/1000, Avg Training Loss: 0.2512017564945628, Avg Validation Loss: 256.70530921609407\n",
      "Epoch 175/1000, Avg Training Loss: 0.25119883061034864, Avg Validation Loss: 256.701592585742\n",
      "Epoch 176/1000, Avg Training Loss: 0.2511959279784649, Avg Validation Loss: 256.69791196577967\n",
      "Epoch 177/1000, Avg Training Loss: 0.2511930482425727, Avg Validation Loss: 256.6942668527044\n",
      "Epoch 178/1000, Avg Training Loss: 0.251190191050133, Avg Validation Loss: 256.69065657338723\n",
      "Epoch 179/1000, Avg Training Loss: 0.25118735600503916, Avg Validation Loss: 256.6870805992278\n",
      "Epoch 180/1000, Avg Training Loss: 0.2511845428120128, Avg Validation Loss: 256.68353847251853\n",
      "Epoch 181/1000, Avg Training Loss: 0.25118175117219066, Avg Validation Loss: 256.6800296832266\n",
      "Epoch 182/1000, Avg Training Loss: 0.2511789807527892, Avg Validation Loss: 256.6765537388866\n",
      "Epoch 183/1000, Avg Training Loss: 0.251176231207754, Avg Validation Loss: 256.6731102339487\n",
      "Epoch 184/1000, Avg Training Loss: 0.2511735022030333, Avg Validation Loss: 256.66969883922667\n",
      "Epoch 185/1000, Avg Training Loss: 0.25117079350087745, Avg Validation Loss: 256.66631906111127\n",
      "Epoch 186/1000, Avg Training Loss: 0.25116810491300384, Avg Validation Loss: 256.66297040291033\n",
      "Epoch 187/1000, Avg Training Loss: 0.2511654361347171, Avg Validation Loss: 256.6596522890263\n",
      "Epoch 188/1000, Avg Training Loss: 0.25116278690834676, Avg Validation Loss: 256.65636423243586\n",
      "Epoch 189/1000, Avg Training Loss: 0.25116015690785704, Avg Validation Loss: 256.65310580258557\n",
      "Epoch 190/1000, Avg Training Loss: 0.2511575458771464, Avg Validation Loss: 256.6498765661113\n",
      "Epoch 191/1000, Avg Training Loss: 0.25115495353140277, Avg Validation Loss: 256.6466761574194\n",
      "Epoch 192/1000, Avg Training Loss: 0.25115237961606834, Avg Validation Loss: 256.6435042360956\n",
      "Epoch 193/1000, Avg Training Loss: 0.25114982393301916, Avg Validation Loss: 256.6403604113633\n",
      "Epoch 194/1000, Avg Training Loss: 0.2511472862008383, Avg Validation Loss: 256.6372443836419\n",
      "Epoch 195/1000, Avg Training Loss: 0.2511447662199822, Avg Validation Loss: 256.6341556614734\n",
      "Epoch 196/1000, Avg Training Loss: 0.2511422637586598, Avg Validation Loss: 256.6310938207244\n",
      "Epoch 197/1000, Avg Training Loss: 0.2511397786350504, Avg Validation Loss: 256.6280584601618\n",
      "Epoch 198/1000, Avg Training Loss: 0.2511373105638866, Avg Validation Loss: 256.6250493007084\n",
      "Epoch 199/1000, Avg Training Loss: 0.25113485928849966, Avg Validation Loss: 256.62206610516773\n",
      "Epoch 200/1000, Avg Training Loss: 0.2511324247197106, Avg Validation Loss: 256.6191083806482\n",
      "Epoch 201/1000, Avg Training Loss: 0.25113000661819945, Avg Validation Loss: 256.6161758457641\n",
      "Epoch 202/1000, Avg Training Loss: 0.25112760476362506, Avg Validation Loss: 256.61326809216126\n",
      "Epoch 203/1000, Avg Training Loss: 0.25112521895111, Avg Validation Loss: 256.61038473109977\n",
      "Epoch 204/1000, Avg Training Loss: 0.2511228489789224, Avg Validation Loss: 256.60752547568046\n",
      "Epoch 205/1000, Avg Training Loss: 0.2511204946376836, Avg Validation Loss: 256.604690008133\n",
      "Epoch 206/1000, Avg Training Loss: 0.2511181557084512, Avg Validation Loss: 256.6018780495999\n",
      "Epoch 207/1000, Avg Training Loss: 0.2511158320608106, Avg Validation Loss: 256.59908925456926\n",
      "Epoch 208/1000, Avg Training Loss: 0.25111352350535476, Avg Validation Loss: 256.5963232839607\n",
      "Epoch 209/1000, Avg Training Loss: 0.25111122988468876, Avg Validation Loss: 256.59357979050407\n",
      "Epoch 210/1000, Avg Training Loss: 0.25110895096437696, Avg Validation Loss: 256.59085859811483\n",
      "Epoch 211/1000, Avg Training Loss: 0.25110668660103375, Avg Validation Loss: 256.58815939211155\n",
      "Epoch 212/1000, Avg Training Loss: 0.2511044366263936, Avg Validation Loss: 256.58548191710986\n",
      "Epoch 213/1000, Avg Training Loss: 0.2511022008676229, Avg Validation Loss: 256.58282588415835\n",
      "Epoch 214/1000, Avg Training Loss: 0.25109997919791666, Avg Validation Loss: 256.58019100455067\n",
      "Epoch 215/1000, Avg Training Loss: 0.2510977714176292, Avg Validation Loss: 256.5775768462354\n",
      "Epoch 216/1000, Avg Training Loss: 0.2510955772864293, Avg Validation Loss: 256.57498322197273\n",
      "Epoch 217/1000, Avg Training Loss: 0.2510933966675702, Avg Validation Loss: 256.5724099823026\n",
      "Epoch 218/1000, Avg Training Loss: 0.2510912293972972, Avg Validation Loss: 256.56985697330225\n",
      "Epoch 219/1000, Avg Training Loss: 0.25108907534194586, Avg Validation Loss: 256.56732386251036\n",
      "Epoch 220/1000, Avg Training Loss: 0.25108693435126617, Avg Validation Loss: 256.5648103999408\n",
      "Epoch 221/1000, Avg Training Loss: 0.25108480628070234, Avg Validation Loss: 256.5623163370626\n",
      "Epoch 222/1000, Avg Training Loss: 0.2510826909712258, Avg Validation Loss: 256.5598413628494\n",
      "Epoch 223/1000, Avg Training Loss: 0.2510805882933171, Avg Validation Loss: 256.5573853562363\n",
      "Epoch 224/1000, Avg Training Loss: 0.2510784980939656, Avg Validation Loss: 256.5549481495977\n",
      "Epoch 225/1000, Avg Training Loss: 0.251076420275, Avg Validation Loss: 256.55252942083047\n",
      "Epoch 226/1000, Avg Training Loss: 0.25107435467248357, Avg Validation Loss: 256.55012889637777\n",
      "Epoch 227/1000, Avg Training Loss: 0.25107230117908, Avg Validation Loss: 256.54774628133475\n",
      "Epoch 228/1000, Avg Training Loss: 0.25107025968788665, Avg Validation Loss: 256.5453813484636\n",
      "Epoch 229/1000, Avg Training Loss: 0.2510682300409904, Avg Validation Loss: 256.54303406455205\n",
      "Epoch 230/1000, Avg Training Loss: 0.25106621212711433, Avg Validation Loss: 256.54070411026044\n",
      "Epoch 231/1000, Avg Training Loss: 0.25106420582014105, Avg Validation Loss: 256.5383912511199\n",
      "Epoch 232/1000, Avg Training Loss: 0.25106221093606806, Avg Validation Loss: 256.5360953218981\n",
      "Epoch 233/1000, Avg Training Loss: 0.2510602273333703, Avg Validation Loss: 256.5338161587056\n",
      "Epoch 234/1000, Avg Training Loss: 0.25105825501022416, Avg Validation Loss: 256.53155354791204\n",
      "Epoch 235/1000, Avg Training Loss: 0.25105629382516725, Avg Validation Loss: 256.5293074096435\n",
      "Epoch 236/1000, Avg Training Loss: 0.25105434368417495, Avg Validation Loss: 256.5270773971466\n",
      "Epoch 237/1000, Avg Training Loss: 0.2510524044470199, Avg Validation Loss: 256.5248632541716\n",
      "Epoch 238/1000, Avg Training Loss: 0.2510504759908375, Avg Validation Loss: 256.5226647544997\n",
      "Epoch 239/1000, Avg Training Loss: 0.2510485581849596, Avg Validation Loss: 256.52048175002216\n",
      "Epoch 240/1000, Avg Training Loss: 0.2510466508947154, Avg Validation Loss: 256.51831412641263\n",
      "Epoch 241/1000, Avg Training Loss: 0.25104475406301835, Avg Validation Loss: 256.5161616698742\n",
      "Epoch 242/1000, Avg Training Loss: 0.2510428675433275, Avg Validation Loss: 256.51402415280575\n",
      "Epoch 243/1000, Avg Training Loss: 0.2510409911998742, Avg Validation Loss: 256.5119014023236\n",
      "Epoch 244/1000, Avg Training Loss: 0.25103912494065467, Avg Validation Loss: 256.5097932969959\n",
      "Epoch 245/1000, Avg Training Loss: 0.2510372686500645, Avg Validation Loss: 256.50769960690775\n",
      "Epoch 246/1000, Avg Training Loss: 0.2510354222159243, Avg Validation Loss: 256.5056201515119\n",
      "Epoch 247/1000, Avg Training Loss: 0.25103358550940946, Avg Validation Loss: 256.50355487705565\n",
      "Epoch 248/1000, Avg Training Loss: 0.2510317584702735, Avg Validation Loss: 256.50150362289014\n",
      "Epoch 249/1000, Avg Training Loss: 0.2510299410659681, Avg Validation Loss: 256.4994661208481\n",
      "Epoch 250/1000, Avg Training Loss: 0.2510281331282395, Avg Validation Loss: 256.49744232004275\n",
      "Epoch 251/1000, Avg Training Loss: 0.2510263345440444, Avg Validation Loss: 256.4954320762857\n",
      "Epoch 252/1000, Avg Training Loss: 0.25102454525690926, Avg Validation Loss: 256.49343520089906\n",
      "Epoch 253/1000, Avg Training Loss: 0.2510227651494349, Avg Validation Loss: 256.491451720679\n",
      "Epoch 254/1000, Avg Training Loss: 0.2510209941989089, Avg Validation Loss: 256.4894814924591\n",
      "Epoch 255/1000, Avg Training Loss: 0.25101923234731605, Avg Validation Loss: 256.48752423646016\n",
      "Epoch 256/1000, Avg Training Loss: 0.2510174794422446, Avg Validation Loss: 256.48557978989356\n",
      "Epoch 257/1000, Avg Training Loss: 0.2510157354139952, Avg Validation Loss: 256.4836481240202\n",
      "Epoch 258/1000, Avg Training Loss: 0.25101400018167946, Avg Validation Loss: 256.48172915971503\n",
      "Epoch 259/1000, Avg Training Loss: 0.2510122736873973, Avg Validation Loss: 256.4798226923907\n",
      "Epoch 260/1000, Avg Training Loss: 0.2510105558141924, Avg Validation Loss: 256.47792868856084\n",
      "Epoch 261/1000, Avg Training Loss: 0.25100884655359373, Avg Validation Loss: 256.4760470049373\n",
      "Epoch 262/1000, Avg Training Loss: 0.2510071458298771, Avg Validation Loss: 256.47417744657326\n",
      "Epoch 263/1000, Avg Training Loss: 0.2510054535533391, Avg Validation Loss: 256.4723198558275\n",
      "Epoch 264/1000, Avg Training Loss: 0.2510037696352676, Avg Validation Loss: 256.470474073766\n",
      "Epoch 265/1000, Avg Training Loss: 0.25100209399058815, Avg Validation Loss: 256.46864000229505\n",
      "Epoch 266/1000, Avg Training Loss: 0.25100042656163246, Avg Validation Loss: 256.4668176174911\n",
      "Epoch 267/1000, Avg Training Loss: 0.2509987673149823, Avg Validation Loss: 256.46500676754977\n",
      "Epoch 268/1000, Avg Training Loss: 0.25099711621270565, Avg Validation Loss: 256.4632072111685\n",
      "Epoch 269/1000, Avg Training Loss: 0.2509954730943553, Avg Validation Loss: 256.4614190679699\n",
      "Epoch 270/1000, Avg Training Loss: 0.25099383796500174, Avg Validation Loss: 256.4596421713836\n",
      "Epoch 271/1000, Avg Training Loss: 0.2509922107432525, Avg Validation Loss: 256.4578764013594\n",
      "Epoch 272/1000, Avg Training Loss: 0.25099059138180196, Avg Validation Loss: 256.4561216083699\n",
      "Epoch 273/1000, Avg Training Loss: 0.2509889798037958, Avg Validation Loss: 256.45437767070325\n",
      "Epoch 274/1000, Avg Training Loss: 0.25098737594895015, Avg Validation Loss: 256.4526445356905\n",
      "Epoch 275/1000, Avg Training Loss: 0.250985779791175, Avg Validation Loss: 256.4509220529431\n",
      "Epoch 276/1000, Avg Training Loss: 0.2509841912681966, Avg Validation Loss: 256.4492100019629\n",
      "Epoch 277/1000, Avg Training Loss: 0.2509826102504674, Avg Validation Loss: 256.447508265092\n",
      "Epoch 278/1000, Avg Training Loss: 0.25098103669774596, Avg Validation Loss: 256.4458167260792\n",
      "Epoch 279/1000, Avg Training Loss: 0.2509794705347298, Avg Validation Loss: 256.4441353458544\n",
      "Epoch 280/1000, Avg Training Loss: 0.25097791171895456, Avg Validation Loss: 256.44246395987454\n",
      "Epoch 281/1000, Avg Training Loss: 0.2509763601940414, Avg Validation Loss: 256.44080249202574\n",
      "Epoch 282/1000, Avg Training Loss: 0.2509748158681052, Avg Validation Loss: 256.4391508945267\n",
      "Epoch 283/1000, Avg Training Loss: 0.25097327870642777, Avg Validation Loss: 256.4375090524674\n",
      "Epoch 284/1000, Avg Training Loss: 0.2509717486350191, Avg Validation Loss: 256.43587689677685\n",
      "Epoch 285/1000, Avg Training Loss: 0.2509702256182419, Avg Validation Loss: 256.4342542701283\n",
      "Epoch 286/1000, Avg Training Loss: 0.250968709635173, Avg Validation Loss: 256.43264106834084\n",
      "Epoch 287/1000, Avg Training Loss: 0.2509672006021342, Avg Validation Loss: 256.43103725197875\n",
      "Epoch 288/1000, Avg Training Loss: 0.2509656985068388, Avg Validation Loss: 256.42944268197215\n",
      "Epoch 289/1000, Avg Training Loss: 0.25096420320079593, Avg Validation Loss: 256.42785727142933\n",
      "Epoch 290/1000, Avg Training Loss: 0.25096271467993114, Avg Validation Loss: 256.42628104251696\n",
      "Epoch 291/1000, Avg Training Loss: 0.2509612329594439, Avg Validation Loss: 256.42471392802224\n",
      "Epoch 292/1000, Avg Training Loss: 0.2509597579629996, Avg Validation Loss: 256.4231558056968\n",
      "Epoch 293/1000, Avg Training Loss: 0.2509582896877274, Avg Validation Loss: 256.4216065038982\n",
      "Epoch 294/1000, Avg Training Loss: 0.2509568279559716, Avg Validation Loss: 256.42006606769894\n",
      "Epoch 295/1000, Avg Training Loss: 0.2509553727686952, Avg Validation Loss: 256.4185344884065\n",
      "Epoch 296/1000, Avg Training Loss: 0.25095392408004674, Avg Validation Loss: 256.41701171632366\n",
      "Epoch 297/1000, Avg Training Loss: 0.2509524818565748, Avg Validation Loss: 256.415497605623\n",
      "Epoch 298/1000, Avg Training Loss: 0.2509510460586872, Avg Validation Loss: 256.4139920294489\n",
      "Epoch 299/1000, Avg Training Loss: 0.2509496166447519, Avg Validation Loss: 256.41249491341125\n",
      "Epoch 300/1000, Avg Training Loss: 0.2509481935527307, Avg Validation Loss: 256.4110061867067\n",
      "Epoch 301/1000, Avg Training Loss: 0.25094677679331134, Avg Validation Loss: 256.40952573599566\n",
      "Epoch 302/1000, Avg Training Loss: 0.25094536629856695, Avg Validation Loss: 256.4080535139757\n",
      "Epoch 303/1000, Avg Training Loss: 0.25094396198223196, Avg Validation Loss: 256.4065894609838\n",
      "Epoch 304/1000, Avg Training Loss: 0.25094256379999025, Avg Validation Loss: 256.40513349253945\n",
      "Epoch 305/1000, Avg Training Loss: 0.25094117172082975, Avg Validation Loss: 256.4036854968134\n",
      "Epoch 306/1000, Avg Training Loss: 0.25093978569334413, Avg Validation Loss: 256.4022453622233\n",
      "Epoch 307/1000, Avg Training Loss: 0.25093840566734715, Avg Validation Loss: 256.40081296489177\n",
      "Epoch 308/1000, Avg Training Loss: 0.2509370315742917, Avg Validation Loss: 256.39938822550215\n",
      "Epoch 309/1000, Avg Training Loss: 0.25093566337772316, Avg Validation Loss: 256.3979712064888\n",
      "Epoch 310/1000, Avg Training Loss: 0.2509343010706119, Avg Validation Loss: 256.3965618529463\n",
      "Epoch 311/1000, Avg Training Loss: 0.25093294462655324, Avg Validation Loss: 256.3951600553254\n",
      "Epoch 312/1000, Avg Training Loss: 0.2509315940251497, Avg Validation Loss: 256.3937657897701\n",
      "Epoch 313/1000, Avg Training Loss: 0.250930249250008, Avg Validation Loss: 256.392379057139\n",
      "Epoch 314/1000, Avg Training Loss: 0.25092891020029856, Avg Validation Loss: 256.3909997399736\n",
      "Epoch 315/1000, Avg Training Loss: 0.2509275768181938, Avg Validation Loss: 256.3896277232372\n",
      "Epoch 316/1000, Avg Training Loss: 0.2509262490946534, Avg Validation Loss: 256.38826295570186\n",
      "Epoch 317/1000, Avg Training Loss: 0.2509249269402642, Avg Validation Loss: 256.38690543059977\n",
      "Epoch 318/1000, Avg Training Loss: 0.25092361034673333, Avg Validation Loss: 256.38555505945754\n",
      "Epoch 319/1000, Avg Training Loss: 0.2509222992646593, Avg Validation Loss: 256.3842117893711\n",
      "Epoch 320/1000, Avg Training Loss: 0.2509209936129344, Avg Validation Loss: 256.38287564644804\n",
      "Epoch 321/1000, Avg Training Loss: 0.25091969350733084, Avg Validation Loss: 256.38154645859504\n",
      "Epoch 322/1000, Avg Training Loss: 0.2509183988527997, Avg Validation Loss: 256.3802242510601\n",
      "Epoch 323/1000, Avg Training Loss: 0.2509171095865802, Avg Validation Loss: 256.3789091339398\n",
      "Epoch 324/1000, Avg Training Loss: 0.25091582570885895, Avg Validation Loss: 256.37760097597055\n",
      "Epoch 325/1000, Avg Training Loss: 0.2509145471930468, Avg Validation Loss: 256.37629975833556\n",
      "Epoch 326/1000, Avg Training Loss: 0.2509132740151494, Avg Validation Loss: 256.3750053122587\n",
      "Epoch 327/1000, Avg Training Loss: 0.2509120061324568, Avg Validation Loss: 256.3737176634258\n",
      "Epoch 328/1000, Avg Training Loss: 0.25091074353734416, Avg Validation Loss: 256.3724367688879\n",
      "Epoch 329/1000, Avg Training Loss: 0.2509094862112572, Avg Validation Loss: 256.3711625072027\n",
      "Epoch 330/1000, Avg Training Loss: 0.25090823408265955, Avg Validation Loss: 256.36989492242947\n",
      "Epoch 331/1000, Avg Training Loss: 0.2509069872059171, Avg Validation Loss: 256.3686338419881\n",
      "Epoch 332/1000, Avg Training Loss: 0.25090574549228256, Avg Validation Loss: 256.36737911573306\n",
      "Epoch 333/1000, Avg Training Loss: 0.2509045088832449, Avg Validation Loss: 256.3661307775026\n",
      "Epoch 334/1000, Avg Training Loss: 0.2509032774175313, Avg Validation Loss: 256.36488880373287\n",
      "Epoch 335/1000, Avg Training Loss: 0.250902051044402, Avg Validation Loss: 256.363653110286\n",
      "Epoch 336/1000, Avg Training Loss: 0.25090082972695327, Avg Validation Loss: 256.36242370822436\n",
      "Epoch 337/1000, Avg Training Loss: 0.25089961346811, Avg Validation Loss: 256.36120042463375\n",
      "Epoch 338/1000, Avg Training Loss: 0.25089840220787485, Avg Validation Loss: 256.3599832852688\n",
      "Epoch 339/1000, Avg Training Loss: 0.25089719587256604, Avg Validation Loss: 256.35877220583626\n",
      "Epoch 340/1000, Avg Training Loss: 0.2508959944317657, Avg Validation Loss: 256.3575671423004\n",
      "Epoch 341/1000, Avg Training Loss: 0.25089479786940083, Avg Validation Loss: 256.35636807967614\n",
      "Epoch 342/1000, Avg Training Loss: 0.2508936060992865, Avg Validation Loss: 256.35517498377516\n",
      "Epoch 343/1000, Avg Training Loss: 0.2508924191409099, Avg Validation Loss: 256.3539877965028\n",
      "Epoch 344/1000, Avg Training Loss: 0.2508912369531683, Avg Validation Loss: 256.35280656438556\n",
      "Epoch 345/1000, Avg Training Loss: 0.25089005953323124, Avg Validation Loss: 256.35163122235247\n",
      "Epoch 346/1000, Avg Training Loss: 0.2508888868726451, Avg Validation Loss: 256.35046166532976\n",
      "Epoch 347/1000, Avg Training Loss: 0.2508877189282925, Avg Validation Loss: 256.3492978396766\n",
      "Epoch 348/1000, Avg Training Loss: 0.2508865556480998, Avg Validation Loss: 256.34813963588647\n",
      "Epoch 349/1000, Avg Training Loss: 0.25088539695269146, Avg Validation Loss: 256.3469870678048\n",
      "Epoch 350/1000, Avg Training Loss: 0.2508842428000842, Avg Validation Loss: 256.3458401746952\n",
      "Epoch 351/1000, Avg Training Loss: 0.25088309315656215, Avg Validation Loss: 256.3446988293624\n",
      "Epoch 352/1000, Avg Training Loss: 0.2508819480629567, Avg Validation Loss: 256.3435630594028\n",
      "Epoch 353/1000, Avg Training Loss: 0.250880807496968, Avg Validation Loss: 256.34243282322313\n",
      "Epoch 354/1000, Avg Training Loss: 0.2508796714128204, Avg Validation Loss: 256.34130812774094\n",
      "Epoch 355/1000, Avg Training Loss: 0.25087853986261976, Avg Validation Loss: 256.3401889081562\n",
      "Epoch 356/1000, Avg Training Loss: 0.2508774127849455, Avg Validation Loss: 256.33907509290134\n",
      "Epoch 357/1000, Avg Training Loss: 0.25087629015084084, Avg Validation Loss: 256.3379666134369\n",
      "Epoch 358/1000, Avg Training Loss: 0.25087517194663483, Avg Validation Loss: 256.33686343086623\n",
      "Epoch 359/1000, Avg Training Loss: 0.25087405814589064, Avg Validation Loss: 256.33576552579785\n",
      "Epoch 360/1000, Avg Training Loss: 0.25087294871812227, Avg Validation Loss: 256.33467284040836\n",
      "Epoch 361/1000, Avg Training Loss: 0.2508718436420413, Avg Validation Loss: 256.33358525748145\n",
      "Epoch 362/1000, Avg Training Loss: 0.2508707428951871, Avg Validation Loss: 256.3325027805218\n",
      "Epoch 363/1000, Avg Training Loss: 0.250869646434847, Avg Validation Loss: 256.33142541410353\n",
      "Epoch 364/1000, Avg Training Loss: 0.250868554226504, Avg Validation Loss: 256.3303531994572\n",
      "Epoch 365/1000, Avg Training Loss: 0.2508674662896192, Avg Validation Loss: 256.3292861279398\n",
      "Epoch 366/1000, Avg Training Loss: 0.25086638262076927, Avg Validation Loss: 256.32822399131953\n",
      "Epoch 367/1000, Avg Training Loss: 0.250865303155311, Avg Validation Loss: 256.3271667976554\n",
      "Epoch 368/1000, Avg Training Loss: 0.2508642278548554, Avg Validation Loss: 256.3261144929304\n",
      "Epoch 369/1000, Avg Training Loss: 0.2508631567156304, Avg Validation Loss: 256.3250669814935\n",
      "Epoch 370/1000, Avg Training Loss: 0.2508620896731049, Avg Validation Loss: 256.3240243589539\n",
      "Epoch 371/1000, Avg Training Loss: 0.25086102674744015, Avg Validation Loss: 256.32298649292517\n",
      "Epoch 372/1000, Avg Training Loss: 0.25085996787820164, Avg Validation Loss: 256.32195339441\n",
      "Epoch 373/1000, Avg Training Loss: 0.2508589130317762, Avg Validation Loss: 256.3209250747782\n",
      "Epoch 374/1000, Avg Training Loss: 0.2508578622188072, Avg Validation Loss: 256.31990156169104\n",
      "Epoch 375/1000, Avg Training Loss: 0.25085681539533566, Avg Validation Loss: 256.31888284394887\n",
      "Epoch 376/1000, Avg Training Loss: 0.2508557725758806, Avg Validation Loss: 256.31786880816185\n",
      "Epoch 377/1000, Avg Training Loss: 0.25085473376742257, Avg Validation Loss: 256.3168593315879\n",
      "Epoch 378/1000, Avg Training Loss: 0.2508536988947689, Avg Validation Loss: 256.31585447877444\n",
      "Epoch 379/1000, Avg Training Loss: 0.2508526679467622, Avg Validation Loss: 256.31485423567625\n",
      "Epoch 380/1000, Avg Training Loss: 0.2508516409142616, Avg Validation Loss: 256.313858605163\n",
      "Epoch 381/1000, Avg Training Loss: 0.250850617778609, Avg Validation Loss: 256.31286754450247\n",
      "Epoch 382/1000, Avg Training Loss: 0.2508495984943548, Avg Validation Loss: 256.31188100694266\n",
      "Epoch 383/1000, Avg Training Loss: 0.250848583035737, Avg Validation Loss: 256.3108989396787\n",
      "Epoch 384/1000, Avg Training Loss: 0.2508475714079686, Avg Validation Loss: 256.30992127352914\n",
      "Epoch 385/1000, Avg Training Loss: 0.25084656354171775, Avg Validation Loss: 256.30894803251505\n",
      "Epoch 386/1000, Avg Training Loss: 0.2508455594290685, Avg Validation Loss: 256.3079791884268\n",
      "Epoch 387/1000, Avg Training Loss: 0.2508445590408666, Avg Validation Loss: 256.30701469928124\n",
      "Epoch 388/1000, Avg Training Loss: 0.2508435623775008, Avg Validation Loss: 256.3060545765776\n",
      "Epoch 389/1000, Avg Training Loss: 0.2508425694147678, Avg Validation Loss: 256.3050988528388\n",
      "Epoch 390/1000, Avg Training Loss: 0.2508415801462128, Avg Validation Loss: 256.3041475166639\n",
      "Epoch 391/1000, Avg Training Loss: 0.25084059458156904, Avg Validation Loss: 256.3032004663092\n",
      "Epoch 392/1000, Avg Training Loss: 0.25083961266102917, Avg Validation Loss: 256.3022578059678\n",
      "Epoch 393/1000, Avg Training Loss: 0.2508386344022454, Avg Validation Loss: 256.3013194267981\n",
      "Epoch 394/1000, Avg Training Loss: 0.25083765968589344, Avg Validation Loss: 256.3003853502304\n",
      "Epoch 395/1000, Avg Training Loss: 0.25083668862790753, Avg Validation Loss: 256.2994555292285\n",
      "Epoch 396/1000, Avg Training Loss: 0.2508357212153728, Avg Validation Loss: 256.29852984064325\n",
      "Epoch 397/1000, Avg Training Loss: 0.25083475731848975, Avg Validation Loss: 256.297608280993\n",
      "Epoch 398/1000, Avg Training Loss: 0.2508337969842443, Avg Validation Loss: 256.2966908406329\n",
      "Epoch 399/1000, Avg Training Loss: 0.25083284018980906, Avg Validation Loss: 256.2957774240175\n",
      "Epoch 400/1000, Avg Training Loss: 0.25083188695352204, Avg Validation Loss: 256.29486803138985\n",
      "Epoch 401/1000, Avg Training Loss: 0.25083093726951633, Avg Validation Loss: 256.2939625377569\n",
      "Epoch 402/1000, Avg Training Loss: 0.2508299910559897, Avg Validation Loss: 256.29306106048125\n",
      "Epoch 403/1000, Avg Training Loss: 0.2508290483152542, Avg Validation Loss: 256.2921635930827\n",
      "Epoch 404/1000, Avg Training Loss: 0.25082810904691305, Avg Validation Loss: 256.29127007929094\n",
      "Epoch 405/1000, Avg Training Loss: 0.2508271732136539, Avg Validation Loss: 256.29038047298354\n",
      "Epoch 406/1000, Avg Training Loss: 0.250826240763378, Avg Validation Loss: 256.28949478125\n",
      "Epoch 407/1000, Avg Training Loss: 0.25082531169907685, Avg Validation Loss: 256.2886129720338\n",
      "Epoch 408/1000, Avg Training Loss: 0.25082438598850776, Avg Validation Loss: 256.28773504689514\n",
      "Epoch 409/1000, Avg Training Loss: 0.25082346366056196, Avg Validation Loss: 256.2868609315679\n",
      "Epoch 410/1000, Avg Training Loss: 0.25082254467296405, Avg Validation Loss: 256.28599054503377\n",
      "Epoch 411/1000, Avg Training Loss: 0.25082162893148013, Avg Validation Loss: 256.2851240153134\n",
      "Epoch 412/1000, Avg Training Loss: 0.25082071645229886, Avg Validation Loss: 256.28426141683957\n",
      "Epoch 413/1000, Avg Training Loss: 0.2508198073094597, Avg Validation Loss: 256.28340264681\n",
      "Epoch 414/1000, Avg Training Loss: 0.2508189014391503, Avg Validation Loss: 256.2825477216645\n",
      "Epoch 415/1000, Avg Training Loss: 0.2508179988737798, Avg Validation Loss: 256.28169656752704\n",
      "Epoch 416/1000, Avg Training Loss: 0.25081709956124576, Avg Validation Loss: 256.2808491482866\n",
      "Epoch 417/1000, Avg Training Loss: 0.2508162034601786, Avg Validation Loss: 256.28000537036434\n",
      "Epoch 418/1000, Avg Training Loss: 0.25081531057366296, Avg Validation Loss: 256.2791651440526\n",
      "Epoch 419/1000, Avg Training Loss: 0.2508144208827515, Avg Validation Loss: 256.2783285089963\n",
      "Epoch 420/1000, Avg Training Loss: 0.25081353438919574, Avg Validation Loss: 256.2774954322844\n",
      "Epoch 421/1000, Avg Training Loss: 0.25081265108844697, Avg Validation Loss: 256.2766657778121\n",
      "Epoch 422/1000, Avg Training Loss: 0.25081177094113327, Avg Validation Loss: 256.27583960674906\n",
      "Epoch 423/1000, Avg Training Loss: 0.25081089391415673, Avg Validation Loss: 256.27501698429313\n",
      "Epoch 424/1000, Avg Training Loss: 0.2508100200001548, Avg Validation Loss: 256.2741977703711\n",
      "Epoch 425/1000, Avg Training Loss: 0.25080914917048197, Avg Validation Loss: 256.2733820587755\n",
      "Epoch 426/1000, Avg Training Loss: 0.2508082814592383, Avg Validation Loss: 256.27256990504793\n",
      "Epoch 427/1000, Avg Training Loss: 0.25080741686234503, Avg Validation Loss: 256.27176124444213\n",
      "Epoch 428/1000, Avg Training Loss: 0.25080655536604735, Avg Validation Loss: 256.2709560500428\n",
      "Epoch 429/1000, Avg Training Loss: 0.2508056969774783, Avg Validation Loss: 256.2701542785352\n",
      "Epoch 430/1000, Avg Training Loss: 0.2508048416356884, Avg Validation Loss: 256.269355942052\n",
      "Epoch 431/1000, Avg Training Loss: 0.2508039893604533, Avg Validation Loss: 256.2685609951116\n",
      "Epoch 432/1000, Avg Training Loss: 0.25080314013216504, Avg Validation Loss: 256.26776933863863\n",
      "Epoch 433/1000, Avg Training Loss: 0.2508022938890155, Avg Validation Loss: 256.2669811357451\n",
      "Epoch 434/1000, Avg Training Loss: 0.25080145062991926, Avg Validation Loss: 256.26619647997154\n",
      "Epoch 435/1000, Avg Training Loss: 0.2508006104350848, Avg Validation Loss: 256.26541522373066\n",
      "Epoch 436/1000, Avg Training Loss: 0.2507997732701332, Avg Validation Loss: 256.26463734758266\n",
      "Epoch 437/1000, Avg Training Loss: 0.2507989391033984, Avg Validation Loss: 256.2638627859066\n",
      "Epoch 438/1000, Avg Training Loss: 0.25079810792014484, Avg Validation Loss: 256.2630915000461\n",
      "Epoch 439/1000, Avg Training Loss: 0.25079727969418886, Avg Validation Loss: 256.2623234796819\n",
      "Epoch 440/1000, Avg Training Loss: 0.25079645441916654, Avg Validation Loss: 256.26155864576106\n",
      "Epoch 441/1000, Avg Training Loss: 0.2507956320664467, Avg Validation Loss: 256.2607970462408\n",
      "Epoch 442/1000, Avg Training Loss: 0.2507948125815732, Avg Validation Loss: 256.26003865518726\n",
      "Epoch 443/1000, Avg Training Loss: 0.2507939959892571, Avg Validation Loss: 256.2592833610355\n",
      "Epoch 444/1000, Avg Training Loss: 0.2507931822718226, Avg Validation Loss: 256.25853128079655\n",
      "Epoch 445/1000, Avg Training Loss: 0.25079237145905764, Avg Validation Loss: 256.25778240120377\n",
      "Epoch 446/1000, Avg Training Loss: 0.2507915635083409, Avg Validation Loss: 256.257036689524\n",
      "Epoch 447/1000, Avg Training Loss: 0.25079075847432275, Avg Validation Loss: 256.2562941708057\n",
      "Epoch 448/1000, Avg Training Loss: 0.250789956303936, Avg Validation Loss: 256.25555473401255\n",
      "Epoch 449/1000, Avg Training Loss: 0.2507891569579527, Avg Validation Loss: 256.2548184204949\n",
      "Epoch 450/1000, Avg Training Loss: 0.25078836043215336, Avg Validation Loss: 256.25408519463826\n",
      "Epoch 451/1000, Avg Training Loss: 0.25078756673613933, Avg Validation Loss: 256.25335502515384\n",
      "Epoch 452/1000, Avg Training Loss: 0.2507867758703516, Avg Validation Loss: 256.2526278261921\n",
      "Epoch 453/1000, Avg Training Loss: 0.25078598778301175, Avg Validation Loss: 256.2519036553393\n",
      "Epoch 454/1000, Avg Training Loss: 0.25078520241917246, Avg Validation Loss: 256.25118256528316\n",
      "Epoch 455/1000, Avg Training Loss: 0.2507844198508023, Avg Validation Loss: 256.2504644020738\n",
      "Epoch 456/1000, Avg Training Loss: 0.25078363998538244, Avg Validation Loss: 256.2497492220531\n",
      "Epoch 457/1000, Avg Training Loss: 0.25078286278864476, Avg Validation Loss: 256.2490371126306\n",
      "Epoch 458/1000, Avg Training Loss: 0.2507820883056086, Avg Validation Loss: 256.248328077976\n",
      "Epoch 459/1000, Avg Training Loss: 0.2507813165419117, Avg Validation Loss: 256.2476222359012\n",
      "Epoch 460/1000, Avg Training Loss: 0.2507805475121419, Avg Validation Loss: 256.2469196490015\n",
      "Epoch 461/1000, Avg Training Loss: 0.2507797812727879, Avg Validation Loss: 256.2462201102244\n",
      "Epoch 462/1000, Avg Training Loss: 0.2507790177491963, Avg Validation Loss: 256.24552349963096\n",
      "Epoch 463/1000, Avg Training Loss: 0.250778256924624, Avg Validation Loss: 256.24482974639074\n",
      "Epoch 464/1000, Avg Training Loss: 0.2507774987851703, Avg Validation Loss: 256.2441388387339\n",
      "Epoch 465/1000, Avg Training Loss: 0.25077674333624767, Avg Validation Loss: 256.2434507428822\n",
      "Epoch 466/1000, Avg Training Loss: 0.25077599057061867, Avg Validation Loss: 256.2427655345313\n",
      "Epoch 467/1000, Avg Training Loss: 0.2507752404928162, Avg Validation Loss: 256.2420832141416\n",
      "Epoch 468/1000, Avg Training Loss: 0.2507744930891206, Avg Validation Loss: 256.2414035928772\n",
      "Epoch 469/1000, Avg Training Loss: 0.25077374828485377, Avg Validation Loss: 256.240726689692\n",
      "Epoch 470/1000, Avg Training Loss: 0.25077300605855846, Avg Validation Loss: 256.24005251941935\n",
      "Epoch 471/1000, Avg Training Loss: 0.2507722664449329, Avg Validation Loss: 256.2393810321147\n",
      "Epoch 472/1000, Avg Training Loss: 0.2507715293807492, Avg Validation Loss: 256.2387123812139\n",
      "Epoch 473/1000, Avg Training Loss: 0.2507707948907963, Avg Validation Loss: 256.2380465619577\n",
      "Epoch 474/1000, Avg Training Loss: 0.25077006300433924, Avg Validation Loss: 256.23738342075643\n",
      "Epoch 475/1000, Avg Training Loss: 0.2507693336105921, Avg Validation Loss: 256.2367230978881\n",
      "Epoch 476/1000, Avg Training Loss: 0.2507686067578718, Avg Validation Loss: 256.2360654978737\n",
      "Epoch 477/1000, Avg Training Loss: 0.250767882449899, Avg Validation Loss: 256.2354105245994\n",
      "Epoch 478/1000, Avg Training Loss: 0.2507671606293033, Avg Validation Loss: 256.2347582932297\n",
      "Epoch 479/1000, Avg Training Loss: 0.2507664413154497, Avg Validation Loss: 256.2341087953235\n",
      "Epoch 480/1000, Avg Training Loss: 0.2507657245154251, Avg Validation Loss: 256.23346200628856\n",
      "Epoch 481/1000, Avg Training Loss: 0.25076501022543024, Avg Validation Loss: 256.2328178603633\n",
      "Epoch 482/1000, Avg Training Loss: 0.25076429839472475, Avg Validation Loss: 256.23217634440596\n",
      "Epoch 483/1000, Avg Training Loss: 0.2507635890109312, Avg Validation Loss: 256.2315375064586\n",
      "Epoch 484/1000, Avg Training Loss: 0.2507628820775946, Avg Validation Loss: 256.2309012548671\n",
      "Epoch 485/1000, Avg Training Loss: 0.25076217756553454, Avg Validation Loss: 256.2302676502255\n",
      "Epoch 486/1000, Avg Training Loss: 0.2507614754936532, Avg Validation Loss: 256.22963680639685\n",
      "Epoch 487/1000, Avg Training Loss: 0.2507607758353775, Avg Validation Loss: 256.22900868299575\n",
      "Epoch 488/1000, Avg Training Loss: 0.25076007861026334, Avg Validation Loss: 256.22838323541214\n",
      "Epoch 489/1000, Avg Training Loss: 0.2507593837830106, Avg Validation Loss: 256.22776038572664\n",
      "Epoch 490/1000, Avg Training Loss: 0.2507586913323393, Avg Validation Loss: 256.2271400802256\n",
      "Epoch 491/1000, Avg Training Loss: 0.25075800120840086, Avg Validation Loss: 256.22652230144695\n",
      "Epoch 492/1000, Avg Training Loss: 0.25075731338775986, Avg Validation Loss: 256.2259070219841\n",
      "Epoch 493/1000, Avg Training Loss: 0.2507566278661024, Avg Validation Loss: 256.22529430635063\n",
      "Epoch 494/1000, Avg Training Loss: 0.2507559446430625, Avg Validation Loss: 256.2246841314845\n",
      "Epoch 495/1000, Avg Training Loss: 0.2507552637318134, Avg Validation Loss: 256.2240764510999\n",
      "Epoch 496/1000, Avg Training Loss: 0.25075458506696224, Avg Validation Loss: 256.2234713384382\n",
      "Epoch 497/1000, Avg Training Loss: 0.25075390865698727, Avg Validation Loss: 256.2228688243175\n",
      "Epoch 498/1000, Avg Training Loss: 0.2507532344939858, Avg Validation Loss: 256.2222689335921\n",
      "Epoch 499/1000, Avg Training Loss: 0.25075256259540885, Avg Validation Loss: 256.22167169986733\n",
      "Epoch 500/1000, Avg Training Loss: 0.25075189297652234, Avg Validation Loss: 256.2210770359738\n",
      "Epoch 501/1000, Avg Training Loss: 0.250751225612256, Avg Validation Loss: 256.22048489127513\n",
      "Epoch 502/1000, Avg Training Loss: 0.2507505604406972, Avg Validation Loss: 256.21989529199016\n",
      "Epoch 503/1000, Avg Training Loss: 0.2507498975110119, Avg Validation Loss: 256.2193082757897\n",
      "Epoch 504/1000, Avg Training Loss: 0.25074923682679234, Avg Validation Loss: 256.2187237546016\n",
      "Epoch 505/1000, Avg Training Loss: 0.2507485783210012, Avg Validation Loss: 256.21814179930664\n",
      "Epoch 506/1000, Avg Training Loss: 0.2507479220338294, Avg Validation Loss: 256.2175623841777\n",
      "Epoch 507/1000, Avg Training Loss: 0.2507472679277445, Avg Validation Loss: 256.2169855154066\n",
      "Epoch 508/1000, Avg Training Loss: 0.25074661598399, Avg Validation Loss: 256.21641123185253\n",
      "Epoch 509/1000, Avg Training Loss: 0.25074596623552947, Avg Validation Loss: 256.2158394961996\n",
      "Epoch 510/1000, Avg Training Loss: 0.2507453186527657, Avg Validation Loss: 256.21527022992746\n",
      "Epoch 511/1000, Avg Training Loss: 0.25074467318934895, Avg Validation Loss: 256.21470349931934\n",
      "Epoch 512/1000, Avg Training Loss: 0.250744029806358, Avg Validation Loss: 256.2141393878003\n",
      "Epoch 513/1000, Avg Training Loss: 0.2507433885740004, Avg Validation Loss: 256.21357779979564\n",
      "Epoch 514/1000, Avg Training Loss: 0.2507427495070861, Avg Validation Loss: 256.2130186282396\n",
      "Epoch 515/1000, Avg Training Loss: 0.2507421125203739, Avg Validation Loss: 256.2124619846805\n",
      "Epoch 516/1000, Avg Training Loss: 0.2507414775938473, Avg Validation Loss: 256.21190791450783\n",
      "Epoch 517/1000, Avg Training Loss: 0.25074084480013825, Avg Validation Loss: 256.2113562945608\n",
      "Epoch 518/1000, Avg Training Loss: 0.2507402140875539, Avg Validation Loss: 256.210807106758\n",
      "Epoch 519/1000, Avg Training Loss: 0.250739585444422, Avg Validation Loss: 256.21026033073247\n",
      "Epoch 520/1000, Avg Training Loss: 0.2507389588796437, Avg Validation Loss: 256.20971592970386\n",
      "Epoch 521/1000, Avg Training Loss: 0.25073833441614984, Avg Validation Loss: 256.2091737782523\n",
      "Epoch 522/1000, Avg Training Loss: 0.2507377119933942, Avg Validation Loss: 256.2086339233075\n",
      "Epoch 523/1000, Avg Training Loss: 0.2507370915581517, Avg Validation Loss: 256.2080964895994\n",
      "Epoch 524/1000, Avg Training Loss: 0.25073647316345016, Avg Validation Loss: 256.2075613376658\n",
      "Epoch 525/1000, Avg Training Loss: 0.25073585673842824, Avg Validation Loss: 256.2070284927398\n",
      "Epoch 526/1000, Avg Training Loss: 0.2507352422884278, Avg Validation Loss: 256.2064980058957\n",
      "Epoch 527/1000, Avg Training Loss: 0.25073462984763983, Avg Validation Loss: 256.2059698521605\n",
      "Epoch 528/1000, Avg Training Loss: 0.25073401942451806, Avg Validation Loss: 256.2054439140385\n",
      "Epoch 529/1000, Avg Training Loss: 0.2507334109731929, Avg Validation Loss: 256.20492021756854\n",
      "Epoch 530/1000, Avg Training Loss: 0.25073280447295093, Avg Validation Loss: 256.20439885086836\n",
      "Epoch 531/1000, Avg Training Loss: 0.2507321999284311, Avg Validation Loss: 256.2038798066285\n",
      "Epoch 532/1000, Avg Training Loss: 0.2507315973750191, Avg Validation Loss: 256.20336297091063\n",
      "Epoch 533/1000, Avg Training Loss: 0.25073099675929045, Avg Validation Loss: 256.20284830284305\n",
      "Epoch 534/1000, Avg Training Loss: 0.25073039805516373, Avg Validation Loss: 256.2023359033527\n",
      "Epoch 535/1000, Avg Training Loss: 0.25072980126111516, Avg Validation Loss: 256.20182579537214\n",
      "Epoch 536/1000, Avg Training Loss: 0.2507292063915127, Avg Validation Loss: 256.201318063739\n",
      "Epoch 537/1000, Avg Training Loss: 0.2507286134080663, Avg Validation Loss: 256.20081272773496\n",
      "Epoch 538/1000, Avg Training Loss: 0.25072802234979363, Avg Validation Loss: 256.20030964900684\n",
      "Epoch 539/1000, Avg Training Loss: 0.25072743321842317, Avg Validation Loss: 256.19980879046165\n",
      "Epoch 540/1000, Avg Training Loss: 0.2507268459775765, Avg Validation Loss: 256.1993102451934\n",
      "Epoch 541/1000, Avg Training Loss: 0.2507262606611183, Avg Validation Loss: 256.1988139710123\n",
      "Epoch 542/1000, Avg Training Loss: 0.25072567725787853, Avg Validation Loss: 256.19831992059085\n",
      "Epoch 543/1000, Avg Training Loss: 0.2507250957308982, Avg Validation Loss: 256.1978280912967\n",
      "Epoch 544/1000, Avg Training Loss: 0.25072451607492674, Avg Validation Loss: 256.19733839388255\n",
      "Epoch 545/1000, Avg Training Loss: 0.25072393830078965, Avg Validation Loss: 256.1968508365438\n",
      "Epoch 546/1000, Avg Training Loss: 0.25072336235937415, Avg Validation Loss: 256.19636541390025\n",
      "Epoch 547/1000, Avg Training Loss: 0.2507227882557726, Avg Validation Loss: 256.19588206879143\n",
      "Epoch 548/1000, Avg Training Loss: 0.25072221601237227, Avg Validation Loss: 256.19540071743324\n",
      "Epoch 549/1000, Avg Training Loss: 0.25072164556946375, Avg Validation Loss: 256.19492138998686\n",
      "Epoch 550/1000, Avg Training Loss: 0.25072107690608963, Avg Validation Loss: 256.1944441754979\n",
      "Epoch 551/1000, Avg Training Loss: 0.25072051002908996, Avg Validation Loss: 256.193969029895\n",
      "Epoch 552/1000, Avg Training Loss: 0.2507199449033359, Avg Validation Loss: 256.1934960216448\n",
      "Epoch 553/1000, Avg Training Loss: 0.25071938156165674, Avg Validation Loss: 256.19302508725434\n",
      "Epoch 554/1000, Avg Training Loss: 0.2507188199690752, Avg Validation Loss: 256.1925561986103\n",
      "Epoch 555/1000, Avg Training Loss: 0.2507182601191079, Avg Validation Loss: 256.19208939987607\n",
      "Epoch 556/1000, Avg Training Loss: 0.2507177020104471, Avg Validation Loss: 256.1916246902847\n",
      "Epoch 557/1000, Avg Training Loss: 0.25071714559778063, Avg Validation Loss: 256.1911621895711\n",
      "Epoch 558/1000, Avg Training Loss: 0.2507165908844179, Avg Validation Loss: 256.1907019201839\n",
      "Epoch 559/1000, Avg Training Loss: 0.2507160378863473, Avg Validation Loss: 256.1902438247557\n",
      "Epoch 560/1000, Avg Training Loss: 0.25071548665222443, Avg Validation Loss: 256.1897877831128\n",
      "Epoch 561/1000, Avg Training Loss: 0.2507149371189904, Avg Validation Loss: 256.18933376140706\n",
      "Epoch 562/1000, Avg Training Loss: 0.25071438926347256, Avg Validation Loss: 256.1888817431435\n",
      "Epoch 563/1000, Avg Training Loss: 0.2507138430588314, Avg Validation Loss: 256.1884317383967\n",
      "Epoch 564/1000, Avg Training Loss: 0.2507132984902571, Avg Validation Loss: 256.18798379643636\n",
      "Epoch 565/1000, Avg Training Loss: 0.25071275557604966, Avg Validation Loss: 256.18753789324404\n",
      "Epoch 566/1000, Avg Training Loss: 0.25071221433595364, Avg Validation Loss: 256.1870939726549\n",
      "Epoch 567/1000, Avg Training Loss: 0.2507116747169791, Avg Validation Loss: 256.1866521228378\n",
      "Epoch 568/1000, Avg Training Loss: 0.2507111367029459, Avg Validation Loss: 256.1862124395768\n",
      "Epoch 569/1000, Avg Training Loss: 0.2507106003298502, Avg Validation Loss: 256.1857748534301\n",
      "Epoch 570/1000, Avg Training Loss: 0.2507100656002395, Avg Validation Loss: 256.18533931470216\n",
      "Epoch 571/1000, Avg Training Loss: 0.250709532496666, Avg Validation Loss: 256.1849057636817\n",
      "Epoch 572/1000, Avg Training Loss: 0.2507090009958937, Avg Validation Loss: 256.18447421870496\n",
      "Epoch 573/1000, Avg Training Loss: 0.25070847113004663, Avg Validation Loss: 256.18404461127153\n",
      "Epoch 574/1000, Avg Training Loss: 0.2507079428193853, Avg Validation Loss: 256.1836169774547\n",
      "Epoch 575/1000, Avg Training Loss: 0.2507074160616478, Avg Validation Loss: 256.18319137331525\n",
      "Epoch 576/1000, Avg Training Loss: 0.2507068909058944, Avg Validation Loss: 256.18276770789805\n",
      "Epoch 577/1000, Avg Training Loss: 0.2507063673320512, Avg Validation Loss: 256.1823460172694\n",
      "Epoch 578/1000, Avg Training Loss: 0.2507058453410383, Avg Validation Loss: 256.18192625930425\n",
      "Epoch 579/1000, Avg Training Loss: 0.250705324892182, Avg Validation Loss: 256.1815085190193\n",
      "Epoch 580/1000, Avg Training Loss: 0.2507048059990062, Avg Validation Loss: 256.18109279368866\n",
      "Epoch 581/1000, Avg Training Loss: 0.2507042886898226, Avg Validation Loss: 256.18067897680226\n",
      "Epoch 582/1000, Avg Training Loss: 0.2507037729325055, Avg Validation Loss: 256.1802670396438\n",
      "Epoch 583/1000, Avg Training Loss: 0.2507032586848821, Avg Validation Loss: 256.1798570702524\n",
      "Epoch 584/1000, Avg Training Loss: 0.25070274595655984, Avg Validation Loss: 256.1794491177733\n",
      "Epoch 585/1000, Avg Training Loss: 0.2507022347565467, Avg Validation Loss: 256.1790430702281\n",
      "Epoch 586/1000, Avg Training Loss: 0.2507017250442515, Avg Validation Loss: 256.1786389390901\n",
      "Epoch 587/1000, Avg Training Loss: 0.250701216834063, Avg Validation Loss: 256.17823671754536\n",
      "Epoch 588/1000, Avg Training Loss: 0.25070071014437, Avg Validation Loss: 256.1778362490404\n",
      "Epoch 589/1000, Avg Training Loss: 0.2507002048933577, Avg Validation Loss: 256.17743761954705\n",
      "Epoch 590/1000, Avg Training Loss: 0.25069970108844786, Avg Validation Loss: 256.17704087272114\n",
      "Epoch 591/1000, Avg Training Loss: 0.25069919873274754, Avg Validation Loss: 256.17664603052515\n",
      "Epoch 592/1000, Avg Training Loss: 0.25069869785669263, Avg Validation Loss: 256.1762530189251\n",
      "Epoch 593/1000, Avg Training Loss: 0.2506981983913303, Avg Validation Loss: 256.1758618487099\n",
      "Epoch 594/1000, Avg Training Loss: 0.25069770036012007, Avg Validation Loss: 256.1754725419445\n",
      "Epoch 595/1000, Avg Training Loss: 0.25069720376388654, Avg Validation Loss: 256.1750850751264\n",
      "Epoch 596/1000, Avg Training Loss: 0.2506967085699172, Avg Validation Loss: 256.1746994768105\n",
      "Epoch 597/1000, Avg Training Loss: 0.25069621478920506, Avg Validation Loss: 256.1743157185317\n",
      "Epoch 598/1000, Avg Training Loss: 0.25069572240542803, Avg Validation Loss: 256.17393379604766\n",
      "Epoch 599/1000, Avg Training Loss: 0.25069523140435845, Avg Validation Loss: 256.17355364540435\n",
      "Epoch 600/1000, Avg Training Loss: 0.25069474176440354, Avg Validation Loss: 256.1731753288797\n",
      "Epoch 601/1000, Avg Training Loss: 0.2506942534766607, Avg Validation Loss: 256.1727989023923\n",
      "Epoch 602/1000, Avg Training Loss: 0.25069376656702647, Avg Validation Loss: 256.1724242868522\n",
      "Epoch 603/1000, Avg Training Loss: 0.25069328101040655, Avg Validation Loss: 256.172051480157\n",
      "Epoch 604/1000, Avg Training Loss: 0.2506927968046257, Avg Validation Loss: 256.17168048378335\n",
      "Epoch 605/1000, Avg Training Loss: 0.25069231396238356, Avg Validation Loss: 256.17131131521626\n",
      "Epoch 606/1000, Avg Training Loss: 0.25069183246360716, Avg Validation Loss: 256.17094399347786\n",
      "Epoch 607/1000, Avg Training Loss: 0.2506913523139904, Avg Validation Loss: 256.1705785053108\n",
      "Epoch 608/1000, Avg Training Loss: 0.25069087352403224, Avg Validation Loss: 256.1702147514993\n",
      "Epoch 609/1000, Avg Training Loss: 0.25069039605921833, Avg Validation Loss: 256.1698527096819\n",
      "Epoch 610/1000, Avg Training Loss: 0.25068991992261763, Avg Validation Loss: 256.1694922945908\n",
      "Epoch 611/1000, Avg Training Loss: 0.25068944504176044, Avg Validation Loss: 256.1691336186507\n",
      "Epoch 612/1000, Avg Training Loss: 0.2506889714476673, Avg Validation Loss: 256.1687766997984\n",
      "Epoch 613/1000, Avg Training Loss: 0.2506884991104022, Avg Validation Loss: 256.16842159408293\n",
      "Epoch 614/1000, Avg Training Loss: 0.25068802805529183, Avg Validation Loss: 256.168068262577\n",
      "Epoch 615/1000, Avg Training Loss: 0.25068755827600775, Avg Validation Loss: 256.1677166700941\n",
      "Epoch 616/1000, Avg Training Loss: 0.25068708977752396, Avg Validation Loss: 256.1673667332844\n",
      "Epoch 617/1000, Avg Training Loss: 0.2506866225028435, Avg Validation Loss: 256.16701857131625\n",
      "Epoch 618/1000, Avg Training Loss: 0.25068615646402426, Avg Validation Loss: 256.16667222557595\n",
      "Epoch 619/1000, Avg Training Loss: 0.25068569164798016, Avg Validation Loss: 256.1663277020648\n",
      "Epoch 620/1000, Avg Training Loss: 0.2506852280619467, Avg Validation Loss: 256.1659849536719\n",
      "Epoch 621/1000, Avg Training Loss: 0.250684765686453, Avg Validation Loss: 256.1656439563309\n",
      "Epoch 622/1000, Avg Training Loss: 0.2506843045096216, Avg Validation Loss: 256.165304801208\n",
      "Epoch 623/1000, Avg Training Loss: 0.2506838445503548, Avg Validation Loss: 256.16496748178594\n",
      "Epoch 624/1000, Avg Training Loss: 0.25068338579380883, Avg Validation Loss: 256.16463200421305\n",
      "Epoch 625/1000, Avg Training Loss: 0.2506829282783156, Avg Validation Loss: 256.16429832892595\n",
      "Epoch 626/1000, Avg Training Loss: 0.2506824719567591, Avg Validation Loss: 256.1639664113078\n",
      "Epoch 627/1000, Avg Training Loss: 0.25068201681211777, Avg Validation Loss: 256.1636362695996\n",
      "Epoch 628/1000, Avg Training Loss: 0.2506815628444301, Avg Validation Loss: 256.1633079381379\n",
      "Epoch 629/1000, Avg Training Loss: 0.25068111008095567, Avg Validation Loss: 256.1629814017643\n",
      "Epoch 630/1000, Avg Training Loss: 0.2506806584905015, Avg Validation Loss: 256.1626566202193\n",
      "Epoch 631/1000, Avg Training Loss: 0.2506802080681802, Avg Validation Loss: 256.16233362620346\n",
      "Epoch 632/1000, Avg Training Loss: 0.2506797587939429, Avg Validation Loss: 256.16201244748703\n",
      "Epoch 633/1000, Avg Training Loss: 0.25067931070221494, Avg Validation Loss: 256.16169303874176\n",
      "Epoch 634/1000, Avg Training Loss: 0.25067886379077137, Avg Validation Loss: 256.1613753430269\n",
      "Epoch 635/1000, Avg Training Loss: 0.2506784180392125, Avg Validation Loss: 256.1610593427223\n",
      "Epoch 636/1000, Avg Training Loss: 0.2506779734262516, Avg Validation Loss: 256.16074505086885\n",
      "Epoch 637/1000, Avg Training Loss: 0.25067752997083786, Avg Validation Loss: 256.1604324266258\n",
      "Epoch 638/1000, Avg Training Loss: 0.2506770876525788, Avg Validation Loss: 256.1601214431254\n",
      "Epoch 639/1000, Avg Training Loss: 0.2506766464632581, Avg Validation Loss: 256.15981212832105\n",
      "Epoch 640/1000, Avg Training Loss: 0.25067620640701743, Avg Validation Loss: 256.1595044483056\n",
      "Epoch 641/1000, Avg Training Loss: 0.2506757674775249, Avg Validation Loss: 256.159198411118\n",
      "Epoch 642/1000, Avg Training Loss: 0.2506753296647712, Avg Validation Loss: 256.1588940497368\n",
      "Epoch 643/1000, Avg Training Loss: 0.25067489297036394, Avg Validation Loss: 256.1585912862464\n",
      "Epoch 644/1000, Avg Training Loss: 0.25067445736303795, Avg Validation Loss: 256.158290178236\n",
      "Epoch 645/1000, Avg Training Loss: 0.2506740228343717, Avg Validation Loss: 256.1579907408894\n",
      "Epoch 646/1000, Avg Training Loss: 0.2506735894102428, Avg Validation Loss: 256.1576930370031\n",
      "Epoch 647/1000, Avg Training Loss: 0.2506731570963553, Avg Validation Loss: 256.1573969999125\n",
      "Epoch 648/1000, Avg Training Loss: 0.2506727258582066, Avg Validation Loss: 256.15710258509944\n",
      "Epoch 649/1000, Avg Training Loss: 0.2506722956783598, Avg Validation Loss: 256.15680981315893\n",
      "Epoch 650/1000, Avg Training Loss: 0.25067186658482576, Avg Validation Loss: 256.1565186468677\n",
      "Epoch 651/1000, Avg Training Loss: 0.25067143860374724, Avg Validation Loss: 256.15622904953904\n",
      "Epoch 652/1000, Avg Training Loss: 0.2506710116762257, Avg Validation Loss: 256.15594107029375\n",
      "Epoch 653/1000, Avg Training Loss: 0.2506705858245387, Avg Validation Loss: 256.1556546849815\n",
      "Epoch 654/1000, Avg Training Loss: 0.2506701610344955, Avg Validation Loss: 256.15536988739456\n",
      "Epoch 655/1000, Avg Training Loss: 0.25066973730413794, Avg Validation Loss: 256.1550867040749\n",
      "Epoch 656/1000, Avg Training Loss: 0.25066931461878655, Avg Validation Loss: 256.15480513362957\n",
      "Epoch 657/1000, Avg Training Loss: 0.2506688929839993, Avg Validation Loss: 256.15452512370973\n",
      "Epoch 658/1000, Avg Training Loss: 0.2506684723900892, Avg Validation Loss: 256.15424667338704\n",
      "Epoch 659/1000, Avg Training Loss: 0.2506680528282918, Avg Validation Loss: 256.1539698594037\n",
      "Epoch 660/1000, Avg Training Loss: 0.2506676343041978, Avg Validation Loss: 256.1536946300771\n",
      "Epoch 661/1000, Avg Training Loss: 0.250667216784575, Avg Validation Loss: 256.1534210050475\n",
      "Epoch 662/1000, Avg Training Loss: 0.25066680027969307, Avg Validation Loss: 256.1531489095587\n",
      "Epoch 663/1000, Avg Training Loss: 0.2506663847657562, Avg Validation Loss: 256.1528783942404\n",
      "Epoch 664/1000, Avg Training Loss: 0.25066597023470805, Avg Validation Loss: 256.1526095410975\n",
      "Epoch 665/1000, Avg Training Loss: 0.25066555670753365, Avg Validation Loss: 256.15234232381397\n",
      "Epoch 666/1000, Avg Training Loss: 0.2506651442022113, Avg Validation Loss: 256.1520766185315\n",
      "Epoch 667/1000, Avg Training Loss: 0.2506647326891013, Avg Validation Loss: 256.15181250826254\n",
      "Epoch 668/1000, Avg Training Loss: 0.25066432214580253, Avg Validation Loss: 256.1515500241271\n",
      "Epoch 669/1000, Avg Training Loss: 0.2506639126145744, Avg Validation Loss: 256.15128917713406\n",
      "Epoch 670/1000, Avg Training Loss: 0.2506635040770117, Avg Validation Loss: 256.15102989923923\n",
      "Epoch 671/1000, Avg Training Loss: 0.2506630965209238, Avg Validation Loss: 256.150772132326\n",
      "Epoch 672/1000, Avg Training Loss: 0.25066268993645563, Avg Validation Loss: 256.1505158130656\n",
      "Epoch 673/1000, Avg Training Loss: 0.25066228429781556, Avg Validation Loss: 256.15026099467866\n",
      "Epoch 674/1000, Avg Training Loss: 0.2506618795909764, Avg Validation Loss: 256.1500077878386\n",
      "Epoch 675/1000, Avg Training Loss: 0.2506614758283569, Avg Validation Loss: 256.14975610907857\n",
      "Epoch 676/1000, Avg Training Loss: 0.2506610730089005, Avg Validation Loss: 256.1495059471514\n",
      "Epoch 677/1000, Avg Training Loss: 0.2506606711295229, Avg Validation Loss: 256.1492572835398\n",
      "Epoch 678/1000, Avg Training Loss: 0.25066027017362824, Avg Validation Loss: 256.1490101404504\n",
      "Epoch 679/1000, Avg Training Loss: 0.2506598701403587, Avg Validation Loss: 256.1487645095771\n",
      "Epoch 680/1000, Avg Training Loss: 0.25065947100712177, Avg Validation Loss: 256.1485203307765\n",
      "Epoch 681/1000, Avg Training Loss: 0.2506590727422911, Avg Validation Loss: 256.14827761278696\n",
      "Epoch 682/1000, Avg Training Loss: 0.250658675371355, Avg Validation Loss: 256.1480363859611\n",
      "Epoch 683/1000, Avg Training Loss: 0.25065827887640707, Avg Validation Loss: 256.1477966515061\n",
      "Epoch 684/1000, Avg Training Loss: 0.25065788328573285, Avg Validation Loss: 256.14755841946453\n",
      "Epoch 685/1000, Avg Training Loss: 0.2506574885514803, Avg Validation Loss: 256.14732176243984\n",
      "Epoch 686/1000, Avg Training Loss: 0.250657094713319, Avg Validation Loss: 256.1470866324304\n",
      "Epoch 687/1000, Avg Training Loss: 0.2506567017830191, Avg Validation Loss: 256.14685296079966\n",
      "Epoch 688/1000, Avg Training Loss: 0.25065630975024417, Avg Validation Loss: 256.1466208069701\n",
      "Epoch 689/1000, Avg Training Loss: 0.25065591860071207, Avg Validation Loss: 256.1463902019906\n",
      "Epoch 690/1000, Avg Training Loss: 0.2506555283195372, Avg Validation Loss: 256.1461611771364\n",
      "Epoch 691/1000, Avg Training Loss: 0.25065513892268027, Avg Validation Loss: 256.1459336228263\n",
      "Epoch 692/1000, Avg Training Loss: 0.2506547504329905, Avg Validation Loss: 256.1457075082069\n",
      "Epoch 693/1000, Avg Training Loss: 0.2506543628206225, Avg Validation Loss: 256.1454828425857\n",
      "Epoch 694/1000, Avg Training Loss: 0.25065397608077655, Avg Validation Loss: 256.1452596778132\n",
      "Epoch 695/1000, Avg Training Loss: 0.250653590172714, Avg Validation Loss: 256.14503815370153\n",
      "Epoch 696/1000, Avg Training Loss: 0.25065320518083956, Avg Validation Loss: 256.1448180754454\n",
      "Epoch 697/1000, Avg Training Loss: 0.2506528210791966, Avg Validation Loss: 256.14459939389616\n",
      "Epoch 698/1000, Avg Training Loss: 0.25065243780775837, Avg Validation Loss: 256.14438216631464\n",
      "Epoch 699/1000, Avg Training Loss: 0.25065205539171675, Avg Validation Loss: 256.1441663786999\n",
      "Epoch 700/1000, Avg Training Loss: 0.25065167382842773, Avg Validation Loss: 256.1439519393886\n",
      "Epoch 701/1000, Avg Training Loss: 0.25065129303248923, Avg Validation Loss: 256.14373905952766\n",
      "Epoch 702/1000, Avg Training Loss: 0.25065091311134596, Avg Validation Loss: 256.1435275704329\n",
      "Epoch 703/1000, Avg Training Loss: 0.25065053399426335, Avg Validation Loss: 256.14331748825134\n",
      "Epoch 704/1000, Avg Training Loss: 0.2506501557041253, Avg Validation Loss: 256.1431088557646\n",
      "Epoch 705/1000, Avg Training Loss: 0.25064977824859636, Avg Validation Loss: 256.1429016611378\n",
      "Epoch 706/1000, Avg Training Loss: 0.25064940163678373, Avg Validation Loss: 256.142695874442\n",
      "Epoch 707/1000, Avg Training Loss: 0.2506490258603654, Avg Validation Loss: 256.1424914864165\n",
      "Epoch 708/1000, Avg Training Loss: 0.25064865090032484, Avg Validation Loss: 256.1422885174513\n",
      "Epoch 709/1000, Avg Training Loss: 0.2506482767421579, Avg Validation Loss: 256.14208704786256\n",
      "Epoch 710/1000, Avg Training Loss: 0.2506479034142358, Avg Validation Loss: 256.1418870356292\n",
      "Epoch 711/1000, Avg Training Loss: 0.25064753088271025, Avg Validation Loss: 256.14168848284174\n",
      "Epoch 712/1000, Avg Training Loss: 0.25064715917843805, Avg Validation Loss: 256.14149128324465\n",
      "Epoch 713/1000, Avg Training Loss: 0.25064678828759895, Avg Validation Loss: 256.1412954904366\n",
      "Epoch 714/1000, Avg Training Loss: 0.25064641818452904, Avg Validation Loss: 256.1411011050573\n",
      "Epoch 715/1000, Avg Training Loss: 0.25064604887932385, Avg Validation Loss: 256.14090816018006\n",
      "Epoch 716/1000, Avg Training Loss: 0.2506456803765452, Avg Validation Loss: 256.14071662814877\n",
      "Epoch 717/1000, Avg Training Loss: 0.25064531268939755, Avg Validation Loss: 256.1405264238223\n",
      "Epoch 718/1000, Avg Training Loss: 0.2506449457994411, Avg Validation Loss: 256.1403375558236\n",
      "Epoch 719/1000, Avg Training Loss: 0.250644579724744, Avg Validation Loss: 256.1401499553382\n",
      "Epoch 720/1000, Avg Training Loss: 0.25064421442730267, Avg Validation Loss: 256.1399636435068\n",
      "Epoch 721/1000, Avg Training Loss: 0.25064384991664374, Avg Validation Loss: 256.13977854794683\n",
      "Epoch 722/1000, Avg Training Loss: 0.2506434861753036, Avg Validation Loss: 256.1395946464068\n",
      "Epoch 723/1000, Avg Training Loss: 0.2506431231804059, Avg Validation Loss: 256.1394119792872\n",
      "Epoch 724/1000, Avg Training Loss: 0.25064276093392, Avg Validation Loss: 256.13923058391254\n",
      "Epoch 725/1000, Avg Training Loss: 0.25064239944180566, Avg Validation Loss: 256.1390504093258\n",
      "Epoch 726/1000, Avg Training Loss: 0.25064203871139423, Avg Validation Loss: 256.1388714778332\n",
      "Epoch 727/1000, Avg Training Loss: 0.2506416787297015, Avg Validation Loss: 256.13869377506563\n",
      "Epoch 728/1000, Avg Training Loss: 0.25064131949084184, Avg Validation Loss: 256.1385172862651\n",
      "Epoch 729/1000, Avg Training Loss: 0.25064096097706906, Avg Validation Loss: 256.13834204014745\n",
      "Epoch 730/1000, Avg Training Loss: 0.2506406031949352, Avg Validation Loss: 256.13816805400023\n",
      "Epoch 731/1000, Avg Training Loss: 0.25064024616467595, Avg Validation Loss: 256.13799526857827\n",
      "Epoch 732/1000, Avg Training Loss: 0.2506398898596716, Avg Validation Loss: 256.1378236951572\n",
      "Epoch 733/1000, Avg Training Loss: 0.2506395342584924, Avg Validation Loss: 256.1376534054954\n",
      "Epoch 734/1000, Avg Training Loss: 0.2506391793928258, Avg Validation Loss: 256.1374843771238\n",
      "Epoch 735/1000, Avg Training Loss: 0.25063882525241515, Avg Validation Loss: 256.13731659979294\n",
      "Epoch 736/1000, Avg Training Loss: 0.250638471824375, Avg Validation Loss: 256.1371500721506\n",
      "Epoch 737/1000, Avg Training Loss: 0.2506381191087479, Avg Validation Loss: 256.13698476265864\n",
      "Epoch 738/1000, Avg Training Loss: 0.2506377671277305, Avg Validation Loss: 256.13682059309326\n",
      "Epoch 739/1000, Avg Training Loss: 0.2506374158188408, Avg Validation Loss: 256.13665762792897\n",
      "Epoch 740/1000, Avg Training Loss: 0.25063706521872825, Avg Validation Loss: 256.13649583043656\n",
      "Epoch 741/1000, Avg Training Loss: 0.2506367153339741, Avg Validation Loss: 256.1363352085866\n",
      "Epoch 742/1000, Avg Training Loss: 0.2506363661240772, Avg Validation Loss: 256.1361757646246\n",
      "Epoch 743/1000, Avg Training Loss: 0.25063601760704207, Avg Validation Loss: 256.13601744969367\n",
      "Epoch 744/1000, Avg Training Loss: 0.25063566976131757, Avg Validation Loss: 256.13586030156705\n",
      "Epoch 745/1000, Avg Training Loss: 0.2506353225831907, Avg Validation Loss: 256.13570432782956\n",
      "Epoch 746/1000, Avg Training Loss: 0.2506349760523849, Avg Validation Loss: 256.13554957620795\n",
      "Epoch 747/1000, Avg Training Loss: 0.2506346301842758, Avg Validation Loss: 256.13539605437495\n",
      "Epoch 748/1000, Avg Training Loss: 0.25063428497417417, Avg Validation Loss: 256.13524372485\n",
      "Epoch 749/1000, Avg Training Loss: 0.250633940420649, Avg Validation Loss: 256.1350925877749\n",
      "Epoch 750/1000, Avg Training Loss: 0.2506335965317794, Avg Validation Loss: 256.13494261670303\n",
      "Epoch 751/1000, Avg Training Loss: 0.25063325328139313, Avg Validation Loss: 256.1347937853999\n",
      "Epoch 752/1000, Avg Training Loss: 0.25063291066635596, Avg Validation Loss: 256.13464612184436\n",
      "Epoch 753/1000, Avg Training Loss: 0.250632568702523, Avg Validation Loss: 256.1344996395527\n",
      "Epoch 754/1000, Avg Training Loss: 0.2506322273772413, Avg Validation Loss: 256.1343543989938\n",
      "Epoch 755/1000, Avg Training Loss: 0.2506318866528811, Avg Validation Loss: 256.1342104482068\n",
      "Epoch 756/1000, Avg Training Loss: 0.25063154659470843, Avg Validation Loss: 256.1340676972093\n",
      "Epoch 757/1000, Avg Training Loss: 0.25063120719496623, Avg Validation Loss: 256.13392611160117\n",
      "Epoch 758/1000, Avg Training Loss: 0.25063086843178906, Avg Validation Loss: 256.1337856284987\n",
      "Epoch 759/1000, Avg Training Loss: 0.25063053029306154, Avg Validation Loss: 256.1336462990486\n",
      "Epoch 760/1000, Avg Training Loss: 0.2506301927860331, Avg Validation Loss: 256.1335081015342\n",
      "Epoch 761/1000, Avg Training Loss: 0.25062985589589076, Avg Validation Loss: 256.1333710589922\n",
      "Epoch 762/1000, Avg Training Loss: 0.2506295196234286, Avg Validation Loss: 256.13323516785357\n",
      "Epoch 763/1000, Avg Training Loss: 0.2506291839685321, Avg Validation Loss: 256.1331003482637\n",
      "Epoch 764/1000, Avg Training Loss: 0.25062884890964043, Avg Validation Loss: 256.13296660945633\n",
      "Epoch 765/1000, Avg Training Loss: 0.2506285144495036, Avg Validation Loss: 256.132833996778\n",
      "Epoch 766/1000, Avg Training Loss: 0.25062818059373754, Avg Validation Loss: 256.13270250799116\n",
      "Epoch 767/1000, Avg Training Loss: 0.25062784735331683, Avg Validation Loss: 256.1325721610169\n",
      "Epoch 768/1000, Avg Training Loss: 0.250627514720893, Avg Validation Loss: 256.13244291453503\n",
      "Epoch 769/1000, Avg Training Loss: 0.2506271826878718, Avg Validation Loss: 256.13231478249907\n",
      "Epoch 770/1000, Avg Training Loss: 0.25062685124801426, Avg Validation Loss: 256.1321877652706\n",
      "Epoch 771/1000, Avg Training Loss: 0.2506265204114399, Avg Validation Loss: 256.13206181429996\n",
      "Epoch 772/1000, Avg Training Loss: 0.250626190165362, Avg Validation Loss: 256.1319368843823\n",
      "Epoch 773/1000, Avg Training Loss: 0.2506258604734444, Avg Validation Loss: 256.1318129931235\n",
      "Epoch 774/1000, Avg Training Loss: 0.25062553132798465, Avg Validation Loss: 256.1316901787776\n",
      "Epoch 775/1000, Avg Training Loss: 0.2506252027517005, Avg Validation Loss: 256.13156843354614\n",
      "Epoch 776/1000, Avg Training Loss: 0.2506248747101421, Avg Validation Loss: 256.13144779492916\n",
      "Epoch 777/1000, Avg Training Loss: 0.250624547243434, Avg Validation Loss: 256.13132821209376\n",
      "Epoch 778/1000, Avg Training Loss: 0.2506242203448909, Avg Validation Loss: 256.13120967626634\n",
      "Epoch 779/1000, Avg Training Loss: 0.25062389400463353, Avg Validation Loss: 256.13109214361356\n",
      "Epoch 780/1000, Avg Training Loss: 0.2506235682158343, Avg Validation Loss: 256.13097563477277\n",
      "Epoch 781/1000, Avg Training Loss: 0.2506232429630055, Avg Validation Loss: 256.1308602031852\n",
      "Epoch 782/1000, Avg Training Loss: 0.25062291825065147, Avg Validation Loss: 256.13074585847073\n",
      "Epoch 783/1000, Avg Training Loss: 0.25062259407891746, Avg Validation Loss: 256.1306326119573\n",
      "Epoch 784/1000, Avg Training Loss: 0.25062227044943036, Avg Validation Loss: 256.13052049621785\n",
      "Epoch 785/1000, Avg Training Loss: 0.25062194735021825, Avg Validation Loss: 256.1304094689623\n",
      "Epoch 786/1000, Avg Training Loss: 0.2506216247922286, Avg Validation Loss: 256.1302995040402\n",
      "Epoch 787/1000, Avg Training Loss: 0.2506213027550609, Avg Validation Loss: 256.1301906732729\n",
      "Epoch 788/1000, Avg Training Loss: 0.2506209812414463, Avg Validation Loss: 256.1300829968629\n",
      "Epoch 789/1000, Avg Training Loss: 0.2506206602881074, Avg Validation Loss: 256.1299763981108\n",
      "Epoch 790/1000, Avg Training Loss: 0.2506203398849854, Avg Validation Loss: 256.1298709005937\n",
      "Epoch 791/1000, Avg Training Loss: 0.25062002004345657, Avg Validation Loss: 256.1297663974176\n",
      "Epoch 792/1000, Avg Training Loss: 0.25061970077013535, Avg Validation Loss: 256.12966279454224\n",
      "Epoch 793/1000, Avg Training Loss: 0.2506193820258264, Avg Validation Loss: 256.1295601632994\n",
      "Epoch 794/1000, Avg Training Loss: 0.25061906381771526, Avg Validation Loss: 256.12945851607793\n",
      "Epoch 795/1000, Avg Training Loss: 0.2506187461229803, Avg Validation Loss: 256.12935791624136\n",
      "Epoch 796/1000, Avg Training Loss: 0.250618428988239, Avg Validation Loss: 256.1292583288092\n",
      "Epoch 797/1000, Avg Training Loss: 0.2506181123879286, Avg Validation Loss: 256.12915975922135\n",
      "Epoch 798/1000, Avg Training Loss: 0.25061779629641445, Avg Validation Loss: 256.12906225386973\n",
      "Epoch 799/1000, Avg Training Loss: 0.25061748077152823, Avg Validation Loss: 256.12896573500024\n",
      "Epoch 800/1000, Avg Training Loss: 0.25061716576666004, Avg Validation Loss: 256.12887018045285\n",
      "Epoch 801/1000, Avg Training Loss: 0.2506168512982578, Avg Validation Loss: 256.1287755498799\n",
      "Epoch 802/1000, Avg Training Loss: 0.2506165373348591, Avg Validation Loss: 256.12868187670927\n",
      "Epoch 803/1000, Avg Training Loss: 0.2506162238671513, Avg Validation Loss: 256.12858919517066\n",
      "Epoch 804/1000, Avg Training Loss: 0.2506159109030714, Avg Validation Loss: 256.1284975137535\n",
      "Epoch 805/1000, Avg Training Loss: 0.2506155984329279, Avg Validation Loss: 256.1284068209109\n",
      "Epoch 806/1000, Avg Training Loss: 0.25061528646117043, Avg Validation Loss: 256.1283171287913\n",
      "Epoch 807/1000, Avg Training Loss: 0.25061497502849683, Avg Validation Loss: 256.128228393113\n",
      "Epoch 808/1000, Avg Training Loss: 0.2506146640998124, Avg Validation Loss: 256.1281405894113\n",
      "Epoch 809/1000, Avg Training Loss: 0.2506143536400289, Avg Validation Loss: 256.12805381254174\n",
      "Epoch 810/1000, Avg Training Loss: 0.2506140436393672, Avg Validation Loss: 256.1279681565967\n",
      "Epoch 811/1000, Avg Training Loss: 0.25061373415705124, Avg Validation Loss: 256.12788350446715\n",
      "Epoch 812/1000, Avg Training Loss: 0.25061342516609175, Avg Validation Loss: 256.1277998505577\n",
      "Epoch 813/1000, Avg Training Loss: 0.25061311667445824, Avg Validation Loss: 256.12771719180245\n",
      "Epoch 814/1000, Avg Training Loss: 0.2506128086820095, Avg Validation Loss: 256.1276355200233\n",
      "Epoch 815/1000, Avg Training Loss: 0.25061250118193507, Avg Validation Loss: 256.1275548412474\n",
      "Epoch 816/1000, Avg Training Loss: 0.25061219416338854, Avg Validation Loss: 256.127475146369\n",
      "Epoch 817/1000, Avg Training Loss: 0.2506118876322298, Avg Validation Loss: 256.12739643321254\n",
      "Epoch 818/1000, Avg Training Loss: 0.25061158157863134, Avg Validation Loss: 256.1273187675826\n",
      "Epoch 819/1000, Avg Training Loss: 0.2506112760076038, Avg Validation Loss: 256.12724215621733\n",
      "Epoch 820/1000, Avg Training Loss: 0.2506109709354116, Avg Validation Loss: 256.12716655682516\n",
      "Epoch 821/1000, Avg Training Loss: 0.25061066634773443, Avg Validation Loss: 256.1270919330183\n",
      "Epoch 822/1000, Avg Training Loss: 0.2506103622393526, Avg Validation Loss: 256.1270182923946\n",
      "Epoch 823/1000, Avg Training Loss: 0.2506100585952734, Avg Validation Loss: 256.12694562749715\n",
      "Epoch 824/1000, Avg Training Loss: 0.25060975540798575, Avg Validation Loss: 256.1268739362374\n",
      "Epoch 825/1000, Avg Training Loss: 0.25060945269654555, Avg Validation Loss: 256.1268032225739\n",
      "Epoch 826/1000, Avg Training Loss: 0.2506091504576363, Avg Validation Loss: 256.126733508721\n",
      "Epoch 827/1000, Avg Training Loss: 0.25060884870703903, Avg Validation Loss: 256.126664721084\n",
      "Epoch 828/1000, Avg Training Loss: 0.2506085474196091, Avg Validation Loss: 256.12659690795266\n",
      "Epoch 829/1000, Avg Training Loss: 0.2506082466107819, Avg Validation Loss: 256.1265300360521\n",
      "Epoch 830/1000, Avg Training Loss: 0.250607946278413, Avg Validation Loss: 256.1264641026504\n",
      "Epoch 831/1000, Avg Training Loss: 0.25060764640676125, Avg Validation Loss: 256.12639912696926\n",
      "Epoch 832/1000, Avg Training Loss: 0.2506073470122928, Avg Validation Loss: 256.1263350941576\n",
      "Epoch 833/1000, Avg Training Loss: 0.2506070481140506, Avg Validation Loss: 256.1262719517099\n",
      "Epoch 834/1000, Avg Training Loss: 0.25060674969004915, Avg Validation Loss: 256.1262097424234\n",
      "Epoch 835/1000, Avg Training Loss: 0.25060645174041324, Avg Validation Loss: 256.1261484719871\n",
      "Epoch 836/1000, Avg Training Loss: 0.25060615425694704, Avg Validation Loss: 256.12608814021246\n",
      "Epoch 837/1000, Avg Training Loss: 0.25060585724269274, Avg Validation Loss: 256.12602876235076\n",
      "Epoch 838/1000, Avg Training Loss: 0.25060556069529216, Avg Validation Loss: 256.1259703152044\n",
      "Epoch 839/1000, Avg Training Loss: 0.2506052646209539, Avg Validation Loss: 256.1259127595995\n",
      "Epoch 840/1000, Avg Training Loss: 0.2506049690104336, Avg Validation Loss: 256.1258560647821\n",
      "Epoch 841/1000, Avg Training Loss: 0.25060467385551033, Avg Validation Loss: 256.1258002717174\n",
      "Epoch 842/1000, Avg Training Loss: 0.25060437915217826, Avg Validation Loss: 256.1257453876118\n",
      "Epoch 843/1000, Avg Training Loss: 0.25060408489947406, Avg Validation Loss: 256.12569145799785\n",
      "Epoch 844/1000, Avg Training Loss: 0.2506037911130805, Avg Validation Loss: 256.1256384371901\n",
      "Epoch 845/1000, Avg Training Loss: 0.2506034977790355, Avg Validation Loss: 256.1255862996478\n",
      "Epoch 846/1000, Avg Training Loss: 0.2506032048675636, Avg Validation Loss: 256.1255351075921\n",
      "Epoch 847/1000, Avg Training Loss: 0.250602912415117, Avg Validation Loss: 256.12548481666545\n",
      "Epoch 848/1000, Avg Training Loss: 0.2506026204286654, Avg Validation Loss: 256.1254354550978\n",
      "Epoch 849/1000, Avg Training Loss: 0.2506023289189779, Avg Validation Loss: 256.1253870752934\n",
      "Epoch 850/1000, Avg Training Loss: 0.25060203786441476, Avg Validation Loss: 256.1253396007309\n",
      "Epoch 851/1000, Avg Training Loss: 0.25060174727158213, Avg Validation Loss: 256.1252929564172\n",
      "Epoch 852/1000, Avg Training Loss: 0.250601457119397, Avg Validation Loss: 256.1252471650569\n",
      "Epoch 853/1000, Avg Training Loss: 0.25060116741139093, Avg Validation Loss: 256.1252021532067\n",
      "Epoch 854/1000, Avg Training Loss: 0.2506008781492418, Avg Validation Loss: 256.12515801868426\n",
      "Epoch 855/1000, Avg Training Loss: 0.2506005893524399, Avg Validation Loss: 256.1251147247606\n",
      "Epoch 856/1000, Avg Training Loss: 0.25060030097570224, Avg Validation Loss: 256.1250722688871\n",
      "Epoch 857/1000, Avg Training Loss: 0.25060001303974566, Avg Validation Loss: 256.12503057291224\n",
      "Epoch 858/1000, Avg Training Loss: 0.2505997255166923, Avg Validation Loss: 256.12498969655996\n",
      "Epoch 859/1000, Avg Training Loss: 0.25059943841295984, Avg Validation Loss: 256.124949632062\n",
      "Epoch 860/1000, Avg Training Loss: 0.25059915173908326, Avg Validation Loss: 256.12491036159133\n",
      "Epoch 861/1000, Avg Training Loss: 0.25059886542691917, Avg Validation Loss: 256.12487197243007\n",
      "Epoch 862/1000, Avg Training Loss: 0.25059857954295267, Avg Validation Loss: 256.1248344194013\n",
      "Epoch 863/1000, Avg Training Loss: 0.2505982940710576, Avg Validation Loss: 256.1247976856148\n",
      "Epoch 864/1000, Avg Training Loss: 0.2505980090050401, Avg Validation Loss: 256.12476179635564\n",
      "Epoch 865/1000, Avg Training Loss: 0.2505977243353276, Avg Validation Loss: 256.1247268080445\n",
      "Epoch 866/1000, Avg Training Loss: 0.25059744008594176, Avg Validation Loss: 256.12469265308187\n",
      "Epoch 867/1000, Avg Training Loss: 0.25059715624842943, Avg Validation Loss: 256.1246592444395\n",
      "Epoch 868/1000, Avg Training Loss: 0.25059687282309007, Avg Validation Loss: 256.1246265854679\n",
      "Epoch 869/1000, Avg Training Loss: 0.2505965897874209, Avg Validation Loss: 256.124594675613\n",
      "Epoch 870/1000, Avg Training Loss: 0.2505963071387802, Avg Validation Loss: 256.12456353522236\n",
      "Epoch 871/1000, Avg Training Loss: 0.2505960248847937, Avg Validation Loss: 256.1245331497689\n",
      "Epoch 872/1000, Avg Training Loss: 0.25059574301754206, Avg Validation Loss: 256.1245035333286\n",
      "Epoch 873/1000, Avg Training Loss: 0.25059546154128104, Avg Validation Loss: 256.1244746930066\n",
      "Epoch 874/1000, Avg Training Loss: 0.2505951804553784, Avg Validation Loss: 256.12444664718055\n",
      "Epoch 875/1000, Avg Training Loss: 0.25059489976046984, Avg Validation Loss: 256.12441940016686\n",
      "Epoch 876/1000, Avg Training Loss: 0.2505946194467644, Avg Validation Loss: 256.12439294531225\n",
      "Epoch 877/1000, Avg Training Loss: 0.2505943395101875, Avg Validation Loss: 256.1243672824053\n",
      "Epoch 878/1000, Avg Training Loss: 0.25059405993240924, Avg Validation Loss: 256.1243423777704\n",
      "Epoch 879/1000, Avg Training Loss: 0.25059378071371236, Avg Validation Loss: 256.12431824122933\n",
      "Epoch 880/1000, Avg Training Loss: 0.2505935018568462, Avg Validation Loss: 256.12429488915393\n",
      "Epoch 881/1000, Avg Training Loss: 0.2505932233410657, Avg Validation Loss: 256.12427235621703\n",
      "Epoch 882/1000, Avg Training Loss: 0.25059294519044395, Avg Validation Loss: 256.12425059570444\n",
      "Epoch 883/1000, Avg Training Loss: 0.2505926674002154, Avg Validation Loss: 256.12422946817026\n",
      "Epoch 884/1000, Avg Training Loss: 0.2505923899506925, Avg Validation Loss: 256.12420901528253\n",
      "Epoch 885/1000, Avg Training Loss: 0.25059211283011634, Avg Validation Loss: 256.12418931382854\n",
      "Epoch 886/1000, Avg Training Loss: 0.25059183606062657, Avg Validation Loss: 256.1241703562222\n",
      "Epoch 887/1000, Avg Training Loss: 0.2505915596419002, Avg Validation Loss: 256.12415214242685\n",
      "Epoch 888/1000, Avg Training Loss: 0.25059128356428134, Avg Validation Loss: 256.1241346988055\n",
      "Epoch 889/1000, Avg Training Loss: 0.25059100782149074, Avg Validation Loss: 256.1241180608644\n",
      "Epoch 890/1000, Avg Training Loss: 0.2505907324124219, Avg Validation Loss: 256.1241022016993\n",
      "Epoch 891/1000, Avg Training Loss: 0.25059045732903357, Avg Validation Loss: 256.12408714569796\n",
      "Epoch 892/1000, Avg Training Loss: 0.25059018258466037, Avg Validation Loss: 256.1240728844394\n",
      "Epoch 893/1000, Avg Training Loss: 0.2505899081792333, Avg Validation Loss: 256.12405939676006\n",
      "Epoch 894/1000, Avg Training Loss: 0.2505896341216646, Avg Validation Loss: 256.1240466285773\n",
      "Epoch 895/1000, Avg Training Loss: 0.2505893603920271, Avg Validation Loss: 256.1240345940512\n",
      "Epoch 896/1000, Avg Training Loss: 0.2505890869943574, Avg Validation Loss: 256.1240233053568\n",
      "Epoch 897/1000, Avg Training Loss: 0.25058881392309185, Avg Validation Loss: 256.1240128095589\n",
      "Epoch 898/1000, Avg Training Loss: 0.25058854119227436, Avg Validation Loss: 256.1240030626701\n",
      "Epoch 899/1000, Avg Training Loss: 0.25058826877877033, Avg Validation Loss: 256.1239940722395\n",
      "Epoch 900/1000, Avg Training Loss: 0.25058799668121606, Avg Validation Loss: 256.12398581223715\n",
      "Epoch 901/1000, Avg Training Loss: 0.25058772491264114, Avg Validation Loss: 256.12397827974314\n",
      "Epoch 902/1000, Avg Training Loss: 0.2505874534559917, Avg Validation Loss: 256.1239715173317\n",
      "Epoch 903/1000, Avg Training Loss: 0.25058718231973065, Avg Validation Loss: 256.12396564697883\n",
      "Epoch 904/1000, Avg Training Loss: 0.2505869115105293, Avg Validation Loss: 256.1239605423486\n",
      "Epoch 905/1000, Avg Training Loss: 0.25058664101908706, Avg Validation Loss: 256.12395617600106\n",
      "Epoch 906/1000, Avg Training Loss: 0.2505863708786997, Avg Validation Loss: 256.1239524292132\n",
      "Epoch 907/1000, Avg Training Loss: 0.2505861010620574, Avg Validation Loss: 256.1239493337003\n",
      "Epoch 908/1000, Avg Training Loss: 0.250585831546862, Avg Validation Loss: 256.12394701098594\n",
      "Epoch 909/1000, Avg Training Loss: 0.2505855623615879, Avg Validation Loss: 256.12394543284097\n",
      "Epoch 910/1000, Avg Training Loss: 0.25058529349230024, Avg Validation Loss: 256.1239445960408\n",
      "Epoch 911/1000, Avg Training Loss: 0.2505850249397602, Avg Validation Loss: 256.1239444984678\n",
      "Epoch 912/1000, Avg Training Loss: 0.2505847566990612, Avg Validation Loss: 256.12394515492\n",
      "Epoch 913/1000, Avg Training Loss: 0.250584488779218, Avg Validation Loss: 256.12394654951174\n",
      "Epoch 914/1000, Avg Training Loss: 0.2505842211797321, Avg Validation Loss: 256.1239486635425\n",
      "Epoch 915/1000, Avg Training Loss: 0.25058395388985594, Avg Validation Loss: 256.1239514530565\n",
      "Epoch 916/1000, Avg Training Loss: 0.25058368690959004, Avg Validation Loss: 256.12395491278244\n",
      "Epoch 917/1000, Avg Training Loss: 0.2505834202309165, Avg Validation Loss: 256.1239590990492\n",
      "Epoch 918/1000, Avg Training Loss: 0.25058315387219765, Avg Validation Loss: 256.1239640213738\n",
      "Epoch 919/1000, Avg Training Loss: 0.2505828878403498, Avg Validation Loss: 256.1239696180925\n",
      "Epoch 920/1000, Avg Training Loss: 0.25058262211830784, Avg Validation Loss: 256.1239758729686\n",
      "Epoch 921/1000, Avg Training Loss: 0.25058235669996964, Avg Validation Loss: 256.1239827809518\n",
      "Epoch 922/1000, Avg Training Loss: 0.250582091583602, Avg Validation Loss: 256.12399035358726\n",
      "Epoch 923/1000, Avg Training Loss: 0.25058182676637303, Avg Validation Loss: 256.12399859676754\n",
      "Epoch 924/1000, Avg Training Loss: 0.25058156225314165, Avg Validation Loss: 256.12400751099256\n",
      "Epoch 925/1000, Avg Training Loss: 0.25058129804083085, Avg Validation Loss: 256.12401710570276\n",
      "Epoch 926/1000, Avg Training Loss: 0.2505810341229611, Avg Validation Loss: 256.12402737403283\n",
      "Epoch 927/1000, Avg Training Loss: 0.25058077050034283, Avg Validation Loss: 256.12403829829776\n",
      "Epoch 928/1000, Avg Training Loss: 0.2505805071763192, Avg Validation Loss: 256.1240498877606\n",
      "Epoch 929/1000, Avg Training Loss: 0.2505802441438233, Avg Validation Loss: 256.124062195875\n",
      "Epoch 930/1000, Avg Training Loss: 0.2505799814108151, Avg Validation Loss: 256.12407521481487\n",
      "Epoch 931/1000, Avg Training Loss: 0.2505797189892536, Avg Validation Loss: 256.1240888897659\n",
      "Epoch 932/1000, Avg Training Loss: 0.2505794568614037, Avg Validation Loss: 256.12410323106633\n",
      "Epoch 933/1000, Avg Training Loss: 0.25057919502070375, Avg Validation Loss: 256.12411825432486\n",
      "Epoch 934/1000, Avg Training Loss: 0.2505789334717606, Avg Validation Loss: 256.1241339679983\n",
      "Epoch 935/1000, Avg Training Loss: 0.2505786722255775, Avg Validation Loss: 256.1241503358093\n",
      "Epoch 936/1000, Avg Training Loss: 0.25057841127498653, Avg Validation Loss: 256.1241674166992\n",
      "Epoch 937/1000, Avg Training Loss: 0.2505781506265942, Avg Validation Loss: 256.1241851824532\n",
      "Epoch 938/1000, Avg Training Loss: 0.2505778902754228, Avg Validation Loss: 256.12420355775146\n",
      "Epoch 939/1000, Avg Training Loss: 0.250577630204744, Avg Validation Loss: 256.12422255277704\n",
      "Epoch 940/1000, Avg Training Loss: 0.2505773704271701, Avg Validation Loss: 256.1242421500097\n",
      "Epoch 941/1000, Avg Training Loss: 0.25057711093238727, Avg Validation Loss: 256.124262330083\n",
      "Epoch 942/1000, Avg Training Loss: 0.2505768517190747, Avg Validation Loss: 256.12428311705924\n",
      "Epoch 943/1000, Avg Training Loss: 0.25057659279872907, Avg Validation Loss: 256.124304551837\n",
      "Epoch 944/1000, Avg Training Loss: 0.25057633417555064, Avg Validation Loss: 256.12432661824755\n",
      "Epoch 945/1000, Avg Training Loss: 0.25057607584282965, Avg Validation Loss: 256.12434929989\n",
      "Epoch 946/1000, Avg Training Loss: 0.2505758178003147, Avg Validation Loss: 256.12437259630497\n",
      "Epoch 947/1000, Avg Training Loss: 0.2505755600476498, Avg Validation Loss: 256.1243964940337\n",
      "Epoch 948/1000, Avg Training Loss: 0.2505753025814832, Avg Validation Loss: 256.12442098218105\n",
      "Epoch 949/1000, Avg Training Loss: 0.2505750454005882, Avg Validation Loss: 256.12444605619055\n",
      "Epoch 950/1000, Avg Training Loss: 0.25057478849894804, Avg Validation Loss: 256.124471730256\n",
      "Epoch 951/1000, Avg Training Loss: 0.25057453188721374, Avg Validation Loss: 256.1244979734249\n",
      "Epoch 952/1000, Avg Training Loss: 0.2505742755712845, Avg Validation Loss: 256.1245247289135\n",
      "Epoch 953/1000, Avg Training Loss: 0.25057401952836467, Avg Validation Loss: 256.12455203861873\n",
      "Epoch 954/1000, Avg Training Loss: 0.2505737637568849, Avg Validation Loss: 256.1245799315483\n",
      "Epoch 955/1000, Avg Training Loss: 0.250573508269213, Avg Validation Loss: 256.1246083739467\n",
      "Epoch 956/1000, Avg Training Loss: 0.2505732530568624, Avg Validation Loss: 256.1246374094748\n",
      "Epoch 957/1000, Avg Training Loss: 0.25057299812152317, Avg Validation Loss: 256.1246670580765\n",
      "Epoch 958/1000, Avg Training Loss: 0.25057274346946873, Avg Validation Loss: 256.1246972909654\n",
      "Epoch 959/1000, Avg Training Loss: 0.2505724890994274, Avg Validation Loss: 256.1247280556412\n",
      "Epoch 960/1000, Avg Training Loss: 0.25057223500179804, Avg Validation Loss: 256.1247593399026\n",
      "Epoch 961/1000, Avg Training Loss: 0.2505719811656868, Avg Validation Loss: 256.12479117631676\n",
      "Epoch 962/1000, Avg Training Loss: 0.25057172759314206, Avg Validation Loss: 256.1248235506974\n",
      "Epoch 963/1000, Avg Training Loss: 0.2505714742879257, Avg Validation Loss: 256.1248564697903\n",
      "Epoch 964/1000, Avg Training Loss: 0.25057122125420844, Avg Validation Loss: 256.1248899536152\n",
      "Epoch 965/1000, Avg Training Loss: 0.25057096848499816, Avg Validation Loss: 256.1249239969055\n",
      "Epoch 966/1000, Avg Training Loss: 0.2505707159762986, Avg Validation Loss: 256.1249586151332\n",
      "Epoch 967/1000, Avg Training Loss: 0.2505704637275804, Avg Validation Loss: 256.1249938024555\n",
      "Epoch 968/1000, Avg Training Loss: 0.25057021173856747, Avg Validation Loss: 256.1250295972792\n",
      "Epoch 969/1000, Avg Training Loss: 0.25056996002296267, Avg Validation Loss: 256.1250659464933\n",
      "Epoch 970/1000, Avg Training Loss: 0.250569708574812, Avg Validation Loss: 256.1251028365381\n",
      "Epoch 971/1000, Avg Training Loss: 0.2505694573736248, Avg Validation Loss: 256.12514030409284\n",
      "Epoch 972/1000, Avg Training Loss: 0.2505692064289438, Avg Validation Loss: 256.1251783473052\n",
      "Epoch 973/1000, Avg Training Loss: 0.250568955748852, Avg Validation Loss: 256.12521697387547\n",
      "Epoch 974/1000, Avg Training Loss: 0.25056870534173553, Avg Validation Loss: 256.1252562350296\n",
      "Epoch 975/1000, Avg Training Loss: 0.25056845521089927, Avg Validation Loss: 256.12529604712375\n",
      "Epoch 976/1000, Avg Training Loss: 0.250568205349682, Avg Validation Loss: 256.12533641182824\n",
      "Epoch 977/1000, Avg Training Loss: 0.2505679557594632, Avg Validation Loss: 256.1253772977708\n",
      "Epoch 978/1000, Avg Training Loss: 0.2505677064347188, Avg Validation Loss: 256.12541870145105\n",
      "Epoch 979/1000, Avg Training Loss: 0.25056745737275526, Avg Validation Loss: 256.12546062732423\n",
      "Epoch 980/1000, Avg Training Loss: 0.25056720856927917, Avg Validation Loss: 256.1255030788693\n",
      "Epoch 981/1000, Avg Training Loss: 0.2505669600347121, Avg Validation Loss: 256.1255460166135\n",
      "Epoch 982/1000, Avg Training Loss: 0.25056671175514367, Avg Validation Loss: 256.1255894642052\n",
      "Epoch 983/1000, Avg Training Loss: 0.25056646373208363, Avg Validation Loss: 256.12563340244026\n",
      "Epoch 984/1000, Avg Training Loss: 0.25056621596258344, Avg Validation Loss: 256.1256778302608\n",
      "Epoch 985/1000, Avg Training Loss: 0.2505659684412441, Avg Validation Loss: 256.12572276455865\n",
      "Epoch 986/1000, Avg Training Loss: 0.25056572116638015, Avg Validation Loss: 256.1257681555423\n",
      "Epoch 987/1000, Avg Training Loss: 0.25056547412580166, Avg Validation Loss: 256.1258140373509\n",
      "Epoch 988/1000, Avg Training Loss: 0.25056522731062014, Avg Validation Loss: 256.12586043905185\n",
      "Epoch 989/1000, Avg Training Loss: 0.25056498073510886, Avg Validation Loss: 256.1259073518979\n",
      "Epoch 990/1000, Avg Training Loss: 0.2505647344048086, Avg Validation Loss: 256.12595479345055\n",
      "Epoch 991/1000, Avg Training Loss: 0.2505644883204397, Avg Validation Loss: 256.1260027508922\n",
      "Epoch 992/1000, Avg Training Loss: 0.25056424248103176, Avg Validation Loss: 256.12605122480943\n",
      "Epoch 993/1000, Avg Training Loss: 0.25056399689033626, Avg Validation Loss: 256.1261001749043\n",
      "Epoch 994/1000, Avg Training Loss: 0.25056375153044114, Avg Validation Loss: 256.1261496645999\n",
      "Epoch 995/1000, Avg Training Loss: 0.25056350641636926, Avg Validation Loss: 256.1261996882524\n",
      "Epoch 996/1000, Avg Training Loss: 0.25056326154875064, Avg Validation Loss: 256.1262502348295\n",
      "Epoch 997/1000, Avg Training Loss: 0.25056301694100375, Avg Validation Loss: 256.1263012673166\n",
      "Epoch 998/1000, Avg Training Loss: 0.25056277257021137, Avg Validation Loss: 256.1263528125746\n",
      "Epoch 999/1000, Avg Training Loss: 0.2505625284276947, Avg Validation Loss: 256.12640490086926\n",
      "Epoch 1000/1000, Avg Training Loss: 0.2505622845213256, Avg Validation Loss: 256.12645754950165\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAaZ0lEQVR4nO3dfXRV9Z3v8fc3J4EgD1HQ+gBqoPJQNCaBgChFwY4jKsJVsZq6llJcPjBWW+6tFDttddpx7tSy7nScq97SaunqZYmOWAYslhmpDli6Kg+iBZUBNF5iqwJqwPJgEr73j7OzPYkncE5ydk52zue11jF7//Y++3z32ZhP9v7t8zvm7oiIiAAU5bsAERHpPhQKIiISUiiIiEhIoSAiIiGFgoiIhIrzXUBnnHjiiV5eXp7vMkREYmXjxo173P2kdMtiHQrl5eVs2LAh32WIiMSKmb3d3jJdPhIRkVAsQ8HMrjSzhQ0NDfkuRUSkR4llKLj7Cne/taysLN+liIj0KLEMBRERiYZCQUREQgoFEREJKRRERCRUmKHw5n/Cb+/PdxUiIt1OYYbCrj/Amgeg6ZN8VyIi0q3EMhQ6/TmFPickfx78MHdFiYj0ALEMhU5/TuG4gcmfBz/IXVEiIj1ALEOh0/oEoXBAoSAikirWA+J12PFnJH8+XgufGwWlZVB6PPTuD8WlUNwLEimP4t7Jn0UJsKI0D2sz33Y9CP6TXDeUMh22Z9p2jG12+nWQDtEb1yGm9y1rx5/56VWPHCrMUBj0ebjmUdj2LBzYAx+/D3u2w+F9yc7n5sPQrE5oEenGrvoJVF6f880WZigAVMxMPtrjDs2NyYBoCQo/0ubhreePNLe/PLnR1tv/dCa7tlbtmbalbpM0be28jmTO9b51jN63DjmlIpLNFm4oHItZ8jJScS/one9iRES6RmF2NIuISFoKBRERCSkUREQkpFAQEZFQLENBX8cpIhKNWIaCvo5TRCQasQwFERGJhkJBRERCCgUREQkpFEREJKRQEBGRkEJBRERCCgUREQkpFEREJKRQEBGRkEJBRERCCgUREQkpFEREJKRQEBGRkEJBRERCxfkuIJWZ/TfgCmAA8Ki7/3t+KxIRKSyRnymY2WNm9r6ZbWnTPtXMtpnZDjObD+Duy9z9FuB24LqoaxMRkda64vLRImBqaoOZJYCHgMuA0UCtmY1OWeU7wXIREelCkYeCu68BPmjTPB7Y4e5vuvsnwBJghiX9EHjW3Tel256Z3WpmG8xsw+7du6MtXkSkwOSro3kwsCtlvj5ouxP4K2Cmmd2e7onuvtDda9y95qSTToq+UhGRAtKtOprd/UHgwXzXISJSqPJ1pvAOcHrK/JCgLSNmdqWZLWxoaMh5YSIihSxfobAeGG5mQ82sF3A9sDzTJ7v7Cne/taysLLICRUQKUVfckvo48HtgpJnVm9nN7t4EfA1YBbwOPOnuW6OuRUREji7yPgV3r22nfSWwMurXFxGRzMVymAv1KYiIRCOWoaA+BRGRaMQyFEREJBqxDAVdPhIRiUYsQ0GXj0REohHLUBARkWgoFEREJKRQEBGRUCxDQR3NIiLRiGUoqKNZRCQasQwFERGJhkJBRERCCgUREQnFMhTU0SwiEo1YhoI6mkVEohHLUBARkWgoFEREJKRQEBGRkEJBRERCsQwF3X0kIhKNWIaC7j4SEYlGLENBRESioVAQEZGQQkFEREIKBRERCSkUREQkpFAQEZFQLENBn1MQEYlGLENBn1MQEYlGLENBRESioVAQEZGQQkFEREIKBRERCSkUREQkpFAQEZGQQkFEREIKBRERCSkUREQkFMtQ0DAXIiLRiGUoaJgLEZFoxDIUREQkGgoFEREJKRRERCSkUBARkZBCQUREQsX5LkBEuq/Gxkbq6+s5dOhQvkuRDigtLWXIkCGUlJRk/ByFgoi0q76+nv79+1NeXo6Z5bscyYK7s3fvXurr6xk6dGjGz9PlIxFp16FDhxg0aJACIYbMjEGDBmV9lqdQEJGjUiDEV0eOnUJBRLqtvXv3UlVVRVVVFaeccgqDBw8O5z/55JOjPnfDhg3cddddx3yNCy64ICe1vvDCC5SVlYX1VVVV8dxzz+Vk211JfQoi0m0NGjSIzZs3A3DffffRr18/vvnNb4bLm5qaKC5O/2uspqaGmpqaY77GunXrclIrwKRJk3jmmWfaXe7uuDtFRUVp59tztP3MtWOeKZhZkZnlJkpFRDpp1qxZ3H777Zx33nnMmzePl156ifPPP5/q6mouuOACtm3bBiT/cp82bRqQDJTZs2czefJkhg0bxoMPPhhur1+/fuH6kydPZubMmYwaNYobbrgBdwdg5cqVjBo1irFjx3LXXXeF281EXV0dI0eO5MYbb+Scc85h7dq1reZ37drF3XffzTnnnENFRQVPPPFEWM+kSZOYPn06o0ePzsl7l4ljRo+7HzGzh4DqLqhHRLqpv1uxldf+tC+n2xx92gDuvfLsrJ9XX1/PunXrSCQS7Nu3j7Vr11JcXMxzzz3Ht7/9bZYuXfqZ57zxxhs8//zz7N+/n5EjRzJnzpzP3Kr58ssvs3XrVk477TQmTpzI7373O2pqarjttttYs2YNQ4cOpba2tt261q5dS1VVVTi/dOlSEokE27dv5xe/+AUTJkygrq6u1fzSpUvZvHkzr7zyCnv27GHcuHFceOGFAGzatIktW7ZkdfdQZ2V6PrLazK4BnvaW6BQRyZNrr72WRCIBQENDAzfddBPbt2/HzGhsbEz7nCuuuILevXvTu3dvPve5z/Hee+8xZMiQVuuMHz8+bKuqqqKuro5+/foxbNiw8BdzbW0tCxcuTPsa6S4f1dXVceaZZzJhwoSwLXX+xRdfpLa2lkQiwcknn8xFF13E+vXrGTBgAOPHj+/SQIDMQ+E24L8DzWZ2EDDA3X1Argoxs2HA3wJl7j4zV9sVkdzoyF/0Uenbt284/d3vfpcpU6bwq1/9irq6OiZPnpz2Ob179w6nE4kETU1NHVqns/Wmm8/0eV0ho7uP3L2/uxe5e4m7DwjmjxkIZvaYmb1vZlvatE81s21mtsPM5gev8aa739yx3RCRQtXQ0MDgwYMBWLRoUc63P3LkSN58803q6uoAwmv+uTJp0iSeeOIJmpub2b17N2vWrGH8+PE5fY1sZHxLqplNN7MFwSPTXpZFwNQ220kADwGXAaOBWjPrul4UEelR5s2bxz333EN1dXXO/rJP1adPHx5++GGmTp3K2LFj6d+/P+19wVdLn0LL46mnnjrm9q+66irOPfdcKisrufjii3nggQc45ZRTcr0bGbNMugjM7B+BccDioKkW2ODu92Tw3HLgGXc/J5g/H7jP3S8N5u8BcPf/Gcw/dbTLR2Z2K3ArwBlnnDH27bffPmb9ItIxr7/+Ol/4whfyXUbeffzxx/Tr1w9354477mD48OHMnTs332VlJN0xNLON7p72ft1MzxQuBy5x98fc/TGSf/1f0cEaBwO7UubrgcFmNsjM/g9Q3RIU6bj7Qnevcfeak046qYMliIhk7qc//SlVVVWcffbZNDQ0cNttt+W7pMhk82mI44EPgumcfzmyu+8Fbs/1dkVEOmvu3LmxOTPorExD4R+Al83seZJ3Hl0IzO/ga74DnJ4yPyRoy5iZXQlcedZZZ3WwBBERSSejTzQDR4AJwNPAUuB8d+9oF/x6YLiZDTWzXsD1wPJsNuDuK9z91vY6e0REpGOOGQrufgSY5+5/dvflwePdTDZuZo8DvwdGmlm9md3s7k3A14BVwOvAk+6+tRP7ICIiOZLp5aPnzOybwBPAX1oa3f2D9p8C7p728+DuvhJYmWmRIiLSNTK9++g64A5gDbAxeGyIqqhjMbMrzWxhQ0NDvkoQkS4wZcoUVq1a1artxz/+MXPmzGn3OZMnT2bDhuSvp8svv5yPPvroM+vcd999LFiw4KivvWzZMl577bVw/nvf+15OhsLu7kNsH/NMIehTmN+JPoScc/cVwIqamppb8l2LiESntraWJUuWcOmll4ZtS5Ys4YEHHsjo+StXdvyCxLJly5g2bVo4Qun3v//9Dm+rre48xHamfQp3d+pVREQ6YObMmfz6178Ov1Cnrq6OP/3pT0yaNIk5c+ZQU1PD2Wefzb333pv2+eXl5ezZsweA+++/nxEjRvDFL34xHF4bkp9BGDduHJWVlVxzzTUcOHCAdevWsXz5cu6++26qqqrYuXMns2bNCj+hvHr1aqqrq6moqGD27NkcPnw4fL17772XMWPGUFFRwRtvvJHxvnaXIbYj7VOIim5JFcmDZ+fDu3/M7TZPqYDL/rHdxQMHDmT8+PE8++yzzJgxgyVLlvDlL38ZM+P+++9n4MCBNDc386UvfYlXX32Vc889N+12Nm7cyJIlS9i8eTNNTU2MGTOGsWPHAnD11Vdzyy3Jiw7f+c53ePTRR7nzzjuZPn0606ZNY+bM1gMsHDp0iFmzZrF69WpGjBjBjTfeyCOPPMI3vvENAE488UQ2bdrEww8/zIIFC/jZz372mXq68xDbsexT0C2pIoWj5RISJC8dtXyfwZNPPsmYMWOorq5m69atra7/t7V27VquuuoqjjvuOAYMGMD06dPDZVu2bGHSpElUVFSwePFitm49+s2Q27ZtY+jQoYwYMQKAm266iTVr1oTLr776agDGjh0bDqLX1qRJk9i8eXP4+PznPw/QoSG2gZwOsZ3RmYK7d+2A3iLS/RzlL/oozZgxg7lz57Jp0yYOHDjA2LFjeeutt1iwYAHr16/nhBNOYNasWRw6dKhD2581axbLli2jsrKSRYsW8cILL3Sq3pbhtzsy9HZ3GGL7qGcKZjYvZfraNsv+IWdViIi0o1+/fkyZMoXZs2eHZwn79u2jb9++lJWV8d577/Hss88edRsXXnghy5Yt4+DBg+zfv58VK1aEy/bv38+pp55KY2MjixcvDtv79+/P/v37P7OtkSNHUldXx44dOwD45S9/yUUXXZSLXT2qrhpi+1iXj65PmW47SN1URES6QG1tLa+88koYCpWVlVRXVzNq1Ci+8pWvMHHixKM+f8yYMVx33XVUVlZy2WWXMW7cuHDZD37wA8477zwmTpzIqFGjwvbrr7+eH/3oR1RXV7Nz586wvbS0lJ///Odce+21VFRUUFRUxO23ZzdsW3ceYvuoQ2eb2cvuXt12Ot18V0rpaL5l+/bt+ShBpCBo6Oz4y/XQ2d7OdLr5LqOOZhGRaByro7nSzPaRHBm1TzBNMF8aaWUiItLljhoK7p7oqkJERCT/Mv6OZhEpTJl8Za90Tx05drEMBQ2IJ9I1SktL2bt3r4IhhtydvXv3Ulqa3ZX+o9591N3V1NR4y2iIIpJ7jY2N1NfXd/iDYZJfpaWlDBkyhJKSklbtR7v7qHPD6YlIj1ZSUpKz4RMkHmJ5+UhERKKhUBARkZBCQUREQrEMBd19JCISjViGgoa5EBGJRixDQUREoqFQEBGRkEJBRERCCgUREQkpFEREJKRQEBGRUCxDQZ9TEBGJRixDQZ9TEBGJRixDQUREoqFQEBGRkEJBRERCCgUREQkpFEREJKRQEBGRkEJBRERCCgUREQkpFEREJBTLUNAwFyIi0YhlKGiYCxGRaMQyFEREJBoKBRERCSkUREQkpFAQEZGQQkFEREIKBRERCSkUREQkpFAQEZGQQkFEREIKBRERCSkUREQkpFAQEZGQQkFEREIKBRERCRXnu4AWZtYXeBj4BHjB3RfnuSQRkYIT6ZmCmT1mZu+b2ZY27VPNbJuZ7TCz+UHz1cBT7n4LMD3KukREJL2oLx8tAqamNphZAngIuAwYDdSa2WhgCLArWK054rpERCSNSEPB3dcAH7RpHg/scPc33f0TYAkwA6gnGQyR1yUiIunl45fvYD49I4BkGAwGngauMbNHgBXtPdnMbjWzDWa2Yffu3dFWKiJSYLpNR7O7/wX4agbrLQQWAtTU1HjUdYmIFJJ8nCm8A5yeMj8kaMuYmV1pZgsbGhpyWpiISKHLRyisB4ab2VAz6wVcDyzPZgPuvsLdby0rK4ukQBGRQhX1LamPA78HRppZvZnd7O5NwNeAVcDrwJPuvjXKOkREJDOR9im4e2077SuBlVG+toiIZC+Wt36qT0FEJBqxDAX1KYiIRCOWoSAiItGIZSjo8pGISDRiGQq6fCQiEo1YhoKIiERDoSAiIiGFgoiIhGIZCupoFhGJRixDQR3NIiLRiGUoiIhINAoyFNbXfcAjL+zMdxkiIt1OQYbCi9v38MPfvIG7vqNHRCRVLEOhsx3NxUUGQNMRhYKISKpYhkJnO5qLE8ndblYoiIi0EstQ6KyWM4XG5iN5rkREpHspzFBIJENBZwoiIq0VZiiEZwoKBRGRVIUZCkGfQtMRXT4SEUkVy1Do7N1HiZa7j3SmICLSSixDobN3H5UkdEuqiEg6sQyFzkoUtdySqstHIiKpCjIUStTRLCKSVkGGgj68JiKSXmGGgj68JiKSVmGGgjqaRUTSKshQ0C2pIiLpxTIUOvs5hRJ9eE1EJK1YhkJnP6eQ0NDZIiJpxTIUOqtf72IA9h1szHMlIiLdS3G+C8iH8kF9KUkY//LbHdR/eJCBfXtxfJ8SSnsl6F1cRGlJ8mfv4uTP4oRRZMlHoshImFFURDj/6U8ws3zvnohIhxVkKPQqLuJvJp/Foy++xY9Wbcvpts2SYWHBNEByDjBaplot+3S6ZVn4DFKeGgaOpWtrtd3W62VUd+arZrnd7EIyupqjCeus6o3ofesO71nkfwpF+AJR1x7Vv71vTR3FJaNPzvl2CzIUAOZeMoK5l4zgUGMzHx74hI8ONHKosZlDjUc43NTM4aYjHGpM/jxyxGl2T/484jQ7uLdMt7TDEffw0fL1zy29Fu7gtG50CL8n2lu1tUy3Xkbq+u2sl/p6mct85Wy2m+1XYHtUdWRTQ1bbjaaI7OrN4j3LartZrJvFdjsiyu9Sj7xXMcIXGFAaza/vgg2FFqUlCU4t68OpZX3yXYqISN4VZEeziIikp1AQEZGQQkFEREIKBRERCcUyFDo7zIWIiKQXy1Do7DAXIiKSXixDQUREoqFQEBGRkEX5acGomdlu4O0OPv1EYE8Oy4kD7XNh0D4Xhs7s85nuflK6BbEOhc4wsw3uXpPvOrqS9rkwaJ8LQ1T7rMtHIiISUiiIiEiokENhYb4LyAPtc2HQPheGSPa5YPsURETkswr5TEFERNpQKIiISKggQ8HMpprZNjPbYWbz811PLpjZ6Wb2vJm9ZmZbzezrQftAM/sPM9se/DwhaDczezB4D141szH53YOOM7OEmb1sZs8E80PN7A/Bvj1hZr2C9t7B/I5geXleC+8gMzvezJ4yszfM7HUzO7+nH2czmxv8u95iZo+bWWlPO85m9piZvW9mW1Lasj6uZnZTsP52M7sp2zoKLhTMLAE8BFwGjAZqzWx0fqvKiSbgf7j7aGACcEewX/OB1e4+HFgdzENy/4cHj1uBR7q+5Jz5OvB6yvwPgX9y97OAD4Gbg/abgQ+D9n8K1oujfwZ+4+6jgEqS+95jj7OZDQbuAmrc/RwgAVxPzzvOi4CpbdqyOq5mNhC4FzgPGA/c2xIkGXP3gnoA5wOrUubvAe7Jd10R7Oe/AZcA24BTg7ZTgW3B9E+A2pT1w/Xi9ACGBP+zXAw8Q/J72PcAxW2PN7AKOD+YLg7Ws3zvQ5b7Wwa81bbunnycgcHALmBgcNyeAS7ticcZKAe2dPS4ArXAT1LaW62XyaPgzhT49B9Yi/qgrccITpergT8AJ7v7n4NF7wInB9M95X34MTAPOBLMDwI+cvemYD51v8J9DpY3BOvHyVBgN/Dz4JLZz8ysLz34OLv7O8AC4P8BfyZ53DbSs49zi2yPa6ePdyGGQo9mZv2ApcA33H1f6jJP/unQY+5BNrNpwPvuvjHftXShYmAM8Ii7VwN/4dNLCkCPPM4nADNIBuJpQF8+e5mlx+uq41qIofAOcHrK/JCgLfbMrIRkICx296eD5vfM7NRg+anA+0F7T3gfJgLTzawOWELyEtI/A8ebWXGwTup+hfscLC8D9nZlwTlQD9S7+x+C+adIhkRPPs5/Bbzl7rvdvRF4muSx78nHuUW2x7XTx7sQQ2E9MDy4c6EXyQ6r5XmuqdPMzIBHgdfd/X+lLFoOtNyBcBPJvoaW9huDuxgmAA0pp6mx4O73uPsQdy8neRx/6+43AM8DM4PV2u5zy3sxM1g/Vn9Ru/u7wC4zGxk0fQl4jR58nEleNppgZscF/85b9rnHHucU2R7XVcBfm9kJwRnWXwdtmct3x0qeOnMuB/4L2An8bb7rydE+fZHkqeWrwObgcTnJa6mrge3Ac8DAYH0jeRfWTuCPJO/syPt+dGL/JwPPBNPDgJeAHcC/Ar2D9tJgfkewfFi+6+7gvlYBG4JjvQw4oacfZ+DvgDeALcAvgd497TgDj5PsM2kkeUZ4c0eOKzA72PcdwFezrUPDXIiISKgQLx+JiEg7FAoiIhJSKIiISEihICIiIYWCiIiEFAoiaZhZs5ltTnnkbDRdMytPHQlTpDspPvYqIgXpoLtX5bsIka6mMwWRLJhZnZk9YGZ/NLOXzOysoL3czH4bjG2/2szOCNpPNrNfmdkrweOCYFMJM/tp8B0B/25mfYL177Lkd2K8amZL8rSbUsAUCiLp9Wlz+ei6lGUN7l4B/G+So7QC/AvwC3c/F1gMPBi0Pwj8p7tXkhyjaGvQPhx4yN3PBj4Crgna5wPVwXZuj2bXRNqnTzSLpGFmH7t7vzTtdcDF7v5mMADhu+4+yMz2kBz3vjFo/7O7n2hmu4Eh7n44ZRvlwH948otTMLNvASXu/vdm9hvgY5LDVyxz948j3lWRVnSmIJI9b2c6G4dTppv5tH/vCpJj2owB1qeMAirSJRQKItm7LuXn74PpdSRHagW4AVgbTK8G5kD4XdJl7W3UzIqA0939eeBbJId8/szZikiU9FeISHp9zGxzyvxv3L3lttQTzOxVkn/t1wZtd5L8NrS7SX4z2leD9q8DC83sZpJnBHNIjoSZTgL4v0FwGPCgu3+Uo/0RyYj6FESyEPQp1Lj7nnzXIhIFXT4SEZGQzhRERCSkMwUREQkpFEREJKRQEBGRkEJBRERCCgUREQn9f40ToZw/XdvRAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IL TEST NON CONVERGE E IMPAZZISCE DOPO TOT EPOCHE\n",
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x_tot = np.random.rand(5000, 15)\n",
    "target = np.random.rand(5000, 3)\n",
    "\n",
    "layer_one = Layer(15, 8, ELU, d_ELU)\n",
    "layer_two = Layer(8, 5, ELU, d_ELU)\n",
    "layer_out = Layer(5, 3, linear, d_linear)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "NN.add_layer(layer_one)\n",
    "NN.add_layer(layer_two)\n",
    "NN.add_layer(layer_out)\n",
    "\n",
    "# Parametri di training\n",
    "K = 5\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "\n",
    "# Cross-validation\n",
    "train_error, val_error = NN.k_fold_cross_validation(\n",
    "    x_tot, target, K, epochs, learning_rate, mean_squared_error, d_mean_squared_error, batch_size\n",
    ")\n",
    "\n",
    "# Plot degli errori\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_error, label='Training Error')\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
