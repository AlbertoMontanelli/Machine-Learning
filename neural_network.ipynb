{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0zBZ8ZOoCBL"
      },
      "source": [
        "# Cose da fare\n",
        "* tocca fa adam\n",
        "* inserire documentazione per le regolarizzazioni (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
        "* inserire loss function per problemi di classificazione: BCE o altro\n",
        "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
        "* analisi training error vs validation error vs test error\n",
        "### Novelties\n",
        "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
        "* momentum -> OBBLIGATORIO\n",
        "* early stopping\n",
        "* learning rate variabile/adam\n",
        "* standarditation e normalization (FACOLTATIVO FORSE)\n",
        "# Cose da fare secondo le (!) Micheli\n",
        "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting\n",
        "*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "qv1wwlgWsFM8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n",
        "\n",
        "# np.vectorize returns an object that acts like pyfunc, but takes arrays as input\n",
        "d_ReLU = np.vectorize(d_ReLU)\n",
        "d_leaky_relu = np.vectorize(d_leaky_relu)\n",
        "ELU = np.vectorize(ELU)\n",
        "d_ELU = np.vectorize(d_ELU)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return - 2 * (y_true - y_pred)\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return - (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return - (y_true - y_pred) if(np.abs(y_true-y_pred)<=delta) else - delta * np.sign(y_true-y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters:**\n",
        " * input_size: $k_{i-1}$;\n",
        " * output_size: $k_i$;\n",
        " * activation_function;\n",
        " * activation_derivative.\n",
        "\n",
        "**Constructor attributes:**\n",
        "* self.input_size = input_size;\n",
        "* self.output_size = output_size;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative;\n",
        "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
        "\n",
        "**Methods :**\n",
        "\n",
        "* **initialize_weights**: initialize weights and biases\n",
        "  * attributes:\n",
        "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
        "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
        "    \n",
        "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
        "  * parameter:\n",
        "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
        "  * attributes:\n",
        "    * self.input: input_array;\n",
        "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
        "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
        "  \n",
        "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        "  * parameters:\n",
        "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "    * learning_rate.\n",
        "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "H8Ap9rLxLlNU",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "da307f31-1bbc-4d71-826e-718b36ac131a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\\n        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\\n\\n        # tikhonov and lasso implementation   \\n        if (reg_type=='tikhonov'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\\n        elif (reg_type=='lasso'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\\n        elif (reg_type=='elastic'): # lasso + tikhonov regularization\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights) \\n        \\n        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\\n        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\\n        return sum_delta_weights\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "        self.initialize_weights()\n",
        "        self.t = 1 # number of iterations for adam\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialization of the parameters of the network\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
        "        self.biases = np.zeros((1, self.output_size))\n",
        "\n",
        "        # Initialization of the parameters for Nesterov optimization\n",
        "        self.velocity_weights = np.zeros_like(self.weights) # zeros array with dim = dim(self.weights)\n",
        "        self.velocity_biases = np.zeros_like(self.biases)\n",
        "\n",
        "        # Initialization of the parameters for Adam optimization\n",
        "        self.m_weights = np.zeros_like(self.weights)\n",
        "        self.v_weights = np.zeros_like(self.weights)\n",
        "        self.m_biases = np.zeros_like(self.biases)\n",
        "        self.v_biases = np.zeros_like(self.biases)\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        self.delta = - d_Ep * self.activation_derivative(self.net)\n",
        "        if opt_type == 'NAG':\n",
        "          weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights in order to use the Nesterov momentum,\n",
        "                                                                        # used to evaluate the gradient after the momentum is applied\n",
        "          bias_pred = self.biases + momentum * self.velocity_biases # same thing for the biases\n",
        "          net_pred = np.dot(self.input, weights_pred) + bias_pred  #  Net has been computed with respect to the predicted weights and the predicted biases\n",
        "          delta_pred = - d_Ep * self.activation_derivative(net_pred)  # Loss gradient with respect to net, minus sign due to the definition\n",
        "          grad_weights = learning_rate_w * np.dot(self.input.T, delta_pred)  # Loss gradient multiplied by the learning rate.\n",
        "                                                                          # The gradient has been computed with respect to the predicted weights and biases\n",
        "          self.velocity_weights = momentum * self.velocity_weights + grad_weights  # Delta w new before regularization\n",
        "\n",
        "          # Tikhonov, lasso, elastic regularization. To be implemented during optimization\n",
        "          if reg_type == 'tikhonov':\n",
        "              self.velocity_weights -= 2 * Lambda_t * weights_pred # learning rule of tikhonov regularization,\n",
        "                                                                   # the minus sign is due to the application of gradient descent algorithm.\n",
        "                                                                   # We use the predicted weights in the Nesterov optimization in the regularization too\n",
        "          elif reg_type == 'lasso':\n",
        "              self.velocity_weights -= Lambda_l * np.sign(weights_pred) # learning rule of lasso regularization\n",
        "          elif reg_type == 'elastic':\n",
        "              self.velocity_weights -= (2 * Lambda_t * weights_pred + Lambda_l * np.sign(weights_pred)) # lasso + tikhonov regularization\n",
        "\n",
        "\n",
        "          self.weights += self.velocity_weights  # Updating the weights\n",
        "          self.velocity_biases = momentum * self.velocity_biases + learning_rate_b * np.sum(delta_pred, axis=0, keepdims=True)\n",
        "          self.biases += self.velocity_biases # Updating the biases\n",
        "\n",
        "        elif opt_type == 'adam':\n",
        "\n",
        "          # In order to include the regularization term in the optimization process:\n",
        "          if reg_type == 'tikhonov':\n",
        "              reg_term = 2 * Lambda_t * self.weights # learning rule of tikhonov regularization,\n",
        "                                                                  # the minus sign is due to the application of gradient descent algorithm.\n",
        "                                                                  # We use the predicted weights in the Nesterov optimization in the regularization too\n",
        "          elif reg_type == 'lasso':\n",
        "              reg_term = Lambda_l * np.sign(self.weights) # learning rule of lasso regularization\n",
        "          elif reg_type == 'elastic':\n",
        "              reg_term = (2 * Lambda_t * self.weights + Lambda_l * np.sign(self.weights)) # lasso + tikhonov regularization\n",
        "\n",
        "\n",
        "\n",
        "          self.m_weights = beta_1 * self.m_weights + (1 - beta_1) * (- np.dot(self.input.T, self.delta) - reg_term) # np.dot(self.input.T, delta) is dLoss/dw,\n",
        "                                                                                                                    # since self.delta is defined with a minus sign\n",
        "                                                                                                                    # and the formula is with a plus sign, we put a minus sign\n",
        "                                                                                                                    # in front of np.dot(xxx)\n",
        "          self.v_weights = beta_2* self.v_weights + (1 - beta_2) * ((np.dot(self.input.T, self.delta) + reg_term)**2) # here we have a plus sign in front of (1 - beta_2) since\n",
        "                                                                                                                      # self.delta is squared\n",
        "          m_weights_hat = self.m_weights / (1 - beta_1**self.t)\n",
        "          v_weights_hat = self.v_weights / (1 - beta_2**self.t)\n",
        "\n",
        "          self.m_biases = beta_1 * self.m_biases - (1 - beta_1) * np.sum(self.delta, axis=0, keepdims=True)\n",
        "          self.v_biases = beta_2* self.v_biases + (1 - beta_2) * np.sum(self.delta**2, axis=0, keepdims=True)\n",
        "          m_biases_hat = self.m_biases / (1 - beta_1**self.t)\n",
        "          v_biases_hat = self.v_biases / (1 - beta_2**self.t)\n",
        "\n",
        "          self.weights -= learning_rate_w * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon)\n",
        "          self.biases -= learning_rate_b * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)\n",
        "\n",
        "\n",
        "        sum_delta_weights = np.dot(self.delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights\n",
        "\n",
        "\n",
        "'''\n",
        "    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\n",
        "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
        "\n",
        "        # tikhonov and lasso implementation\n",
        "        if (reg_type=='tikhonov'):\n",
        "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\n",
        "        elif (reg_type=='lasso'):\n",
        "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\n",
        "        elif (reg_type=='elastic'): # lasso + tikhonov regularization\n",
        "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights)\n",
        "\n",
        "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
        "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        "\n",
        "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
        "   * parameters:\n",
        "     * x_tot: total data given as input;\n",
        "     * target: total data labels given as input;\n",
        "     * test_split: percentile of test set with respect to the total data.\n",
        "    * return->:\n",
        "      * x_train_val: training & validation set extracted from input data;\n",
        "      * target_train_val: training & validation set labels;\n",
        "      * x_test_val: test set extracted from input data;\n",
        "      * target_test_val: test set for input data labels.\n",
        "\n",
        "     \n",
        "* **add_layer**: appends a layer to the empty list self.layers\n",
        "   * parameter:\n",
        "     * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        "  * parameter:\n",
        "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        "  * return -> input = $o_i$ for layer $L_i$.\n",
        "  \n",
        "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        "  * parameters:\n",
        "    * d_Ep;\n",
        "    * learning_rate.\n",
        "\n",
        "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
        "\n",
        "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
        "  * parameters:\n",
        "    * x_train: set of the original dataset used for training;\n",
        "    * target_train: labels corresponding to the training set;\n",
        "    * x_val: set of the original dataset used for validation;\n",
        "    * target_val: labels corresponding to the validation set;\n",
        "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
        "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
        "    * loss_function: hyperparameter;\n",
        "    * loss_function_derivative: hyperparameter;\n",
        "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
        "\n",
        "* **train_val**: actual training and validation process.\n",
        "  * parameters:\n",
        "    * x_train_val;\n",
        "    * target_train_val;\n",
        "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
        "    * epochs;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def data_split(self, x_tot, target, test_split):\n",
        "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
        "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
        "\n",
        "        x_test = x_tot[:test_size]\n",
        "        target_test = target[:test_size]\n",
        "        x_train_val = x_tot[test_size:]\n",
        "        target_train_val = target[test_size:]\n",
        "\n",
        "        return x_train_val, target_train_val, x_test, target_test\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "\n",
        "    def reinitialize_weights(self):\n",
        "        for layer in self.layers:\n",
        "            layer.initialize_weights() # does it layer-by-layer\n",
        "\n",
        "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        train_error_epoch = np.zeros(epochs)\n",
        "        val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
        "            epoch_val_loss = 0\n",
        "\n",
        "            # shuffling training data before splitting it into batches.\n",
        "            # done in order to avoid reinforcing neurons in the same way\n",
        "            # in different epochs due to invisible patterns in the data\n",
        "            train_indices = np.arange(x_train.shape[0])\n",
        "            np.random.shuffle(train_indices)\n",
        "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
        "            target_train = target_train[train_indices]\n",
        "\n",
        "            # if batch_size=1 we get the online version,\n",
        "            # else we get mini-batch version with batches of size batch_size\n",
        "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
        "                                                             # Even if the number of examples is not divisible\n",
        "                                                             # for batch_size the last, smaller batch is processed anyway\n",
        "\n",
        "                x_batch = x_train[i:i+batch_size]\n",
        "                target_batch = target_train[i:i+batch_size]\n",
        "\n",
        "                # forward propagation\n",
        "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
        "                # computing loss and gradient\n",
        "                loss = loss_function(target_batch, predictions)\n",
        "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "                epoch_tr_loss += np.sum(loss)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(loss_gradient, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, beta_1, beta_2, epsilon, reg_type, opt_type)\n",
        "                Layer.t += 1 # number of iterations for adam\n",
        "\n",
        "\n",
        "            # validation\n",
        "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
        "            val_loss = loss_function(target_val, val_predictions)\n",
        "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished.\n",
        "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
        "            # Store average errors for the epoch\n",
        "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
        "            val_error_epoch[epoch] = epoch_val_loss\n",
        "\n",
        "        return train_error_epoch, val_error_epoch\n",
        "\n",
        "\n",
        "    def train_val(self, x_train_val, target_train_val, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type):\n",
        "        num_samples = x_train_val.shape[0]\n",
        "        fold_size = num_samples // K\n",
        "\n",
        "        # error storage for averaging\n",
        "        avg_train_error_epoch = np.zeros(epochs)\n",
        "        avg_val_error_epoch = np.zeros(epochs)\n",
        "\n",
        "        if K==1: # hold-out validation\n",
        "            train_indices = np.arange(0, int(0.75*num_samples)) # training set is 75% of the training & validation set\n",
        "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type\n",
        "            ) # computation of errors via train_val_setup method\n",
        "            return train_error_epoch, val_error_epoch\n",
        "\n",
        "\n",
        "        for k in range(K):\n",
        "            # creating fold indices\n",
        "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
        "                                                                        # It contains the indices of the examples used in validation set for\n",
        "                                                                        # this fold.\n",
        "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples -\n",
        "                                                                              # len(val_indices). It contains the indices of all the examples\n",
        "                                                                              # but the ones used in the validation set for this fold.\n",
        "                                                                              # It corresponds to the training set for the current fold.\n",
        "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
        "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
        "\n",
        "            # re-initializing weights for each fold\n",
        "            self.reinitialize_weights()\n",
        "            Layer.t = 1\n",
        "            # training on the current fold\n",
        "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
        "            x_train, target_train, x_val, target_val, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, loss_function, loss_function_derivative, batch_size, beta_1, beta_2, epsilon, reg_type, opt_type\n",
        "            )\n",
        "\n",
        "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
        "            avg_train_error_epoch += train_error_epoch\n",
        "            avg_val_error_epoch += val_error_epoch\n",
        "\n",
        "            print(f\"Fold {k+1} completed., t = {Layer.t}\")\n",
        "\n",
        "        # averaging errors across all folds\n",
        "        avg_train_error_epoch /= K\n",
        "        avg_val_error_epoch /= K\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
        "\n",
        "        return avg_train_error_epoch, avg_val_error_epoch\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "# Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "5be15b38-62a4-439a-dd86-57790daaffe2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 completed., t = 8001\n",
            "Fold 2 completed., t = 8001\n",
            "Fold 3 completed., t = 8001\n",
            "Fold 4 completed., t = 8001\n",
            "Fold 5 completed., t = 8001\n",
            "Epoch 1/1000, Avg Training Loss: 0.8700562399837979, Avg Validation Loss: 130.85233914722613\n",
            "Epoch 2/1000, Avg Training Loss: 0.5504634485546184, Avg Validation Loss: 86.373060523737\n",
            "Epoch 3/1000, Avg Training Loss: 0.4029727533130638, Avg Validation Loss: 73.2853416777971\n",
            "Epoch 4/1000, Avg Training Loss: 0.3623433127709606, Avg Validation Loss: 67.9719783980407\n",
            "Epoch 5/1000, Avg Training Loss: 0.33175936512586435, Avg Validation Loss: 61.81207535037184\n",
            "Epoch 6/1000, Avg Training Loss: 0.30528340307892365, Avg Validation Loss: 57.40227932400188\n",
            "Epoch 7/1000, Avg Training Loss: 0.2879511403626918, Avg Validation Loss: 54.58787498231396\n",
            "Epoch 8/1000, Avg Training Loss: 0.27599870528799725, Avg Validation Loss: 53.02224780270831\n",
            "Epoch 9/1000, Avg Training Loss: 0.26942698197860776, Avg Validation Loss: 51.975005185789044\n",
            "Epoch 10/1000, Avg Training Loss: 0.26434678420487645, Avg Validation Loss: 51.29477832624356\n",
            "Epoch 11/1000, Avg Training Loss: 0.2610711529954444, Avg Validation Loss: 50.94232449799565\n",
            "Epoch 12/1000, Avg Training Loss: 0.2592572740804291, Avg Validation Loss: 50.81025957559538\n",
            "Epoch 13/1000, Avg Training Loss: 0.25854338480818484, Avg Validation Loss: 50.74280371024467\n",
            "Epoch 14/1000, Avg Training Loss: 0.2580666882297192, Avg Validation Loss: 50.70445696776255\n",
            "Epoch 15/1000, Avg Training Loss: 0.2577070705056656, Avg Validation Loss: 50.67029153323335\n",
            "Epoch 16/1000, Avg Training Loss: 0.2575224864095087, Avg Validation Loss: 50.6531130313962\n",
            "Epoch 17/1000, Avg Training Loss: 0.25742968432301866, Avg Validation Loss: 50.64320367542095\n",
            "Epoch 18/1000, Avg Training Loss: 0.257339382234319, Avg Validation Loss: 50.63784013425459\n",
            "Epoch 19/1000, Avg Training Loss: 0.25733793396421223, Avg Validation Loss: 50.65216321088495\n",
            "Epoch 20/1000, Avg Training Loss: 0.25712885161069, Avg Validation Loss: 50.622343750201054\n",
            "Epoch 21/1000, Avg Training Loss: 0.257021521396669, Avg Validation Loss: 50.603013664701194\n",
            "Epoch 22/1000, Avg Training Loss: 0.2569023788508242, Avg Validation Loss: 50.57826462398958\n",
            "Epoch 23/1000, Avg Training Loss: 0.2567898960295906, Avg Validation Loss: 50.554962285468875\n",
            "Epoch 24/1000, Avg Training Loss: 0.256743333665279, Avg Validation Loss: 50.55859331213431\n",
            "Epoch 25/1000, Avg Training Loss: 0.25663253025289046, Avg Validation Loss: 50.545855313148095\n",
            "Epoch 26/1000, Avg Training Loss: 0.2566214346881422, Avg Validation Loss: 50.54129908811611\n",
            "Epoch 27/1000, Avg Training Loss: 0.2565806447507948, Avg Validation Loss: 50.52935750849872\n",
            "Epoch 28/1000, Avg Training Loss: 0.2564641149621234, Avg Validation Loss: 50.51747663445427\n",
            "Epoch 29/1000, Avg Training Loss: 0.25633945279326203, Avg Validation Loss: 50.49046370175156\n",
            "Epoch 30/1000, Avg Training Loss: 0.25621093627271974, Avg Validation Loss: 50.467042384894086\n",
            "Epoch 31/1000, Avg Training Loss: 0.2560649148548768, Avg Validation Loss: 50.43319543898505\n",
            "Epoch 32/1000, Avg Training Loss: 0.25591862480181365, Avg Validation Loss: 50.410896409744346\n",
            "Epoch 33/1000, Avg Training Loss: 0.25578171055769994, Avg Validation Loss: 50.40224187568168\n",
            "Epoch 34/1000, Avg Training Loss: 0.2556935644399284, Avg Validation Loss: 50.38730083480209\n",
            "Epoch 35/1000, Avg Training Loss: 0.25560805662054276, Avg Validation Loss: 50.38182080108166\n",
            "Epoch 36/1000, Avg Training Loss: 0.25553336096009227, Avg Validation Loss: 50.37604123259002\n",
            "Epoch 37/1000, Avg Training Loss: 0.25549162641936524, Avg Validation Loss: 50.37841630867474\n",
            "Epoch 38/1000, Avg Training Loss: 0.25545963703249286, Avg Validation Loss: 50.37820162813392\n",
            "Epoch 39/1000, Avg Training Loss: 0.25544247711093143, Avg Validation Loss: 50.376937817604\n",
            "Epoch 40/1000, Avg Training Loss: 0.2553931233650091, Avg Validation Loss: 50.38423359431383\n",
            "Epoch 41/1000, Avg Training Loss: 0.25533277104576046, Avg Validation Loss: 50.375170188099275\n",
            "Epoch 42/1000, Avg Training Loss: 0.255313749426041, Avg Validation Loss: 50.3748609011609\n",
            "Epoch 43/1000, Avg Training Loss: 0.25525719309857686, Avg Validation Loss: 50.37594131633256\n",
            "Epoch 44/1000, Avg Training Loss: 0.2552342484342088, Avg Validation Loss: 50.36654126539492\n",
            "Epoch 45/1000, Avg Training Loss: 0.2552028835992184, Avg Validation Loss: 50.36856822605833\n",
            "Epoch 46/1000, Avg Training Loss: 0.25519225684798846, Avg Validation Loss: 50.371115538998325\n",
            "Epoch 47/1000, Avg Training Loss: 0.2551490168028735, Avg Validation Loss: 50.37720122813934\n",
            "Epoch 48/1000, Avg Training Loss: 0.2551283945407775, Avg Validation Loss: 50.38428012594993\n",
            "Epoch 49/1000, Avg Training Loss: 0.25513784512720716, Avg Validation Loss: 50.379042756291696\n",
            "Epoch 50/1000, Avg Training Loss: 0.2551054417143153, Avg Validation Loss: 50.376260073120754\n",
            "Epoch 51/1000, Avg Training Loss: 0.2550900916832439, Avg Validation Loss: 50.386949665225515\n",
            "Epoch 52/1000, Avg Training Loss: 0.2550777033534651, Avg Validation Loss: 50.391756852721144\n",
            "Epoch 53/1000, Avg Training Loss: 0.2550694969776853, Avg Validation Loss: 50.39814748556613\n",
            "Epoch 54/1000, Avg Training Loss: 0.25511721524665065, Avg Validation Loss: 50.410826701291946\n",
            "Epoch 55/1000, Avg Training Loss: 0.25504647058941493, Avg Validation Loss: 50.41961099012501\n",
            "Epoch 56/1000, Avg Training Loss: 0.2550632937483292, Avg Validation Loss: 50.43269503906096\n",
            "Epoch 57/1000, Avg Training Loss: 0.2550842320819176, Avg Validation Loss: 50.441673021074955\n",
            "Epoch 58/1000, Avg Training Loss: 0.2550830953307184, Avg Validation Loss: 50.44626412245729\n",
            "Epoch 59/1000, Avg Training Loss: 0.2550997911498278, Avg Validation Loss: 50.4533842917964\n",
            "Epoch 60/1000, Avg Training Loss: 0.2551125304959541, Avg Validation Loss: 50.4609662311405\n",
            "Epoch 61/1000, Avg Training Loss: 0.255114585491808, Avg Validation Loss: 50.46326024553731\n",
            "Epoch 62/1000, Avg Training Loss: 0.2551377091578061, Avg Validation Loss: 50.465473286224125\n",
            "Epoch 63/1000, Avg Training Loss: 0.2551015378529799, Avg Validation Loss: 50.47604045137787\n",
            "Epoch 64/1000, Avg Training Loss: 0.25512512180916214, Avg Validation Loss: 50.48551075344621\n",
            "Epoch 65/1000, Avg Training Loss: 0.2551146092934028, Avg Validation Loss: 50.49248112002134\n",
            "Epoch 66/1000, Avg Training Loss: 0.25511898406202277, Avg Validation Loss: 50.50303886929095\n",
            "Epoch 67/1000, Avg Training Loss: 0.2551521367377207, Avg Validation Loss: 50.50701671842368\n",
            "Epoch 68/1000, Avg Training Loss: 0.2551425928546632, Avg Validation Loss: 50.518115102773955\n",
            "Epoch 69/1000, Avg Training Loss: 0.25517565348810584, Avg Validation Loss: 50.52893837491891\n",
            "Epoch 70/1000, Avg Training Loss: 0.2551800255508586, Avg Validation Loss: 50.539816823699\n",
            "Epoch 71/1000, Avg Training Loss: 0.2552282972846116, Avg Validation Loss: 50.56061649034193\n",
            "Epoch 72/1000, Avg Training Loss: 0.2552137627889024, Avg Validation Loss: 50.56991193163216\n",
            "Epoch 73/1000, Avg Training Loss: 0.2552207524306285, Avg Validation Loss: 50.573967282681544\n",
            "Epoch 74/1000, Avg Training Loss: 0.2552410111976059, Avg Validation Loss: 50.580728865325824\n",
            "Epoch 75/1000, Avg Training Loss: 0.2552712956498717, Avg Validation Loss: 50.59329821157144\n",
            "Epoch 76/1000, Avg Training Loss: 0.25529670779938, Avg Validation Loss: 50.59755586281905\n",
            "Epoch 77/1000, Avg Training Loss: 0.25527116967413066, Avg Validation Loss: 50.60091387848481\n",
            "Epoch 78/1000, Avg Training Loss: 0.2552492948145867, Avg Validation Loss: 50.61494974167539\n",
            "Epoch 79/1000, Avg Training Loss: 0.25534192117841625, Avg Validation Loss: 50.64044291569062\n",
            "Epoch 80/1000, Avg Training Loss: 0.2553057267673553, Avg Validation Loss: 50.63392212490658\n",
            "Epoch 81/1000, Avg Training Loss: 0.2552853350543364, Avg Validation Loss: 50.63927555383981\n",
            "Epoch 82/1000, Avg Training Loss: 0.25528442249874744, Avg Validation Loss: 50.65472142234094\n",
            "Epoch 83/1000, Avg Training Loss: 0.25530455165341154, Avg Validation Loss: 50.67238202480382\n",
            "Epoch 84/1000, Avg Training Loss: 0.2553247948949339, Avg Validation Loss: 50.68622805227204\n",
            "Epoch 85/1000, Avg Training Loss: 0.25533755425309124, Avg Validation Loss: 50.693412995029746\n",
            "Epoch 86/1000, Avg Training Loss: 0.2553427380920563, Avg Validation Loss: 50.705751315773156\n",
            "Epoch 87/1000, Avg Training Loss: 0.25535195474519007, Avg Validation Loss: 50.720151337348184\n",
            "Epoch 88/1000, Avg Training Loss: 0.2553817725282164, Avg Validation Loss: 50.72714350759892\n",
            "Epoch 89/1000, Avg Training Loss: 0.255376127519201, Avg Validation Loss: 50.73415518611026\n",
            "Epoch 90/1000, Avg Training Loss: 0.25539337225812453, Avg Validation Loss: 50.75307570026358\n",
            "Epoch 91/1000, Avg Training Loss: 0.25539767495625015, Avg Validation Loss: 50.75923465335765\n",
            "Epoch 92/1000, Avg Training Loss: 0.25540384510756925, Avg Validation Loss: 50.76703669710933\n",
            "Epoch 93/1000, Avg Training Loss: 0.25540488803818195, Avg Validation Loss: 50.77743989072421\n",
            "Epoch 94/1000, Avg Training Loss: 0.2554050327193347, Avg Validation Loss: 50.78938576098044\n",
            "Epoch 95/1000, Avg Training Loss: 0.25541642029049033, Avg Validation Loss: 50.80494480262668\n",
            "Epoch 96/1000, Avg Training Loss: 0.2554441488262261, Avg Validation Loss: 50.8231161443438\n",
            "Epoch 97/1000, Avg Training Loss: 0.2554391181745884, Avg Validation Loss: 50.83329433228263\n",
            "Epoch 98/1000, Avg Training Loss: 0.25542810919399805, Avg Validation Loss: 50.83600695802179\n",
            "Epoch 99/1000, Avg Training Loss: 0.2554579707745396, Avg Validation Loss: 50.84286492543589\n",
            "Epoch 100/1000, Avg Training Loss: 0.2554484880818794, Avg Validation Loss: 50.85589177606643\n",
            "Epoch 101/1000, Avg Training Loss: 0.2554610328079384, Avg Validation Loss: 50.87134181241589\n",
            "Epoch 102/1000, Avg Training Loss: 0.25548746744481116, Avg Validation Loss: 50.88809134806609\n",
            "Epoch 103/1000, Avg Training Loss: 0.25550951694841423, Avg Validation Loss: 50.90313149514776\n",
            "Epoch 104/1000, Avg Training Loss: 0.2555514639379257, Avg Validation Loss: 50.92002323602041\n",
            "Epoch 105/1000, Avg Training Loss: 0.25560066767240064, Avg Validation Loss: 50.93270529949045\n",
            "Epoch 106/1000, Avg Training Loss: 0.25564412884842175, Avg Validation Loss: 50.958584500110874\n",
            "Epoch 107/1000, Avg Training Loss: 0.2556912224117268, Avg Validation Loss: 50.97952653602609\n",
            "Epoch 108/1000, Avg Training Loss: 0.25567839257886976, Avg Validation Loss: 50.99641620559973\n",
            "Epoch 109/1000, Avg Training Loss: 0.2557553066944168, Avg Validation Loss: 51.01563267727541\n",
            "Epoch 110/1000, Avg Training Loss: 0.2557705863810254, Avg Validation Loss: 51.04125313616906\n",
            "Epoch 111/1000, Avg Training Loss: 0.2558670609015205, Avg Validation Loss: 51.077938496027784\n",
            "Epoch 112/1000, Avg Training Loss: 0.255917121666614, Avg Validation Loss: 51.104261276563165\n",
            "Epoch 113/1000, Avg Training Loss: 0.25598943646652655, Avg Validation Loss: 51.12039852582518\n",
            "Epoch 114/1000, Avg Training Loss: 0.25601767486040006, Avg Validation Loss: 51.14403653340747\n",
            "Epoch 115/1000, Avg Training Loss: 0.25603199693485806, Avg Validation Loss: 51.171521177659535\n",
            "Epoch 116/1000, Avg Training Loss: 0.25601271588497454, Avg Validation Loss: 51.18802521186187\n",
            "Epoch 117/1000, Avg Training Loss: 0.2559818593648938, Avg Validation Loss: 51.192284776047345\n",
            "Epoch 118/1000, Avg Training Loss: 0.25592309431193727, Avg Validation Loss: 51.211489601155016\n",
            "Epoch 119/1000, Avg Training Loss: 0.2558818895145183, Avg Validation Loss: 51.222320049443596\n",
            "Epoch 120/1000, Avg Training Loss: 0.2558219579424359, Avg Validation Loss: 51.231845750092546\n",
            "Epoch 121/1000, Avg Training Loss: 0.2557569700189031, Avg Validation Loss: 51.235440830001245\n",
            "Epoch 122/1000, Avg Training Loss: 0.2557313875469441, Avg Validation Loss: 51.25177670984223\n",
            "Epoch 123/1000, Avg Training Loss: 0.2556337043090622, Avg Validation Loss: 51.251879639269134\n",
            "Epoch 124/1000, Avg Training Loss: 0.2555585456044255, Avg Validation Loss: 51.24370057886481\n",
            "Epoch 125/1000, Avg Training Loss: 0.2554872783148966, Avg Validation Loss: 51.25515668506829\n",
            "Epoch 126/1000, Avg Training Loss: 0.2553790939333291, Avg Validation Loss: 51.25350255635386\n",
            "Epoch 127/1000, Avg Training Loss: 0.255250767276494, Avg Validation Loss: 51.25082314539061\n",
            "Epoch 128/1000, Avg Training Loss: 0.2551624966718972, Avg Validation Loss: 51.243295626984974\n",
            "Epoch 129/1000, Avg Training Loss: 0.2550552129524699, Avg Validation Loss: 51.24259601999837\n",
            "Epoch 130/1000, Avg Training Loss: 0.2550049803005929, Avg Validation Loss: 51.25740366719823\n",
            "Epoch 131/1000, Avg Training Loss: 0.25485607555368633, Avg Validation Loss: 51.24505881672937\n",
            "Epoch 132/1000, Avg Training Loss: 0.2547859868121474, Avg Validation Loss: 51.23559221837083\n",
            "Epoch 133/1000, Avg Training Loss: 0.2546912094466254, Avg Validation Loss: 51.22988720688227\n",
            "Epoch 134/1000, Avg Training Loss: 0.2546578195558228, Avg Validation Loss: 51.22274448068451\n",
            "Epoch 135/1000, Avg Training Loss: 0.2545628849684726, Avg Validation Loss: 51.20601762226993\n",
            "Epoch 136/1000, Avg Training Loss: 0.2544653132514067, Avg Validation Loss: 51.21225843860531\n",
            "Epoch 137/1000, Avg Training Loss: 0.25442040942629923, Avg Validation Loss: 51.21596098954683\n",
            "Epoch 138/1000, Avg Training Loss: 0.2543185265061173, Avg Validation Loss: 51.21371068643296\n",
            "Epoch 139/1000, Avg Training Loss: 0.2542562720313066, Avg Validation Loss: 51.19427752838227\n",
            "Epoch 140/1000, Avg Training Loss: 0.25418615297571523, Avg Validation Loss: 51.198033709075204\n",
            "Epoch 141/1000, Avg Training Loss: 0.2541240809386862, Avg Validation Loss: 51.18700123505427\n",
            "Epoch 142/1000, Avg Training Loss: 0.25406655548606993, Avg Validation Loss: 51.1867449987984\n",
            "Epoch 143/1000, Avg Training Loss: 0.25400326105831006, Avg Validation Loss: 51.1696272184131\n",
            "Epoch 144/1000, Avg Training Loss: 0.25392057165688375, Avg Validation Loss: 51.16475435790747\n",
            "Epoch 145/1000, Avg Training Loss: 0.2538792793796793, Avg Validation Loss: 51.1526823755837\n",
            "Epoch 146/1000, Avg Training Loss: 0.2537951261035405, Avg Validation Loss: 51.1475643997286\n",
            "Epoch 147/1000, Avg Training Loss: 0.2537392542106477, Avg Validation Loss: 51.14706367457007\n",
            "Epoch 148/1000, Avg Training Loss: 0.25368376241628043, Avg Validation Loss: 51.14631049952045\n",
            "Epoch 149/1000, Avg Training Loss: 0.2536469363977216, Avg Validation Loss: 51.124573792497856\n",
            "Epoch 150/1000, Avg Training Loss: 0.25358929005234593, Avg Validation Loss: 51.12006937745666\n",
            "Epoch 151/1000, Avg Training Loss: 0.25349294001313316, Avg Validation Loss: 51.1115218331221\n",
            "Epoch 152/1000, Avg Training Loss: 0.2534531348970853, Avg Validation Loss: 51.112658915477034\n",
            "Epoch 153/1000, Avg Training Loss: 0.25337499967814253, Avg Validation Loss: 51.10440369270385\n",
            "Epoch 154/1000, Avg Training Loss: 0.253315521790828, Avg Validation Loss: 51.08321162499736\n",
            "Epoch 155/1000, Avg Training Loss: 0.253256679413274, Avg Validation Loss: 51.07297414070611\n",
            "Epoch 156/1000, Avg Training Loss: 0.2532126900880507, Avg Validation Loss: 51.076797184789214\n",
            "Epoch 157/1000, Avg Training Loss: 0.25315737771973235, Avg Validation Loss: 51.05697049480227\n",
            "Epoch 158/1000, Avg Training Loss: 0.25311098089383594, Avg Validation Loss: 51.06294785705106\n",
            "Epoch 159/1000, Avg Training Loss: 0.2530474333566253, Avg Validation Loss: 51.03281580726248\n",
            "Epoch 160/1000, Avg Training Loss: 0.25295737277264346, Avg Validation Loss: 51.02700349991215\n",
            "Epoch 161/1000, Avg Training Loss: 0.2529018233609802, Avg Validation Loss: 51.02139838080454\n",
            "Epoch 162/1000, Avg Training Loss: 0.2529162471144173, Avg Validation Loss: 51.040019063833626\n",
            "Epoch 163/1000, Avg Training Loss: 0.25283237767990185, Avg Validation Loss: 51.011332116721704\n",
            "Epoch 164/1000, Avg Training Loss: 0.2528108004774813, Avg Validation Loss: 51.018564879966895\n",
            "Epoch 165/1000, Avg Training Loss: 0.25274261573823764, Avg Validation Loss: 51.01649849907115\n",
            "Epoch 166/1000, Avg Training Loss: 0.2527006708926275, Avg Validation Loss: 50.97738318570117\n",
            "Epoch 167/1000, Avg Training Loss: 0.2526336636862862, Avg Validation Loss: 50.95936259971052\n",
            "Epoch 168/1000, Avg Training Loss: 0.25259090018824876, Avg Validation Loss: 50.96338248140506\n",
            "Epoch 169/1000, Avg Training Loss: 0.25252677941742624, Avg Validation Loss: 50.962014728329116\n",
            "Epoch 170/1000, Avg Training Loss: 0.2524890064826393, Avg Validation Loss: 50.94952792819735\n",
            "Epoch 171/1000, Avg Training Loss: 0.25243587603025913, Avg Validation Loss: 50.92509410662781\n",
            "Epoch 172/1000, Avg Training Loss: 0.25237736813381306, Avg Validation Loss: 50.931395725592836\n",
            "Epoch 173/1000, Avg Training Loss: 0.2523193281604961, Avg Validation Loss: 50.92357608423057\n",
            "Epoch 174/1000, Avg Training Loss: 0.25227841261669726, Avg Validation Loss: 50.91054150683826\n",
            "Epoch 175/1000, Avg Training Loss: 0.2522261997032784, Avg Validation Loss: 50.899019168723925\n",
            "Epoch 176/1000, Avg Training Loss: 0.2521785646189724, Avg Validation Loss: 50.8890212218879\n",
            "Epoch 177/1000, Avg Training Loss: 0.25213400300628186, Avg Validation Loss: 50.89314667606703\n",
            "Epoch 178/1000, Avg Training Loss: 0.2520953338731962, Avg Validation Loss: 50.8912077054933\n",
            "Epoch 179/1000, Avg Training Loss: 0.2520673700193358, Avg Validation Loss: 50.873890808319956\n",
            "Epoch 180/1000, Avg Training Loss: 0.2520031897873998, Avg Validation Loss: 50.86751742032659\n",
            "Epoch 181/1000, Avg Training Loss: 0.2519609375599646, Avg Validation Loss: 50.86433908592619\n",
            "Epoch 182/1000, Avg Training Loss: 0.2519638056182105, Avg Validation Loss: 50.870974001311495\n",
            "Epoch 183/1000, Avg Training Loss: 0.2519078611361818, Avg Validation Loss: 50.86587118923643\n",
            "Epoch 184/1000, Avg Training Loss: 0.25184765295189104, Avg Validation Loss: 50.84813964701945\n",
            "Epoch 185/1000, Avg Training Loss: 0.25182704968901076, Avg Validation Loss: 50.82239310064034\n",
            "Epoch 186/1000, Avg Training Loss: 0.251770102947771, Avg Validation Loss: 50.81332012701087\n",
            "Epoch 187/1000, Avg Training Loss: 0.2517369990812092, Avg Validation Loss: 50.80465291369653\n",
            "Epoch 188/1000, Avg Training Loss: 0.2517171490522674, Avg Validation Loss: 50.81206623688724\n",
            "Epoch 189/1000, Avg Training Loss: 0.2516796428502542, Avg Validation Loss: 50.804670148414274\n",
            "Epoch 190/1000, Avg Training Loss: 0.2516794250697749, Avg Validation Loss: 50.81964754098708\n",
            "Epoch 191/1000, Avg Training Loss: 0.25162657327738364, Avg Validation Loss: 50.801900625038044\n",
            "Epoch 192/1000, Avg Training Loss: 0.25159651516632037, Avg Validation Loss: 50.77257033516092\n",
            "Epoch 193/1000, Avg Training Loss: 0.2515251801131779, Avg Validation Loss: 50.75848557200384\n",
            "Epoch 194/1000, Avg Training Loss: 0.2515115278634448, Avg Validation Loss: 50.76418604604949\n",
            "Epoch 195/1000, Avg Training Loss: 0.25145326724923583, Avg Validation Loss: 50.76022292429984\n",
            "Epoch 196/1000, Avg Training Loss: 0.2514455994768561, Avg Validation Loss: 50.75155807334421\n",
            "Epoch 197/1000, Avg Training Loss: 0.2514398037556324, Avg Validation Loss: 50.7452596067614\n",
            "Epoch 198/1000, Avg Training Loss: 0.25138516484856954, Avg Validation Loss: 50.73954671724649\n",
            "Epoch 199/1000, Avg Training Loss: 0.2513693909702489, Avg Validation Loss: 50.73266462654459\n",
            "Epoch 200/1000, Avg Training Loss: 0.2513576782017654, Avg Validation Loss: 50.73079406812753\n",
            "Epoch 201/1000, Avg Training Loss: 0.2513504950652575, Avg Validation Loss: 50.732737023453105\n",
            "Epoch 202/1000, Avg Training Loss: 0.25131418736620115, Avg Validation Loss: 50.725243380512154\n",
            "Epoch 203/1000, Avg Training Loss: 0.2513006501848661, Avg Validation Loss: 50.71109522912301\n",
            "Epoch 204/1000, Avg Training Loss: 0.25129746266527536, Avg Validation Loss: 50.69471131689409\n",
            "Epoch 205/1000, Avg Training Loss: 0.25126668271451846, Avg Validation Loss: 50.69969338057254\n",
            "Epoch 206/1000, Avg Training Loss: 0.2512735634790274, Avg Validation Loss: 50.697886255797464\n",
            "Epoch 207/1000, Avg Training Loss: 0.25125932012356933, Avg Validation Loss: 50.69035367003086\n",
            "Epoch 208/1000, Avg Training Loss: 0.25126014548108044, Avg Validation Loss: 50.6882041105375\n",
            "Epoch 209/1000, Avg Training Loss: 0.25124571184198574, Avg Validation Loss: 50.686629306341686\n",
            "Epoch 210/1000, Avg Training Loss: 0.2513097978694159, Avg Validation Loss: 50.68821213586473\n",
            "Epoch 211/1000, Avg Training Loss: 0.251274713993739, Avg Validation Loss: 50.690913623347924\n",
            "Epoch 212/1000, Avg Training Loss: 0.25129634377754156, Avg Validation Loss: 50.69251797612783\n",
            "Epoch 213/1000, Avg Training Loss: 0.25135412715316297, Avg Validation Loss: 50.691588228525966\n",
            "Epoch 214/1000, Avg Training Loss: 0.2513841069417474, Avg Validation Loss: 50.71861596519052\n",
            "Epoch 215/1000, Avg Training Loss: 0.25147249875114214, Avg Validation Loss: 50.749790620503546\n",
            "Epoch 216/1000, Avg Training Loss: 0.2515574150092965, Avg Validation Loss: 50.77962333339209\n",
            "Epoch 217/1000, Avg Training Loss: 0.25168679603034705, Avg Validation Loss: 50.788145061193234\n",
            "Epoch 218/1000, Avg Training Loss: 0.25185403557090635, Avg Validation Loss: 50.822303605968145\n",
            "Epoch 219/1000, Avg Training Loss: 0.2520555230495036, Avg Validation Loss: 50.87487705251706\n",
            "Epoch 220/1000, Avg Training Loss: 0.25238608053989403, Avg Validation Loss: 50.943845355805294\n",
            "Epoch 221/1000, Avg Training Loss: 0.25273867279380086, Avg Validation Loss: 51.02796286473081\n",
            "Epoch 222/1000, Avg Training Loss: 0.2531005062355309, Avg Validation Loss: 51.08964652137419\n",
            "Epoch 223/1000, Avg Training Loss: 0.2535311913949174, Avg Validation Loss: 51.155058174488886\n",
            "Epoch 224/1000, Avg Training Loss: 0.2539012936816133, Avg Validation Loss: 51.20965640939613\n",
            "Epoch 225/1000, Avg Training Loss: 0.2542331433917457, Avg Validation Loss: 51.256061120204066\n",
            "Epoch 226/1000, Avg Training Loss: 0.2545450927415901, Avg Validation Loss: 51.2606963148508\n",
            "Epoch 227/1000, Avg Training Loss: 0.2546596691023968, Avg Validation Loss: 51.26287274144481\n",
            "Epoch 228/1000, Avg Training Loss: 0.2546365238437657, Avg Validation Loss: 51.22857574885069\n",
            "Epoch 229/1000, Avg Training Loss: 0.25449243806485433, Avg Validation Loss: 51.13865048103497\n",
            "Epoch 230/1000, Avg Training Loss: 0.25426929052878683, Avg Validation Loss: 51.02087456635215\n",
            "Epoch 231/1000, Avg Training Loss: 0.2538994499017971, Avg Validation Loss: 50.93519934517737\n",
            "Epoch 232/1000, Avg Training Loss: 0.25366270965165916, Avg Validation Loss: 50.88675966661538\n",
            "Epoch 233/1000, Avg Training Loss: 0.25346948740891995, Avg Validation Loss: 50.837321262248935\n",
            "Epoch 234/1000, Avg Training Loss: 0.2533262557961521, Avg Validation Loss: 50.78446652732067\n",
            "Epoch 235/1000, Avg Training Loss: 0.2532181555514905, Avg Validation Loss: 50.7288887768834\n",
            "Epoch 236/1000, Avg Training Loss: 0.2530712923843303, Avg Validation Loss: 50.69429692182388\n",
            "Epoch 237/1000, Avg Training Loss: 0.2530041586495677, Avg Validation Loss: 50.68150365459951\n",
            "Epoch 238/1000, Avg Training Loss: 0.2529743124087935, Avg Validation Loss: 50.67351148641421\n",
            "Epoch 239/1000, Avg Training Loss: 0.25297291542466954, Avg Validation Loss: 50.64797635085472\n",
            "Epoch 240/1000, Avg Training Loss: 0.25291947156229105, Avg Validation Loss: 50.648779336029264\n",
            "Epoch 241/1000, Avg Training Loss: 0.2529003326785827, Avg Validation Loss: 50.64666696870886\n",
            "Epoch 242/1000, Avg Training Loss: 0.25291272219417704, Avg Validation Loss: 50.612309474633754\n",
            "Epoch 243/1000, Avg Training Loss: 0.25291529551071007, Avg Validation Loss: 50.62602337249355\n",
            "Epoch 244/1000, Avg Training Loss: 0.252865543135702, Avg Validation Loss: 50.60522439879199\n",
            "Epoch 245/1000, Avg Training Loss: 0.25285620636421385, Avg Validation Loss: 50.609468969554385\n",
            "Epoch 246/1000, Avg Training Loss: 0.2528393814744813, Avg Validation Loss: 50.61099618686816\n",
            "Epoch 247/1000, Avg Training Loss: 0.25283735983337596, Avg Validation Loss: 50.609345622846405\n",
            "Epoch 248/1000, Avg Training Loss: 0.25283149193665166, Avg Validation Loss: 50.61264850976934\n",
            "Epoch 249/1000, Avg Training Loss: 0.2528413865451978, Avg Validation Loss: 50.61389144915876\n",
            "Epoch 250/1000, Avg Training Loss: 0.25282829914767563, Avg Validation Loss: 50.61649101672893\n",
            "Epoch 251/1000, Avg Training Loss: 0.25281918961924976, Avg Validation Loss: 50.60448793212943\n",
            "Epoch 252/1000, Avg Training Loss: 0.2528138548525367, Avg Validation Loss: 50.60345386640696\n",
            "Epoch 253/1000, Avg Training Loss: 0.25281696901701545, Avg Validation Loss: 50.604835469963234\n",
            "Epoch 254/1000, Avg Training Loss: 0.2528178348106472, Avg Validation Loss: 50.62406401997728\n",
            "Epoch 255/1000, Avg Training Loss: 0.2527643342729927, Avg Validation Loss: 50.60739031521951\n",
            "Epoch 256/1000, Avg Training Loss: 0.25276230400939087, Avg Validation Loss: 50.58991341321769\n",
            "Epoch 257/1000, Avg Training Loss: 0.25273442442522104, Avg Validation Loss: 50.60024857101392\n",
            "Epoch 258/1000, Avg Training Loss: 0.25272037798870367, Avg Validation Loss: 50.583247493385684\n",
            "Epoch 259/1000, Avg Training Loss: 0.2527302551157533, Avg Validation Loss: 50.56945257959431\n",
            "Epoch 260/1000, Avg Training Loss: 0.2526869637802081, Avg Validation Loss: 50.58797257172617\n",
            "Epoch 261/1000, Avg Training Loss: 0.2526817733396097, Avg Validation Loss: 50.59574445448355\n",
            "Epoch 262/1000, Avg Training Loss: 0.25270073118956954, Avg Validation Loss: 50.59698440740722\n",
            "Epoch 263/1000, Avg Training Loss: 0.25266576002728725, Avg Validation Loss: 50.5988377513949\n",
            "Epoch 264/1000, Avg Training Loss: 0.2526541519855705, Avg Validation Loss: 50.60059253074411\n",
            "Epoch 265/1000, Avg Training Loss: 0.252662071132713, Avg Validation Loss: 50.59663853835219\n",
            "Epoch 266/1000, Avg Training Loss: 0.2526721166633353, Avg Validation Loss: 50.59407272570153\n",
            "Epoch 267/1000, Avg Training Loss: 0.25262137466794415, Avg Validation Loss: 50.58325857754697\n",
            "Epoch 268/1000, Avg Training Loss: 0.2526103030463267, Avg Validation Loss: 50.5828813873764\n",
            "Epoch 269/1000, Avg Training Loss: 0.25261177049116346, Avg Validation Loss: 50.58216360948518\n",
            "Epoch 270/1000, Avg Training Loss: 0.25260425525188507, Avg Validation Loss: 50.58100688919528\n",
            "Epoch 271/1000, Avg Training Loss: 0.25259948748137073, Avg Validation Loss: 50.581761786589844\n",
            "Epoch 272/1000, Avg Training Loss: 0.25263189515952783, Avg Validation Loss: 50.5781869465697\n",
            "Epoch 273/1000, Avg Training Loss: 0.2526036108670707, Avg Validation Loss: 50.58515963103626\n",
            "Epoch 274/1000, Avg Training Loss: 0.252591860213397, Avg Validation Loss: 50.59442729672481\n",
            "Epoch 275/1000, Avg Training Loss: 0.25255753985560725, Avg Validation Loss: 50.58769379148417\n",
            "Epoch 276/1000, Avg Training Loss: 0.25253651516244036, Avg Validation Loss: 50.58963934936287\n",
            "Epoch 277/1000, Avg Training Loss: 0.25260667894799155, Avg Validation Loss: 50.561152877614035\n",
            "Epoch 278/1000, Avg Training Loss: 0.2525453329383201, Avg Validation Loss: 50.59709682293104\n",
            "Epoch 279/1000, Avg Training Loss: 0.25250772617932177, Avg Validation Loss: 50.59025076920346\n",
            "Epoch 280/1000, Avg Training Loss: 0.25250603526631776, Avg Validation Loss: 50.5857155097105\n",
            "Epoch 281/1000, Avg Training Loss: 0.2524819398643519, Avg Validation Loss: 50.59647472420164\n",
            "Epoch 282/1000, Avg Training Loss: 0.25248553982167093, Avg Validation Loss: 50.60327201917032\n",
            "Epoch 283/1000, Avg Training Loss: 0.2524556767203632, Avg Validation Loss: 50.60570378641475\n",
            "Epoch 284/1000, Avg Training Loss: 0.25244454153921386, Avg Validation Loss: 50.60093718952733\n",
            "Epoch 285/1000, Avg Training Loss: 0.25243479235934, Avg Validation Loss: 50.59264961949772\n",
            "Epoch 286/1000, Avg Training Loss: 0.2524335191809848, Avg Validation Loss: 50.613375975726136\n",
            "Epoch 287/1000, Avg Training Loss: 0.25241560583537953, Avg Validation Loss: 50.61391137968411\n",
            "Epoch 288/1000, Avg Training Loss: 0.2524199489704982, Avg Validation Loss: 50.61832275805877\n",
            "Epoch 289/1000, Avg Training Loss: 0.2524137319946381, Avg Validation Loss: 50.62773434518482\n",
            "Epoch 290/1000, Avg Training Loss: 0.2524113593955247, Avg Validation Loss: 50.637029527189355\n",
            "Epoch 291/1000, Avg Training Loss: 0.2524195709403972, Avg Validation Loss: 50.62919337884138\n",
            "Epoch 292/1000, Avg Training Loss: 0.25242928913833107, Avg Validation Loss: 50.644248788038034\n",
            "Epoch 293/1000, Avg Training Loss: 0.25243125120785465, Avg Validation Loss: 50.66077154549413\n",
            "Epoch 294/1000, Avg Training Loss: 0.2524373496480865, Avg Validation Loss: 50.64664944183973\n",
            "Epoch 295/1000, Avg Training Loss: 0.2523973219696732, Avg Validation Loss: 50.65301804618858\n",
            "Epoch 296/1000, Avg Training Loss: 0.2523892384503579, Avg Validation Loss: 50.65208443664755\n",
            "Epoch 297/1000, Avg Training Loss: 0.25240590705439625, Avg Validation Loss: 50.660882091947514\n",
            "Epoch 298/1000, Avg Training Loss: 0.25240159654446603, Avg Validation Loss: 50.66619992813306\n",
            "Epoch 299/1000, Avg Training Loss: 0.25241946153499245, Avg Validation Loss: 50.660887799634835\n",
            "Epoch 300/1000, Avg Training Loss: 0.2523796181264182, Avg Validation Loss: 50.6666471063231\n",
            "Epoch 301/1000, Avg Training Loss: 0.25238990371777104, Avg Validation Loss: 50.6733245774128\n",
            "Epoch 302/1000, Avg Training Loss: 0.252395144979383, Avg Validation Loss: 50.689111287555804\n",
            "Epoch 303/1000, Avg Training Loss: 0.25240620795231783, Avg Validation Loss: 50.70542222309828\n",
            "Epoch 304/1000, Avg Training Loss: 0.2524307765585671, Avg Validation Loss: 50.713455568812215\n",
            "Epoch 305/1000, Avg Training Loss: 0.25245853447539723, Avg Validation Loss: 50.72435491814349\n",
            "Epoch 306/1000, Avg Training Loss: 0.25243633872387206, Avg Validation Loss: 50.71440919205893\n",
            "Epoch 307/1000, Avg Training Loss: 0.2524399657263846, Avg Validation Loss: 50.72333405712641\n",
            "Epoch 308/1000, Avg Training Loss: 0.25240441935360447, Avg Validation Loss: 50.71946731529525\n",
            "Epoch 309/1000, Avg Training Loss: 0.2523889295002123, Avg Validation Loss: 50.71127228306097\n",
            "Epoch 310/1000, Avg Training Loss: 0.2523559765294478, Avg Validation Loss: 50.709412063432076\n",
            "Epoch 311/1000, Avg Training Loss: 0.2523730181356673, Avg Validation Loss: 50.71049345890805\n",
            "Epoch 312/1000, Avg Training Loss: 0.2523020943056249, Avg Validation Loss: 50.701351670319184\n",
            "Epoch 313/1000, Avg Training Loss: 0.2522538841666264, Avg Validation Loss: 50.70110474655603\n",
            "Epoch 314/1000, Avg Training Loss: 0.2522632511709379, Avg Validation Loss: 50.6990586695247\n",
            "Epoch 315/1000, Avg Training Loss: 0.2522088099475803, Avg Validation Loss: 50.701155227714196\n",
            "Epoch 316/1000, Avg Training Loss: 0.2521780851859981, Avg Validation Loss: 50.69336245499426\n",
            "Epoch 317/1000, Avg Training Loss: 0.25215850039378024, Avg Validation Loss: 50.684542622609484\n",
            "Epoch 318/1000, Avg Training Loss: 0.25215272656739385, Avg Validation Loss: 50.70069611608153\n",
            "Epoch 319/1000, Avg Training Loss: 0.25214411447345675, Avg Validation Loss: 50.677873094022694\n",
            "Epoch 320/1000, Avg Training Loss: 0.25210952835762407, Avg Validation Loss: 50.68472888015087\n",
            "Epoch 321/1000, Avg Training Loss: 0.2520922516276673, Avg Validation Loss: 50.69133469935555\n",
            "Epoch 322/1000, Avg Training Loss: 0.25206679754532596, Avg Validation Loss: 50.68683089075837\n",
            "Epoch 323/1000, Avg Training Loss: 0.2520674795271037, Avg Validation Loss: 50.684041299429865\n",
            "Epoch 324/1000, Avg Training Loss: 0.25200597030922045, Avg Validation Loss: 50.67114756463731\n",
            "Epoch 325/1000, Avg Training Loss: 0.2519810328742143, Avg Validation Loss: 50.65685324685856\n",
            "Epoch 326/1000, Avg Training Loss: 0.2519536364977638, Avg Validation Loss: 50.64928095222237\n",
            "Epoch 327/1000, Avg Training Loss: 0.2519114847470364, Avg Validation Loss: 50.65209767672641\n",
            "Epoch 328/1000, Avg Training Loss: 0.25189327194288724, Avg Validation Loss: 50.63788458197887\n",
            "Epoch 329/1000, Avg Training Loss: 0.25186251266162185, Avg Validation Loss: 50.64100553744684\n",
            "Epoch 330/1000, Avg Training Loss: 0.25185372862884453, Avg Validation Loss: 50.62628821006068\n",
            "Epoch 331/1000, Avg Training Loss: 0.2518446253028047, Avg Validation Loss: 50.648557568603195\n",
            "Epoch 332/1000, Avg Training Loss: 0.2518125936646374, Avg Validation Loss: 50.63966506826516\n",
            "Epoch 333/1000, Avg Training Loss: 0.25176588765817437, Avg Validation Loss: 50.625099232622645\n",
            "Epoch 334/1000, Avg Training Loss: 0.25177278434948713, Avg Validation Loss: 50.620116521787494\n",
            "Epoch 335/1000, Avg Training Loss: 0.25173188779217126, Avg Validation Loss: 50.60801040653378\n",
            "Epoch 336/1000, Avg Training Loss: 0.25174803084745745, Avg Validation Loss: 50.612280970481436\n",
            "Epoch 337/1000, Avg Training Loss: 0.2516850336892714, Avg Validation Loss: 50.59581468475794\n",
            "Epoch 338/1000, Avg Training Loss: 0.2516686530450911, Avg Validation Loss: 50.58676456158344\n",
            "Epoch 339/1000, Avg Training Loss: 0.2516535668345296, Avg Validation Loss: 50.57754193568883\n",
            "Epoch 340/1000, Avg Training Loss: 0.2516283499874908, Avg Validation Loss: 50.563742467964005\n",
            "Epoch 341/1000, Avg Training Loss: 0.25159374762415454, Avg Validation Loss: 50.549329528614436\n",
            "Epoch 342/1000, Avg Training Loss: 0.2516413907649746, Avg Validation Loss: 50.54319632268192\n",
            "Epoch 343/1000, Avg Training Loss: 0.2515727898661051, Avg Validation Loss: 50.52104382438093\n",
            "Epoch 344/1000, Avg Training Loss: 0.25152894747055216, Avg Validation Loss: 50.50713131852882\n",
            "Epoch 345/1000, Avg Training Loss: 0.25150794781028724, Avg Validation Loss: 50.513025331497474\n",
            "Epoch 346/1000, Avg Training Loss: 0.25149129056569675, Avg Validation Loss: 50.512230290612685\n",
            "Epoch 347/1000, Avg Training Loss: 0.25148614160983235, Avg Validation Loss: 50.51482288783305\n",
            "Epoch 348/1000, Avg Training Loss: 0.2514453962175709, Avg Validation Loss: 50.51676980741375\n",
            "Epoch 349/1000, Avg Training Loss: 0.25141466125845086, Avg Validation Loss: 50.50695928691162\n",
            "Epoch 350/1000, Avg Training Loss: 0.25141853465977554, Avg Validation Loss: 50.495912437671535\n",
            "Epoch 351/1000, Avg Training Loss: 0.25135981466973034, Avg Validation Loss: 50.48441444990473\n",
            "Epoch 352/1000, Avg Training Loss: 0.2513766794802655, Avg Validation Loss: 50.4720850220506\n",
            "Epoch 353/1000, Avg Training Loss: 0.2513383378593856, Avg Validation Loss: 50.461848479094726\n",
            "Epoch 354/1000, Avg Training Loss: 0.2513045836673741, Avg Validation Loss: 50.45215974221264\n",
            "Epoch 355/1000, Avg Training Loss: 0.2513409011874856, Avg Validation Loss: 50.45088079632333\n",
            "Epoch 356/1000, Avg Training Loss: 0.25132598280207424, Avg Validation Loss: 50.44185671242953\n",
            "Epoch 357/1000, Avg Training Loss: 0.25133131424164956, Avg Validation Loss: 50.44624925649134\n",
            "Epoch 358/1000, Avg Training Loss: 0.2513432138154077, Avg Validation Loss: 50.44149606579446\n",
            "Epoch 359/1000, Avg Training Loss: 0.2513184056353156, Avg Validation Loss: 50.44277454770736\n",
            "Epoch 360/1000, Avg Training Loss: 0.25127260071554, Avg Validation Loss: 50.43178932345728\n",
            "Epoch 361/1000, Avg Training Loss: 0.2512637233633351, Avg Validation Loss: 50.42664938526542\n",
            "Epoch 362/1000, Avg Training Loss: 0.2512273775639956, Avg Validation Loss: 50.41923927827682\n",
            "Epoch 363/1000, Avg Training Loss: 0.25118984145399703, Avg Validation Loss: 50.40616939422472\n",
            "Epoch 364/1000, Avg Training Loss: 0.2511943923418221, Avg Validation Loss: 50.403895616946485\n",
            "Epoch 365/1000, Avg Training Loss: 0.25113989526766767, Avg Validation Loss: 50.393873451111546\n",
            "Epoch 366/1000, Avg Training Loss: 0.25114093238347235, Avg Validation Loss: 50.38755060378383\n",
            "Epoch 367/1000, Avg Training Loss: 0.2511243090054339, Avg Validation Loss: 50.37308944136337\n",
            "Epoch 368/1000, Avg Training Loss: 0.25110606266739277, Avg Validation Loss: 50.36895282438889\n",
            "Epoch 369/1000, Avg Training Loss: 0.2510963248426805, Avg Validation Loss: 50.37547925624698\n",
            "Epoch 370/1000, Avg Training Loss: 0.25110487272115306, Avg Validation Loss: 50.36824138664457\n",
            "Epoch 371/1000, Avg Training Loss: 0.25111820112803485, Avg Validation Loss: 50.38487833952178\n",
            "Epoch 372/1000, Avg Training Loss: 0.25108281411881916, Avg Validation Loss: 50.378248660549666\n",
            "Epoch 373/1000, Avg Training Loss: 0.2511074266390012, Avg Validation Loss: 50.36804624529168\n",
            "Epoch 374/1000, Avg Training Loss: 0.25107578385511337, Avg Validation Loss: 50.35548002823594\n",
            "Epoch 375/1000, Avg Training Loss: 0.25107919439086596, Avg Validation Loss: 50.360323502326786\n",
            "Epoch 376/1000, Avg Training Loss: 0.251066668796725, Avg Validation Loss: 50.34485585426974\n",
            "Epoch 377/1000, Avg Training Loss: 0.25105224713003016, Avg Validation Loss: 50.348318447219796\n",
            "Epoch 378/1000, Avg Training Loss: 0.2510851374290318, Avg Validation Loss: 50.32573818000797\n",
            "Epoch 379/1000, Avg Training Loss: 0.2510384072452363, Avg Validation Loss: 50.32440868421271\n",
            "Epoch 380/1000, Avg Training Loss: 0.25107437395986093, Avg Validation Loss: 50.35107805921588\n",
            "Epoch 381/1000, Avg Training Loss: 0.25107387261358033, Avg Validation Loss: 50.35325922923075\n",
            "Epoch 382/1000, Avg Training Loss: 0.25105965197830704, Avg Validation Loss: 50.33666970266884\n",
            "Epoch 383/1000, Avg Training Loss: 0.2510608782319227, Avg Validation Loss: 50.31260865111817\n",
            "Epoch 384/1000, Avg Training Loss: 0.25103257826593434, Avg Validation Loss: 50.309188036092905\n",
            "Epoch 385/1000, Avg Training Loss: 0.2510581997278195, Avg Validation Loss: 50.3181174154581\n",
            "Epoch 386/1000, Avg Training Loss: 0.25101682087731375, Avg Validation Loss: 50.31337477322998\n",
            "Epoch 387/1000, Avg Training Loss: 0.2510304301700737, Avg Validation Loss: 50.31723162341547\n",
            "Epoch 388/1000, Avg Training Loss: 0.25101315291689874, Avg Validation Loss: 50.30433854301597\n",
            "Epoch 389/1000, Avg Training Loss: 0.25100684319097233, Avg Validation Loss: 50.28593172663527\n",
            "Epoch 390/1000, Avg Training Loss: 0.2510188195807267, Avg Validation Loss: 50.30497325619144\n",
            "Epoch 391/1000, Avg Training Loss: 0.25098158729678355, Avg Validation Loss: 50.29119973087362\n",
            "Epoch 392/1000, Avg Training Loss: 0.2510010449351626, Avg Validation Loss: 50.29320709814655\n",
            "Epoch 393/1000, Avg Training Loss: 0.25099765267565405, Avg Validation Loss: 50.307766946041454\n",
            "Epoch 394/1000, Avg Training Loss: 0.25098914065660144, Avg Validation Loss: 50.31646020095935\n",
            "Epoch 395/1000, Avg Training Loss: 0.2510094535938615, Avg Validation Loss: 50.31483436625582\n",
            "Epoch 396/1000, Avg Training Loss: 0.25101252455001327, Avg Validation Loss: 50.317667572447704\n",
            "Epoch 397/1000, Avg Training Loss: 0.2510354582736496, Avg Validation Loss: 50.32835481460185\n",
            "Epoch 398/1000, Avg Training Loss: 0.2510183686696119, Avg Validation Loss: 50.33618675630527\n",
            "Epoch 399/1000, Avg Training Loss: 0.2510359672897835, Avg Validation Loss: 50.33533034468647\n",
            "Epoch 400/1000, Avg Training Loss: 0.2510636173046262, Avg Validation Loss: 50.33934125039852\n",
            "Epoch 401/1000, Avg Training Loss: 0.2510897055039164, Avg Validation Loss: 50.34784644512642\n",
            "Epoch 402/1000, Avg Training Loss: 0.2511132146173043, Avg Validation Loss: 50.37223945676057\n",
            "Epoch 403/1000, Avg Training Loss: 0.2511368735096611, Avg Validation Loss: 50.39293503787866\n",
            "Epoch 404/1000, Avg Training Loss: 0.2511611306793275, Avg Validation Loss: 50.41108534181196\n",
            "Epoch 405/1000, Avg Training Loss: 0.25119087798925127, Avg Validation Loss: 50.44116174191946\n",
            "Epoch 406/1000, Avg Training Loss: 0.25122098500900547, Avg Validation Loss: 50.458357893236965\n",
            "Epoch 407/1000, Avg Training Loss: 0.25122046914294816, Avg Validation Loss: 50.476353636258985\n",
            "Epoch 408/1000, Avg Training Loss: 0.2512605496038628, Avg Validation Loss: 50.500825112895114\n",
            "Epoch 409/1000, Avg Training Loss: 0.2512850694576449, Avg Validation Loss: 50.514872964802535\n",
            "Epoch 410/1000, Avg Training Loss: 0.2513180623746677, Avg Validation Loss: 50.528836872914596\n",
            "Epoch 411/1000, Avg Training Loss: 0.2513271256556594, Avg Validation Loss: 50.53268052868763\n",
            "Epoch 412/1000, Avg Training Loss: 0.2513373539719087, Avg Validation Loss: 50.541011691770585\n",
            "Epoch 413/1000, Avg Training Loss: 0.2513783563803327, Avg Validation Loss: 50.548837988527204\n",
            "Epoch 414/1000, Avg Training Loss: 0.2513965350773553, Avg Validation Loss: 50.567390612706255\n",
            "Epoch 415/1000, Avg Training Loss: 0.25142416404855966, Avg Validation Loss: 50.58531842849864\n",
            "Epoch 416/1000, Avg Training Loss: 0.2514667659154151, Avg Validation Loss: 50.61043875009365\n",
            "Epoch 417/1000, Avg Training Loss: 0.2514959872851828, Avg Validation Loss: 50.63202045963044\n",
            "Epoch 418/1000, Avg Training Loss: 0.2515601629491693, Avg Validation Loss: 50.665185485957906\n",
            "Epoch 419/1000, Avg Training Loss: 0.2516188913548959, Avg Validation Loss: 50.69401146989438\n",
            "Epoch 420/1000, Avg Training Loss: 0.2516442634386997, Avg Validation Loss: 50.7108526820898\n",
            "Epoch 421/1000, Avg Training Loss: 0.25171418606024765, Avg Validation Loss: 50.729927982462925\n",
            "Epoch 422/1000, Avg Training Loss: 0.2517869032340005, Avg Validation Loss: 50.76183630658475\n",
            "Epoch 423/1000, Avg Training Loss: 0.2518145821754103, Avg Validation Loss: 50.79198556510189\n",
            "Epoch 424/1000, Avg Training Loss: 0.25191472823694894, Avg Validation Loss: 50.80388865387546\n",
            "Epoch 425/1000, Avg Training Loss: 0.25197238390097704, Avg Validation Loss: 50.845381304959545\n",
            "Epoch 426/1000, Avg Training Loss: 0.25208834965464516, Avg Validation Loss: 50.90027289490597\n",
            "Epoch 427/1000, Avg Training Loss: 0.25222176810336394, Avg Validation Loss: 50.950942458179696\n",
            "Epoch 428/1000, Avg Training Loss: 0.2523370135701231, Avg Validation Loss: 51.01664964556447\n",
            "Epoch 429/1000, Avg Training Loss: 0.2525029474504492, Avg Validation Loss: 51.06431927993435\n",
            "Epoch 430/1000, Avg Training Loss: 0.25263486075343716, Avg Validation Loss: 51.1400039358269\n",
            "Epoch 431/1000, Avg Training Loss: 0.2528590762180698, Avg Validation Loss: 51.235625629967096\n",
            "Epoch 432/1000, Avg Training Loss: 0.25297925051628817, Avg Validation Loss: 51.30614997611082\n",
            "Epoch 433/1000, Avg Training Loss: 0.2531456414061838, Avg Validation Loss: 51.3696302891606\n",
            "Epoch 434/1000, Avg Training Loss: 0.25334673993969914, Avg Validation Loss: 51.43909721596184\n",
            "Epoch 435/1000, Avg Training Loss: 0.25347860829357566, Avg Validation Loss: 51.50701875147875\n",
            "Epoch 436/1000, Avg Training Loss: 0.25365823391278447, Avg Validation Loss: 51.59589670128135\n",
            "Epoch 437/1000, Avg Training Loss: 0.2539076299732721, Avg Validation Loss: 51.709158164127075\n",
            "Epoch 438/1000, Avg Training Loss: 0.25399478011007065, Avg Validation Loss: 51.72948538592141\n",
            "Epoch 439/1000, Avg Training Loss: 0.25410734558294373, Avg Validation Loss: 51.7795563039131\n",
            "Epoch 440/1000, Avg Training Loss: 0.2541798192504524, Avg Validation Loss: 51.8256538458729\n",
            "Epoch 441/1000, Avg Training Loss: 0.2543020369643597, Avg Validation Loss: 51.90421046256378\n",
            "Epoch 442/1000, Avg Training Loss: 0.2544474988881258, Avg Validation Loss: 51.938465898151804\n",
            "Epoch 443/1000, Avg Training Loss: 0.254559541499112, Avg Validation Loss: 51.982941803751814\n",
            "Epoch 444/1000, Avg Training Loss: 0.2546902974962651, Avg Validation Loss: 52.03726048824994\n",
            "Epoch 445/1000, Avg Training Loss: 0.2547317086079604, Avg Validation Loss: 52.04436041431643\n",
            "Epoch 446/1000, Avg Training Loss: 0.25478921491765244, Avg Validation Loss: 52.075216126991485\n",
            "Epoch 447/1000, Avg Training Loss: 0.25486111047661525, Avg Validation Loss: 52.109980441541346\n",
            "Epoch 448/1000, Avg Training Loss: 0.2549076763048576, Avg Validation Loss: 52.108873265789015\n",
            "Epoch 449/1000, Avg Training Loss: 0.2549461567413742, Avg Validation Loss: 52.09340992004438\n",
            "Epoch 450/1000, Avg Training Loss: 0.25496680591921994, Avg Validation Loss: 52.1136404500012\n",
            "Epoch 451/1000, Avg Training Loss: 0.2550301034496842, Avg Validation Loss: 52.14441681610415\n",
            "Epoch 452/1000, Avg Training Loss: 0.25508273136553816, Avg Validation Loss: 52.149604047611774\n",
            "Epoch 453/1000, Avg Training Loss: 0.25509241821406314, Avg Validation Loss: 52.136072803008915\n",
            "Epoch 454/1000, Avg Training Loss: 0.2550879066471752, Avg Validation Loss: 52.12356815366806\n",
            "Epoch 455/1000, Avg Training Loss: 0.2551465596876794, Avg Validation Loss: 52.115820021289565\n",
            "Epoch 456/1000, Avg Training Loss: 0.2551881673006106, Avg Validation Loss: 52.12746302424526\n",
            "Epoch 457/1000, Avg Training Loss: 0.2552216679526537, Avg Validation Loss: 52.1405836942251\n",
            "Epoch 458/1000, Avg Training Loss: 0.25527331503115513, Avg Validation Loss: 52.134363363498046\n",
            "Epoch 459/1000, Avg Training Loss: 0.2552805003493701, Avg Validation Loss: 52.12467162003077\n",
            "Epoch 460/1000, Avg Training Loss: 0.25533676143734474, Avg Validation Loss: 52.11105179282212\n",
            "Epoch 461/1000, Avg Training Loss: 0.25534776165105044, Avg Validation Loss: 52.12386819323951\n",
            "Epoch 462/1000, Avg Training Loss: 0.2554448415446962, Avg Validation Loss: 52.190611467013994\n",
            "Epoch 463/1000, Avg Training Loss: 0.25546601252756135, Avg Validation Loss: 52.16118562801445\n",
            "Epoch 464/1000, Avg Training Loss: 0.25545121414721966, Avg Validation Loss: 52.17186418357467\n",
            "Epoch 465/1000, Avg Training Loss: 0.2555219411490986, Avg Validation Loss: 52.185148637472096\n",
            "Epoch 466/1000, Avg Training Loss: 0.2555176473639431, Avg Validation Loss: 52.18086080818194\n",
            "Epoch 467/1000, Avg Training Loss: 0.2555420596308008, Avg Validation Loss: 52.19087816579261\n",
            "Epoch 468/1000, Avg Training Loss: 0.25557818474057176, Avg Validation Loss: 52.19081679590327\n",
            "Epoch 469/1000, Avg Training Loss: 0.255552372586194, Avg Validation Loss: 52.1682104858005\n",
            "Epoch 470/1000, Avg Training Loss: 0.2555356196803298, Avg Validation Loss: 52.16605357988465\n",
            "Epoch 471/1000, Avg Training Loss: 0.25560002811405946, Avg Validation Loss: 52.15526039115641\n",
            "Epoch 472/1000, Avg Training Loss: 0.25552844461739765, Avg Validation Loss: 52.129960396987045\n",
            "Epoch 473/1000, Avg Training Loss: 0.2554734923352391, Avg Validation Loss: 52.11605196728662\n",
            "Epoch 474/1000, Avg Training Loss: 0.25544428850215756, Avg Validation Loss: 52.09225470259352\n",
            "Epoch 475/1000, Avg Training Loss: 0.2553807853966978, Avg Validation Loss: 52.08294829658196\n",
            "Epoch 476/1000, Avg Training Loss: 0.25533930529786225, Avg Validation Loss: 52.0704742542497\n",
            "Epoch 477/1000, Avg Training Loss: 0.2553244278200383, Avg Validation Loss: 52.05803999153676\n",
            "Epoch 478/1000, Avg Training Loss: 0.25531386638254366, Avg Validation Loss: 52.014578992496624\n",
            "Epoch 479/1000, Avg Training Loss: 0.25524407874282473, Avg Validation Loss: 52.00486326641358\n",
            "Epoch 480/1000, Avg Training Loss: 0.255279955257555, Avg Validation Loss: 52.02749155170532\n",
            "Epoch 481/1000, Avg Training Loss: 0.25524463913560735, Avg Validation Loss: 52.02558689146084\n",
            "Epoch 482/1000, Avg Training Loss: 0.2552767907838833, Avg Validation Loss: 52.03425204526906\n",
            "Epoch 483/1000, Avg Training Loss: 0.25527379984503784, Avg Validation Loss: 52.022143182924566\n",
            "Epoch 484/1000, Avg Training Loss: 0.25527981275039613, Avg Validation Loss: 52.02446242170535\n",
            "Epoch 485/1000, Avg Training Loss: 0.2552608492366638, Avg Validation Loss: 52.00958038096163\n",
            "Epoch 486/1000, Avg Training Loss: 0.25522714999631807, Avg Validation Loss: 52.009342291338115\n",
            "Epoch 487/1000, Avg Training Loss: 0.2552212646013849, Avg Validation Loss: 51.99700368241378\n",
            "Epoch 488/1000, Avg Training Loss: 0.2551952276305969, Avg Validation Loss: 52.00386018381712\n",
            "Epoch 489/1000, Avg Training Loss: 0.2551946980744038, Avg Validation Loss: 52.019670208327035\n",
            "Epoch 490/1000, Avg Training Loss: 0.25517965930792186, Avg Validation Loss: 52.03008219305006\n",
            "Epoch 491/1000, Avg Training Loss: 0.2551327919950373, Avg Validation Loss: 51.998817642657066\n",
            "Epoch 492/1000, Avg Training Loss: 0.2551075283724192, Avg Validation Loss: 51.96435488599286\n",
            "Epoch 493/1000, Avg Training Loss: 0.25503241401752635, Avg Validation Loss: 51.95009431112523\n",
            "Epoch 494/1000, Avg Training Loss: 0.25498483927673543, Avg Validation Loss: 51.92949633841835\n",
            "Epoch 495/1000, Avg Training Loss: 0.2549372242633293, Avg Validation Loss: 51.92833688512504\n",
            "Epoch 496/1000, Avg Training Loss: 0.25489147143699153, Avg Validation Loss: 51.90639453686923\n",
            "Epoch 497/1000, Avg Training Loss: 0.2548330605524097, Avg Validation Loss: 51.87428645937129\n",
            "Epoch 498/1000, Avg Training Loss: 0.2547471983608409, Avg Validation Loss: 51.85981237338831\n",
            "Epoch 499/1000, Avg Training Loss: 0.2546695849718634, Avg Validation Loss: 51.825113915169354\n",
            "Epoch 500/1000, Avg Training Loss: 0.25461294153585157, Avg Validation Loss: 51.80857680625182\n",
            "Epoch 501/1000, Avg Training Loss: 0.2546185358261283, Avg Validation Loss: 51.7920980460774\n",
            "Epoch 502/1000, Avg Training Loss: 0.2545992524119387, Avg Validation Loss: 51.760417961257275\n",
            "Epoch 503/1000, Avg Training Loss: 0.2545133232859508, Avg Validation Loss: 51.74046139010457\n",
            "Epoch 504/1000, Avg Training Loss: 0.2544943476173913, Avg Validation Loss: 51.758612009269626\n",
            "Epoch 505/1000, Avg Training Loss: 0.25444000335068667, Avg Validation Loss: 51.734923129268466\n",
            "Epoch 506/1000, Avg Training Loss: 0.2543846727447994, Avg Validation Loss: 51.713370286105096\n",
            "Epoch 507/1000, Avg Training Loss: 0.25438345605795143, Avg Validation Loss: 51.72403764090073\n",
            "Epoch 508/1000, Avg Training Loss: 0.2543365465159298, Avg Validation Loss: 51.670969584961775\n",
            "Epoch 509/1000, Avg Training Loss: 0.2543158628439463, Avg Validation Loss: 51.65837767127313\n",
            "Epoch 510/1000, Avg Training Loss: 0.2542858424844832, Avg Validation Loss: 51.64411389140142\n",
            "Epoch 511/1000, Avg Training Loss: 0.2542135810288595, Avg Validation Loss: 51.671838964242625\n",
            "Epoch 512/1000, Avg Training Loss: 0.25419750261105634, Avg Validation Loss: 51.67040138504146\n",
            "Epoch 513/1000, Avg Training Loss: 0.2542476115390499, Avg Validation Loss: 51.694820950772495\n",
            "Epoch 514/1000, Avg Training Loss: 0.2541237793425092, Avg Validation Loss: 51.65728242137939\n",
            "Epoch 515/1000, Avg Training Loss: 0.2540683020824367, Avg Validation Loss: 51.61964639081691\n",
            "Epoch 516/1000, Avg Training Loss: 0.25404289264579605, Avg Validation Loss: 51.625827246562686\n",
            "Epoch 517/1000, Avg Training Loss: 0.25395216309547775, Avg Validation Loss: 51.59322395713163\n",
            "Epoch 518/1000, Avg Training Loss: 0.253977507134631, Avg Validation Loss: 51.57115406371402\n",
            "Epoch 519/1000, Avg Training Loss: 0.2539233740604556, Avg Validation Loss: 51.58411553830309\n",
            "Epoch 520/1000, Avg Training Loss: 0.25384756307218553, Avg Validation Loss: 51.54815367759528\n",
            "Epoch 521/1000, Avg Training Loss: 0.25376399088364093, Avg Validation Loss: 51.50999910439066\n",
            "Epoch 522/1000, Avg Training Loss: 0.2536712716114705, Avg Validation Loss: 51.493306414817695\n",
            "Epoch 523/1000, Avg Training Loss: 0.2536234145866615, Avg Validation Loss: 51.472231244947295\n",
            "Epoch 524/1000, Avg Training Loss: 0.25358922352663327, Avg Validation Loss: 51.46692714669962\n",
            "Epoch 525/1000, Avg Training Loss: 0.25356414349775236, Avg Validation Loss: 51.46747103884975\n",
            "Epoch 526/1000, Avg Training Loss: 0.25347997695986796, Avg Validation Loss: 51.457606675137306\n",
            "Epoch 527/1000, Avg Training Loss: 0.25342412988895124, Avg Validation Loss: 51.456206612098484\n",
            "Epoch 528/1000, Avg Training Loss: 0.2533909767225936, Avg Validation Loss: 51.41790949739622\n",
            "Epoch 529/1000, Avg Training Loss: 0.253317686586409, Avg Validation Loss: 51.38764154335574\n",
            "Epoch 530/1000, Avg Training Loss: 0.2533077746709941, Avg Validation Loss: 51.39262235760856\n",
            "Epoch 531/1000, Avg Training Loss: 0.25328476683035644, Avg Validation Loss: 51.40889564643857\n",
            "Epoch 532/1000, Avg Training Loss: 0.2532256045926745, Avg Validation Loss: 51.374429169804635\n",
            "Epoch 533/1000, Avg Training Loss: 0.2531428740246459, Avg Validation Loss: 51.335883243645654\n",
            "Epoch 534/1000, Avg Training Loss: 0.2531388521132661, Avg Validation Loss: 51.33912755767935\n",
            "Epoch 535/1000, Avg Training Loss: 0.2530260748044178, Avg Validation Loss: 51.29783190102906\n",
            "Epoch 536/1000, Avg Training Loss: 0.25296510526453586, Avg Validation Loss: 51.27635512861579\n",
            "Epoch 537/1000, Avg Training Loss: 0.25293990817745315, Avg Validation Loss: 51.255143711813176\n",
            "Epoch 538/1000, Avg Training Loss: 0.25288872147537655, Avg Validation Loss: 51.26337573590265\n",
            "Epoch 539/1000, Avg Training Loss: 0.2528301755124902, Avg Validation Loss: 51.25811472729177\n",
            "Epoch 540/1000, Avg Training Loss: 0.25275985741210827, Avg Validation Loss: 51.2566262275689\n",
            "Epoch 541/1000, Avg Training Loss: 0.2527344199531873, Avg Validation Loss: 51.231693019002634\n",
            "Epoch 542/1000, Avg Training Loss: 0.25268715375002304, Avg Validation Loss: 51.238631576698076\n",
            "Epoch 543/1000, Avg Training Loss: 0.2526343631941833, Avg Validation Loss: 51.212063751835984\n",
            "Epoch 544/1000, Avg Training Loss: 0.25258909471831686, Avg Validation Loss: 51.19092509111083\n",
            "Epoch 545/1000, Avg Training Loss: 0.252505293469339, Avg Validation Loss: 51.168419244446866\n",
            "Epoch 546/1000, Avg Training Loss: 0.25249937614939044, Avg Validation Loss: 51.11709485361207\n",
            "Epoch 547/1000, Avg Training Loss: 0.25242224979593686, Avg Validation Loss: 51.128783532959986\n",
            "Epoch 548/1000, Avg Training Loss: 0.2523656437765377, Avg Validation Loss: 51.125992985481915\n",
            "Epoch 549/1000, Avg Training Loss: 0.2523137743017406, Avg Validation Loss: 51.08561062050244\n",
            "Epoch 550/1000, Avg Training Loss: 0.2522142939516512, Avg Validation Loss: 51.06516345372851\n",
            "Epoch 551/1000, Avg Training Loss: 0.25216484239767284, Avg Validation Loss: 51.05073388769971\n",
            "Epoch 552/1000, Avg Training Loss: 0.2521235823371519, Avg Validation Loss: 51.06860834184535\n",
            "Epoch 553/1000, Avg Training Loss: 0.2520861118941075, Avg Validation Loss: 51.053684476119955\n",
            "Epoch 554/1000, Avg Training Loss: 0.2520818795074378, Avg Validation Loss: 51.016792710226156\n",
            "Epoch 555/1000, Avg Training Loss: 0.2519695456245249, Avg Validation Loss: 50.99983610837737\n",
            "Epoch 556/1000, Avg Training Loss: 0.2519038734990635, Avg Validation Loss: 50.99280802069967\n",
            "Epoch 557/1000, Avg Training Loss: 0.25183415045690377, Avg Validation Loss: 50.959760257647325\n",
            "Epoch 558/1000, Avg Training Loss: 0.25180320746936263, Avg Validation Loss: 50.956474966164954\n",
            "Epoch 559/1000, Avg Training Loss: 0.2517334446257282, Avg Validation Loss: 50.93563387075504\n",
            "Epoch 560/1000, Avg Training Loss: 0.2516827324389482, Avg Validation Loss: 50.895383965464305\n",
            "Epoch 561/1000, Avg Training Loss: 0.2516112177678858, Avg Validation Loss: 50.84974221734219\n",
            "Epoch 562/1000, Avg Training Loss: 0.251540372962816, Avg Validation Loss: 50.83101062179011\n",
            "Epoch 563/1000, Avg Training Loss: 0.2514810959284386, Avg Validation Loss: 50.82930256729708\n",
            "Epoch 564/1000, Avg Training Loss: 0.25142475692957, Avg Validation Loss: 50.82234822408303\n",
            "Epoch 565/1000, Avg Training Loss: 0.2513654548263795, Avg Validation Loss: 50.80800718418854\n",
            "Epoch 566/1000, Avg Training Loss: 0.25138443440803165, Avg Validation Loss: 50.7593530987111\n",
            "Epoch 567/1000, Avg Training Loss: 0.2512916975464524, Avg Validation Loss: 50.7793910984647\n",
            "Epoch 568/1000, Avg Training Loss: 0.2512177690706589, Avg Validation Loss: 50.75334524197177\n",
            "Epoch 569/1000, Avg Training Loss: 0.25118991581633165, Avg Validation Loss: 50.76566050177834\n",
            "Epoch 570/1000, Avg Training Loss: 0.25110067473521147, Avg Validation Loss: 50.725332353126134\n",
            "Epoch 571/1000, Avg Training Loss: 0.2510617230472144, Avg Validation Loss: 50.676705241894275\n",
            "Epoch 572/1000, Avg Training Loss: 0.2510327210562583, Avg Validation Loss: 50.69116956530711\n",
            "Epoch 573/1000, Avg Training Loss: 0.25101497540883233, Avg Validation Loss: 50.64078903524177\n",
            "Epoch 574/1000, Avg Training Loss: 0.2509233013578875, Avg Validation Loss: 50.63357886733925\n",
            "Epoch 575/1000, Avg Training Loss: 0.2508923617194657, Avg Validation Loss: 50.61618096497451\n",
            "Epoch 576/1000, Avg Training Loss: 0.2508131204784435, Avg Validation Loss: 50.5954180289871\n",
            "Epoch 577/1000, Avg Training Loss: 0.2507558970259332, Avg Validation Loss: 50.59367326934989\n",
            "Epoch 578/1000, Avg Training Loss: 0.25078848090722483, Avg Validation Loss: 50.549622387784524\n",
            "Epoch 579/1000, Avg Training Loss: 0.25068101270061743, Avg Validation Loss: 50.55508910008045\n",
            "Epoch 580/1000, Avg Training Loss: 0.2506818240752497, Avg Validation Loss: 50.58512198953603\n",
            "Epoch 581/1000, Avg Training Loss: 0.25059310661606954, Avg Validation Loss: 50.53864761722983\n",
            "Epoch 582/1000, Avg Training Loss: 0.2505870190393709, Avg Validation Loss: 50.49253927471697\n",
            "Epoch 583/1000, Avg Training Loss: 0.2504981952783585, Avg Validation Loss: 50.513819474681284\n",
            "Epoch 584/1000, Avg Training Loss: 0.25044872273684143, Avg Validation Loss: 50.50791614201172\n",
            "Epoch 585/1000, Avg Training Loss: 0.25040565446862517, Avg Validation Loss: 50.46209054948123\n",
            "Epoch 586/1000, Avg Training Loss: 0.25040190145817043, Avg Validation Loss: 50.456898879720846\n",
            "Epoch 587/1000, Avg Training Loss: 0.25035972073841833, Avg Validation Loss: 50.40030561616157\n",
            "Epoch 588/1000, Avg Training Loss: 0.2502446685595492, Avg Validation Loss: 50.41357723093259\n",
            "Epoch 589/1000, Avg Training Loss: 0.25023608083502075, Avg Validation Loss: 50.4201266378935\n",
            "Epoch 590/1000, Avg Training Loss: 0.25020740266512503, Avg Validation Loss: 50.40336743787756\n",
            "Epoch 591/1000, Avg Training Loss: 0.2501405465483487, Avg Validation Loss: 50.39223230169628\n",
            "Epoch 592/1000, Avg Training Loss: 0.25013413180540567, Avg Validation Loss: 50.36672225845405\n",
            "Epoch 593/1000, Avg Training Loss: 0.25005483022234776, Avg Validation Loss: 50.35452805906744\n",
            "Epoch 594/1000, Avg Training Loss: 0.24999430025762825, Avg Validation Loss: 50.308343851762004\n",
            "Epoch 595/1000, Avg Training Loss: 0.24996589980326506, Avg Validation Loss: 50.30016345870141\n",
            "Epoch 596/1000, Avg Training Loss: 0.24993057250664294, Avg Validation Loss: 50.27892817067911\n",
            "Epoch 597/1000, Avg Training Loss: 0.24986702950286804, Avg Validation Loss: 50.27582274500199\n",
            "Epoch 598/1000, Avg Training Loss: 0.24983868286529806, Avg Validation Loss: 50.279421161709806\n",
            "Epoch 599/1000, Avg Training Loss: 0.2498235159945083, Avg Validation Loss: 50.24319763426063\n",
            "Epoch 600/1000, Avg Training Loss: 0.24978388443081737, Avg Validation Loss: 50.23532062988353\n",
            "Epoch 601/1000, Avg Training Loss: 0.24971796550827718, Avg Validation Loss: 50.22185809083619\n",
            "Epoch 602/1000, Avg Training Loss: 0.24968444919556926, Avg Validation Loss: 50.173092664336146\n",
            "Epoch 603/1000, Avg Training Loss: 0.24962684985413053, Avg Validation Loss: 50.189564240777976\n",
            "Epoch 604/1000, Avg Training Loss: 0.24956094150742858, Avg Validation Loss: 50.16766567624587\n",
            "Epoch 605/1000, Avg Training Loss: 0.24952589055990299, Avg Validation Loss: 50.13083362223429\n",
            "Epoch 606/1000, Avg Training Loss: 0.24947136457481844, Avg Validation Loss: 50.11889813416531\n",
            "Epoch 607/1000, Avg Training Loss: 0.24949380830353746, Avg Validation Loss: 50.08434114782361\n",
            "Epoch 608/1000, Avg Training Loss: 0.24944230470419387, Avg Validation Loss: 50.069610278178615\n",
            "Epoch 609/1000, Avg Training Loss: 0.24936385829558744, Avg Validation Loss: 50.060559803659366\n",
            "Epoch 610/1000, Avg Training Loss: 0.24937656284007076, Avg Validation Loss: 50.1070409412587\n",
            "Epoch 611/1000, Avg Training Loss: 0.24932242159170886, Avg Validation Loss: 50.05079652062099\n",
            "Epoch 612/1000, Avg Training Loss: 0.24933273862162367, Avg Validation Loss: 50.03133960986293\n",
            "Epoch 613/1000, Avg Training Loss: 0.24921705848819575, Avg Validation Loss: 50.0401241285687\n",
            "Epoch 614/1000, Avg Training Loss: 0.24917674384161986, Avg Validation Loss: 50.01909070022069\n",
            "Epoch 615/1000, Avg Training Loss: 0.24914671947135517, Avg Validation Loss: 50.01209337341564\n",
            "Epoch 616/1000, Avg Training Loss: 0.2490953127100176, Avg Validation Loss: 49.96447997705177\n",
            "Epoch 617/1000, Avg Training Loss: 0.24905973173069168, Avg Validation Loss: 49.94302256484795\n",
            "Epoch 618/1000, Avg Training Loss: 0.24902932781572354, Avg Validation Loss: 49.9367901471176\n",
            "Epoch 619/1000, Avg Training Loss: 0.24903037770336922, Avg Validation Loss: 49.96410258996123\n",
            "Epoch 620/1000, Avg Training Loss: 0.2489338098439225, Avg Validation Loss: 49.89487112920354\n",
            "Epoch 621/1000, Avg Training Loss: 0.24889146821121078, Avg Validation Loss: 49.88752178417748\n",
            "Epoch 622/1000, Avg Training Loss: 0.2488499518548578, Avg Validation Loss: 49.85552054402368\n",
            "Epoch 623/1000, Avg Training Loss: 0.24881651610480918, Avg Validation Loss: 49.84185516178802\n",
            "Epoch 624/1000, Avg Training Loss: 0.24881963387070263, Avg Validation Loss: 49.823573139973696\n",
            "Epoch 625/1000, Avg Training Loss: 0.24885014473561562, Avg Validation Loss: 49.841831968060724\n",
            "Epoch 626/1000, Avg Training Loss: 0.24876154071809423, Avg Validation Loss: 49.811247964531915\n",
            "Epoch 627/1000, Avg Training Loss: 0.24866556531473766, Avg Validation Loss: 49.77123459588864\n",
            "Epoch 628/1000, Avg Training Loss: 0.248640515624263, Avg Validation Loss: 49.73995326181078\n",
            "Epoch 629/1000, Avg Training Loss: 0.2485936647505647, Avg Validation Loss: 49.76036066549209\n",
            "Epoch 630/1000, Avg Training Loss: 0.2485705262842621, Avg Validation Loss: 49.73638927726226\n",
            "Epoch 631/1000, Avg Training Loss: 0.2485479666247122, Avg Validation Loss: 49.70763455338094\n",
            "Epoch 632/1000, Avg Training Loss: 0.2484802902538577, Avg Validation Loss: 49.69823118605812\n",
            "Epoch 633/1000, Avg Training Loss: 0.24848171092418267, Avg Validation Loss: 49.69938178764245\n",
            "Epoch 634/1000, Avg Training Loss: 0.24840122142540697, Avg Validation Loss: 49.68231741824188\n",
            "Epoch 635/1000, Avg Training Loss: 0.24839667900669887, Avg Validation Loss: 49.65774421800221\n",
            "Epoch 636/1000, Avg Training Loss: 0.248333960350994, Avg Validation Loss: 49.65015899823017\n",
            "Epoch 637/1000, Avg Training Loss: 0.24836058812164244, Avg Validation Loss: 49.629245715136584\n",
            "Epoch 638/1000, Avg Training Loss: 0.24826220631612148, Avg Validation Loss: 49.59061898496854\n",
            "Epoch 639/1000, Avg Training Loss: 0.24823775806905593, Avg Validation Loss: 49.594323982495\n",
            "Epoch 640/1000, Avg Training Loss: 0.24822352879699344, Avg Validation Loss: 49.58966304230026\n",
            "Epoch 641/1000, Avg Training Loss: 0.24819313577235158, Avg Validation Loss: 49.5517002028858\n",
            "Epoch 642/1000, Avg Training Loss: 0.24810583223169588, Avg Validation Loss: 49.54741326551755\n",
            "Epoch 643/1000, Avg Training Loss: 0.2481574205389318, Avg Validation Loss: 49.56263204821089\n",
            "Epoch 644/1000, Avg Training Loss: 0.2481013154994513, Avg Validation Loss: 49.51520758185263\n",
            "Epoch 645/1000, Avg Training Loss: 0.24803225271104332, Avg Validation Loss: 49.518621468060275\n",
            "Epoch 646/1000, Avg Training Loss: 0.24799400260289928, Avg Validation Loss: 49.457836559430355\n",
            "Epoch 647/1000, Avg Training Loss: 0.24795464465275338, Avg Validation Loss: 49.446441770853376\n",
            "Epoch 648/1000, Avg Training Loss: 0.24791410962329435, Avg Validation Loss: 49.4281964864352\n",
            "Epoch 649/1000, Avg Training Loss: 0.2479028681481106, Avg Validation Loss: 49.404389802076246\n",
            "Epoch 650/1000, Avg Training Loss: 0.24789092199211116, Avg Validation Loss: 49.44019968354833\n",
            "Epoch 651/1000, Avg Training Loss: 0.24786683942809645, Avg Validation Loss: 49.38409441827504\n",
            "Epoch 652/1000, Avg Training Loss: 0.24778637614970309, Avg Validation Loss: 49.37035628796043\n",
            "Epoch 653/1000, Avg Training Loss: 0.24773679627139988, Avg Validation Loss: 49.378766354811134\n",
            "Epoch 654/1000, Avg Training Loss: 0.24774748832292842, Avg Validation Loss: 49.392696936814545\n",
            "Epoch 655/1000, Avg Training Loss: 0.24770862557272644, Avg Validation Loss: 49.35816606036962\n",
            "Epoch 656/1000, Avg Training Loss: 0.24767242658264849, Avg Validation Loss: 49.34866820549143\n",
            "Epoch 657/1000, Avg Training Loss: 0.24768125515235795, Avg Validation Loss: 49.31684007284949\n",
            "Epoch 658/1000, Avg Training Loss: 0.24764242921564975, Avg Validation Loss: 49.28544042842025\n",
            "Epoch 659/1000, Avg Training Loss: 0.24761001900555699, Avg Validation Loss: 49.288150152692324\n",
            "Epoch 660/1000, Avg Training Loss: 0.24752746511985013, Avg Validation Loss: 49.27903627077033\n",
            "Epoch 661/1000, Avg Training Loss: 0.2475562714868361, Avg Validation Loss: 49.28128234601688\n",
            "Epoch 662/1000, Avg Training Loss: 0.24749863648103773, Avg Validation Loss: 49.23339745458544\n",
            "Epoch 663/1000, Avg Training Loss: 0.24758088907384723, Avg Validation Loss: 49.231011207180536\n",
            "Epoch 664/1000, Avg Training Loss: 0.24747752554144462, Avg Validation Loss: 49.22092584855161\n",
            "Epoch 665/1000, Avg Training Loss: 0.2474245825499329, Avg Validation Loss: 49.211540051296204\n",
            "Epoch 666/1000, Avg Training Loss: 0.24743659631353376, Avg Validation Loss: 49.1875723330211\n",
            "Epoch 667/1000, Avg Training Loss: 0.24746351040849948, Avg Validation Loss: 49.168355333674285\n",
            "Epoch 668/1000, Avg Training Loss: 0.2473614192697663, Avg Validation Loss: 49.188289752153885\n",
            "Epoch 669/1000, Avg Training Loss: 0.24731855851433557, Avg Validation Loss: 49.17542180656762\n",
            "Epoch 670/1000, Avg Training Loss: 0.24732045934607946, Avg Validation Loss: 49.14835562316263\n",
            "Epoch 671/1000, Avg Training Loss: 0.24726606663541884, Avg Validation Loss: 49.13841802842632\n",
            "Epoch 672/1000, Avg Training Loss: 0.24724144761442154, Avg Validation Loss: 49.121908857162964\n",
            "Epoch 673/1000, Avg Training Loss: 0.2473066910032409, Avg Validation Loss: 49.138025764082315\n",
            "Epoch 674/1000, Avg Training Loss: 0.24722363973970243, Avg Validation Loss: 49.13455882709718\n",
            "Epoch 675/1000, Avg Training Loss: 0.24719060595749467, Avg Validation Loss: 49.08494813196645\n",
            "Epoch 676/1000, Avg Training Loss: 0.24723285541313725, Avg Validation Loss: 49.05272037543551\n",
            "Epoch 677/1000, Avg Training Loss: 0.24714414269153687, Avg Validation Loss: 49.030279813638735\n",
            "Epoch 678/1000, Avg Training Loss: 0.24711456926296235, Avg Validation Loss: 49.04885469443572\n",
            "Epoch 679/1000, Avg Training Loss: 0.24707215791025042, Avg Validation Loss: 49.040946050865855\n",
            "Epoch 680/1000, Avg Training Loss: 0.24707298248884957, Avg Validation Loss: 49.05384704949177\n",
            "Epoch 681/1000, Avg Training Loss: 0.24708958938251321, Avg Validation Loss: 49.03562551778909\n",
            "Epoch 682/1000, Avg Training Loss: 0.2470581919756981, Avg Validation Loss: 49.021800462660494\n",
            "Epoch 683/1000, Avg Training Loss: 0.24709616368793008, Avg Validation Loss: 49.01063163136169\n",
            "Epoch 684/1000, Avg Training Loss: 0.24699994603380226, Avg Validation Loss: 48.98689733754611\n",
            "Epoch 685/1000, Avg Training Loss: 0.24697999188206793, Avg Validation Loss: 48.96580272001907\n",
            "Epoch 686/1000, Avg Training Loss: 0.24693985671003424, Avg Validation Loss: 48.95673407777745\n",
            "Epoch 687/1000, Avg Training Loss: 0.2469443338888215, Avg Validation Loss: 48.9626996014562\n",
            "Epoch 688/1000, Avg Training Loss: 0.24688909070932882, Avg Validation Loss: 48.96036349275535\n",
            "Epoch 689/1000, Avg Training Loss: 0.24685957288804652, Avg Validation Loss: 48.95884256236014\n",
            "Epoch 690/1000, Avg Training Loss: 0.24690210918764702, Avg Validation Loss: 48.9174669218997\n",
            "Epoch 691/1000, Avg Training Loss: 0.24686359436314353, Avg Validation Loss: 48.916266423997946\n",
            "Epoch 692/1000, Avg Training Loss: 0.2468234277166549, Avg Validation Loss: 48.921162839532435\n",
            "Epoch 693/1000, Avg Training Loss: 0.24682592758795607, Avg Validation Loss: 48.90948829713406\n",
            "Epoch 694/1000, Avg Training Loss: 0.24679684197753285, Avg Validation Loss: 48.90403737326585\n",
            "Epoch 695/1000, Avg Training Loss: 0.24682131255626827, Avg Validation Loss: 48.91169617167412\n",
            "Epoch 696/1000, Avg Training Loss: 0.24679221266229615, Avg Validation Loss: 48.87377518545122\n",
            "Epoch 697/1000, Avg Training Loss: 0.24678864783000642, Avg Validation Loss: 48.84292543376246\n",
            "Epoch 698/1000, Avg Training Loss: 0.2467688143923035, Avg Validation Loss: 48.88678397366209\n",
            "Epoch 699/1000, Avg Training Loss: 0.24680003338587825, Avg Validation Loss: 48.918529077610216\n",
            "Epoch 700/1000, Avg Training Loss: 0.24680801550233297, Avg Validation Loss: 48.849317052943135\n",
            "Epoch 701/1000, Avg Training Loss: 0.24671408240001416, Avg Validation Loss: 48.84811588445663\n",
            "Epoch 702/1000, Avg Training Loss: 0.24666820851607268, Avg Validation Loss: 48.84174128895761\n",
            "Epoch 703/1000, Avg Training Loss: 0.24670087253786513, Avg Validation Loss: 48.86730467982184\n",
            "Epoch 704/1000, Avg Training Loss: 0.24673450144885195, Avg Validation Loss: 48.879892272241705\n",
            "Epoch 705/1000, Avg Training Loss: 0.2466758565389436, Avg Validation Loss: 48.86347739142137\n",
            "Epoch 706/1000, Avg Training Loss: 0.2466553742719492, Avg Validation Loss: 48.80341388437738\n",
            "Epoch 707/1000, Avg Training Loss: 0.2466343461633219, Avg Validation Loss: 48.83102904478524\n",
            "Epoch 708/1000, Avg Training Loss: 0.24663123967126413, Avg Validation Loss: 48.80131522542991\n",
            "Epoch 709/1000, Avg Training Loss: 0.24661745662863443, Avg Validation Loss: 48.80126350723492\n",
            "Epoch 710/1000, Avg Training Loss: 0.24659599889239445, Avg Validation Loss: 48.807552921901205\n",
            "Epoch 711/1000, Avg Training Loss: 0.2466100111935353, Avg Validation Loss: 48.823763330975325\n",
            "Epoch 712/1000, Avg Training Loss: 0.2466163800643408, Avg Validation Loss: 48.81968161298408\n",
            "Epoch 713/1000, Avg Training Loss: 0.24660902583653563, Avg Validation Loss: 48.79330043806861\n",
            "Epoch 714/1000, Avg Training Loss: 0.24658988839860463, Avg Validation Loss: 48.781847490669755\n",
            "Epoch 715/1000, Avg Training Loss: 0.24660326579727682, Avg Validation Loss: 48.80945916258703\n",
            "Epoch 716/1000, Avg Training Loss: 0.24660138297142417, Avg Validation Loss: 48.76823999498373\n",
            "Epoch 717/1000, Avg Training Loss: 0.24661979640250717, Avg Validation Loss: 48.788284492654746\n",
            "Epoch 718/1000, Avg Training Loss: 0.2465605020059925, Avg Validation Loss: 48.78313603712226\n",
            "Epoch 719/1000, Avg Training Loss: 0.2465865390583288, Avg Validation Loss: 48.77295044873845\n",
            "Epoch 720/1000, Avg Training Loss: 0.2465850621237506, Avg Validation Loss: 48.776598257466674\n",
            "Epoch 721/1000, Avg Training Loss: 0.24653561584147318, Avg Validation Loss: 48.76673387064762\n",
            "Epoch 722/1000, Avg Training Loss: 0.24651297270633626, Avg Validation Loss: 48.7801254608551\n",
            "Epoch 723/1000, Avg Training Loss: 0.2465097294083188, Avg Validation Loss: 48.776159596533034\n",
            "Epoch 724/1000, Avg Training Loss: 0.24652277762843497, Avg Validation Loss: 48.78558939895064\n",
            "Epoch 725/1000, Avg Training Loss: 0.24654575440399454, Avg Validation Loss: 48.74393939470343\n",
            "Epoch 726/1000, Avg Training Loss: 0.24655789994206756, Avg Validation Loss: 48.75449053663888\n",
            "Epoch 727/1000, Avg Training Loss: 0.24658663866392894, Avg Validation Loss: 48.7651579836716\n",
            "Epoch 728/1000, Avg Training Loss: 0.24653778774481672, Avg Validation Loss: 48.733596918646974\n",
            "Epoch 729/1000, Avg Training Loss: 0.24653574089745725, Avg Validation Loss: 48.72451818399896\n",
            "Epoch 730/1000, Avg Training Loss: 0.24651746435011707, Avg Validation Loss: 48.75068453920032\n",
            "Epoch 731/1000, Avg Training Loss: 0.24668406165010107, Avg Validation Loss: 48.75178647537446\n",
            "Epoch 732/1000, Avg Training Loss: 0.24656336668880222, Avg Validation Loss: 48.76869618830116\n",
            "Epoch 733/1000, Avg Training Loss: 0.24653829799067284, Avg Validation Loss: 48.739113765658075\n",
            "Epoch 734/1000, Avg Training Loss: 0.24651974376268668, Avg Validation Loss: 48.71205251815931\n",
            "Epoch 735/1000, Avg Training Loss: 0.24650582568229254, Avg Validation Loss: 48.73205399952061\n",
            "Epoch 736/1000, Avg Training Loss: 0.24647188744181844, Avg Validation Loss: 48.745630233176286\n",
            "Epoch 737/1000, Avg Training Loss: 0.2464984800593391, Avg Validation Loss: 48.72825519957895\n",
            "Epoch 738/1000, Avg Training Loss: 0.2464938101847284, Avg Validation Loss: 48.769482229238356\n",
            "Epoch 739/1000, Avg Training Loss: 0.24650018011972719, Avg Validation Loss: 48.73530987722286\n",
            "Epoch 740/1000, Avg Training Loss: 0.2465054813904943, Avg Validation Loss: 48.75338551491602\n",
            "Epoch 741/1000, Avg Training Loss: 0.24653718469687916, Avg Validation Loss: 48.75131116311833\n",
            "Epoch 742/1000, Avg Training Loss: 0.24649938461593, Avg Validation Loss: 48.68956277845331\n",
            "Epoch 743/1000, Avg Training Loss: 0.24652676825304531, Avg Validation Loss: 48.741124655457014\n",
            "Epoch 744/1000, Avg Training Loss: 0.2465485690531071, Avg Validation Loss: 48.73876078148895\n",
            "Epoch 745/1000, Avg Training Loss: 0.24659609057891202, Avg Validation Loss: 48.66441567232235\n",
            "Epoch 746/1000, Avg Training Loss: 0.24647264554915116, Avg Validation Loss: 48.71973660820041\n",
            "Epoch 747/1000, Avg Training Loss: 0.2464806717105572, Avg Validation Loss: 48.74442719216462\n",
            "Epoch 748/1000, Avg Training Loss: 0.24652998890446207, Avg Validation Loss: 48.769316495200556\n",
            "Epoch 749/1000, Avg Training Loss: 0.24652861582319208, Avg Validation Loss: 48.80548998783386\n",
            "Epoch 750/1000, Avg Training Loss: 0.24656277379801383, Avg Validation Loss: 48.71255685769076\n",
            "Epoch 751/1000, Avg Training Loss: 0.2465469449707952, Avg Validation Loss: 48.69699768037101\n",
            "Epoch 752/1000, Avg Training Loss: 0.2465202592990648, Avg Validation Loss: 48.71799383531766\n",
            "Epoch 753/1000, Avg Training Loss: 0.24648529710889006, Avg Validation Loss: 48.748567453895085\n",
            "Epoch 754/1000, Avg Training Loss: 0.24652518479315838, Avg Validation Loss: 48.74615967060791\n",
            "Epoch 755/1000, Avg Training Loss: 0.24650713447257747, Avg Validation Loss: 48.73243300696069\n",
            "Epoch 756/1000, Avg Training Loss: 0.2464615640437781, Avg Validation Loss: 48.73842645453431\n",
            "Epoch 757/1000, Avg Training Loss: 0.24645962302374005, Avg Validation Loss: 48.69181915101203\n",
            "Epoch 758/1000, Avg Training Loss: 0.24648047918142452, Avg Validation Loss: 48.728591323684\n",
            "Epoch 759/1000, Avg Training Loss: 0.2464684680851329, Avg Validation Loss: 48.73806727845877\n",
            "Epoch 760/1000, Avg Training Loss: 0.24646083981720224, Avg Validation Loss: 48.750128936319626\n",
            "Epoch 761/1000, Avg Training Loss: 0.24644854432904928, Avg Validation Loss: 48.72011789961551\n",
            "Epoch 762/1000, Avg Training Loss: 0.24649459267209803, Avg Validation Loss: 48.67397412993196\n",
            "Epoch 763/1000, Avg Training Loss: 0.24648903166810623, Avg Validation Loss: 48.68459867747798\n",
            "Epoch 764/1000, Avg Training Loss: 0.24646134692800256, Avg Validation Loss: 48.717447033668776\n",
            "Epoch 765/1000, Avg Training Loss: 0.2464472717303088, Avg Validation Loss: 48.75162068727719\n",
            "Epoch 766/1000, Avg Training Loss: 0.24649774225553464, Avg Validation Loss: 48.75135554194967\n",
            "Epoch 767/1000, Avg Training Loss: 0.24651244700648747, Avg Validation Loss: 48.70501734129414\n",
            "Epoch 768/1000, Avg Training Loss: 0.24651897608221282, Avg Validation Loss: 48.7312433850108\n",
            "Epoch 769/1000, Avg Training Loss: 0.24652207960536812, Avg Validation Loss: 48.760881364063856\n",
            "Epoch 770/1000, Avg Training Loss: 0.24646774871225205, Avg Validation Loss: 48.74287608223739\n",
            "Epoch 771/1000, Avg Training Loss: 0.24652410021863036, Avg Validation Loss: 48.69837426657236\n",
            "Epoch 772/1000, Avg Training Loss: 0.24642937709812307, Avg Validation Loss: 48.7394881136294\n",
            "Epoch 773/1000, Avg Training Loss: 0.24648591262638322, Avg Validation Loss: 48.798010989079835\n",
            "Epoch 774/1000, Avg Training Loss: 0.24642908003217007, Avg Validation Loss: 48.77672983934026\n",
            "Epoch 775/1000, Avg Training Loss: 0.24648006069136788, Avg Validation Loss: 48.73148996304061\n",
            "Epoch 776/1000, Avg Training Loss: 0.24662020470600055, Avg Validation Loss: 48.65687244126664\n",
            "Epoch 777/1000, Avg Training Loss: 0.24646743306466146, Avg Validation Loss: 48.71785838174558\n",
            "Epoch 778/1000, Avg Training Loss: 0.2464836961175264, Avg Validation Loss: 48.73789450666218\n",
            "Epoch 779/1000, Avg Training Loss: 0.24652376258780784, Avg Validation Loss: 48.80618005815808\n",
            "Epoch 780/1000, Avg Training Loss: 0.24648040937703264, Avg Validation Loss: 48.750350775644264\n",
            "Epoch 781/1000, Avg Training Loss: 0.24658709570193188, Avg Validation Loss: 48.69083851589002\n",
            "Epoch 782/1000, Avg Training Loss: 0.2464400273360286, Avg Validation Loss: 48.714980858685365\n",
            "Epoch 783/1000, Avg Training Loss: 0.2464550050341256, Avg Validation Loss: 48.700810297768925\n",
            "Epoch 784/1000, Avg Training Loss: 0.2464184604796885, Avg Validation Loss: 48.76632370885791\n",
            "Epoch 785/1000, Avg Training Loss: 0.24656211924545135, Avg Validation Loss: 48.770015992286496\n",
            "Epoch 786/1000, Avg Training Loss: 0.24650510960117683, Avg Validation Loss: 48.78235972836211\n",
            "Epoch 787/1000, Avg Training Loss: 0.24659098950989272, Avg Validation Loss: 48.795374162883334\n",
            "Epoch 788/1000, Avg Training Loss: 0.24648408804433192, Avg Validation Loss: 48.72075432797044\n",
            "Epoch 789/1000, Avg Training Loss: 0.24640477030093572, Avg Validation Loss: 48.71503791872297\n",
            "Epoch 790/1000, Avg Training Loss: 0.24653399822528854, Avg Validation Loss: 48.71230224591423\n",
            "Epoch 791/1000, Avg Training Loss: 0.24643245140426825, Avg Validation Loss: 48.740154874689225\n",
            "Epoch 792/1000, Avg Training Loss: 0.24639204563796097, Avg Validation Loss: 48.74525706936755\n",
            "Epoch 793/1000, Avg Training Loss: 0.24645858535344936, Avg Validation Loss: 48.745996193597094\n",
            "Epoch 794/1000, Avg Training Loss: 0.24642433931120825, Avg Validation Loss: 48.76347695291669\n",
            "Epoch 795/1000, Avg Training Loss: 0.2464213264701548, Avg Validation Loss: 48.756971931951185\n",
            "Epoch 796/1000, Avg Training Loss: 0.24643382863683816, Avg Validation Loss: 48.7370974993503\n",
            "Epoch 797/1000, Avg Training Loss: 0.2464314368791263, Avg Validation Loss: 48.75135103664912\n",
            "Epoch 798/1000, Avg Training Loss: 0.24645560650314155, Avg Validation Loss: 48.70691160890648\n",
            "Epoch 799/1000, Avg Training Loss: 0.24646565473864362, Avg Validation Loss: 48.76328365199086\n",
            "Epoch 800/1000, Avg Training Loss: 0.24651946821871643, Avg Validation Loss: 48.691959996546046\n",
            "Epoch 801/1000, Avg Training Loss: 0.24642339818105705, Avg Validation Loss: 48.7718655245333\n",
            "Epoch 802/1000, Avg Training Loss: 0.24642198772180218, Avg Validation Loss: 48.773091893423924\n",
            "Epoch 803/1000, Avg Training Loss: 0.24645099254047875, Avg Validation Loss: 48.741059067325175\n",
            "Epoch 804/1000, Avg Training Loss: 0.24644254100322852, Avg Validation Loss: 48.7560023196929\n",
            "Epoch 805/1000, Avg Training Loss: 0.24658747528279118, Avg Validation Loss: 48.82574167962675\n",
            "Epoch 806/1000, Avg Training Loss: 0.24646879031827174, Avg Validation Loss: 48.75608543386477\n",
            "Epoch 807/1000, Avg Training Loss: 0.2464663873439931, Avg Validation Loss: 48.74121805623702\n",
            "Epoch 808/1000, Avg Training Loss: 0.24639980900487587, Avg Validation Loss: 48.72207645242324\n",
            "Epoch 809/1000, Avg Training Loss: 0.24655525562959824, Avg Validation Loss: 48.82167505330279\n",
            "Epoch 810/1000, Avg Training Loss: 0.2463989110593292, Avg Validation Loss: 48.777784153413826\n",
            "Epoch 811/1000, Avg Training Loss: 0.24641388310482226, Avg Validation Loss: 48.74814346783889\n",
            "Epoch 812/1000, Avg Training Loss: 0.2464457795352979, Avg Validation Loss: 48.76700876572296\n",
            "Epoch 813/1000, Avg Training Loss: 0.24648740478194295, Avg Validation Loss: 48.724516662953135\n",
            "Epoch 814/1000, Avg Training Loss: 0.24647088667118183, Avg Validation Loss: 48.734260423793884\n",
            "Epoch 815/1000, Avg Training Loss: 0.24656157131329193, Avg Validation Loss: 48.834539923845995\n",
            "Epoch 816/1000, Avg Training Loss: 0.24649392413434673, Avg Validation Loss: 48.708883975457596\n",
            "Epoch 817/1000, Avg Training Loss: 0.24644703226750522, Avg Validation Loss: 48.735277346283254\n",
            "Epoch 818/1000, Avg Training Loss: 0.24641110529697727, Avg Validation Loss: 48.73851187063531\n",
            "Epoch 819/1000, Avg Training Loss: 0.24638984655462032, Avg Validation Loss: 48.77800410094308\n",
            "Epoch 820/1000, Avg Training Loss: 0.2464447647503733, Avg Validation Loss: 48.80608197279014\n",
            "Epoch 821/1000, Avg Training Loss: 0.24645154290155916, Avg Validation Loss: 48.7155559259561\n",
            "Epoch 822/1000, Avg Training Loss: 0.24640692322734586, Avg Validation Loss: 48.74924726368627\n",
            "Epoch 823/1000, Avg Training Loss: 0.24645659443279544, Avg Validation Loss: 48.70749928703379\n",
            "Epoch 824/1000, Avg Training Loss: 0.2464277059771137, Avg Validation Loss: 48.77874307321384\n",
            "Epoch 825/1000, Avg Training Loss: 0.24638043239079915, Avg Validation Loss: 48.75416888437515\n",
            "Epoch 826/1000, Avg Training Loss: 0.24641066563935055, Avg Validation Loss: 48.774071402698155\n",
            "Epoch 827/1000, Avg Training Loss: 0.2465039389642294, Avg Validation Loss: 48.77060744785063\n",
            "Epoch 828/1000, Avg Training Loss: 0.24645272388647663, Avg Validation Loss: 48.786096620135915\n",
            "Epoch 829/1000, Avg Training Loss: 0.24643865069843618, Avg Validation Loss: 48.769221522376085\n",
            "Epoch 830/1000, Avg Training Loss: 0.24640917418206773, Avg Validation Loss: 48.738890055335624\n",
            "Epoch 831/1000, Avg Training Loss: 0.2464222382879202, Avg Validation Loss: 48.75065795912997\n",
            "Epoch 832/1000, Avg Training Loss: 0.24644424085697655, Avg Validation Loss: 48.79574663808033\n",
            "Epoch 833/1000, Avg Training Loss: 0.24638516409658337, Avg Validation Loss: 48.78224157246032\n",
            "Epoch 834/1000, Avg Training Loss: 0.2464076992776976, Avg Validation Loss: 48.72500543068948\n",
            "Epoch 835/1000, Avg Training Loss: 0.24643125282327502, Avg Validation Loss: 48.785629358784945\n",
            "Epoch 836/1000, Avg Training Loss: 0.24645708178351733, Avg Validation Loss: 48.738121389827924\n",
            "Epoch 837/1000, Avg Training Loss: 0.246474263520116, Avg Validation Loss: 48.78843957411028\n",
            "Epoch 838/1000, Avg Training Loss: 0.2463964093642361, Avg Validation Loss: 48.811842605108964\n",
            "Epoch 839/1000, Avg Training Loss: 0.24640194721742173, Avg Validation Loss: 48.75884773462013\n",
            "Epoch 840/1000, Avg Training Loss: 0.246457572663454, Avg Validation Loss: 48.77397064165077\n",
            "Epoch 841/1000, Avg Training Loss: 0.24637397872267855, Avg Validation Loss: 48.77981490175996\n",
            "Epoch 842/1000, Avg Training Loss: 0.2463707245035279, Avg Validation Loss: 48.7450378598517\n",
            "Epoch 843/1000, Avg Training Loss: 0.24647813587390333, Avg Validation Loss: 48.698803404855056\n",
            "Epoch 844/1000, Avg Training Loss: 0.2464173111176205, Avg Validation Loss: 48.76526931638753\n",
            "Epoch 845/1000, Avg Training Loss: 0.24632698982574922, Avg Validation Loss: 48.81822133927035\n",
            "Epoch 846/1000, Avg Training Loss: 0.24641505825299248, Avg Validation Loss: 48.8044723140316\n",
            "Epoch 847/1000, Avg Training Loss: 0.24636078473289197, Avg Validation Loss: 48.795636653303674\n",
            "Epoch 848/1000, Avg Training Loss: 0.2464431870950567, Avg Validation Loss: 48.73687743927502\n",
            "Epoch 849/1000, Avg Training Loss: 0.24637194677241817, Avg Validation Loss: 48.81367462228195\n",
            "Epoch 850/1000, Avg Training Loss: 0.2464652061284972, Avg Validation Loss: 48.74804997075999\n",
            "Epoch 851/1000, Avg Training Loss: 0.24643383600675764, Avg Validation Loss: 48.81408535479784\n",
            "Epoch 852/1000, Avg Training Loss: 0.2464658990684023, Avg Validation Loss: 48.8301390354866\n",
            "Epoch 853/1000, Avg Training Loss: 0.2463562337154105, Avg Validation Loss: 48.73947674264295\n",
            "Epoch 854/1000, Avg Training Loss: 0.24636566698060144, Avg Validation Loss: 48.71774872378214\n",
            "Epoch 855/1000, Avg Training Loss: 0.246500013073302, Avg Validation Loss: 48.77460137667158\n",
            "Epoch 856/1000, Avg Training Loss: 0.24636798403365745, Avg Validation Loss: 48.77142036224386\n",
            "Epoch 857/1000, Avg Training Loss: 0.24646331425248008, Avg Validation Loss: 48.74140998383402\n",
            "Epoch 858/1000, Avg Training Loss: 0.246485186778173, Avg Validation Loss: 48.743463949420274\n",
            "Epoch 859/1000, Avg Training Loss: 0.2463571896734449, Avg Validation Loss: 48.82645093024864\n",
            "Epoch 860/1000, Avg Training Loss: 0.24636911795663344, Avg Validation Loss: 48.83082564250876\n",
            "Epoch 861/1000, Avg Training Loss: 0.2464813620739271, Avg Validation Loss: 48.730888120829945\n",
            "Epoch 862/1000, Avg Training Loss: 0.2463843139089326, Avg Validation Loss: 48.743649285250385\n",
            "Epoch 863/1000, Avg Training Loss: 0.24641152349746082, Avg Validation Loss: 48.85310371662779\n",
            "Epoch 864/1000, Avg Training Loss: 0.24647488448517163, Avg Validation Loss: 48.837429371914894\n",
            "Epoch 865/1000, Avg Training Loss: 0.2464523916725079, Avg Validation Loss: 48.85690816893901\n",
            "Epoch 866/1000, Avg Training Loss: 0.24643747644702027, Avg Validation Loss: 48.70820189456139\n",
            "Epoch 867/1000, Avg Training Loss: 0.24641751032284506, Avg Validation Loss: 48.7253754064339\n",
            "Epoch 868/1000, Avg Training Loss: 0.24640378083139147, Avg Validation Loss: 48.72960450349896\n",
            "Epoch 869/1000, Avg Training Loss: 0.24634398699546456, Avg Validation Loss: 48.76987847738861\n",
            "Epoch 870/1000, Avg Training Loss: 0.24642672053422301, Avg Validation Loss: 48.80425584032085\n",
            "Epoch 871/1000, Avg Training Loss: 0.2466824426814096, Avg Validation Loss: 48.92288430586204\n",
            "Epoch 872/1000, Avg Training Loss: 0.24643218425258795, Avg Validation Loss: 48.841732932297035\n",
            "Epoch 873/1000, Avg Training Loss: 0.24639710957566519, Avg Validation Loss: 48.75236167866606\n",
            "Epoch 874/1000, Avg Training Loss: 0.24643571717468807, Avg Validation Loss: 48.64233307049404\n",
            "Epoch 875/1000, Avg Training Loss: 0.24650840604102733, Avg Validation Loss: 48.78768396887053\n",
            "Epoch 876/1000, Avg Training Loss: 0.24642704693178183, Avg Validation Loss: 48.693687783032075\n",
            "Epoch 877/1000, Avg Training Loss: 0.2464203820643133, Avg Validation Loss: 48.762662895198375\n",
            "Epoch 878/1000, Avg Training Loss: 0.24644585670298202, Avg Validation Loss: 48.884949235793854\n",
            "Epoch 879/1000, Avg Training Loss: 0.24633117522109632, Avg Validation Loss: 48.83027911290212\n",
            "Epoch 880/1000, Avg Training Loss: 0.2465618091484956, Avg Validation Loss: 48.69275982287116\n",
            "Epoch 881/1000, Avg Training Loss: 0.24632345675890907, Avg Validation Loss: 48.726536025439515\n",
            "Epoch 882/1000, Avg Training Loss: 0.24634173059464165, Avg Validation Loss: 48.795937040489726\n",
            "Epoch 883/1000, Avg Training Loss: 0.24646283852722037, Avg Validation Loss: 48.87528744894594\n",
            "Epoch 884/1000, Avg Training Loss: 0.24652565851692998, Avg Validation Loss: 48.777323201784256\n",
            "Epoch 885/1000, Avg Training Loss: 0.24647636915496654, Avg Validation Loss: 48.77092485785066\n",
            "Epoch 886/1000, Avg Training Loss: 0.24659982878137263, Avg Validation Loss: 48.76631461986433\n",
            "Epoch 887/1000, Avg Training Loss: 0.2464196045722774, Avg Validation Loss: 48.760728444007356\n",
            "Epoch 888/1000, Avg Training Loss: 0.24643052805957807, Avg Validation Loss: 48.819201313353005\n",
            "Epoch 889/1000, Avg Training Loss: 0.24640530108844275, Avg Validation Loss: 48.73569348713946\n",
            "Epoch 890/1000, Avg Training Loss: 0.2463898719648032, Avg Validation Loss: 48.81845781711366\n",
            "Epoch 891/1000, Avg Training Loss: 0.2463818569830434, Avg Validation Loss: 48.77001432153455\n",
            "Epoch 892/1000, Avg Training Loss: 0.24650005578531936, Avg Validation Loss: 48.87378281366627\n",
            "Epoch 893/1000, Avg Training Loss: 0.24642023624611498, Avg Validation Loss: 48.766331845384364\n",
            "Epoch 894/1000, Avg Training Loss: 0.24636606334128164, Avg Validation Loss: 48.770474943690886\n",
            "Epoch 895/1000, Avg Training Loss: 0.2463488478333063, Avg Validation Loss: 48.71876659494478\n",
            "Epoch 896/1000, Avg Training Loss: 0.24631005478221454, Avg Validation Loss: 48.758208372140146\n",
            "Epoch 897/1000, Avg Training Loss: 0.24641037729971665, Avg Validation Loss: 48.818703320209735\n",
            "Epoch 898/1000, Avg Training Loss: 0.24637098780753064, Avg Validation Loss: 48.73055753593587\n",
            "Epoch 899/1000, Avg Training Loss: 0.24632818565603273, Avg Validation Loss: 48.716681342546906\n",
            "Epoch 900/1000, Avg Training Loss: 0.2463122298906533, Avg Validation Loss: 48.79831020619828\n",
            "Epoch 901/1000, Avg Training Loss: 0.24643714223996738, Avg Validation Loss: 48.824181729481225\n",
            "Epoch 902/1000, Avg Training Loss: 0.24628929824472878, Avg Validation Loss: 48.805706474217786\n",
            "Epoch 903/1000, Avg Training Loss: 0.24663688339143447, Avg Validation Loss: 48.656963534269394\n",
            "Epoch 904/1000, Avg Training Loss: 0.24628134962431447, Avg Validation Loss: 48.81988442028259\n",
            "Epoch 905/1000, Avg Training Loss: 0.24640807603707654, Avg Validation Loss: 48.92502996201519\n",
            "Epoch 906/1000, Avg Training Loss: 0.2463622334242567, Avg Validation Loss: 48.77078517683834\n",
            "Epoch 907/1000, Avg Training Loss: 0.24632660114162327, Avg Validation Loss: 48.76162227963412\n",
            "Epoch 908/1000, Avg Training Loss: 0.24626865292380573, Avg Validation Loss: 48.73651713832643\n",
            "Epoch 909/1000, Avg Training Loss: 0.2463366784563449, Avg Validation Loss: 48.76419888568117\n",
            "Epoch 910/1000, Avg Training Loss: 0.246434436884641, Avg Validation Loss: 48.73942800718855\n",
            "Epoch 911/1000, Avg Training Loss: 0.24630746650877205, Avg Validation Loss: 48.79841919396092\n",
            "Epoch 912/1000, Avg Training Loss: 0.24633248575078703, Avg Validation Loss: 48.82440210413799\n",
            "Epoch 913/1000, Avg Training Loss: 0.24633180388352383, Avg Validation Loss: 48.826550277168884\n",
            "Epoch 914/1000, Avg Training Loss: 0.24638481703235662, Avg Validation Loss: 48.815546891604505\n",
            "Epoch 915/1000, Avg Training Loss: 0.24625897047297662, Avg Validation Loss: 48.74700257710404\n",
            "Epoch 916/1000, Avg Training Loss: 0.24655851812913324, Avg Validation Loss: 48.732039398179566\n",
            "Epoch 917/1000, Avg Training Loss: 0.24634721120410444, Avg Validation Loss: 48.796522955189246\n",
            "Epoch 918/1000, Avg Training Loss: 0.2463829303810229, Avg Validation Loss: 48.760927313880956\n",
            "Epoch 919/1000, Avg Training Loss: 0.24651626331161475, Avg Validation Loss: 48.91317041921154\n",
            "Epoch 920/1000, Avg Training Loss: 0.24641989642232492, Avg Validation Loss: 48.78208238073345\n",
            "Epoch 921/1000, Avg Training Loss: 0.24643714193269578, Avg Validation Loss: 48.834326010501556\n",
            "Epoch 922/1000, Avg Training Loss: 0.24627613797818157, Avg Validation Loss: 48.7442423864595\n",
            "Epoch 923/1000, Avg Training Loss: 0.24633505539013734, Avg Validation Loss: 48.70227067222279\n",
            "Epoch 924/1000, Avg Training Loss: 0.24636588054820344, Avg Validation Loss: 48.70508844883031\n",
            "Epoch 925/1000, Avg Training Loss: 0.24635114302737318, Avg Validation Loss: 48.83694008446369\n",
            "Epoch 926/1000, Avg Training Loss: 0.24621714632144076, Avg Validation Loss: 48.833265245757964\n",
            "Epoch 927/1000, Avg Training Loss: 0.24643541234748484, Avg Validation Loss: 48.75609451633231\n",
            "Epoch 928/1000, Avg Training Loss: 0.24643570312971974, Avg Validation Loss: 48.838854155215174\n",
            "Epoch 929/1000, Avg Training Loss: 0.24642128141866657, Avg Validation Loss: 48.728503546137375\n",
            "Epoch 930/1000, Avg Training Loss: 0.246412732617017, Avg Validation Loss: 48.8167490378639\n",
            "Epoch 931/1000, Avg Training Loss: 0.24644201977904476, Avg Validation Loss: 48.74719043566515\n",
            "Epoch 932/1000, Avg Training Loss: 0.24633881395827803, Avg Validation Loss: 48.819801482723655\n",
            "Epoch 933/1000, Avg Training Loss: 0.24653011381910858, Avg Validation Loss: 48.876961397573055\n",
            "Epoch 934/1000, Avg Training Loss: 0.24631239572670688, Avg Validation Loss: 48.70532939325554\n",
            "Epoch 935/1000, Avg Training Loss: 0.24627768266052563, Avg Validation Loss: 48.74986806869141\n",
            "Epoch 936/1000, Avg Training Loss: 0.24683276981265959, Avg Validation Loss: 48.95441544285815\n",
            "Epoch 937/1000, Avg Training Loss: 0.24649122349525546, Avg Validation Loss: 48.755785967176756\n",
            "Epoch 938/1000, Avg Training Loss: 0.24635808533119163, Avg Validation Loss: 48.65250691189044\n",
            "Epoch 939/1000, Avg Training Loss: 0.24631070670189895, Avg Validation Loss: 48.81309943838303\n",
            "Epoch 940/1000, Avg Training Loss: 0.24633603830619685, Avg Validation Loss: 48.79915208400291\n",
            "Epoch 941/1000, Avg Training Loss: 0.2461872935511934, Avg Validation Loss: 48.83923995992737\n",
            "Epoch 942/1000, Avg Training Loss: 0.24635921117801363, Avg Validation Loss: 48.7897561371568\n",
            "Epoch 943/1000, Avg Training Loss: 0.24640719660023802, Avg Validation Loss: 48.859376456223124\n",
            "Epoch 944/1000, Avg Training Loss: 0.24625847217496244, Avg Validation Loss: 48.73131682623389\n",
            "Epoch 945/1000, Avg Training Loss: 0.24627095587265516, Avg Validation Loss: 48.761271023237434\n",
            "Epoch 946/1000, Avg Training Loss: 0.24620832701851977, Avg Validation Loss: 48.823334606181675\n",
            "Epoch 947/1000, Avg Training Loss: 0.24624058309172794, Avg Validation Loss: 48.82877554081895\n",
            "Epoch 948/1000, Avg Training Loss: 0.2462792813666373, Avg Validation Loss: 48.76572607454221\n",
            "Epoch 949/1000, Avg Training Loss: 0.2463605692115763, Avg Validation Loss: 48.8017223834683\n",
            "Epoch 950/1000, Avg Training Loss: 0.24621863365784682, Avg Validation Loss: 48.77878322483285\n",
            "Epoch 951/1000, Avg Training Loss: 0.24628730858562356, Avg Validation Loss: 48.819630955822205\n",
            "Epoch 952/1000, Avg Training Loss: 0.24624422295058937, Avg Validation Loss: 48.81668128632357\n",
            "Epoch 953/1000, Avg Training Loss: 0.24650423802346275, Avg Validation Loss: 48.74613108722612\n",
            "Epoch 954/1000, Avg Training Loss: 0.24633458964348207, Avg Validation Loss: 48.71812594569484\n",
            "Epoch 955/1000, Avg Training Loss: 0.24636134565601991, Avg Validation Loss: 48.77717186040942\n",
            "Epoch 956/1000, Avg Training Loss: 0.24617844113383336, Avg Validation Loss: 48.863850542207174\n",
            "Epoch 957/1000, Avg Training Loss: 0.246288080201265, Avg Validation Loss: 48.765015732875725\n",
            "Epoch 958/1000, Avg Training Loss: 0.24628446245970437, Avg Validation Loss: 48.86435517878941\n",
            "Epoch 959/1000, Avg Training Loss: 0.24628243445118692, Avg Validation Loss: 48.69562718569277\n",
            "Epoch 960/1000, Avg Training Loss: 0.24643878687357415, Avg Validation Loss: 48.815661010178914\n",
            "Epoch 961/1000, Avg Training Loss: 0.24631273377129928, Avg Validation Loss: 48.809797588008415\n",
            "Epoch 962/1000, Avg Training Loss: 0.246279123483298, Avg Validation Loss: 48.86406482302495\n",
            "Epoch 963/1000, Avg Training Loss: 0.24620102091729645, Avg Validation Loss: 48.76384628857514\n",
            "Epoch 964/1000, Avg Training Loss: 0.24618539552744237, Avg Validation Loss: 48.772291047566355\n",
            "Epoch 965/1000, Avg Training Loss: 0.246292943284299, Avg Validation Loss: 48.812486662909194\n",
            "Epoch 966/1000, Avg Training Loss: 0.2461728591146852, Avg Validation Loss: 48.74563440087894\n",
            "Epoch 967/1000, Avg Training Loss: 0.24619369321880416, Avg Validation Loss: 48.750362825014065\n",
            "Epoch 968/1000, Avg Training Loss: 0.2462457668738872, Avg Validation Loss: 48.82183539367514\n",
            "Epoch 969/1000, Avg Training Loss: 0.24629845637256065, Avg Validation Loss: 48.88067710992929\n",
            "Epoch 970/1000, Avg Training Loss: 0.24631757487834874, Avg Validation Loss: 48.714774720110505\n",
            "Epoch 971/1000, Avg Training Loss: 0.24618493903519872, Avg Validation Loss: 48.70341323964888\n",
            "Epoch 972/1000, Avg Training Loss: 0.24626436416120526, Avg Validation Loss: 48.79316939903901\n",
            "Epoch 973/1000, Avg Training Loss: 0.24622331642497833, Avg Validation Loss: 48.86532923818335\n",
            "Epoch 974/1000, Avg Training Loss: 0.2463345439088714, Avg Validation Loss: 48.780833284498726\n",
            "Epoch 975/1000, Avg Training Loss: 0.24616456624516683, Avg Validation Loss: 48.85518982596747\n",
            "Epoch 976/1000, Avg Training Loss: 0.24622576971638124, Avg Validation Loss: 48.77706388133869\n",
            "Epoch 977/1000, Avg Training Loss: 0.24615867556574894, Avg Validation Loss: 48.76554268633474\n",
            "Epoch 978/1000, Avg Training Loss: 0.24611682359509784, Avg Validation Loss: 48.81724873556303\n",
            "Epoch 979/1000, Avg Training Loss: 0.2462837932426796, Avg Validation Loss: 48.77285203834425\n",
            "Epoch 980/1000, Avg Training Loss: 0.2463807203923351, Avg Validation Loss: 48.799987809119855\n",
            "Epoch 981/1000, Avg Training Loss: 0.24629562704696803, Avg Validation Loss: 48.81961918669353\n",
            "Epoch 982/1000, Avg Training Loss: 0.24642939303666414, Avg Validation Loss: 48.8352342866897\n",
            "Epoch 983/1000, Avg Training Loss: 0.24637364498333064, Avg Validation Loss: 48.715801604209034\n",
            "Epoch 984/1000, Avg Training Loss: 0.24624466472109596, Avg Validation Loss: 48.78432576159311\n",
            "Epoch 985/1000, Avg Training Loss: 0.24617261462602147, Avg Validation Loss: 48.848889900041556\n",
            "Epoch 986/1000, Avg Training Loss: 0.24620469729663358, Avg Validation Loss: 48.801454488630355\n",
            "Epoch 987/1000, Avg Training Loss: 0.24624246024712018, Avg Validation Loss: 48.76397235881679\n",
            "Epoch 988/1000, Avg Training Loss: 0.2462788068146539, Avg Validation Loss: 48.80602018998414\n",
            "Epoch 989/1000, Avg Training Loss: 0.2462239904305929, Avg Validation Loss: 48.77306565228471\n",
            "Epoch 990/1000, Avg Training Loss: 0.24632990982123265, Avg Validation Loss: 48.89355333586245\n",
            "Epoch 991/1000, Avg Training Loss: 0.24640119769323884, Avg Validation Loss: 48.855405928107324\n",
            "Epoch 992/1000, Avg Training Loss: 0.24628131131810868, Avg Validation Loss: 48.642733162377745\n",
            "Epoch 993/1000, Avg Training Loss: 0.2462786721768559, Avg Validation Loss: 48.811481845855866\n",
            "Epoch 994/1000, Avg Training Loss: 0.24617141751732913, Avg Validation Loss: 48.81319023049283\n",
            "Epoch 995/1000, Avg Training Loss: 0.24621534175556875, Avg Validation Loss: 48.77006538793527\n",
            "Epoch 996/1000, Avg Training Loss: 0.24615375696174194, Avg Validation Loss: 48.75967349572504\n",
            "Epoch 997/1000, Avg Training Loss: 0.24662690369694976, Avg Validation Loss: 48.71499237218771\n",
            "Epoch 998/1000, Avg Training Loss: 0.24623576493921648, Avg Validation Loss: 48.86273793113705\n",
            "Epoch 999/1000, Avg Training Loss: 0.2463910338728172, Avg Validation Loss: 48.80525958245363\n",
            "Epoch 1000/1000, Avg Training Loss: 0.24659055680550854, Avg Validation Loss: 48.66668034335517\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGwCAYAAACgi8/jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+V0lEQVR4nO3de3wU1cH/8e/s5g4kAQIJgXAREQlFQAgU0IqKBvRBQXy8PKgBrTxa8FKqorVeW8Uq+qCy1doWqK0VpK3Unxco4B1REARBLooiIhBSBBISIJfd8/tjkyWbC+Sym0lmP+8X+9qdmbMzZw+b3e+eOTNjGWOMAAAAHMhldwUAAADChaADAAAci6ADAAAci6ADAAAci6ADAAAci6ADAAAci6ADAAAcK8ruCtjN5/Npz549atOmjSzLsrs6AACgDowxOnz4sNLT0+Vy1d5vE/FBZ8+ePcrIyLC7GgAAoAF27dqlLl261Lo84oNOmzZtJPkbKjEx0ebaAACAuigoKFBGRkbge7w2ER90KnZXJSYmEnQAAGhhTjbshMHIAADAsQg6AADAsQg6AADAsSJ+jA4AoH68Xq9KS0vtrgYcLjo6Wm63u9HrIegAAOrEGKPc3FwdOnTI7qogQiQnJystLa1R57kj6AAA6qQi5HTs2FEJCQmcZBVhY4zRkSNHlJeXJ0nq1KlTg9dF0AEAnJTX6w2EnPbt29tdHUSA+Ph4SVJeXp46duzY4N1YDEYGAJxUxZichIQEm2uCSFLxfmvMmDCCDgCgzthdhaYUivcbQQcAADgWQQcAADgWQQcAgHrq3r27Zs+eXefy7777rizL4tB8GxB0wqVgr3TwW8nLSbUAwC6WZZ3w9uCDDzZovWvWrNGUKVPqXH748OHau3evkpKSGrS9uqoIVDXdcnNzw7rt5orDy8Pld0OlY/nStLVSyql21wYAItLevXsDjxcuXKj7779f27ZtC8xr3bp14LExRl6vV1FRJ/9q7NChQ73qERMTo7S0tHo9pzG2bdumxMTEoHkdO3assWxJSYliYmKqzS8tLVV0dHS9t93Q54ULPTrh4ir/T/aW2FsPAAgTY4yOlJTZcjPG1KmOaWlpgVtSUpIsywpMb926VW3atNFbb72lQYMGKTY2Vh9++KG+/vprXXrppUpNTVXr1q2VlZWl5cuXB6236q4ry7L0xz/+UePHj1dCQoJ69eql1157LbC86q6r+fPnKzk5WUuXLlWfPn3UunVrjR49OiiYlZWV6dZbb1VycrLat2+vGTNmKCcnR+PGjTvp6+7YsWPQa09LS5PL5f/KnzRpksaNG6dHHnlE6enp6t27t7799ltZlqWFCxfqnHPOUVxcnF566SX5fD49/PDD6tKli2JjYzVgwAAtWbIksJ3antec0KMTLu7ydOxj1xUAZzpa6lXm/Utt2fbmh7OVEBOar7C7775bs2bN0imnnKK2bdtq165duuiii/TII48oNjZWL774osaOHatt27apa9euta7noYce0uOPP64nnnhCzz77rCZOnKidO3eqXbt2NZY/cuSIZs2apb/85S9yuVy65pprdMcddwSCwm9/+1u99NJLmjdvnvr06aOnn35aixcv1rnnntvo17xixQolJiZq2bJl1driySef1MCBAxUXF6enn35aTz75pH7/+99r4MCBmjt3ri655BJ98cUX6tWrV63Pa04IOuHiLm9axugAQLP28MMP64ILLghMt2vXTv379w9M//rXv9arr76q1157TdOmTat1PZMmTdLVV18tSXr00Uf1zDPPaPXq1Ro9enSN5UtLS/X888+rZ8+ekqRp06bp4YcfDix/9tlndc8992j8+PGSpDlz5ujNN9+s02vq0qVL0HS3bt30xRdfBKZbtWqlP/7xj4FdVt9++60k6fbbb9dll10WKDdr1izNmDFDV111lSR/+HrnnXc0e/ZseTyeQLmqz2tOCDrhUtGjw64rAA4VH+3W5oezbdt2qAwePDhourCwUA8++KDeeOMN7d27V2VlZTp69Ki+++67E67njDPOCDxu1aqVEhMTA9dqqklCQkIg5Ej+6zlVlM/Pz9e+ffs0ZMiQwHK3261BgwbJ5/Od9DV98MEHatOmTWC66piZfv361Tgup3JbFBQUaM+ePRoxYkRQmREjRmjDhg21Pq+5IeiES2CMDj06AJzJsqyQ7T6yU6tWrYKm77jjDi1btkyzZs3Sqaeeqvj4eF1++eUqKTnxD9eqYcKyrBOGkprK13Xs0cn06NFDycnJtS6v+ppPNv9kGvq8psBg5HBxE3QAoCVauXKlJk2apPHjx6tfv35KS0sL7NppKklJSUpNTdWaNWsC87xer9atW9dkdUhMTFR6erpWrlwZNH/lypXKzMxssno0VsuP4s0Vg5EBoEXq1auX/vnPf2rs2LGyLEv33XdfnXYXhdott9yimTNn6tRTT9Xpp5+uZ599VgcPHqzT9Z/y8vJ07NixoHnt27ev92Hfd955px544AH17NlTAwYM0Lx587R+/fpmd2TViRB0wsXN4eUA0BI99dRTuv766zV8+HClpKRoxowZKigoaPJ6zJgxQ7m5ubruuuvkdrs1ZcoUZWdny+0++fik3r17V5u3atUq/fjHP65XHW699Vbl5+frF7/4hfLy8pSZmanXXnst6Iir5s4yodoh2EIVFBQoKSlJ+fn51U6u1Ch/HivteF+a8Cep3+WhWy8A2ODYsWPasWOHevTo0ewOH44UPp9Pffr00RVXXKFf//rXdlenSZzofVfX7296dMKFwcgAgEbYuXOn/v3vf+ucc85RcXGx5syZox07duh//ud/7K5ai8Jg5HDh8HIAQCO4XC7Nnz9fWVlZGjFihDZu3Kjly5erT58+dletRaFHJ1wqxugwGBkA0AAZGRnVjnhC/dGjEy4cXg4AgO0IOuES2HVF0AEAwC4tPujs2rVLI0eOVGZmps444wwtWrTI7ir5uSqudcUYHQAA7NLix+hERUVp9uzZGjBggHJzczVo0CBddNFF9p+Omh4dAABs1+KDTqdOndSpUydJUlpamlJSUnTgwIFmEHQYjAwAgN1s33X1/vvva+zYsUpPT5dlWVq8eHG1Mh6PR927d1dcXJyGDh2q1atX17iutWvXyuv1KiMjI8y1rgPOjAwAjjFy5Ejdfvvtgenu3btr9uzZJ3xObd9p9RWq9UQq24NOUVGR+vfvL4/HU+PyhQsXavr06XrggQe0bt069e/fX9nZ2YFL2Vc4cOCArrvuOr3wwgsn3F5xcbEKCgqCbmER2HVVFp71AwBOauzYsRo9enSNyz744ANZlqXPP/+83utds2aNpkyZ0tjqBXnwwQc1YMCAavP37t2rMWPGhHRbVc2fP1+WZVW7OeEs2LYHnTFjxug3v/mNxo8fX+Pyp556SjfeeKMmT56szMxMPf/880pISNDcuXMDZYqLizVu3DjdfffdGj58+Am3N3PmTCUlJQVuYev9cdGjAwB2u+GGG7Rs2TJ9//331ZbNmzdPgwcP1hlnnFHv9Xbo0EEJCQmhqOJJpaWlKTY2NuzbSUxM1N69e4NuO3furLV8SUn17zdjjMrK6v8Dv6HPqwvbg86JlJSUaO3atRo1alRgnsvl0qhRo7Rq1SpJ/saZNGmSzjvvPF177bUnXec999yj/Pz8wG3Xrl3hqTy7rgDAdv/1X/+lDh06aP78+UHzCwsLtWjRIt1www364YcfdPXVV6tz585KSEhQv3799PLLL59wvVV3XX311Vf6yU9+ori4OGVmZmrZsmXVnjNjxgyddtppSkhI0CmnnKL77rtPpaX+cZzz58/XQw89pA0bNgR6UyrqXHXX1caNG3XeeecpPj5e7du315QpU1RYWBhYPmnSJI0bN06zZs1Sp06d1L59e02dOjWwrdpYlqW0tLSgW2pqamD5yJEjNW3aNN1+++1KSUlRdna23n33XVmWpbfeekuDBg1SbGysPvzwQxUXF+vWW29Vx44dFRcXp7POOktr1qwJrKu254VDsx6MvH//fnm93qCGlqTU1FRt3bpVkrRy5UotXLhQZ5xxRuCN8Je//EX9+vWrcZ2xsbFNkowVVd7dV1Yc/m0BgB2MkUqP2LPt6ATJsk5aLCoqStddd53mz5+ve++9V1b5cxYtWiSv16urr75ahYWFGjRokGbMmKHExES98cYbuvbaa9WzZ08NGTLkpNvw+Xy67LLLlJqaqk8++UT5+flB43kqtGnTRvPnz1d6ero2btyoG2+8UW3atNFdd92lK6+8Ups2bdKSJUu0fPlySVJSUlK1dRQVFSk7O1vDhg3TmjVrlJeXp5/+9KeaNm1aUJh755131KlTJ73zzjvavn27rrzySg0YMEA33njjSV/Pifz5z3/WzTffHDhj8969eyVJd999t2bNmqVTTjlFbdu21V133aV//OMf+vOf/6xu3brp8ccfV3Z2trZv36527doF1lf1eeHQrINOXZx11lny+Xx2V6O66Iqgc9TeegBAuJQekR5Nt2fbv9wjxdTt6Nrrr79eTzzxhN577z2NHDlSkn+31YQJEwLDGO64445A+VtuuUVLly7VK6+8Uqegs3z5cm3dulVLly5Verq/PR599NFq42p+9atfBR53795dd9xxhxYsWKC77rpL8fHxat26taKiopSWllbrtv72t7/p2LFjevHFFwNHF8+ZM0djx47Vb3/720DHQNu2bTVnzhy53W6dfvrpuvjii7VixYoTBp38/Hy1bt06aN7ZZ5+tt956KzDdq1cvPf7444HpiqDz8MMP64ILLpDkD2PPPfec5s+fH2iDP/zhD1q2bJn+9Kc/6c477ww8v/LzwqVZB52UlBS53W7t27cvaP6+fftO+EZoFip6dEqP2VsPAIhwp59+uoYPH665c+dq5MiR2r59uz744AM9/PDDkiSv16tHH31Ur7zyinbv3q2SkhIVFxfXeQzOli1blJGREQg5kjRs2LBq5RYuXKhnnnlGX3/9tQoLC1VWVqbExMR6vZYtW7aof//+QadQGTFihHw+n7Zt2xYIOn379pXb7Q6U6dSpkzZu3HjCdbdp00br1q0LmhcfHx80PWjQoBqfO3jw4MDjr7/+WqWlpRoxYkRgXnR0tIYMGaItW7bU+rxwadZBJyYmRoMGDdKKFSs0btw4Sf4uwhUrVmjatGn2Vu5kAruuCDoAHCo6wd+zYte26+GGG27QLbfcIo/Ho3nz5qlnz54655xzJElPPPGEnn76ac2ePVv9+vVTq1atdPvtt9c42LahVq1apYkTJ+qhhx5Sdna2kpKStGDBAj355JMh20Zl0dHRQdOWZZ1074fL5dKpp556wjK1naOuoeeua4pz3tkedAoLC7V9+/bA9I4dO7R+/Xq1a9dOXbt21fTp05WTk6PBgwdryJAhmj17toqKijR58uRGbdfj8cjj8cjr9Tb2JdQsujwFE3QAOJVl1Xn3kd2uuOIK3Xbbbfrb3/6mF198UTfffHNgvM7KlSt16aWX6pprrpHk/0H95ZdfKjMzs07r7tOnj3bt2qW9e/cGTmD78ccfB5X56KOP1K1bN917772BeVWPaIqJiTnpd1KfPn00f/58FRUVBULCypUr5XK51Lt37zrVN9x69uypmJgYrVy5Ut26dZMklZaWas2aNTWOXQo324+6+vTTTzVw4EANHDhQkjR9+nQNHDhQ999/vyTpyiuv1KxZs3T//fdrwIABWr9+vZYsWVJtgHJ9TZ06VZs3bw4aBR5SUeUDntl1BQC2a926ta688krdc8892rt3ryZNmhRY1qtXLy1btkwfffSRtmzZov/93/+tNmTiREaNGqXTTjtNOTk52rBhgz744IOgQFOxje+++04LFizQ119/rWeeeUavvvpqUJnu3bsHfuzv379fxcXVD2aZOHGi4uLilJOTo02bNumdd97RLbfcomuvvbbR34vGGOXm5la71XccbKtWrXTzzTfrzjvv1JIlS7R582bdeOONOnLkiG644YZG1bEhbA86I0eOlDGm2q3y6PFp06Zp586dKi4u1ieffKKhQ4faV+G6iqro0WEwMgA0BzfccIMOHjyo7OzsoPE0v/rVr3TmmWcqOztbI0eOVFpaWmC4RF24XC69+uqrOnr0qIYMGaKf/vSneuSRR4LKXHLJJfr5z3+uadOmacCAAfroo4903333BZWZMGGCRo8erXPPPVcdOnSo8RD3hIQELV26VAcOHFBWVpYuv/xynX/++ZozZ079GqMGBQUFgcsqVb5VPUFvXTz22GOaMGGCrr32Wp155pnavn27li5dGrYjq07EMsaYJt9qM1JQUKCkpCTl5+fXe1DYCe1eK/3hPCkpQ/r5ptCtFwBscOzYMe3YsUM9evRwxNly0TKc6H1X1+9v23t0HKuiR6eUHh0AAOxC0AmXijE6DEYGAMA2ERt0PB6PMjMzlZWVFZ4NcNQVAAC2i9igE/6jrsr3JfrKuII5AAA2idigE3ZRlQZNceQVAIeI8ONX0MRC8X4j6IRLdLxklZ9+u/iwvXUBgEaqONPukSM2XcQTEani/Vb1TM/1YfuZkR3LsqT4ttKR/dLRg1KiTRe+A4AQcLvdSk5ODpxTJSEhIXBmYSDUjDE6cuSI8vLylJycHHTdrvoi6IRTfHJ50Dlkd00AoNEqLqbckBPIAQ2RnJzc6It4E3TCKb78DJBHD9pbDwAIAcuy1KlTJ3Xs2FGlpaV2VwcOFx0d3aienAoRG3TCflFP6XjQOXYofNsAgCbmdrtD8gUENIWIHYwc9sPLJSku2X9Pjw4AALaI2KDTJBLa+e+L/mNvPQAAiFAEnXBq28N//8PX9tYDAIAIRdAJp5RT/ff7v7K3HgAARCiCTjil9PbfH/haKimyty4AAEQggk44JXWRErv4r3e16xO7awMAQMQh6ISTZUk9fuJ/vON9e+sCAEAEitig4/F4lJmZqaysrPBuqMfZ/vsdH4R3OwAAoBrLRPilaAsKCpSUlKT8/HwlJiaGfgMHv5We7i+5oqVf7paiYkO/DQAAIkxdv78jtkenySR3kxJSJF+plLvJ7toAABBRCDrhZllS5zP9j3evtbcuzY0x0spnpFeuk3I3hm69xYXS1jekr9+WSo+Fbr0AgBYnYq911aQ6D5K++jdBp6rPF0rL7vM//u5jadoaKS6pYes6+K205k/SN+9KeZv9R7pJUlKGdMmzUs9zQ1FjAEALQ49OU+gy2H//7Yf+Xgz4e12WPXB8unCfv3enITYslH43XProGSn3c3/ISewiteog5e+SXvpvaePfQ1NvINR8Pj4XgDCiR6cpdB0uuWOlgu/9X8Sd+ttdI/u9O1MqzPVfJuP8+6S/Xy+t8khZN0iJ6Sd+blmxVLBb+uEb6bMXpc3/8s/P+LE0dIqUMVRK7CyVHpVemyZt+of0jxukw3ulYdP8uxMBO/i8/t7HvC3Svk3Szo+k78svLJwxVBrwP1KfS6ToOFurCTgJR12F+6irCq/kSJsXSx36SGde6x+g7I6WXFGSy13lvvLNLVnl8y1LklXp3nX8S7vivqb/zsA8U/7YSMZ3gsc+/weyzysZb5Xl5feB9ZrySVO9jDH+55celUoK/WeHLiny78L76t/+dVy9QDpttDR3tLTrY6ldT+n0i6ToBP92K55z9ICU/72Uv1sqygt+fZZL+sld0jl3+durMp9PWnK3tPr3/umMoVLfy6SUXlLrjv4AGhXjX4d/ZcHtqROEoqpl6jNdNWw1Zl2hnG5oCDTG35PmLT1+X3as/FZ8/HHp0fLpo8fL+srfY0HvxfJ1VsxvLFPpvV2xndrK1byg9jJWxd+iS4G/S+P1v7ayYqlov//Cvge/lfZ/6W+HE4lvK502RurQW0poL8UnS1FxkjvGf9SmO7r8fVvpsSuq+uur+ppllX+euPyfKZar0rSrynTl5fwwQPNU1+9vgk5TBZ3/fCn9aZR0LD9822hRLH9Pztm/8E/u/0qaN6buV3qPivefeTpjiPTjn0lpP6q9rDHSJ7+Xlj9w8i8ZVFGHoGR8/i921E1UnD/EdOjj363ddZg/VGx5Tfrsr/7drc1NUPCpeOyqJTSVh6PaQlPlQBj4sWbVcq/gx1XLBOpXNYzVFM5O9COwUhlTW1CstO4af7RYCgqZllX+I7VqG7iPv+6qP1oDbVO1naqUrfh7s9zH27nWNjxB+9X7Xg1//pk5Ie+pJOjUUZMFHcnfI7H+b/7BskcPlvealFW5+apMVypTa69JRS9LTb/KraC7oD+c2h4H/bpzq/YPo4r1qYZlldYTkyDFtJZiWvlviZ393fMdTgtunyMHpC9elX7Y7v+1b1n+8tGt/IOUkzofv6xGQrsaPtxO1v67pc8XSN9/6r+i/LFD/l/c3pIq7aoqj0+kcu/WCaYjkSva/6UeFeMPptFx/vuoWCk63t8bUfmDuqJXrcb3ZiN6FYyp9KVa5cu1PqyaJir3aFbq+ax4Xe4YqVWKf7xYYrrU4XSpbffqPY8VfF7/gPrv1xx/jx49JHmLpbIS/723tPx9W+mx8armL8zKr9Uc760lnKKp3bXD/7kdQgSdk/B4PPJ4PPJ6vfryyy+bJugAxpw8FDXZtOpZvrYQZ5XvhnX7w4072n/v4liHZs+Y4OATFIR8qhaMguaZGp5TMe2rZZ1e/9um8voDu9tq2sVedVe5gudVnn98xvEelVqdYFlFOHS5FRwWVUN9qjyu3LNSuUeo8uv3eRUUjCt2owZNmyqPK/cwVXptQe1Yw4/gWu9Vz/I13dfyf1Xb/SVzpNjWJ/g/qT+CTh01aY8OAAAICc6MDAAAIh5BBwAAOBZBBwAAOBZBBwAAOBZBBwAAOBZBBwAAOBZBBwAAOBZBBwAAOBZBBwAAOFbEBh2Px6PMzExlZWXZXRUAABAmXAKCS0AAANDicAkIAAAQ8Qg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsQg6AADAsSI26Hg8HmVmZiorK8vuqgAAgDCxjDHG7krYqaCgQElJScrPz1diYqLd1QEAAHVQ1+/viO3RAQAAzkfQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjkXQAQAAjhWxQcfj8SgzM1NZWVl2VwUAAISJZYwxdlfCTgUFBUpKSlJ+fr4SExPtrg4AAKiDun5/R2yPDgAAcD6CDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCxHBJ3x48erbdu2uvzyy+2uCgAAaEYcEXRuu+02vfjii3ZXAwAANDOOCDojR45UmzZt7K4GAABoZmwPOu+//77Gjh2r9PR0WZalxYsXVyvj8XjUvXt3xcXFaejQoVq9enXTVxQAALQ4tgedoqIi9e/fXx6Pp8blCxcu1PTp0/XAAw9o3bp16t+/v7Kzs5WXl9eg7RUXF6ugoCDoBgAAnMn2oDNmzBj95je/0fjx42tc/tRTT+nGG2/U5MmTlZmZqeeff14JCQmaO3dug7Y3c+ZMJSUlBW4ZGRmNqT4AAGjGbA86J1JSUqK1a9dq1KhRgXkul0ujRo3SqlWrGrTOe+65R/n5+YHbrl27QlVdAADQzETZXYET2b9/v7xer1JTU4Pmp6amauvWrYHpUaNGacOGDSoqKlKXLl20aNEiDRs2rMZ1xsbGKjY2Nqz1BgAAzUOzDjp1tXz5crurAAAAmqFmvesqJSVFbrdb+/btC5q/b98+paWl2VQrAADQUjTroBMTE6NBgwZpxYoVgXk+n08rVqyoddcUAABABdt3XRUWFmr79u2B6R07dmj9+vVq166dunbtqunTpysnJ0eDBw/WkCFDNHv2bBUVFWny5MmN2q7H45HH45HX623sSwAAAM2UZYwxdlbg3Xff1bnnnlttfk5OjubPny9JmjNnjp544gnl5uZqwIABeuaZZzR06NCQbL+goEBJSUnKz89XYmJiSNYJAADCq67f37YHHbsRdAAAaHnq+v3drMfoAAAANAZBBwAAOBZBBwAAOFbEBh2Px6PMzExlZWXZXRUAABAmDEZmMDIAAC0Og5EBAEDEq3fQKS0tVVRUlDZt2hSO+gAAAIRMvYNOdHS0unbtyhmFAQBAs9egXVf33nuvfvnLX+rAgQOhrg8AAEDINOhaV3PmzNH27duVnp6ubt26qVWrVkHL161bF5LKAQAANEaDgs64ceNCXI2mx0U9AQBwPg4v5/ByAABanLp+fzeoR6fC2rVrtWXLFklS3759NXDgwMasDgAAIKQaFHTy8vJ01VVX6d1331VycrIk6dChQzr33HO1YMECdejQIZR1BAAAaJAGHXV1yy236PDhw/riiy904MABHThwQJs2bVJBQYFuvfXWUNcRAACgQRo0RicpKUnLly+vdp2o1atX68ILL9ShQ4dCVb+wY4wOAAAtT1gvAeHz+RQdHV1tfnR0tHw+X0NWCQAAEHINCjrnnXeebrvtNu3Zsycwb/fu3fr5z3+u888/P2SVAwAAaIwGBZ05c+aooKBA3bt3V8+ePdWzZ0/16NFDBQUFevbZZ0Ndx7DweDzKzMystvsNAAA4R4PPo2OM0fLly7V161ZJUp8+fTRq1KiQVq4pMEYHAICWJ2zn0SktLVV8fLzWr1+vCy64QBdccEGjKgoAABAuXL0cAAA4FlcvBwAAjsXVywEAgGNF7NXLAQCA89U76JSVlcmyLF1//fXq0qVLOOoEAAAQEvUeoxMVFaUnnnhCZWVl4agPAABAyDT4zMjvvfdeqOsCAAAQUg0aozNmzBjdfffd2rhxowYNGlRtMPIll1wSksqFk8fjkcfj4TB5AAAcrEFnRna5au8IsiyrRYUHzowMAEDLE7YzI0viCuUAAKBFqNcYnYsuukj5+fmB6ccee0yHDh0KTP/www/KzMwMWeUAAAAao15BZ+nSpSouLg5MP/roo0FnRy4rK9O2bdtCVzsAAIBGqFfQqTqcp4EXPgcAAGgSDTq8HAAAoCWoV9CxLEuWZVWbBwAA0BzV66grY4wmTZqk2NhYSdKxY8d00003Bc6jU3n8DgAAgN3qFXRycnKCpq+55ppqZa677rrG1QgAACBE6hV05s2bF656AAAAhByDkQEAgGNFbNDxeDzKzMxUVlaW3VUBAABh0qBrXTkJ17oCAKDlqev3d8T26AAAAOcj6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMci6AAAAMeK2KDj8XiUmZmprKwsu6sCAADCxDLGGLsrYae6XuYdAAA0H3X9/o7YHh0AAOB8BB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYBB0AAOBYERt0PB6PMjMzlZWVZXdVAABAmFjGGGN3JexUUFCgpKQk5efnKzEx0e7qAACAOqjr93fE9ugAAADnI+gAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHIugAAADHckTQef3119W7d2/16tVLf/zjH+2uDgAAaCai7K5AY5WVlWn69Ol65513lJSUpEGDBmn8+PFq37693VUDAAA2a/E9OqtXr1bfvn3VuXNntW7dWmPGjNG///1vu6sFAACaAduDzvvvv6+xY8cqPT1dlmVp8eLF1cp4PB51795dcXFxGjp0qFavXh1YtmfPHnXu3Dkw3blzZ+3evbspqg4AAJo524NOUVGR+vfvL4/HU+PyhQsXavr06XrggQe0bt069e/fX9nZ2crLy2vQ9oqLi1VQUBB0AwAAzmR70BkzZox+85vfaPz48TUuf+qpp3TjjTdq8uTJyszM1PPPP6+EhATNnTtXkpSenh7Ug7N7926lp6fXur2ZM2cqKSkpcMvIyAjtCwIAAM2G7UHnREpKSrR27VqNGjUqMM/lcmnUqFFatWqVJGnIkCHatGmTdu/ercLCQr311lvKzs6udZ333HOP8vPzA7ddu3aF/XUAAAB7NOujrvbv3y+v16vU1NSg+ampqdq6daskKSoqSk8++aTOPfdc+Xw+3XXXXSc84io2NlaxsbFhrTcAAGgemnXQqatLLrlEl1xyid3VAAAAzUyzDjopKSlyu93at29f0Px9+/YpLS3NplrVzdqdB3Ss1Kczu7ZVfIzb7uoAABCRmvUYnZiYGA0aNEgrVqwIzPP5fFqxYoWGDRtmY81O7vr5n2riHz/RnvyjdlcFAICIZXuPTmFhobZv3x6Y3rFjh9avX6927dqpa9eumj59unJycjR48GANGTJEs2fPVlFRkSZPntyo7Xo8Hnk8Hnm93sa+hBpFu/0ZstTrC8v6AQDAydkedD799FOde+65genp06dLknJycjR//nxdeeWV+s9//qP7779fubm5GjBggJYsWVJtgHJ9TZ06VVOnTlVBQYGSkpIata6axLgtSVJpmQn5ugEAQN3YHnRGjhwpY04cBqZNm6Zp06Y1UY1CIzrK36NTQo8OAAC2adZjdFoydl0BAGA/gk6YEHQAALAfQSdMoivG6BB0AACwTcQGHY/Ho8zMTGVlZYVl/RU9OiUMRgYAwDYRG3SmTp2qzZs3a82aNWFZPz06AADYL2KDTrgxRgcAAPsRdMIkhqADAIDtCDphEhij42WMDgAAdiHohEnFCQPL6NEBAMA2BJ0wYTAyAAD2i9igE+7Dy4+P0WHXFQAAdonYoBP+w8srzqNDjw4AAHaJ2KATbhxeDgCA/Qg6YRIdxRgdAADsRtAJE8boAABgP4JOmBw/jw49OgAA2IWgEyaBMToMRgYAwDYEnTDhPDoAANgvYoNO2M+jE8UYHQAA7BaxQSfc59GJcjFGBwAAu0Vs0Ak3dl0BAGA/gk6YHN91RdABAMAuBJ0wOX7UFWN0AACwC0EnTDiPDgAA9iPohAljdAAAsB9BJ0xiuKgnAAC2I+iESTTn0QEAwHYEnTCJpkcHAADbRWzQCfeZkRmjAwCA/SI26IT7zMjHx+iw6woAALtEbNAJN65eDgCA/Qg6YVIxGJnz6AAAYB+CTpgwRgcAAPsRdMKkYoyOz0heH+N0AACwA0EnTCrG6Ej06gAAYBeCTphUDjrFDEgGAMAWBJ0wiXZbcrv843SOlXptrg0AAJGJoBMmlmUpIdotSSoqLrO5NgAARCaCThglxPqDzpESenQAALBDxAadcF8CQpISYqIkSUfZdQUAgC0iNuiE+xIQkhTPrisAAGwVsUGnKbQq33V1lF1XAADYgqATRvHlu66KCDoAANiCoBNGrWIqenTYdQUAgB0IOmEUH8NRVwAA2ImgE0atKnZdMRgZAABbEHTCqG1CtCTpwJESm2sCAEBkIuiEUUqbWEnS/sMEHQAA7EDQCaMOrf1B5z+FxTbXBACAyETQCaNAjw5BBwAAWxB0wqiiR2dfwTH5fMbm2gAAEHkIOmHUuW284qJdOlbq044fiuyuDgAAEYegE0bRbpf6pidJktZ/d8jeygAAEIEIOmGW1b2dJGnl1/ttrgkAAJEnYoOOx+NRZmamsrKywrqdn5yWIklauZ2gAwBAU4vYoDN16lRt3rxZa9asCet2BmQky+2ytK+gWHvzj4Z1WwAAIFjEBp2mkhATpdNS20iSNuw6ZG9lAACIMASdJjAgI1mS9BlBBwCAJkXQaQIDy4MOPTrV+XxGX+zJV3FZ6K7wvuvAEc1buUOPvLFZ73/5HxnDOYwAIFJF2V2BSNC/POhs/D5fXp+R22XZW6Fmwhij//3rWi3bvE99OiXqnzcPV3yMu0HrOlJSpj99sEP/2rBH2/MKA/P/8MEOje6bpiev6K9WsbzdASDS0KPTBE7t2FqtYtwqKvFqW+5hu6vTbPxr/R4t27xPkrRlb4FeeP+bBq1nx/4ijfd8pCeXfanteYVyuywN7dFOlw5IV7Tb0pIvcnXlC6uUd/hYKKsPAGgB+InbBNwuS0NPaa+3t+Zp2eZ9ykxPtLtKtis4VqrfvLFFktSnU6K27C3Qc+9t12VndlZGu4STPt8Yo/2FJVq2eZ9mvrlFh4vLlNI6VjNG99aFfdOUFB8tSZo0vLtu+POn2rS7QJf97iP9+foh6tmhdVhfG3AiPp/RN/uLtH7XIa3fdVCffXdIR0u96t8lWVdlZWhIj3ayLHp9gVCxTIQPYCgoKFBSUpLy8/OVmBi+APKPtd/rF4s2qHVslKb85BSlJsYqPiZKMW6XYqIsxbjdinZbio5ylc9zKdrtUrTbUkz5PJfLktuy5LIsuVyS27Lkdlkt7kMx/2ipfvnqRr3x+V6dktJKb91+tnLmrtbH3xzQkO7t9OAlfdUxMVY+n1FxmU+FxWXKLTimPYeO6qt9hdqWe1jb9h3WgaKSwDoHd2srz8QzlZoYV2173+4vUs681dr5wxG1iY3SpQPTNahbW6W2iVNcjFtxUW5FuS25LMmyLFmSv40tS5YluVyV5/nL1HTvqvR/47L8/1dW+XI4mzFGxkg+Y+QzUpnPp6Jir46WeHXwSIm+P3hUW3MLtH7XIW3YdUgFx8pqXVff9ERdMThDA7smKy0pTu1bxbK7G6hBXb+/CTpNFHTKvD5d8ftVWheGS0FY1vEv1ooA5LIsyf+vvIz/S1fyz6v48rXKn1+xxAp6jmSVT1lV1iVJLpd/ecX2K9ZVOSwEtmlZKinzf/DvO1wcGKv0lxuGaHjPFG3PO6z/evZDHSv11et190hppf8Z0lU5w7sr2l37ntgfCov10xc/1Wc2XIojEIJc/kB0PKxaNS9z+QNs4HF5O7rL51cErqpqClRV59SUuayqpWosU9P2TrKeGsoYIxmZ8ntJVaaNMeX3FcsrTxv5fMfL1bS+iuerxvX5n68a5h9/vv/JPuMv4ysPL6aWe1+l9dRHXLRL/TonaUBGsgZ2batWsVFasilXr372fY1/AxU/dqLdlqIr/RCqHLAr/vaOz6sewq3A+63i77OO5a0ayruql5dU6XnHP3cqfz5UPFaVzwlL/h8VqlLuRL8TanrPBZY1IBue6EdJ1fdc5TpU/pwLfA6WT1f7265ouxqeV7kt/MusoHUd/4wtf+0V7RZ4XqXP8CrTVg3rrvzjrvLzVWW66rqD11Xz8yv+Jir+vjLaJYQ8sBN06qipgo7kHzD797Xfa93Ogzp8rExFJWUq9RqVen0qKfOpxOsLPC71msC8krK6f/m3FKelttYvL+qjkb07Buat33VITy37Uut2HlRhcZlclv8DPiEmSqmJceqUFKdTO7bWaaltdHpaG/Xs0Lpeg5e9PqMPt+/Xv7/I1Vd5hTpYVKJjZV4dLfGVf6nV7YuNC9GjrhJi3GoTF6UubRPUI6WV+mcka2BGsnqntakxmB8sKtErn+7Sh9v3a8vewzpQVMz7DY6w4f4LlZQQHdJ1EnTqqCmDTkMZY1TmM/L6jn/Ren1GPp+R1/jvfUaBxxXlTOD5Uvnv40opu3rirlhuqpRVDWUrfi37gn4BHw8FgXIV8yTFuF1qHRulDm1ilZZUfRdTZT6fCfy6a46q7qqoeN3eijDkOz6/6v9PbeUq/t+O3/v/n43x/z9Xnl/1z7amP+Lqf9nVS1UtU7f1BP+irb1M1fWYGn/91fXXq1XLr8+6/nqt1vN4gl+vwT0cwb+2K36hW6reC1JRx2i3pbgod6Pfw16f0aEjJSou8/8IKvX6yh/7fyBVvH8q9z5V/hv0+Y6/R2sqE/z+rby8vLwv+P190vKBcsd7zSqvo+J9YFT5s6JST5ov+DPGV9Mbq6Y3V82zAtur4ypq7ZkzUqX3zfH3YcWyqj2RlXsWq/9t1/w5WePjGtZdtZez8uesqs6vUpfG9HJWXZ/PV/t6A39X5W1kWZZWzjjPtqDDYOQWoOKDM7phR163SM055EiVuuRP0HUONJbbZal961i7qwG0aBxeDgAAHIugAwAAHIugAwAAHIugAwAAHIugAwAAHIugAwAAHIugAwAAHIugAwAAHIugAwAAHCtig47H41FmZqaysrLsrgoAAAgTrnXVAq51BQAAgtX1+ztie3QAAIDzEXQAAIBjEXQAAIBjRdldAbtVDFEqKCiwuSYAAKCuKr63TzbUOOKDzuHDhyVJGRkZNtcEAADU1+HDh5WUlFTr8og/6srn82nPnj1q06aNLMsK2XoLCgqUkZGhXbt2cTRXGNHOTYe2bhq0c9OgnZtGONvZGKPDhw8rPT1dLlftI3EivkfH5XKpS5cuYVt/YmIif0RNgHZuOrR106Cdmwbt3DTC1c4n6smpwGBkAADgWAQdAADgWASdMImNjdUDDzyg2NhYu6viaLRz06Gtmwbt3DRo56bRHNo54gcjAwAA56JHBwAAOBZBBwAAOBZBBwAAOBZBBwAAOBZBJ0w8Ho+6d++uuLg4DR06VKtXr7a7Si3GzJkzlZWVpTZt2qhjx44aN26ctm3bFlTm2LFjmjp1qtq3b6/WrVtrwoQJ2rdvX1CZ7777ThdffLESEhLUsWNH3XnnnSorK2vKl9KiPPbYY7IsS7fffntgHu0cOrt379Y111yj9u3bKz4+Xv369dOnn34aWG6M0f33369OnTopPj5eo0aN0ldffRW0jgMHDmjixIlKTExUcnKybrjhBhUWFjb1S2m2vF6v7rvvPvXo0UPx8fHq2bOnfv3rXwddC4l2rr/3339fY8eOVXp6uizL0uLFi4OWh6pNP//8c5199tmKi4tTRkaGHn/88dC8AIOQW7BggYmJiTFz5841X3zxhbnxxhtNcnKy2bdvn91VaxGys7PNvHnzzKZNm8z69evNRRddZLp27WoKCwsDZW666SaTkZFhVqxYYT799FPz4x//2AwfPjywvKyszPzoRz8yo0aNMp999pl58803TUpKirnnnnvseEnN3urVq0337t3NGWecYW677bbAfNo5NA4cOGC6detmJk2aZD755BPzzTffmKVLl5rt27cHyjz22GMmKSnJLF682GzYsMFccsklpkePHubo0aOBMqNHjzb9+/c3H3/8sfnggw/Mqaeeaq6++mo7XlKz9Mgjj5j27dub119/3ezYscMsWrTItG7d2jz99NOBMrRz/b355pvm3nvvNf/85z+NJPPqq68GLQ9Fm+bn55vU1FQzceJEs2nTJvPyyy+b+Ph48/vf/77R9SfohMGQIUPM1KlTA9Ner9ekp6ebmTNn2lirlisvL89IMu+9954xxphDhw6Z6Ohos2jRokCZLVu2GElm1apVxhj/H6bL5TK5ubmBMs8995xJTEw0xcXFTfsCmrnDhw+bXr16mWXLlplzzjknEHRo59CZMWOGOeuss2pd7vP5TFpamnniiScC8w4dOmRiY2PNyy+/bIwxZvPmzUaSWbNmTaDMW2+9ZSzLMrt37w5f5VuQiy++2Fx//fVB8y677DIzceJEYwztHApVg06o2vR3v/udadu2bdDnxowZM0zv3r0bXWd2XYVYSUmJ1q5dq1GjRgXmuVwujRo1SqtWrbKxZi1Xfn6+JKldu3aSpLVr16q0tDSojU8//XR17do10MarVq1Sv379lJqaGiiTnZ2tgoICffHFF01Y++Zv6tSpuvjii4PaU6KdQ+m1117T4MGD9d///d/q2LGjBg4cqD/84Q+B5Tt27FBubm5QWyclJWno0KFBbZ2cnKzBgwcHyowaNUoul0uffPJJ072YZmz48OFasWKFvvzyS0nShg0b9OGHH2rMmDGSaOdwCFWbrlq1Sj/5yU8UExMTKJOdna1t27bp4MGDjapjxF/UM9T2798vr9cb9MEvSampqdq6datNtWq5fD6fbr/9do0YMUI/+tGPJEm5ubmKiYlRcnJyUNnU1FTl5uYGytT0f1CxDH4LFizQunXrtGbNmmrLaOfQ+eabb/Tcc89p+vTp+uUvf6k1a9bo1ltvVUxMjHJycgJtVVNbVm7rjh07Bi2PiopSu3btaOtyd999twoKCnT66afL7XbL6/XqkUce0cSJEyWJdg6DULVpbm6uevToUW0dFcvatm3b4DoSdNCsTZ06VZs2bdKHH35od1UcZ9euXbrtttu0bNkyxcXF2V0dR/P5fBo8eLAeffRRSdLAgQO1adMmPf/888rJybG5ds7xyiuv6KWXXtLf/vY39e3bV+vXr9ftt9+u9PR02jmCsesqxFJSUuR2u6sdmbJv3z6lpaXZVKuWadq0aXr99df1zjvvqEuXLoH5aWlpKikp0aFDh4LKV27jtLS0Gv8PKpbBv2sqLy9PZ555pqKiohQVFaX33ntPzzzzjKKiopSamko7h0inTp2UmZkZNK9Pnz767rvvJB1vqxN9bqSlpSkvLy9oeVlZmQ4cOEBbl7vzzjt1991366qrrlK/fv107bXX6uc//7lmzpwpiXYOh1C1aTg/Swg6IRYTE6NBgwZpxYoVgXk+n08rVqzQsGHDbKxZy2GM0bRp0/Tqq6/q7bffrtadOWjQIEVHRwe18bZt2/Tdd98F2njYsGHauHFj0B/XsmXLlJiYWO0LJ1Kdf/752rhxo9avXx+4DR48WBMnTgw8pp1DY8SIEdVOkfDll1+qW7dukqQePXooLS0tqK0LCgr0ySefBLX1oUOHtHbt2kCZt99+Wz6fT0OHDm2CV9H8HTlyRC5X8Nea2+2Wz+eTRDuHQ6jadNiwYXr//fdVWloaKLNs2TL17t27UbutJHF4eTgsWLDAxMbGmvnz55vNmzebKVOmmOTk5KAjU1C7m2++2SQlJZl3333X7N27N3A7cuRIoMxNN91kunbtat5++23z6aefmmHDhplhw4YFllcc9nzhhRea9evXmyVLlpgOHTpw2PNJVD7qyhjaOVRWr15toqKizCOPPGK++uor89JLL5mEhATz17/+NVDmscceM8nJyeZf//qX+fzzz82ll15a4yG6AwcONJ988on58MMPTa9evSL6sOeqcnJyTOfOnQOHl//zn/80KSkp5q677gqUoZ3r7/Dhw+azzz4zn332mZFknnrqKfPZZ5+ZnTt3GmNC06aHDh0yqamp5tprrzWbNm0yCxYsMAkJCRxe3pw9++yzpmvXriYmJsYMGTLEfPzxx3ZXqcWQVONt3rx5gTJHjx41P/vZz0zbtm1NQkKCGT9+vNm7d2/Qer799lszZswYEx8fb1JSUswvfvELU1pa2sSvpmWpGnRo59D5f//v/5kf/ehHJjY21px++unmhRdeCFru8/nMfffdZ1JTU01sbKw5//zzzbZt24LK/PDDD+bqq682rVu3NomJiWby5Mnm8OHDTfkymrWCggJz2223ma5du5q4uDhzyimnmHvvvTfokGXauf7eeeedGj+Tc3JyjDGha9MNGzaYs846y8TGxprOnTubxx57LCT1t4ypdMpIAAAAB2GMDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDgAAcCyCDoCIZ1mWFi9ebHc1AIQBQQeArSZNmiTLsqrdRo8ebXfVADhAlN0VAIDRo0dr3rx5QfNiY2Ntqg0AJ6FHB4DtYmNjlZaWFnRr27atJP9upeeee05jxoxRfHy8TjnlFP39738Pev7GjRt13nnnKT4+Xu3bt9eUKVNUWFgYVGbu3Lnq27evYmNj1alTJ02bNi1o+f79+zV+/HglJCSoV69eeu211wLLDh48qIkTJ6pDhw6Kj49Xr169qgUzAM0TQQdAs3ffffdpwoQJ2rBhgyZOnKirrrpKW7ZskSQVFRUpOztbbdu21Zo1a7Ro0SItX748KMg899xzmjp1qqZMmaKNGzfqtdde06mnnhq0jYceekhXXHGFPv/8c1100UWaOHGiDhw4ENj+5s2b9dZbb2nLli167rnnlJKS0nQNAKDhQnINdABooJycHON2u02rVq2Cbo888ogxxhhJ5qabbgp6ztChQ83NN99sjDHmhRdeMG3btjWFhYWB5W+88YZxuVwmNzfXGGNMenq6uffee2utgyTzq1/9KjBdWFhoJJm33nrLGGPM2LFjzeTJk0PzggE0KcboALDdueeeq+eeey5oXrt27QKPhw0bFrRs2LBhWr9+vSRpy5Yt6t+/v1q1ahVYPmLECPl8Pm3btk2WZWnPnj06//zzT1iHM844I/C4VatWSkxMVF5eniTp5ptv1oQJE7Ru3TpdeOGFGjdunIYPH96g1wqgaRF0ANiuVatW1XYlhUp8fHydykVHRwdNW5Yln88nSRozZox27typN998U8uWLdP555+vqVOnatasWSGvL4DQYowOgGbv448/rjbdp08fSVKfPn20YcMGFRUVBZavXLlSLpdLvXv3Vps2bdS9e3etWLGiUXXo0KGDcnJy9Ne//lWzZ8/WCy+80Kj1AWga9OgAsF1xcbFyc3OD5kVFRQUG/C5atEiDBw/WWWedpZdeekmrV6/Wn/70J0nSxIkT9cADDygnJ0cPPvig/vOf/+iWW27Rtddeq9TUVEnSgw8+qJtuukkdO3bUmDFjdPjwYa1cuVK33HJLnep3//33a9CgQerbt6+Ki4v1+uuvB4IWgOaNoAPAdkuWLFGnTp2C5vXu3Vtbt26V5D8iasGCBfrZz36mTp066eWXX1ZmZqYkKSEhQUuXLtVtt92mrKwsJSQkaMKECXrqqacC68rJydGxY8f0f//3f7rjjjuUkpKiyy+/vM71i4mJ0T333KNvv/1W8fHxOvvss7VgwYIQvHIA4WYZY4zdlQCA2liWpVdffVXjxo2zuyoAWiDG6AAAAMci6AAAAMdijA6AZo296wAagx4dAADgWAQdAADgWAQdAADgWAQdAADgWAQdAADgWAQdAADgWAQdAADgWAQdAADgWP8fDBLyTQj9+McAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x_tot = np.random.rand(1000, 15)\n",
        "target = np.random.rand(1000, 3)\n",
        "\n",
        "layer_one = Layer(15, 8, ELU, d_ELU)\n",
        "layer_two = Layer(8, 5, ELU, d_ELU)\n",
        "layer_out = Layer(5, 3, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "NN.add_layer(layer_one)\n",
        "NN.add_layer(layer_two)\n",
        "NN.add_layer(layer_out)\n",
        "\n",
        "# Parametri di training\n",
        "K = 5\n",
        "epochs = 1000\n",
        "learning_rate_w = 0.001 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
        "                         # learning_rate of minibatch for number of the examples in the minibatch\n",
        "learning_rate_b = 0.001\n",
        "batch_size = 100\n",
        "Lambda_t = 0.1\n",
        "Lambda_l = 0.1\n",
        "momentum = 0.7 # totalmente a caso\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon = 1e-8\n",
        "\n",
        "# Cross-validation\n",
        "train_error, val_error = NN.train_val(x_tot, target, K, epochs, learning_rate_w, learning_rate_b, Lambda_t, Lambda_l, momentum, mean_squared_error, d_mean_squared_error, batch_size, beta_1, beta_2, epsilon, 'elastic', 'adam')\n",
        "\n",
        "# Plot degli errori\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_error, label='Training Error')\n",
        "plt.plot(val_error, label='Validation Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Error')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}