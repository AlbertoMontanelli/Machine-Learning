{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT3nSmTsZ9NP"
      },
      "source": [
        "# Notation conventions\n",
        "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
        "* **number of examples** = $l$ ;\n",
        "* **number of features** = $n$ ;\n",
        "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
        "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
        "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
        "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
        "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
        "\n",
        "### Input Layer $L_0$ with $k_0$ units :\n",
        "* input_size = $n$;\n",
        "* output_size = $k_0$;\n",
        "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
        "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
        "\n",
        "### Generic Layer $L_i$ with $k_i$ units :\n",
        "* input_size = $k_{i-1}$ ;\n",
        "* output_size = $k_i$ ;\n",
        "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
        "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
        "\n",
        "### Online vs mini-batch version:\n",
        "* online version: $l' = 1$ example;\n",
        "* mini-batch version: $l' =$ number of examples in the mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7La5v5wwHVP"
      },
      "source": [
        "# Activation functions\n",
        "Definition of the activation functions and their derivatives.\n",
        "* **Hidden layers**:\n",
        "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
        "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
        "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
        "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
        "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
        "* **Output layer**:\n",
        "  * **Regression problem**: Linear output;\n",
        "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qv1wwlgWsFM8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
        "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
        "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
        "\n",
        "def sigmoid(net):\n",
        "    return 1 / (1 + np.exp(-net))\n",
        "\n",
        "def d_sigmoid(net):\n",
        "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
        "\n",
        "def tanh(net):\n",
        "    return np.tanh(net)\n",
        "\n",
        "def d_tanh(net):\n",
        "    return 1 - (np.tanh(net))**2\n",
        "\n",
        "\"\"\"   DA RIVEDERE\n",
        "\n",
        "def softmax(net):\n",
        "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
        "\n",
        "def softmax_derivative(net):\n",
        "\n",
        "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
        "    batch_size, current_neuron_size = net.shape\n",
        "\n",
        "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
        "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
        "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
        "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
        "\n",
        "    for i in range(batch_size): # for each example i in the batch\n",
        "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
        "                                   # the example i\n",
        "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    return jacobians\n",
        "\"\"\"\n",
        "\n",
        "def softplus(net):\n",
        "    return np.log(1 + np.exp(net))\n",
        "\n",
        "def d_softplus(net):\n",
        "    return np.exp(net) / (1 + np.exp(net))\n",
        "\n",
        "def linear(net):\n",
        "    return net\n",
        "\n",
        "def d_linear(net):\n",
        "    return 1\n",
        "\n",
        "def ReLU(net):\n",
        "    return np.maximum(net, 0)\n",
        "\n",
        "def d_ReLU(net):\n",
        "    return 1 if(net>=0) else 0\n",
        "\n",
        "def leaky_relu(net, alpha):\n",
        "    return np.maximum(net, alpha*net)\n",
        "\n",
        "def d_leaky_relu(net, alpha):\n",
        "    return 1 if(net>=0) else alpha\n",
        "\n",
        "def ELU(net):\n",
        "    return net if(net>=0) else np.exp(net)-1\n",
        "\n",
        "def d_ELU(net):\n",
        "    return 1 if(net>=0) else np.exp(net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjNvxQOLsFM-"
      },
      "source": [
        "# Loss/Error functions:\n",
        "Definition of loss/error functions and their derivatives. \\\n",
        "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
        "* **mean_squared_error**;\n",
        "* **mean_euclidian_error**;\n",
        "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8Q7LVpTswQAh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2)\n",
        "\n",
        "def d_mean_squared_error(y_true, y_pred):\n",
        "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
        "\n",
        "def mean_euclidian_error(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
        "\n",
        "def d_mean_euclidian_error(y_true, y_pred):\n",
        "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
        "\n",
        "def huber_loss(y_true, y_pred, delta):\n",
        "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
        "\n",
        "def d_huber_loss(y_true, y_pred, delta):\n",
        "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAFcEjML9we1"
      },
      "source": [
        "#  class Layer\n",
        "**Constructor parameters :**\n",
        " * input_size : $k_{i-1}$ ;\n",
        " * output_size : $k_i$ ;\n",
        " * activation_function ;\n",
        " * activation_derivative . \\\\\n",
        "\n",
        "**Constructor attributes :**\n",
        "* self.weights : $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix . \\\\\n",
        "Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$ ;\n",
        "* self.biases : $1 \\ \\textrm{x} \\ k_i$ array. Initialized to zeros;\n",
        "* self.activation_function;\n",
        "* self.activation_derivative .\n",
        "\n",
        "**Methods :**\n",
        "* forward_layer : allows to compute the output of the layer for a given input.\n",
        " * parameter :\n",
        "   * input_array : matrix $X$ (see above for the case $L_0$ or $L_i$) .\n",
        " * attributes :\n",
        "   * self.input : input_array ;\n",
        "   * self.net : net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$) .\n",
        " * return -> output = $f(net)$, where $f$ is the activation function; $f(net)$ has the same dimensions of $net$.\n",
        "* backward_layer : computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
        " * parameters :\n",
        "   * d_Ep : target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
        "   * learning_rate.\n",
        " * return -> sum_delta_weights $= \\delta \\cdot W^T$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "H8Ap9rLxLlNU",
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
        "        self.weights = np.random.uniform(low=-1/np.sqrt(input_size), high=1/np.sqrt(input_size), size=(input_size, output_size))\n",
        "        self.biases = np.zeros((1, output_size))\n",
        "        self.activation_function = activation_function\n",
        "        self.activation_derivative = activation_derivative\n",
        "\n",
        "\n",
        "    def forward_layer(self, input_array):\n",
        "        self.input = input_array\n",
        "        self.net = np.dot(self.input, self.weights) + self.biases\n",
        "        output = self.activation_function(self.net)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def backward_layer(self, d_Ep, learning_rate):\n",
        "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
        "        self.weights += learning_rate * np.dot(self.input.T, delta) # learning rule for the weights\n",
        "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
        "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
        "        return sum_delta_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rQI2lNkn83H"
      },
      "source": [
        "# class NeuralNetwork\n",
        "**Constructor attributes**:\n",
        " * self.layers: an empty list that will contain the layers.\n",
        "\n",
        "**Methods**:\n",
        " * data_split: splits the input data into training set, validation set and test set\n",
        "  * parameter:\n",
        "    * x_tot: total data given as input;\n",
        "    * K: number K of K-folds used in K-folds cross validation;\n",
        "    * step_cycle: number of the step in the permutation cycle of the K-fold validation process.\n",
        "  * attributes:\n",
        "    * self.x_train: training set;\n",
        "    * self.x_val: validation set;\n",
        "    * self.x_test: test set.\n",
        " * add_layer: appends a layer to the empty list self.layers\n",
        "  * parameter:\n",
        "    * layer: the layer appended to the list self.layers.\n",
        "\n",
        "* forward: iterates the layer.forward_layer method through each layer in the list self.layers\n",
        " * parameter:\n",
        "   * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
        " * return -> input = $o_i$ for layer $L_i$.\n",
        "* backward: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
        " * parameter:\n",
        "   * d_Ep;\n",
        "   * learning_rate.\n",
        "\n",
        "* train_online: applies the forward and backward method to the network for a specified number of epochs **one example at a time**.\n",
        " * parameter:\n",
        "   * x_train: input matrix $X$;\n",
        "   * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
        "   * epochs: number of the iterations of the training algorithm;\n",
        "   * learning_rate;\n",
        "   * loss_function;\n",
        "   * loss_function_derivative.\n",
        "\n",
        "* train_minibatch: applies the forward and backward method to the network for a specified number of epochs **to batches of $l' < l$** examples.\n",
        " * parameter:\n",
        "    * x_train: input matrix $X$;\n",
        "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
        "    * epochs: number of the iterations of the training algorithm;\n",
        "    * learning_rate;\n",
        "    * loss_function;\n",
        "    * loss_function_derivative;\n",
        "    * batch_size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EmtKxOEhn91Q",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def data_split(self, x_tot, target, K, step_cycle):\n",
        "        '''# randomization of the input matrix\n",
        "        indices = np.arange(num_samples) # creates an array from 0 to num_samples - 1\n",
        "        np.random.shuffle(indices) # shuffling the indices\n",
        "        x_tot = x_tot[indices] # re-ordering of the rows according to the new indices'''\n",
        "\n",
        "        # splitting of the data batch into x_train_val and x_test\n",
        "        num_samples = x_tot.shape[0]\n",
        "        x_train_val = x_tot[:int(0.8 * num_samples)] # training set and validation set make up 80% of the original data set\n",
        "        target_train_val = target[:int(0.8 * num_samples)]\n",
        "        self.x_test = x_tot[int(0.8 * num_samples):] # test set makes up 20% of the original data set\n",
        "        self.target_test = target[int(0.8 * num_samples):]\n",
        "\n",
        "        # splitting of the x_train_val batch into x_train and x_val\n",
        "        if K == 1: # hold-out cross validation\n",
        "            self.x_train = x_train_val[:int(0.75 * x_train_val.shape[0])] # training set makes up 60% of the original data set\n",
        "            self.target_train = target_train_val[:int(0.75 * target_train_val.shape[0])]\n",
        "            self.x_val = x_train_val[int(0.75 * x_train_val.shape[0]):] # validation set makes up 20% of the original data\n",
        "            self.target_val = target_train_val[int(0.75 * target_train_val.shape[0]):]\n",
        "        else: # K-fold cross validation\n",
        "            fold_size = int(x_train_val.shape[0] / K) # number of rows per fold\n",
        "            self.x_train = np.concatenate([x_train_val[:step_cycle*fold_size], x_train_val[(step_cycle+1)*fold_size:]]) # training set\n",
        "            self.target_train = np.concatenate([target_train_val[:step_cycle*fold_size], target_train_val[(step_cycle+1)*fold_size:]]) \n",
        "            self.x_val = x_train_val[step_cycle*fold_size:(step_cycle+1)*fold_size] # validation set\n",
        "            self.target_val = target_train_val[step_cycle*fold_size:(step_cycle+1)*fold_size]\n",
        "\n",
        "    def forward(self, input):\n",
        "        for layer in self.layers:\n",
        "            input = layer.forward_layer(input)\n",
        "        return input\n",
        "\n",
        "    def backward(self, d_Ep, learning_rate):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_Ep = layer.backward_layer(d_Ep, learning_rate)\n",
        "\n",
        "    # i pesi vanno rinizializzati ??????????\n",
        "    def train_online(self, x_tot, target, epochs, learning_rate, loss_function, loss_function_derivative, K):\n",
        "        val_error_epoch = np.zeros(epochs)\n",
        "        train_error_epoch = np.zeros(epochs)\n",
        "        \n",
        "        for i in range (K):\n",
        "            self.data_split(x_tot, target, K, i)\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                epoch_loss = 0\n",
        "                epoch_error = 0\n",
        "\n",
        "                for x_train_row, target_train_row in zip(self.x_train, self.target_train):\n",
        "\n",
        "                    x_train_row = x_train_row.reshape(1, -1)\n",
        "                    target_train_row = target_train_row.reshape(1, -1)\n",
        "\n",
        "                    # Forward propagation\n",
        "                    predictions = self.forward(x_train_row) # predictions = output of the output layer\n",
        "\n",
        "                    # Compute loss and loss gradient for backward function\n",
        "                    loss = loss_function(target_train_row, predictions)\n",
        "                    loss_gradient = loss_function_derivative(target_train_row, predictions)\n",
        "                    epoch_loss += loss  # accumulates the losses for each example\n",
        "\n",
        "                    # Backward propagation\n",
        "                    self.backward(loss_gradient, learning_rate)\n",
        "\n",
        "                for x_val_row, target_val_row in zip(self.x_val, self.target_val):\n",
        "\n",
        "                    x_val_row = x_val_row.reshape(1, -1)\n",
        "                    target_val_row = target_val_row.reshape(1, -1)\n",
        "\n",
        "                    # Forward propagation\n",
        "                    out_val = self.forward(self.x_val) # out_val = output of the output layer with x_val as input to the input layer\n",
        "\n",
        "                    # Computation of the error for validation set for the single example\n",
        "                    error = loss_function(target_val_row, out_val)\n",
        "                    epoch_error += error  # accumulates the losses for each example\n",
        "                \n",
        "                # computation of the loss and error averaged on examples per epoch\n",
        "                average_epoch_tr_error = epoch_loss / len(self.x_train)\n",
        "                average_epoch_val_error = epoch_error / len(self.x_val)\n",
        "\n",
        "                print(f\"ONLINE: Permutation #{i+1}, Epoch #{epoch}, Training error: {average_epoch_tr_error}, Validation error: {average_epoch_val_error}\")\n",
        "                \n",
        "                train_error_epoch[epoch] += average_epoch_tr_error\n",
        "                val_error_epoch[epoch] += average_epoch_val_error\n",
        "\n",
        "            # computation of the error averaged on K-fold\n",
        "            train_error_epoch = train_error_epoch/K\n",
        "            val_error_epoch = val_error_epoch/K\n",
        "            # c'è da fare un bel grafico sulle epoch\n",
        "            # si fa anche una std dev ? \n",
        "\n",
        "    #da sistemare !!\n",
        "    def train_minibatch(self, x_tot, target, epochs, learning_rate, loss_function, loss_function_derivative, batch_size, K):\n",
        "        for i in range (K):\n",
        "            self.data_split(x_tot, K, i)\n",
        "            num_samples = self.x_train.shape[0] # selection of the number of rows\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                epoch_loss = 0\n",
        "\n",
        "                # the rows of the input matrix are randomized in order to have different examples in each mini-batch for each epoch\n",
        "                indices = np.arange(num_samples) # creates an array from 0 to num_samples - 1\n",
        "                np.random.shuffle(indices) # shuffling the indices\n",
        "                self.x_train = self.x_train[indices] # re-ordering of the rows according to the new indices\n",
        "                target = target[indices] # same but for the targets\n",
        "\n",
        "                # process data in batches\n",
        "                for i in range(0, num_samples, batch_size): # even if the last mini-batch does not have size equal to batch_size it is processed anyway\n",
        "                    x_batch = self.x_train[i:i+batch_size]\n",
        "                    target_batch = target[i:i+batch_size]\n",
        "\n",
        "                    # Forward propagation\n",
        "                    predictions = self.forward(x_batch) # predictions = output of the output layer\n",
        "\n",
        "                    # Compute loss and loss gradient for backward function\n",
        "                    loss = loss_function(target_batch, predictions)\n",
        "                    loss_gradient = loss_function_derivative(target_batch, predictions)\n",
        "                    epoch_loss += np.sum(loss)  # accumulates the loss for all the examples in the mini-batch for each mini-batch\n",
        "\n",
        "                    # Backward propagation\n",
        "                    self.backward(loss_gradient, learning_rate)\n",
        "\n",
        "                # computation of the average loss per epoch\n",
        "                average_epoch_loss = epoch_loss / num_samples\n",
        "                print(f\"MINIBATCH: epoch #{epoch}, Average Loss: {average_epoch_loss}\")\n",
        "                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT0LTc5_oFD2"
      },
      "source": [
        "#Unit Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp0g8MgT3hhO",
        "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ONLINE: Permutation #1, Epoch #0, Training error: 0.24238315138729996, Validation error: 1.5699186171625916\n",
            "ONLINE: Permutation #1, Epoch #1, Training error: 0.19668679561431623, Validation error: 1.535590837174169\n",
            "ONLINE: Permutation #1, Epoch #2, Training error: 0.19452355492149367, Validation error: 1.5215890358385329\n",
            "ONLINE: Permutation #1, Epoch #3, Training error: 0.19270042406194346, Validation error: 1.5104522386565984\n",
            "ONLINE: Permutation #1, Epoch #4, Training error: 0.19123119491682836, Validation error: 1.5016437099120397\n",
            "ONLINE: Permutation #1, Epoch #5, Training error: 0.19006715507198285, Validation error: 1.4946570391994405\n",
            "ONLINE: Permutation #1, Epoch #6, Training error: 0.18913859646296566, Validation error: 1.4890708988473689\n",
            "ONLINE: Permutation #1, Epoch #7, Training error: 0.18838977150597974, Validation error: 1.4845691167796924\n",
            "ONLINE: Permutation #1, Epoch #8, Training error: 0.18777968331755143, Validation error: 1.4809162896530836\n",
            "ONLINE: Permutation #1, Epoch #9, Training error: 0.18727823301440355, Validation error: 1.4779348365121283\n",
            "ONLINE: Permutation #1, Epoch #10, Training error: 0.18686297981675332, Validation error: 1.4754886712401016\n",
            "ONLINE: Permutation #1, Epoch #11, Training error: 0.1865168933633019, Validation error: 1.4734720214152046\n",
            "ONLINE: Permutation #1, Epoch #12, Training error: 0.18622683373894935, Validation error: 1.4718016998767303\n",
            "ONLINE: Permutation #1, Epoch #13, Training error: 0.18598251367492802, Validation error: 1.470411653913825\n",
            "ONLINE: Permutation #1, Epoch #14, Training error: 0.18577577716271448, Validation error: 1.4692490363076205\n",
            "ONLINE: Permutation #2, Epoch #0, Training error: 0.1931322276350302, Validation error: 0.9240306530847094\n",
            "ONLINE: Permutation #2, Epoch #1, Training error: 0.19262288691835927, Validation error: 0.9260187732313143\n",
            "ONLINE: Permutation #2, Epoch #2, Training error: 0.19218819727285086, Validation error: 0.9276805795119683\n",
            "ONLINE: Permutation #2, Epoch #3, Training error: 0.19182941671179185, Validation error: 0.9293357332542512\n",
            "ONLINE: Permutation #2, Epoch #4, Training error: 0.19153306354448882, Validation error: 0.9309933049754769\n",
            "ONLINE: Permutation #2, Epoch #5, Training error: 0.1912863486501631, Validation error: 0.9326221279537874\n",
            "ONLINE: Permutation #2, Epoch #6, Training error: 0.19107917252920684, Validation error: 0.9341961099961259\n",
            "ONLINE: Permutation #2, Epoch #7, Training error: 0.19090378436209143, Validation error: 0.9356986153991543\n",
            "ONLINE: Permutation #2, Epoch #8, Training error: 0.19075421543824317, Validation error: 0.9371204518990669\n",
            "ONLINE: Permutation #2, Epoch #9, Training error: 0.19062581937526857, Validation error: 0.9384575589076639\n",
            "ONLINE: Permutation #2, Epoch #10, Training error: 0.19051493492341218, Validation error: 0.9397092607845399\n",
            "ONLINE: Permutation #2, Epoch #11, Training error: 0.19041864278012696, Validation error: 0.9408770514443017\n",
            "ONLINE: Permutation #2, Epoch #12, Training error: 0.19033458957228577, Validation error: 0.9419637690047563\n",
            "ONLINE: Permutation #2, Epoch #13, Training error: 0.1902608592472229, Validation error: 0.9429730414167549\n",
            "ONLINE: Permutation #2, Epoch #14, Training error: 0.19019587801831617, Validation error: 0.9439089174067692\n",
            "ONLINE: Permutation #3, Epoch #0, Training error: 0.18697611357727226, Validation error: 1.5140029228378071\n",
            "ONLINE: Permutation #3, Epoch #1, Training error: 0.18619863087164082, Validation error: 1.5114250169949874\n",
            "ONLINE: Permutation #3, Epoch #2, Training error: 0.18564026922849672, Validation error: 1.5095152221705699\n",
            "ONLINE: Permutation #3, Epoch #3, Training error: 0.18523121611646678, Validation error: 1.5084958241275348\n",
            "ONLINE: Permutation #3, Epoch #4, Training error: 0.18493029750118495, Validation error: 1.5081274494969108\n",
            "ONLINE: Permutation #3, Epoch #5, Training error: 0.1847070952851635, Validation error: 1.5081871106101723\n",
            "ONLINE: Permutation #3, Epoch #6, Training error: 0.1845398667991754, Validation error: 1.5085138595639616\n",
            "ONLINE: Permutation #3, Epoch #7, Training error: 0.18441316361933766, Validation error: 1.5089984250640402\n",
            "ONLINE: Permutation #3, Epoch #8, Training error: 0.18431597916566414, Validation error: 1.509568438166227\n",
            "ONLINE: Permutation #3, Epoch #9, Training error: 0.18424043355222663, Validation error: 1.5101768925372008\n",
            "ONLINE: Permutation #3, Epoch #10, Training error: 0.18418085584576793, Validation error: 1.510793983248762\n",
            "ONLINE: Permutation #3, Epoch #11, Training error: 0.18413314321004798, Validation error: 1.511401471889072\n",
            "ONLINE: Permutation #3, Epoch #12, Training error: 0.18409431132979495, Validation error: 1.5119888224099736\n",
            "ONLINE: Permutation #3, Epoch #13, Training error: 0.18406217744048015, Validation error: 1.512550560521627\n",
            "ONLINE: Permutation #3, Epoch #14, Training error: 0.18403513581022116, Validation error: 1.5130844763375537\n",
            "ONLINE: Permutation #4, Epoch #0, Training error: 0.19145945428032704, Validation error: 0.9061935723565105\n",
            "ONLINE: Permutation #4, Epoch #1, Training error: 0.1910771092709446, Validation error: 0.9078449840610855\n",
            "ONLINE: Permutation #4, Epoch #2, Training error: 0.19093012356599515, Validation error: 0.9082482330717306\n",
            "ONLINE: Permutation #4, Epoch #3, Training error: 0.1908318422839174, Validation error: 0.9084274040541332\n",
            "ONLINE: Permutation #4, Epoch #4, Training error: 0.19075206991156202, Validation error: 0.9085734903339993\n",
            "ONLINE: Permutation #4, Epoch #5, Training error: 0.19068359457769898, Validation error: 0.9087190446302881\n",
            "ONLINE: Permutation #4, Epoch #6, Training error: 0.19062374468017362, Validation error: 0.9088670021512862\n",
            "ONLINE: Permutation #4, Epoch #7, Training error: 0.19057096952330094, Validation error: 0.9090154626993878\n",
            "ONLINE: Permutation #4, Epoch #8, Training error: 0.1905241278803817, Validation error: 0.909162272698941\n",
            "ONLINE: Permutation #4, Epoch #9, Training error: 0.1904823086061839, Validation error: 0.9093057383780211\n",
            "ONLINE: Permutation #4, Epoch #10, Training error: 0.1904447645898514, Validation error: 0.90944463404601\n",
            "ONLINE: Permutation #4, Epoch #11, Training error: 0.19041087615436705, Validation error: 0.909578108021021\n",
            "ONLINE: Permutation #4, Epoch #12, Training error: 0.19038012562928025, Validation error: 0.9097055937352088\n",
            "ONLINE: Permutation #4, Epoch #13, Training error: 0.19035207813938276, Validation error: 0.9098267399649367\n",
            "ONLINE: Permutation #4, Epoch #14, Training error: 0.1903263666345065, Validation error: 0.9099413579332003\n",
            "ONLINE: Permutation #5, Epoch #0, Training error: 0.17245066289610794, Validation error: 2.3413342375993462\n",
            "ONLINE: Permutation #5, Epoch #1, Training error: 0.17248113134637638, Validation error: 2.35109152772369\n",
            "ONLINE: Permutation #5, Epoch #2, Training error: 0.17243284451350857, Validation error: 2.3528777374966197\n",
            "ONLINE: Permutation #5, Epoch #3, Training error: 0.1723654347257479, Validation error: 2.353235124106845\n",
            "ONLINE: Permutation #5, Epoch #4, Training error: 0.17230436165551788, Validation error: 2.353387392996958\n",
            "ONLINE: Permutation #5, Epoch #5, Training error: 0.17225284989005893, Validation error: 2.3535516685175466\n",
            "ONLINE: Permutation #5, Epoch #6, Training error: 0.1722097939604153, Validation error: 2.353755518639026\n",
            "ONLINE: Permutation #5, Epoch #7, Training error: 0.1721736340647636, Validation error: 2.353993396478059\n",
            "ONLINE: Permutation #5, Epoch #8, Training error: 0.17214302356008843, Validation error: 2.3542559772604563\n",
            "ONLINE: Permutation #5, Epoch #9, Training error: 0.1721168846969366, Validation error: 2.354535200547738\n",
            "ONLINE: Permutation #5, Epoch #10, Training error: 0.17209436518691423, Validation error: 2.3548247841420635\n",
            "ONLINE: Permutation #5, Epoch #11, Training error: 0.17207478961051395, Validation error: 2.355119974741561\n",
            "ONLINE: Permutation #5, Epoch #12, Training error: 0.17205761993558935, Validation error: 2.35541723177867\n",
            "ONLINE: Permutation #5, Epoch #13, Training error: 0.17204242517035562, Validation error: 2.355713958780673\n",
            "ONLINE: Permutation #5, Epoch #14, Training error: 0.1720288582128496, Validation error: 2.35600828999795\n",
            "ONLINE: Permutation #6, Epoch #0, Training error: 0.18723644367662723, Validation error: 1.3627647065523698\n",
            "ONLINE: Permutation #6, Epoch #1, Training error: 0.1864273260167949, Validation error: 1.3692112278721036\n",
            "ONLINE: Permutation #6, Epoch #2, Training error: 0.18613744632449836, Validation error: 1.3710349189520545\n",
            "ONLINE: Permutation #6, Epoch #3, Training error: 0.18594072699081732, Validation error: 1.3722289814664894\n",
            "ONLINE: Permutation #6, Epoch #4, Training error: 0.18578358551093316, Validation error: 1.3733324876771393\n",
            "ONLINE: Permutation #6, Epoch #5, Training error: 0.1856525719682924, Validation error: 1.3744208398172288\n",
            "ONLINE: Permutation #6, Epoch #6, Training error: 0.18554186610902274, Validation error: 1.375491963545819\n",
            "ONLINE: Permutation #6, Epoch #7, Training error: 0.18544769274226608, Validation error: 1.376533807181577\n",
            "ONLINE: Permutation #6, Epoch #8, Training error: 0.1853671587931752, Validation error: 1.3775359911023752\n",
            "ONLINE: Permutation #6, Epoch #9, Training error: 0.18529793493932234, Validation error: 1.3784911824491164\n",
            "ONLINE: Permutation #6, Epoch #10, Training error: 0.1852381183990793, Validation error: 1.3793947063240415\n",
            "ONLINE: Permutation #6, Epoch #11, Training error: 0.18518614620292687, Validation error: 1.380243967029353\n",
            "ONLINE: Permutation #6, Epoch #12, Training error: 0.18514073039636714, Validation error: 1.3810379454296584\n",
            "ONLINE: Permutation #6, Epoch #13, Training error: 0.18510080700856812, Validation error: 1.3817767978617352\n",
            "ONLINE: Permutation #6, Epoch #14, Training error: 0.18506549517336884, Validation error: 1.3824615422236994\n",
            "ONLINE: Permutation #7, Epoch #0, Training error: 0.18065571080807916, Validation error: 1.710524479084204\n",
            "ONLINE: Permutation #7, Epoch #1, Training error: 0.1806744073737287, Validation error: 1.7124438097316164\n",
            "ONLINE: Permutation #7, Epoch #2, Training error: 0.18063875675952495, Validation error: 1.7147815056965248\n",
            "ONLINE: Permutation #7, Epoch #3, Training error: 0.1805973724929656, Validation error: 1.7169896734496617\n",
            "ONLINE: Permutation #7, Epoch #4, Training error: 0.18055999772848114, Validation error: 1.7190432681262886\n",
            "ONLINE: Permutation #7, Epoch #5, Training error: 0.18052745535410666, Validation error: 1.7209478224579715\n",
            "ONLINE: Permutation #7, Epoch #6, Training error: 0.18049909960648935, Validation error: 1.722712370367497\n",
            "ONLINE: Permutation #7, Epoch #7, Training error: 0.18047417279902983, Validation error: 1.7243463008900928\n",
            "ONLINE: Permutation #7, Epoch #8, Training error: 0.1804520245738754, Validation error: 1.7258587563737615\n",
            "ONLINE: Permutation #7, Epoch #9, Training error: 0.1804321255224209, Validation error: 1.7272584728735623\n",
            "ONLINE: Permutation #7, Epoch #10, Training error: 0.18041404855903193, Validation error: 1.7285537231032377\n",
            "ONLINE: Permutation #7, Epoch #11, Training error: 0.18039744903563493, Validation error: 1.7297522943980184\n",
            "ONLINE: Permutation #7, Epoch #12, Training error: 0.18038204824211668, Validation error: 1.7308614844595294\n",
            "ONLINE: Permutation #7, Epoch #13, Training error: 0.18036762030136436, Validation error: 1.7318881081969006\n",
            "ONLINE: Permutation #7, Epoch #14, Training error: 0.18035398181901485, Validation error: 1.7328385118774905\n",
            "ONLINE: Permutation #8, Epoch #0, Training error: 0.1907245967993694, Validation error: 1.093691556556155\n",
            "ONLINE: Permutation #8, Epoch #1, Training error: 0.1905257655754535, Validation error: 1.093962671782515\n",
            "ONLINE: Permutation #8, Epoch #2, Training error: 0.1903752352066551, Validation error: 1.0942600643072768\n",
            "ONLINE: Permutation #8, Epoch #3, Training error: 0.19024893374563445, Validation error: 1.094511827335554\n",
            "ONLINE: Permutation #8, Epoch #4, Training error: 0.19013978225232653, Validation error: 1.0947270616609985\n",
            "ONLINE: Permutation #8, Epoch #5, Training error: 0.1900445519444617, Validation error: 1.094910215416809\n",
            "ONLINE: Permutation #8, Epoch #6, Training error: 0.18996106011044936, Validation error: 1.0950643269387244\n",
            "ONLINE: Permutation #8, Epoch #7, Training error: 0.18988757996771907, Validation error: 1.0951921868402728\n",
            "ONLINE: Permutation #8, Epoch #8, Training error: 0.18982268374317163, Validation error: 1.0952964918917665\n",
            "ONLINE: Permutation #8, Epoch #9, Training error: 0.1897651751307436, Validation error: 1.095379819111937\n",
            "ONLINE: Permutation #8, Epoch #10, Training error: 0.18971404576460402, Validation error: 1.0954445879492927\n",
            "ONLINE: Permutation #8, Epoch #11, Training error: 0.18966844211743117, Validation error: 1.095493035891609\n",
            "ONLINE: Permutation #8, Epoch #12, Training error: 0.1896276390693596, Validation error: 1.0955272073943414\n",
            "ONLINE: Permutation #8, Epoch #13, Training error: 0.18959101847864104, Validation error: 1.0955489526072646\n",
            "ONLINE: Permutation #8, Epoch #14, Training error: 0.1895580516855576, Validation error: 1.095559932723396\n",
            "ONLINE: Permutation #9, Epoch #0, Training error: 0.1809772311338456, Validation error: 1.634942187347678\n",
            "ONLINE: Permutation #9, Epoch #1, Training error: 0.18111728189422138, Validation error: 1.64062882834742\n",
            "ONLINE: Permutation #9, Epoch #2, Training error: 0.18108957314718724, Validation error: 1.6411245774433283\n",
            "ONLINE: Permutation #9, Epoch #3, Training error: 0.18105825033539735, Validation error: 1.6412256467436765\n",
            "ONLINE: Permutation #9, Epoch #4, Training error: 0.18103143648248837, Validation error: 1.641277211150533\n",
            "ONLINE: Permutation #9, Epoch #5, Training error: 0.18100746703722664, Validation error: 1.6413211102918785\n",
            "ONLINE: Permutation #9, Epoch #6, Training error: 0.18098569380705054, Validation error: 1.6413639726811795\n",
            "ONLINE: Permutation #9, Epoch #7, Training error: 0.1809657936157874, Validation error: 1.6414068558661832\n",
            "ONLINE: Permutation #9, Epoch #8, Training error: 0.18094753480549408, Validation error: 1.6414498490065197\n",
            "ONLINE: Permutation #9, Epoch #9, Training error: 0.18093072653900905, Validation error: 1.6414928697679876\n",
            "ONLINE: Permutation #9, Epoch #10, Training error: 0.18091520557797558, Validation error: 1.6415358058350729\n",
            "ONLINE: Permutation #9, Epoch #11, Training error: 0.1809008306768257, Validation error: 1.6415785404916254\n",
            "ONLINE: Permutation #9, Epoch #12, Training error: 0.18088747895107057, Validation error: 1.641620957807844\n",
            "ONLINE: Permutation #9, Epoch #13, Training error: 0.1808750430763752, Validation error: 1.6416629443357784\n",
            "ONLINE: Permutation #9, Epoch #14, Training error: 0.18086342901368258, Validation error: 1.6417043902091821\n",
            "ONLINE: Permutation #10, Epoch #0, Training error: 0.17608536465138905, Validation error: 1.7478064514354141\n",
            "ONLINE: Permutation #10, Epoch #1, Training error: 0.17547352418422413, Validation error: 1.7509314217109566\n",
            "ONLINE: Permutation #10, Epoch #2, Training error: 0.17511193885300513, Validation error: 1.7545964800443072\n",
            "ONLINE: Permutation #10, Epoch #3, Training error: 0.17476199731776693, Validation error: 1.7585188521997894\n",
            "ONLINE: Permutation #10, Epoch #4, Training error: 0.1744371068934986, Validation error: 1.762624565618397\n",
            "ONLINE: Permutation #10, Epoch #5, Training error: 0.17413917682955826, Validation error: 1.7668722006152995\n",
            "ONLINE: Permutation #10, Epoch #6, Training error: 0.17386699031179417, Validation error: 1.7712280557845042\n",
            "ONLINE: Permutation #10, Epoch #7, Training error: 0.17361883665767724, Validation error: 1.7756616990373764\n",
            "ONLINE: Permutation #10, Epoch #8, Training error: 0.17339301076707803, Validation error: 1.7801450534815082\n",
            "ONLINE: Permutation #10, Epoch #9, Training error: 0.1731878898867721, Validation error: 1.784652169737786\n",
            "ONLINE: Permutation #10, Epoch #10, Training error: 0.1730019368683019, Validation error: 1.7891591531976976\n",
            "ONLINE: Permutation #10, Epoch #11, Training error: 0.17283369373351462, Validation error: 1.7936441345063154\n",
            "ONLINE: Permutation #10, Epoch #12, Training error: 0.17268177620928127, Validation error: 1.7980872525208116\n",
            "ONLINE: Permutation #10, Epoch #13, Training error: 0.1725448703411374, Validation error: 1.8024706368807077\n",
            "ONLINE: Permutation #10, Epoch #14, Training error: 0.1724217307189626, Validation error: 1.8067783829691417\n"
          ]
        }
      ],
      "source": [
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.random.rand(100, 3)\n",
        "target = np.random.rand(100, 2)\n",
        "\n",
        "layer_one = Layer(3, 2, linear, d_linear)\n",
        "layer_two = Layer(2, 2, linear, d_linear)\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "NN.add_layer(layer_one)\n",
        "NN.add_layer(layer_two)\n",
        "NN.train_online(x, target, 15, 0.01, mean_squared_error, d_mean_squared_error, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT1dN_rEoKPO",
        "outputId": "55889d57-b879-4df4-bc0e-062e07f0338d",
        "tags": []
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "train_minibatch() missing 1 required positional argument: 'K'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-11-cebcdc2aae12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mNN2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_one2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mNN2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_two2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mNN1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_mean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mNN2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_online\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_mean_squared_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: train_minibatch() missing 1 required positional argument: 'K'"
          ]
        }
      ],
      "source": [
        "#test\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.random.rand(1000, 3)\n",
        "target = np.random.rand(1000, 2)\n",
        "\n",
        "\n",
        "layer_one1 = Layer(3, 2, linear, d_linear)\n",
        "layer_one2 = Layer(3, 2, linear, d_linear)\n",
        "layer_two1 = Layer(2, 2, linear, d_linear)\n",
        "layer_two2 = Layer(2, 2, linear, d_linear)\n",
        "\n",
        "NN1 = NeuralNetwork()\n",
        "NN1.add_layer(layer_one1)\n",
        "NN1.add_layer(layer_two1)\n",
        "NN2 = NeuralNetwork()\n",
        "NN2.add_layer(layer_one2)\n",
        "NN2.add_layer(layer_two2)\n",
        "NN1.train_minibatch(x, target, 10, 0.01, mean_squared_error, d_mean_squared_error, 3)\n",
        "NN2.train_online(x, target, 10, 0.01, mean_squared_error, d_mean_squared_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rNUYJgrTsFNC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
