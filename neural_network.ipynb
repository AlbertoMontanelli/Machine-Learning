{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/class_unit/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives.\n",
    "* **Hidden layers**:\n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qv1wwlgWsFM8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dobbiamo capire che funzioni di attivazione usare e quali derivate\n",
    "# da iniziare a fare successivamente: cross validation, test vs training error, ricerca di iperparametri (grid search, n layer, n unit,\n",
    "# learning rule), nr epochs/early stopping, tikhonov regularization, momentum, adaline e altre novelties\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha):\n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjNvxQOLsFM-"
   },
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters :**\n",
    " * input_size : $k_{i-1}$ ;\n",
    " * output_size : $k_i$ ;\n",
    " * activation_function ;\n",
    " * activation_derivative . \n",
    "\n",
    "**Constructor attributes :**\n",
    "* self.input_size = input_size;\n",
    "* self.output_size = output_size;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative;\n",
    "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
    "\n",
    "**Methods :**\n",
    "\n",
    "* **initialize_weights**: initialize weights and biases\n",
    "  * attributes :\n",
    "    * self.weights : $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix . \\\n",
    "      Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$ ;\n",
    "    * self.biases : $1 \\ \\textrm{x} \\ k_i$ array. Initialized to zeros;\n",
    "* **forward_layer** : allows to compute the output of the layer for a given input.\n",
    "  * parameter :\n",
    "    * input_array : matrix $X$ (see above for the case $L_0$ or $L_i$) .\n",
    "  * attributes :\n",
    "    * self.input : input_array ;\n",
    "    * self.net : net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$) .\n",
    "  * return -> output = $f(net)$, where $f$ is the activation function; $f(net)$ has the same dimensions of $net$.\n",
    "  \n",
    "* **backward_layer** : computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    "  * parameters :\n",
    "    * d_Ep : target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "    * learning_rate.\n",
    "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H8Ap9rLxLlNU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
    "        self.biases = np.zeros((1, self.output_size))\n",
    "        \n",
    "        \n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "\n",
    "\n",
    "    def backward_layer(self, d_Ep, learning_rate):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "        self.weights += learning_rate * np.dot(self.input.T, delta) # learning rule for the weights\n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    " * **data_split**: splits the input data into training set, validation set and test set\n",
    "   * parameter:\n",
    "     * x_tot: total data given as input;\n",
    "     * target: total data labels given as input;\n",
    "     * test_split: percentile of test set with respect to the total data.\n",
    "    * return->:\n",
    "      * x_train_val: training set+validation set for input data;\n",
    "      * target_train_val: training set+validation set for input data labels;\n",
    "      * x_test_val: test set for input data;\n",
    "      * target_test_val: test set for input data labels.\n",
    "\n",
    "     \n",
    " * **add_layer**: appends a layer to the empty list self.layers\n",
    "   * parameter:\n",
    "     * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    "  * parameter:\n",
    "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    "  * return -> input = $o_i$ for layer $L_i$.\n",
    "  \n",
    "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    "  * parameter:\n",
    "    * d_Ep;\n",
    "    * learning_rate.\n",
    "\n",
    "* **train_online**: applies the forward and backward method to the network for a specified number of epochs **one example at a time**.\n",
    "  * parameter:\n",
    "    * x_train: input matrix $X$;\n",
    "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "    * epochs: number of the iterations of the training algorithm;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative.\n",
    "\n",
    "* **train_minibatch**: applies the forward and backward method to the network for a specified number of epochs **to batches of $l' < l$** examples.\n",
    "  * parameter:\n",
    "    * x_train: input matrix $X$;\n",
    "    * target: $l \\ \\textrm{x} \\ d$ matrix;\n",
    "    * epochs: number of the iterations of the training algorithm;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative;\n",
    "    * batch_size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def data_split(self, x_tot, target, test_split):\n",
    "        \n",
    "        num_samples = x_tot.shape[0] # the total number of the examples in input\n",
    "        test_size = int(num_samples * test_split) # the number of the examples in the test set\n",
    "        indices = num_samples # if we don't want the randomization and the shuffle of the examples\n",
    "        \n",
    "        #indices = np.arange(num_samples)\n",
    "        #np.random.shuffle(indices)\n",
    "\n",
    "        x_tot = x_tot[indices]\n",
    "        target = target[indices]\n",
    "\n",
    "        x_test = x_tot[:test_size]\n",
    "        target_test = target[:test_size]\n",
    "        x_train_val = x_tot[test_size:]\n",
    "        target_train_val = target[test_size:]\n",
    "\n",
    "        return x_train_val, target_train_val, x_test, target_test\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, d_Ep, learning_rate):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate)\n",
    "\n",
    "    def reinitialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights()            \n",
    " \n",
    "\n",
    "    def train(self, x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size):\n",
    "        train_error_epoch = np.zeros(epochs)\n",
    "        val_error_epoch = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_tr_loss = 0\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            \"\"\"\n",
    "            # Shuffle training data\n",
    "            train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(train_indices)\n",
    "            x_train = x_train[train_indices]\n",
    "            target_train = target_train[train_indices]\n",
    "            \"\"\"\n",
    "            \n",
    "            # Mini-batch processing, if batch_size=1 we get the online version\n",
    "            for i in range(0, x_train.shape[0], batch_size):\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                target_batch = target_train[i:i+batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                predictions = self.forward(x_batch)\n",
    "\n",
    "                # Compute loss and gradient\n",
    "                loss = loss_function(target_batch, predictions)\n",
    "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "                epoch_tr_loss += np.sum(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(loss_gradient, learning_rate)\n",
    "\n",
    "            # Validation\n",
    "            val_predictions = self.forward(x_val)\n",
    "            val_loss = loss_function(target_val, val_predictions)\n",
    "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished. \n",
    "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
    "            # Store average errors for the epoch\n",
    "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
    "            val_error_epoch[epoch] = epoch_val_loss\n",
    "\n",
    "        return train_error_epoch, val_error_epoch\n",
    "\n",
    "\n",
    "    def k_fold_cross_validation(self, x_tot, target, K, epochs, learning_rate, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_tot.shape[0]\n",
    "        fold_size = num_samples // K # if K=1 we get the hold-out validation\n",
    "\n",
    "        # Error storage for averaging\n",
    "        avg_train_error_epoch = np.zeros(epochs)\n",
    "        avg_val_error_epoch = np.zeros(epochs)\n",
    "        \n",
    "        if K==1: # hold-out validation\n",
    "            print(\"sono entrato in hold-out\")\n",
    "            train_indices = np.arange(0, int(0.8*num_samples))\n",
    "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices)\n",
    "            x_train, target_train = x_tot[train_indices], target[train_indices]\n",
    "            x_val, target_val = x_tot[val_indices], target[val_indices]            \n",
    "            train_error_epoch, val_error_epoch = self.train(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "            return train_error_epoch, val_error_epoch\n",
    "            \n",
    "        # ROBA FOTONICA PAZZESKA DI CHAT GPT, LEGGERE BENE\n",
    "        for k in range(K):\n",
    "            print(\"sono entrato in cross val\")\n",
    "            # Create fold indices\n",
    "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
    "                                                                        # It contains the indices of the examples used in validation set for\n",
    "                                                                        # this fold\n",
    "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples - \n",
    "                                                                              # len(val_indices). It contains the indices of all the examples\n",
    "                                                                              # but the ones used in the validation set for this fold. Thus \n",
    "                                                                              # it corresponds to the training set for this fold \n",
    "            x_train, target_train = x_tot[train_indices], target[train_indices]\n",
    "            x_val, target_val = x_tot[val_indices], target[val_indices]\n",
    "\n",
    "            # Reinitialize weights for each fold\n",
    "            self.reinitialize_weights()\n",
    "\n",
    "            # Train on the current fold\n",
    "            train_error_epoch, val_error_epoch = self.train(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "\n",
    "            # Accumulate errors for averaging, we have the errors for each epoch summed for all the folds\n",
    "            avg_train_error_epoch += train_error_epoch\n",
    "            avg_val_error_epoch += val_error_epoch\n",
    "\n",
    "            print(f\"Fold {k+1} completed.\")\n",
    "\n",
    "        # Average errors across all folds\n",
    "        avg_train_error_epoch /= K\n",
    "        avg_val_error_epoch /= K\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
    "\n",
    "        return avg_train_error_epoch, avg_val_error_epoch\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp0g8MgT3hhO",
    "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sono entrato in cross val\n",
      "Fold 1 completed.\n",
      "sono entrato in cross val\n",
      "Fold 2 completed.\n",
      "sono entrato in cross val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29613/3742339482.py:2: RuntimeWarning: overflow encountered in square\n",
      "  return np.sum((y_true - y_pred)**2)\n",
      "/tmp/ipykernel_29613/456031650.py:24: RuntimeWarning: invalid value encountered in add\n",
      "  self.weights += learning_rate * np.dot(self.input.T, delta) # learning rule for the weights\n",
      "/tmp/ipykernel_29613/456031650.py:25: RuntimeWarning: invalid value encountered in add\n",
      "  self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 completed.\n",
      "sono entrato in cross val\n",
      "Fold 4 completed.\n",
      "sono entrato in cross val\n",
      "Fold 5 completed.\n",
      "Epoch 1/500, Avg Training Loss: 0.48112430858760413, Avg Validation Loss: 4.5380762748730135\n",
      "Epoch 2/500, Avg Training Loss: 0.19474689358919609, Avg Validation Loss: 4.255587547271082\n",
      "Epoch 3/500, Avg Training Loss: 0.19440150408126886, Avg Validation Loss: 4.211875927415239\n",
      "Epoch 4/500, Avg Training Loss: 0.19282800502616837, Avg Validation Loss: 4.182429174245851\n",
      "Epoch 5/500, Avg Training Loss: 0.1915125212705567, Avg Validation Loss: 4.159211800468024\n",
      "Epoch 6/500, Avg Training Loss: 0.19045344379266918, Avg Validation Loss: 4.140535151955014\n",
      "Epoch 7/500, Avg Training Loss: 0.18959046910455235, Avg Validation Loss: 4.1253358617468425\n",
      "Epoch 8/500, Avg Training Loss: 0.18887893400792402, Avg Validation Loss: 4.1128489571483735\n",
      "Epoch 9/500, Avg Training Loss: 0.18828634109519096, Avg Validation Loss: 4.102508545903536\n",
      "Epoch 10/500, Avg Training Loss: 0.18778856869016142, Avg Validation Loss: 4.093887467450566\n",
      "Epoch 11/500, Avg Training Loss: 0.18736736783237795, Avg Validation Loss: 4.086657605668989\n",
      "Epoch 12/500, Avg Training Loss: 0.18700869659685823, Avg Validation Loss: 4.080563046785944\n",
      "Epoch 13/500, Avg Training Loss: 0.18670158623041125, Avg Validation Loss: 4.075401503081764\n",
      "Epoch 14/500, Avg Training Loss: 0.1864373528985654, Avg Validation Loss: 4.071011190765736\n",
      "Epoch 15/500, Avg Training Loss: 0.18620903914262293, Avg Validation Loss: 4.0672613873030965\n",
      "Epoch 16/500, Avg Training Loss: 0.1860110111535301, Avg Validation Loss: 4.064045520728897\n",
      "Epoch 17/500, Avg Training Loss: 0.18583866370967553, Avg Validation Loss: 4.061276032915872\n",
      "Epoch 18/500, Avg Training Loss: 0.18568820077197096, Avg Validation Loss: 4.058880506239708\n",
      "Epoch 19/500, Avg Training Loss: 0.18555647007542947, Avg Validation Loss: 4.056798703691388\n",
      "Epoch 20/500, Avg Training Loss: 0.18544083681535, Avg Validation Loss: 4.054980278717646\n",
      "Epoch 21/500, Avg Training Loss: 0.18533908602053378, Avg Validation Loss: 4.05338298256024\n",
      "Epoch 22/500, Avg Training Loss: 0.18524934624316364, Avg Validation Loss: 4.051971245736826\n",
      "Epoch 23/500, Avg Training Loss: 0.1851700292781418, Avg Validation Loss: 4.050715044199485\n",
      "Epoch 24/500, Avg Training Loss: 0.18509978207309388, Avg Validation Loss: 4.049588984523533\n",
      "Epoch 25/500, Avg Training Loss: 0.18503744801006486, Avg Validation Loss: 4.048571559419026\n",
      "Epoch 26/500, Avg Training Loss: 0.1849820354663963, Avg Validation Loss: 4.047644537042052\n",
      "Epoch 27/500, Avg Training Loss: 0.18493269208543314, Avg Validation Loss: 4.046792456439528\n",
      "Epoch 28/500, Avg Training Loss: 0.1848886835683185, Avg Validation Loss: 4.046002207962728\n",
      "Epoch 29/500, Avg Training Loss: 0.18484937607773771, Avg Validation Loss: 4.045262682302521\n",
      "Epoch 30/500, Avg Training Loss: 0.18481422155177812, Avg Validation Loss: 4.044564475401611\n",
      "Epoch 31/500, Avg Training Loss: 0.18478274538115633, Avg Validation Loss: 4.043899639216447\n",
      "Epoch 32/500, Avg Training Loss: 0.18475453602009143, Avg Validation Loss: 4.043261470369008\n",
      "Epoch 33/500, Avg Training Loss: 0.18472923619017295, Avg Validation Loss: 4.042644330315139\n",
      "Epoch 34/500, Avg Training Loss: 0.1847065354049305, Avg Validation Loss: 4.042043491883787\n",
      "Epoch 35/500, Avg Training Loss: 0.18468616359571738, Avg Validation Loss: 4.041455007999669\n",
      "Epoch 36/500, Avg Training Loss: 0.18466788566080086, Avg Validation Loss: 4.04087559915577\n",
      "Epoch 37/500, Avg Training Loss: 0.18465149679202844, Avg Validation Loss: 4.040302556800404\n",
      "Epoch 38/500, Avg Training Loss: 0.1846368184591913, Avg Validation Loss: 4.039733660282142\n",
      "Epoch 39/500, Avg Training Loss: 0.18462369495278913, Avg Validation Loss: 4.039167105381946\n",
      "Epoch 40/500, Avg Training Loss: 0.18461199040247775, Avg Validation Loss: 4.038601442775496\n",
      "Epoch 41/500, Avg Training Loss: 0.18460158620192832, Avg Validation Loss: 4.038035525025611\n",
      "Epoch 42/500, Avg Training Loss: 0.184592378781814, Avg Validation Loss: 4.0374684609164655\n",
      "Epoch 43/500, Avg Training Loss: 0.184584277681675, Avg Validation Loss: 4.036899576117127\n",
      "Epoch 44/500, Avg Training Loss: 0.18457720387888932, Avg Validation Loss: 4.036328379308895\n",
      "Epoch 45/500, Avg Training Loss: 0.18457108833919908, Avg Validation Loss: 4.03575453303417\n",
      "Epoch 46/500, Avg Training Loss: 0.18456587075844713, Avg Validation Loss: 4.035177828628926\n",
      "Epoch 47/500, Avg Training Loss: 0.18456149846955427, Avg Validation Loss: 4.034598164689054\n",
      "Epoch 48/500, Avg Training Loss: 0.18455792549246142, Avg Validation Loss: 4.034015528596283\n",
      "Epoch 49/500, Avg Training Loss: 0.18455511170789127, Avg Validation Loss: 4.033429980693591\n",
      "Epoch 50/500, Avg Training Loss: 0.18455302213844585, Avg Validation Loss: 4.032841640755287\n",
      "Epoch 51/500, Avg Training Loss: 0.18455162632282696, Avg Validation Loss: 4.032250676444354\n",
      "Epoch 52/500, Avg Training Loss: 0.18455089777090744, Avg Validation Loss: 4.031657293490566\n",
      "Epoch 53/500, Avg Training Loss: 0.18455081348904584, Avg Validation Loss: 4.031061727358183\n",
      "Epoch 54/500, Avg Training Loss: 0.18455135356646593, Avg Validation Loss: 4.030464236202645\n",
      "Epoch 55/500, Avg Training Loss: 0.18455250081475266, Avg Validation Loss: 4.029865094942098\n",
      "Epoch 56/500, Avg Training Loss: 0.184554240453575, Avg Validation Loss: 4.029264590292608\n",
      "Epoch 57/500, Avg Training Loss: 0.18455655983666164, Avg Validation Loss: 4.028663016635845\n",
      "Epoch 58/500, Avg Training Loss: 0.1845594482128433, Avg Validation Loss: 4.02806067260533\n",
      "Epoch 59/500, Avg Training Loss: 0.18456289651766045, Avg Validation Loss: 4.027457858292458\n",
      "Epoch 60/500, Avg Training Loss: 0.18456689719162436, Avg Validation Loss: 4.026854872986565\n",
      "Epoch 61/500, Avg Training Loss: 0.18457144402173237, Avg Validation Loss: 4.0262520133747834\n",
      "Epoch 62/500, Avg Training Loss: 0.1845765320032811, Avg Validation Loss: 4.0256495721373176\n",
      "Epoch 63/500, Avg Training Loss: 0.18458215721940635, Avg Validation Loss: 4.025047836882469\n",
      "Epoch 64/500, Avg Training Loss: 0.1845883167361121, Avg Validation Loss: 4.024447089373254\n",
      "Epoch 65/500, Avg Training Loss: 0.18459500851084004, Avg Validation Loss: 4.023847605004065\n",
      "Epoch 66/500, Avg Training Loss: 0.18460223131288403, Avg Validation Loss: 4.023249652491526\n",
      "Epoch 67/500, Avg Training Loss: 0.18460998465417044, Avg Validation Loss: 4.022653493748706\n",
      "Epoch 68/500, Avg Training Loss: 0.18461826872911696, Avg Validation Loss: 4.022059383916208\n",
      "Epoch 69/500, Avg Training Loss: 0.18462708436244632, Avg Validation Loss: 4.021467571527452\n",
      "Epoch 70/500, Avg Training Loss: 0.18463643296397525, Avg Validation Loss: 4.020878298788798\n",
      "Epoch 71/500, Avg Training Loss: 0.18464631648952395, Avg Validation Loss: 4.0202918019580105\n",
      "Epoch 72/500, Avg Training Loss: 0.18465673740719954, Avg Validation Loss: 4.019708311807173\n",
      "Epoch 73/500, Avg Training Loss: 0.18466769866840155, Avg Validation Loss: 4.019128054158282\n",
      "Epoch 74/500, Avg Training Loss: 0.18467920368297933, Avg Validation Loss: 4.018551250481767\n",
      "Epoch 75/500, Avg Training Loss: 0.18469125629804278, Avg Validation Loss: 4.017978118549819\n",
      "Epoch 76/500, Avg Training Loss: 0.18470386077999074, Avg Validation Loss: 4.01740887313795\n",
      "Epoch 77/500, Avg Training Loss: 0.18471702179937327, Avg Validation Loss: 4.016843726769469\n",
      "Epoch 78/500, Avg Training Loss: 0.1847307444182531, Avg Validation Loss: 4.01628289049877\n",
      "Epoch 79/500, Avg Training Loss: 0.18474503407977125, Avg Validation Loss: 4.015726574730301\n",
      "Epoch 80/500, Avg Training Loss: 0.18475989659965647, Avg Validation Loss: 4.01517499007103\n",
      "Epoch 81/500, Avg Training Loss: 0.18477533815945074, Avg Validation Loss: 4.014628348215033\n",
      "Epoch 82/500, Avg Training Loss: 0.18479136530124635, Avg Validation Loss: 4.014086862859535\n",
      "Epoch 83/500, Avg Training Loss: 0.1848079849237564, Avg Validation Loss: 4.013550750652442\n",
      "Epoch 84/500, Avg Training Loss: 0.18482520427955623, Avg Validation Loss: 4.013020232171972\n",
      "Epoch 85/500, Avg Training Loss: 0.18484303097335375, Avg Validation Loss: 4.012495532939566\n",
      "Epoch 86/500, Avg Training Loss: 0.1848614729611574, Avg Validation Loss: 4.011976884467792\n",
      "Epoch 87/500, Avg Training Loss: 0.18488053855022468, Avg Validation Loss: 4.011464525345433\n",
      "Epoch 88/500, Avg Training Loss: 0.18490023639968206, Avg Validation Loss: 4.010958702362419\n",
      "Epoch 89/500, Avg Training Loss: 0.18492057552171665, Avg Validation Loss: 4.0104596716777365\n",
      "Epoch 90/500, Avg Training Loss: 0.1849415652832452, Avg Validation Loss: 4.009967700033877\n",
      "Epoch 91/500, Avg Training Loss: 0.18496321540797073, Avg Validation Loss: 4.009483066021839\n",
      "Epoch 92/500, Avg Training Loss: 0.1849855359787404, Avg Validation Loss: 4.009006061401149\n",
      "Epoch 93/500, Avg Training Loss: 0.18500853744012022, Avg Validation Loss: 4.008536992479803\n",
      "Epoch 94/500, Avg Training Loss: 0.18503223060110274, Avg Validation Loss: 4.0080761815595105\n",
      "Epoch 95/500, Avg Training Loss: 0.18505662663786202, Avg Validation Loss: 4.007623968452125\n",
      "Epoch 96/500, Avg Training Loss: 0.1850817370964695, Avg Validation Loss: 4.007180712073585\n",
      "Epoch 97/500, Avg Training Loss: 0.18510757389548024, Avg Validation Loss: 4.006746792122302\n",
      "Epoch 98/500, Avg Training Loss: 0.1851341493282947, Avg Validation Loss: 4.006322610849434\n",
      "Epoch 99/500, Avg Training Loss: 0.18516147606519434, Avg Validation Loss: 4.005908594929105\n",
      "Epoch 100/500, Avg Training Loss: 0.1851895671549438, Avg Validation Loss: 4.005505197437257\n",
      "Epoch 101/500, Avg Training Loss: 0.1852184360258418, Avg Validation Loss: 4.005112899948493\n",
      "Epoch 102/500, Avg Training Loss: 0.18524809648609344, Avg Validation Loss: 4.004732214761025\n",
      "Epoch 103/500, Avg Training Loss: 0.18527856272336501, Avg Validation Loss: 4.004363687260571\n",
      "Epoch 104/500, Avg Training Loss: 0.18530984930336747, Avg Validation Loss: 4.004007898434929\n",
      "Epoch 105/500, Avg Training Loss: 0.18534197116730064, Avg Validation Loss: 4.003665467551821\n",
      "Epoch 106/500, Avg Training Loss: 0.1853749436279711, Avg Validation Loss: 4.003337055013576\n",
      "Epoch 107/500, Avg Training Loss: 0.18540878236437813, Avg Validation Loss: 4.003023365403253\n",
      "Epoch 108/500, Avg Training Loss: 0.18544350341453822, Avg Validation Loss: 4.002725150737915\n",
      "Epoch 109/500, Avg Training Loss: 0.18547912316629495, Avg Validation Loss: 4.002443213945963\n",
      "Epoch 110/500, Avg Training Loss: 0.18551565834583209, Avg Validation Loss: 4.002178412586681\n",
      "Epoch 111/500, Avg Training Loss: 0.18555312600357693, Avg Validation Loss: 4.0019316628315496\n",
      "Epoch 112/500, Avg Training Loss: 0.18559154349714574, Avg Validation Loss: 4.001703943728305\n",
      "Epoch 113/500, Avg Training Loss: 0.18563092847094476, Avg Validation Loss: 4.001496301770269\n",
      "Epoch 114/500, Avg Training Loss: 0.1856712988319976, Avg Validation Loss: 4.001309855795122\n",
      "Epoch 115/500, Avg Training Loss: 0.1857126727215211, Avg Validation Loss: 4.001145802239044\n",
      "Epoch 116/500, Avg Training Loss: 0.1857550684817207, Avg Validation Loss: 4.001005420773912\n",
      "Epoch 117/500, Avg Training Loss: 0.1857985046172171, Avg Validation Loss: 4.000890080357248\n",
      "Epoch 118/500, Avg Training Loss: 0.18584299975045182, Avg Validation Loss: 4.000801245726543\n",
      "Epoch 119/500, Avg Training Loss: 0.18588857257035013, Avg Validation Loss: 4.00074048437167\n",
      "Epoch 120/500, Avg Training Loss: 0.18593524177343962, Avg Validation Loss: 4.00070947402125\n",
      "Epoch 121/500, Avg Training Loss: 0.1859830259965404, Avg Validation Loss: 4.000710010680967\n",
      "Epoch 122/500, Avg Training Loss: 0.18603194374004822, Avg Validation Loss: 4.000744017264024\n",
      "Epoch 123/500, Avg Training Loss: 0.18608201328072954, Avg Validation Loss: 4.00081355285606\n",
      "Epoch 124/500, Avg Training Loss: 0.18613325257283936, Avg Validation Loss: 4.00092082265895\n",
      "Epoch 125/500, Avg Training Loss: 0.18618567913624975, Avg Validation Loss: 4.001068188659789\n",
      "Epoch 126/500, Avg Training Loss: 0.18623930993014967, Avg Validation Loss: 4.001258181073183\n",
      "Epoch 127/500, Avg Training Loss: 0.18629416121073586, Avg Validation Loss: 4.00149351060629\n",
      "Epoch 128/500, Avg Training Loss: 0.18635024837116557, Avg Validation Loss: 4.001777081597162\n",
      "Epoch 129/500, Avg Training Loss: 0.1864075857618833, Avg Validation Loss: 4.0021120060773345\n",
      "Epoch 130/500, Avg Training Loss: 0.18646618648926547, Avg Validation Loss: 4.002501618809317\n",
      "Epoch 131/500, Avg Training Loss: 0.18652606219035103, Avg Validation Loss: 4.002949493348442\n",
      "Epoch 132/500, Avg Training Loss: 0.18658722278124418, Avg Validation Loss: 4.0034594591760255\n",
      "Epoch 133/500, Avg Training Loss: 0.18664967617658848, Avg Validation Loss: 4.004035619946857\n",
      "Epoch 134/500, Avg Training Loss: 0.18671342797732393, Avg Validation Loss: 4.004682372888144\n",
      "Epoch 135/500, Avg Training Loss: 0.1867784811237543, Avg Validation Loss: 4.00540442937876\n",
      "Epoch 136/500, Avg Training Loss: 0.18684483551077452, Avg Validation Loss: 4.0062068367266\n",
      "Epoch 137/500, Avg Training Loss: 0.1869124875619478, Avg Validation Loss: 4.00709500114712\n",
      "Epoch 138/500, Avg Training Loss: 0.18698142975898352, Avg Validation Loss: 4.0080747119272235\n",
      "Epoch 139/500, Avg Training Loss: 0.18705165012306524, Avg Validation Loss: 4.0091521667346175\n",
      "Epoch 140/500, Avg Training Loss: 0.1871231316444215, Avg Validation Loss: 4.010333998002448\n",
      "Epoch 141/500, Avg Training Loss: 0.1871958516565454, Avg Validation Loss: 4.01162730028148\n",
      "Epoch 142/500, Avg Training Loss: 0.18726978115155943, Avg Validation Loss: 4.013039658405782\n",
      "Epoch 143/500, Avg Training Loss: 0.18734488403342703, Avg Validation Loss: 4.014579176261395\n",
      "Epoch 144/500, Avg Training Loss: 0.1874211163060496, Avg Validation Loss: 4.016254505879068\n",
      "Epoch 145/500, Avg Training Loss: 0.18749842519379228, Avg Validation Loss: 4.018074876489957\n",
      "Epoch 146/500, Avg Training Loss: 0.1875767481926957, Avg Validation Loss: 4.020050123085154\n",
      "Epoch 147/500, Avg Training Loss: 0.18765601205158677, Avg Validation Loss: 4.022190713903931\n",
      "Epoch 148/500, Avg Training Loss: 0.1877361316835517, Avg Validation Loss: 4.024507776139383\n",
      "Epoch 149/500, Avg Training Loss: 0.1878170090098299, Avg Validation Loss: 4.027013118991518\n",
      "Epoch 150/500, Avg Training Loss: 0.18789853174017082, Avg Validation Loss: 4.0297192530148935\n",
      "Epoch 151/500, Avg Training Loss: 0.18798057209612945, Avg Validation Loss: 4.032639404498832\n",
      "Epoch 152/500, Avg Training Loss: 0.18806298548669848, Avg Validation Loss: 4.035787523382166\n",
      "Epoch 153/500, Avg Training Loss: 0.18814560914913284, Avg Validation Loss: 4.03917828294128\n",
      "Epoch 154/500, Avg Training Loss: 0.18822826077183358, Avg Validation Loss: 4.042827069201395\n",
      "Epoch 155/500, Avg Training Loss: 0.18831073712073146, Avg Validation Loss: 4.04674995770997\n",
      "Epoch 156/500, Avg Training Loss: 0.18839281269571062, Avg Validation Loss: 4.050963674983889\n",
      "Epoch 157/500, Avg Training Loss: 0.18847423844917024, Avg Validation Loss: 4.0554855416084905\n",
      "Epoch 158/500, Avg Training Loss: 0.18855474060469832, Avg Validation Loss: 4.060333393640612\n",
      "Epoch 159/500, Avg Training Loss: 0.18863401961983023, Avg Validation Loss: 4.0655254786693265\n",
      "Epoch 160/500, Avg Training Loss: 0.18871174934268717, Avg Validation Loss: 4.07108032264357\n",
      "Epoch 161/500, Avg Training Loss: 0.18878757641755545, Avg Validation Loss: 4.077016563419102\n",
      "Epoch 162/500, Avg Training Loss: 0.18886111999868122, Avg Validation Loss: 4.08335274695143\n",
      "Epoch 163/500, Avg Training Loss: 0.18893197183412855, Avg Validation Loss: 4.090107082217649\n",
      "Epoch 164/500, Avg Training Loss: 0.1889996967818119, Avg Validation Loss: 4.097297151349715\n",
      "Epoch 165/500, Avg Training Loss: 0.18906383381704875, Avg Validation Loss: 4.104939572172233\n",
      "Epoch 166/500, Avg Training Loss: 0.1891238975845031, Avg Validation Loss: 4.113049611432848\n",
      "Epoch 167/500, Avg Training Loss: 0.1891793805366246, Avg Validation Loss: 4.121640748565875\n",
      "Epoch 168/500, Avg Training Loss: 0.1892297556853288, Avg Validation Loss: 4.130724191906689\n",
      "Epoch 169/500, Avg Training Loss: 0.18927447997379637, Avg Validation Loss: 4.14030835192543\n",
      "Epoch 170/500, Avg Training Loss: 0.18931299825159503, Avg Validation Loss: 4.150398279294286\n",
      "Epoch 171/500, Avg Training Loss: 0.1893447478103074, Avg Validation Loss: 4.16099507941809\n",
      "Epoch 172/500, Avg Training Loss: 0.18936916341085835, Avg Validation Loss: 4.172095319355207\n",
      "Epoch 173/500, Avg Training Loss: 0.18938568271106807, Avg Validation Loss: 4.183690447664751\n",
      "Epoch 174/500, Avg Training Loss: 0.1893937519867265, Avg Validation Loss: 4.195766252368629\n",
      "Epoch 175/500, Avg Training Loss: 0.189392832036187, Avg Validation Loss: 4.20830238653421\n",
      "Epoch 176/500, Avg Training Loss: 0.18938240417133706, Avg Validation Loss: 4.2212719944763455\n",
      "Epoch 177/500, Avg Training Loss: 0.189361976229724, Avg Validation Loss: 4.234641473660469\n",
      "Epoch 178/500, Avg Training Loss: 0.1893310885938624, Avg Validation Loss: 4.248370407417708\n",
      "Epoch 179/500, Avg Training Loss: 0.18928932027055928, Avg Validation Loss: 4.26241170091942\n",
      "Epoch 180/500, Avg Training Loss: 0.1892362951564319, Avg Validation Loss: 4.276711946962188\n",
      "Epoch 181/500, Avg Training Loss: 0.1891716886808731, Avg Validation Loss: 4.291212038662588\n",
      "Epoch 182/500, Avg Training Loss: 0.18909523505470713, Avg Validation Loss: 4.3058480331817055\n",
      "Epoch 183/500, Avg Training Loss: 0.18900673533935458, Avg Validation Loss: 4.3205522545962705\n",
      "Epoch 184/500, Avg Training Loss: 0.18890606646723548, Avg Validation Loss: 4.335254606069766\n",
      "Epoch 185/500, Avg Training Loss: 0.1887931911775408, Avg Validation Loss: 4.349884043181478\n",
      "Epoch 186/500, Avg Training Loss: 0.1886681685859466, Avg Validation Loss: 4.364370143725129\n",
      "Epoch 187/500, Avg Training Loss: 0.18853116480683518, Avg Validation Loss: 4.37864469677657\n",
      "Epoch 188/500, Avg Training Loss: 0.18838246273924777, Avg Validation Loss: 4.392643227468126\n",
      "Epoch 189/500, Avg Training Loss: 0.1882224698780723, Avg Validation Loss: 4.406306375204405\n",
      "Epoch 190/500, Avg Training Loss: 0.1880517228905194, Avg Validation Loss: 4.41958105250965\n",
      "Epoch 191/500, Avg Training Loss: 0.18787088776310554, Avg Validation Loss: 4.43242132855405\n",
      "Epoch 192/500, Avg Training Loss: 0.1876807546028887, Avg Validation Loss: 4.444789003674896\n",
      "Epoch 193/500, Avg Training Loss: 0.18748222664972256, Avg Validation Loss: 4.456653865978684\n",
      "Epoch 194/500, Avg Training Loss: 0.18727630365708484, Avg Validation Loss: 4.467993645116314\n",
      "Epoch 195/500, Avg Training Loss: 0.18706406042538756, Avg Validation Loss: 4.478793698595744\n",
      "Epoch 196/500, Avg Training Loss: 0.1868466218105862, Avg Validation Loss: 4.489046480423976\n",
      "Epoch 197/500, Avg Training Loss: 0.1866251358876087, Avg Validation Loss: 4.4987508495152735\n",
      "Epoch 198/500, Avg Training Loss: 0.18640074706967122, Avg Validation Loss: 4.5079112764054035\n",
      "Epoch 199/500, Avg Training Loss: 0.1861745708698221, Avg Validation Loss: 4.516537002529768\n",
      "Epoch 200/500, Avg Training Loss: 0.18594767168523038, Avg Validation Loss: 4.524641198324627\n",
      "Epoch 201/500, Avg Training Loss: 0.18572104456180846, Avg Validation Loss: 4.532240156459775\n",
      "Epoch 202/500, Avg Training Loss: 0.1854956014378429, Avg Validation Loss: 4.539352546150607\n",
      "Epoch 203/500, Avg Training Loss: 0.18527216194005672, Avg Validation Loss: 4.545998744870889\n",
      "Epoch 204/500, Avg Training Loss: 0.18505144846112825, Avg Validation Loss: 4.552200255594736\n",
      "Epoch 205/500, Avg Training Loss: 0.18483408500629347, Avg Validation Loss: 4.557979211244424\n",
      "Epoch 206/500, Avg Training Loss: 0.1846205991589607, Avg Validation Loss: 4.5633579633205965\n",
      "Epoch 207/500, Avg Training Loss: 0.18441142646740274, Avg Validation Loss: 4.568358748567636\n",
      "Epoch 208/500, Avg Training Loss: 0.1842069165752791, Avg Validation Loss: 4.573003425706519\n",
      "Epoch 209/500, Avg Training Loss: 0.18400734048521294, Avg Validation Loss: 4.57731327345503\n",
      "Epoch 210/500, Avg Training Loss: 0.1838128984364334, Avg Validation Loss: 4.58130884096758\n",
      "Epoch 211/500, Avg Training Loss: 0.18362372797844007, Avg Validation Loss: 4.585009842229547\n",
      "Epoch 212/500, Avg Training Loss: 0.18343991192126874, Avg Validation Loss: 4.588435086635181\n",
      "Epoch 213/500, Avg Training Loss: 0.18326148593219535, Avg Validation Loss: 4.591602438834373\n",
      "Epoch 214/500, Avg Training Loss: 0.18308844562481613, Avg Validation Loss: 4.594528801825169\n",
      "Epoch 215/500, Avg Training Loss: 0.18292075304849764, Avg Validation Loss: 4.597230118176141\n",
      "Epoch 216/500, Avg Training Loss: 0.1827583425342086, Avg Validation Loss: 4.5997213850477525\n",
      "Epoch 217/500, Avg Training Loss: 0.18260112588912375, Avg Validation Loss: 4.602016679523648\n",
      "Epoch 218/500, Avg Training Loss: 0.18244899695699313, Avg Validation Loss: 4.604129191207656\n",
      "Epoch 219/500, Avg Training Loss: 0.18230183558040566, Avg Validation Loss: 4.606071260112675\n",
      "Epoch 220/500, Avg Training Loss: 0.18215951100670602, Avg Validation Loss: 4.607854417102949\n",
      "Epoch 221/500, Avg Training Loss: 0.18202188479650666, Avg Validation Loss: 4.609489427758755\n",
      "Epoch 222/500, Avg Training Loss: 0.18188881326583112, Avg Validation Loss: 4.6109863322643845\n",
      "Epoch 223/500, Avg Training Loss: 0.1817601495706526, Avg Validation Loss: 4.61235449817214\n",
      "Epoch 224/500, Avg Training Loss: 0.18163574532494547, Avg Validation Loss: 4.613602633647993\n",
      "Epoch 225/500, Avg Training Loss: 0.18151545225336138, Avg Validation Loss: 4.614738917744709\n",
      "Epoch 226/500, Avg Training Loss: 0.18139912256187996, Avg Validation Loss: 4.615770754116266\n",
      "Epoch 227/500, Avg Training Loss: 0.1812866122639184, Avg Validation Loss: 4.616705751934946\n",
      "Epoch 228/500, Avg Training Loss: 0.18117777405836272, Avg Validation Loss: 4.617548540842103\n",
      "Epoch 229/500, Avg Training Loss: 0.18107248424152922, Avg Validation Loss: 4.618312153718233\n",
      "Epoch 230/500, Avg Training Loss: 0.18097055175508509, Avg Validation Loss: 4.618976800998739\n",
      "Epoch 231/500, Avg Training Loss: 0.1808720453688079, Avg Validation Loss: 4.619644499409457\n",
      "Epoch 232/500, Avg Training Loss: 0.18077608901186631, Avg Validation Loss: 4.6199423776256\n",
      "Epoch 233/500, Avg Training Loss: 0.18068544161185096, Avg Validation Loss: 4.621393789581285\n",
      "Epoch 234/500, Avg Training Loss: 0.1805886697896934, Avg Validation Loss: 4.617730230464434\n",
      "Epoch 235/500, Avg Training Loss: 0.18053315842570933, Avg Validation Loss: 4.635659361667345\n",
      "Epoch 236/500, Avg Training Loss: 0.18034920892529238, Avg Validation Loss: 4.560030074985964\n",
      "Epoch 237/500, Avg Training Loss: 0.18134551286046025, Avg Validation Loss: 4.93373258733115\n",
      "Epoch 238/500, Avg Training Loss: 0.19030717316881568, Avg Validation Loss: 3.8542602107258634\n",
      "Epoch 239/500, Avg Training Loss: 0.5207149125643872, Avg Validation Loss: 54.4599517283971\n",
      "Epoch 240/500, Avg Training Loss: 25176551.399371065, Avg Validation Loss: 2.4227400011775084e+33\n",
      "Epoch 241/500, Avg Training Loss: inf, Avg Validation Loss: inf\n",
      "Epoch 242/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 243/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 244/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 245/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 246/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 247/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 248/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 249/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 250/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 251/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 252/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 253/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 254/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 255/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 256/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 257/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 258/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 259/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 260/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 261/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 262/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 263/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 264/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 265/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 266/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 267/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 268/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 269/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 270/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 271/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 272/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 273/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 274/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 275/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 276/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 277/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 278/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 279/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 280/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 281/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 282/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 283/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 284/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 285/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 286/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 287/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 288/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 289/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 290/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 291/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 292/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 293/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 294/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 295/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 296/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 297/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 298/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 299/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 300/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 301/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 302/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 303/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 304/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 305/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 306/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 307/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 308/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 309/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 310/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 311/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 312/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 313/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 314/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 315/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 316/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 317/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 318/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 319/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 320/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 321/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 322/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 323/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 324/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 325/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 326/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 327/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 328/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 329/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 330/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 331/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 332/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 333/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 334/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 335/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 336/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 337/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 338/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 339/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 340/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 341/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 342/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 343/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 344/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 345/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 346/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 347/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 348/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 349/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 350/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 351/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 352/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 353/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 354/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 355/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 356/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 357/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 358/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 359/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 360/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 361/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 362/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 363/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 364/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 365/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 366/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 367/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 368/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 369/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 370/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 371/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 372/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 373/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 374/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 375/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 376/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 377/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 378/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 379/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 380/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 381/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 382/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 383/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 384/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 385/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 386/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 387/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 388/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 389/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 390/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 391/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 392/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 393/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 394/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 395/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 396/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 397/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 398/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 399/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 400/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 401/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 402/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 403/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 404/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 405/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 406/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 407/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 408/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 409/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 410/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 411/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 412/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 413/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 414/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 415/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 416/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 417/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 418/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 419/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 420/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 421/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 422/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 423/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 424/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 425/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 426/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 427/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 428/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 429/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 430/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 431/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 432/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 433/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 434/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 435/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 436/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 437/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 438/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 439/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 440/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 441/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 442/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 443/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 444/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 445/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 446/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 447/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 448/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 449/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 450/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 451/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 452/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 453/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 454/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 455/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 456/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 457/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 458/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 459/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 460/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 461/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 462/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 463/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 464/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 465/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 466/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 467/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 468/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 469/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 470/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 471/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 472/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 473/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 474/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 475/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 476/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 477/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 478/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 479/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 480/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 481/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 482/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 483/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 484/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 485/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 486/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 487/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 488/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 489/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 490/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 491/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 492/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 493/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 494/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 495/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 496/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 497/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 498/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 499/500, Avg Training Loss: nan, Avg Validation Loss: nan\n",
      "Epoch 500/500, Avg Training Loss: nan, Avg Validation Loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IL TEST NON CONVERGE E IMPAZZISCE DOPO TOT EPOCHE\n",
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x_tot = np.random.rand(100, 3)\n",
    "target = np.random.rand(100, 2)\n",
    "\n",
    "layer_one = Layer(3, 2, linear, d_linear)\n",
    "layer_two = Layer(2, 2, linear, d_linear)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "NN.add_layer(layer_one)\n",
    "NN.add_layer(layer_two)\n",
    "\n",
    "# Parametri di training\n",
    "K = 5\n",
    "epochs = 500\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "# Cross-validation\n",
    "train_error, val_error = NN.k_fold_cross_validation(\n",
    "    x_tot, target, K, epochs, learning_rate, mean_squared_error, d_mean_squared_error, batch_size\n",
    ")\n",
    "\n",
    "# Plot degli errori\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_error, label='Training Error')\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
