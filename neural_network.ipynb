{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cose da fare\n",
    "* tocca fa adam\n",
    "* inserire documentazione per le regolarizzazioni (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
    "* inserire loss function per problemi di classificazione: BCE o altro\n",
    "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
    "* analisi training error vs validation error vs test error\n",
    "### Novelties\n",
    "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
    "* momentum -> OBBLIGATORIO \n",
    "* early stopping\n",
    "* learning rate variabile/adam \n",
    "* standarditation e normalization (FACOLTATIVO FORSE)\n",
    "# Cose da fare secondo le (!) Micheli\n",
    "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives.\n",
    "* **Hidden layers**:\n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "qv1wwlgWsFM8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha):\n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n",
    "\n",
    "# np.vectorize returns an object that acts like pyfunc, but takes arrays as input\n",
    "d_ReLU = np.vectorize(d_ReLU)\n",
    "d_leaky_relu = np.vectorize(d_leaky_relu)\n",
    "ELU = np.vectorize(ELU)\n",
    "d_ELU = np.vectorize(d_ELU) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjNvxQOLsFM-"
   },
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters:**\n",
    " * input_size: $k_{i-1}$;\n",
    " * output_size: $k_i$;\n",
    " * activation_function;\n",
    " * activation_derivative. \n",
    "\n",
    "**Constructor attributes:**\n",
    "* self.input_size = input_size;\n",
    "* self.output_size = output_size;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative;\n",
    "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
    "\n",
    "**Methods :**\n",
    "\n",
    "* **initialize_weights**: initialize weights and biases\n",
    "  * attributes:\n",
    "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
    "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
    "    \n",
    "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
    "  * parameter:\n",
    "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
    "  * attributes:\n",
    "    * self.input: input_array;\n",
    "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
    "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
    "  \n",
    "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    "  * parameters:\n",
    "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "    * learning_rate.\n",
    "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "H8Ap9rLxLlNU",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\\n        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\\n\\n        # tikhonov and lasso implementation   \\n        if (reg_type=='tikhonov'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\\n        elif (reg_type=='lasso'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\\n        elif (reg_type=='elastic'): # lasso + tikhonov regularization\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights) \\n        \\n        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\\n        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\\n        return sum_delta_weights\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
    "        self.biases = np.zeros((1, self.output_size))\n",
    "\n",
    "        self.velocity_weights = np.zeros_like(self.weights) # zeros array with dim = dim(self.weights)\n",
    "        self.velocity_biases = np.zeros_like(self.biases)\n",
    "        \n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "    \n",
    "    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum):\n",
    "        weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights. \n",
    "        self.net = np.dot(self.input, weights_pred) + self.biases  #  Net has been computed with respect to the predicted weights\n",
    "        delta = d_Ep * self.activation_derivative(self.net)  # Loss gradient\n",
    "        grad_weights = learning_rate * np.dot(self.input.T, delta)  # The gradient has been computed with respect to the predicted weights\n",
    "\n",
    "        # Tikhonov and lasso implementation\n",
    "        if reg_type == 'tikhonov':\n",
    "            grad_weights += 2 * Lambda_t * self.weights # learning rule - tikhonov regularization\n",
    "        elif reg_type == 'lasso':\n",
    "            grad_weights += Lambda_l * np.sign(self.weights) # learning rule - lasso regularization\n",
    "        elif reg_type == 'elastic':\n",
    "            grad_weights += 2 * Lambda_t * self.weights + Lambda_l * np.sign(self.weights) # lasso + tikhonov regularization\n",
    "\n",
    "        self.velocity_weights = momentum * self.velocity_weights + grad_weights  # Delta w new\n",
    "        self.weights -= self.velocity_weights  # Update of the weights\n",
    "\n",
    "        self.velocity_biases = momentum * self.velocity_biases + learning_rate * np.sum(delta, axis=0, keepdims=True) \n",
    "        self.biases += self.velocity_biases\n",
    "\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n",
    "\n",
    "'''\n",
    "    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "\n",
    "        # tikhonov and lasso implementation   \n",
    "        if (reg_type=='tikhonov'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\n",
    "        elif (reg_type=='lasso'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\n",
    "        elif (reg_type=='elastic'): # lasso + tikhonov regularization\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights) \n",
    "        \n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
    "   * parameters:\n",
    "     * x_tot: total data given as input;\n",
    "     * target: total data labels given as input;\n",
    "     * test_split: percentile of test set with respect to the total data.\n",
    "    * return->:\n",
    "      * x_train_val: training & validation set extracted from input data;\n",
    "      * target_train_val: training & validation set labels;\n",
    "      * x_test_val: test set extracted from input data;\n",
    "      * target_test_val: test set for input data labels.\n",
    "\n",
    "     \n",
    "* **add_layer**: appends a layer to the empty list self.layers\n",
    "   * parameter:\n",
    "     * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    "  * parameter:\n",
    "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    "  * return -> input = $o_i$ for layer $L_i$.\n",
    "  \n",
    "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    "  * parameters:\n",
    "    * d_Ep;\n",
    "    * learning_rate.\n",
    "\n",
    "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
    "\n",
    "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
    "  * parameters:\n",
    "    * x_train: set of the original dataset used for training;\n",
    "    * target_train: labels corresponding to the training set;\n",
    "    * x_val: set of the original dataset used for validation;\n",
    "    * target_val: labels corresponding to the validation set;\n",
    "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
    "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
    "    * loss_function: hyperparameter;\n",
    "    * loss_function_derivative: hyperparameter;\n",
    "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
    "\n",
    "* **train_val**: actual training and validation process.\n",
    "  * parameters:\n",
    "    * x_train_val;\n",
    "    * target_train_val;\n",
    "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
    "    * epochs;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative;\n",
    "    * batch_size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def data_split(self, x_tot, target, test_split):\n",
    "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
    "        test_size = int(num_samples * test_split) # the number of the examples in the test set        \n",
    "\n",
    "        x_test = x_tot[:test_size]\n",
    "        target_test = target[:test_size]\n",
    "        x_train_val = x_tot[test_size:]\n",
    "        target_train_val = target[test_size:]\n",
    "\n",
    "        return x_train_val, target_train_val, x_test, target_test\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum)\n",
    "\n",
    "    def reinitialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights() # does it layer-by-layer         \n",
    "\n",
    "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size):\n",
    "        train_error_epoch = np.zeros(epochs)\n",
    "        val_error_epoch = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            # shuffling training data before splitting it into batches.\n",
    "            # done in order to avoid reinforcing neurons in the same way\n",
    "            # in different epochs due to invisible patterns in the data\n",
    "            train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(train_indices)\n",
    "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
    "            target_train = target_train[train_indices]\n",
    "            \n",
    "            # if batch_size=1 we get the online version, \n",
    "            # else we get mini-batch version with batches of size batch_size\n",
    "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
    "                                                             # Even if the number of examples is not divisible \n",
    "                                                             # for batch_size the last, smaller batch is processed anyway\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                target_batch = target_train[i:i+batch_size]\n",
    "\n",
    "                # forward propagation\n",
    "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
    "                # computing loss and gradient\n",
    "                loss = loss_function(target_batch, predictions)\n",
    "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "                epoch_tr_loss += np.sum(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(loss_gradient, learning_rate, Lambda_t, Lambda_l, reg_type, momentum)\n",
    "\n",
    "            # validation\n",
    "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
    "            val_loss = loss_function(target_val, val_predictions)\n",
    "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished. \n",
    "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
    "            # Store average errors for the epoch\n",
    "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
    "            val_error_epoch[epoch] = epoch_val_loss\n",
    "\n",
    "        return train_error_epoch, val_error_epoch\n",
    "\n",
    "\n",
    "    def train_val(self, x_train_val, target_train_val, K, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_train_val.shape[0]\n",
    "        fold_size = num_samples // K\n",
    "\n",
    "        # error storage for averaging\n",
    "        avg_train_error_epoch = np.zeros(epochs)\n",
    "        avg_val_error_epoch = np.zeros(epochs)\n",
    "        \n",
    "        if K==1: # hold-out validation\n",
    "            train_indices = np.arange(0, int(0.75*num_samples)) # training set is 75% of the training & validation set\n",
    "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets           \n",
    "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size\n",
    "            ) # computation of errors via train_val_setup method\n",
    "            return train_error_epoch, val_error_epoch\n",
    "            \n",
    "\n",
    "        for k in range(K):\n",
    "            # creating fold indices\n",
    "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
    "                                                                        # It contains the indices of the examples used in validation set for\n",
    "                                                                        # this fold.\n",
    "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples - \n",
    "                                                                              # len(val_indices). It contains the indices of all the examples\n",
    "                                                                              # but the ones used in the validation set for this fold. \n",
    "                                                                              # It corresponds to the training set for the current fold. \n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
    "\n",
    "            # re-initializing weights for each fold\n",
    "            self.reinitialize_weights()\n",
    "\n",
    "            # training on the current fold\n",
    "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "\n",
    "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
    "            avg_train_error_epoch += train_error_epoch\n",
    "            avg_val_error_epoch += val_error_epoch\n",
    "\n",
    "            print(f\"Fold {k+1} completed.\")\n",
    "\n",
    "        # averaging errors across all folds\n",
    "        avg_train_error_epoch /= K\n",
    "        avg_val_error_epoch /= K\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
    "\n",
    "        return avg_train_error_epoch, avg_val_error_epoch\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp0g8MgT3hhO",
    "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in square\n",
      "  \n",
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-e98944bc61b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Cross-validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mtrain_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'elastic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_mean_squared_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Plot degli errori\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-53dda937935f>\u001b[0m in \u001b[0;36mtrain_val\u001b[1;34m(self, x_train_val, target_train_val, K, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;31m# training on the current fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             train_error_epoch, val_error_epoch = self.train_val_setup(\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function_derivative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             )\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-53dda937935f>\u001b[0m in \u001b[0;36mtrain_val_setup\u001b[1;34m(self, x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_gradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-53dda937935f>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_Ep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0md_Ep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_Ep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLambda_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreinitialize_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-56db6d8e6dab>\u001b[0m in \u001b[0;36mbackward_layer\u001b[1;34m(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_pred\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m  \u001b[1;31m#  Net has been computed with respect to the predicted weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0md_Ep\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Loss gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mgrad_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# The gradient has been computed with respect to the predicted weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Tikhonov and lasso implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x_tot = np.random.rand(1000, 15)\n",
    "target = np.random.rand(1000, 3)\n",
    "\n",
    "layer_one = Layer(15, 8, ELU, d_ELU)\n",
    "layer_two = Layer(8, 5, ELU, d_ELU)\n",
    "layer_out = Layer(5, 3, linear, d_linear)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "NN.add_layer(layer_one)\n",
    "NN.add_layer(layer_two)\n",
    "NN.add_layer(layer_out)\n",
    "\n",
    "# Parametri di training\n",
    "K = 5\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
    "                       # learning_rate of minibatch for number of the examples in the minibatch\n",
    "batch_size = 100\n",
    "Lambda_t = 0.1\n",
    "Lambda_l = 0.1\n",
    "momentum = 0.7 # totalmente a caso\n",
    "\n",
    "# Cross-validation\n",
    "train_error, val_error = NN.train_val(x_tot, target, K, epochs, learning_rate, Lambda_t, Lambda_l, 'elastic', momentum, mean_squared_error, d_mean_squared_error, batch_size)\n",
    "\n",
    "# Plot degli errori\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_error, label='Training Error')\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
