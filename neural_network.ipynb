{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AlbertoMontanelli/Machine-Learning/blob/neural_network/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cose da fare\n",
    "* tocca fa adam\n",
    "* inserire documentazione per le regolarizzazioni (spiegare SOLO PER NOI perché la lasso favorisce i pesi che sono a 0 e porta a sparsity, mentre la ridge favorisce pesi piccoli ma non nulli e tiene di conto di tutti i pesi - meno sparsity)\n",
    "* inserire loss function per problemi di classificazione: BCE o altro\n",
    "* RICERCA DI IPERPARAMETRI -> GRID SEARCH\n",
    "* analisi training error vs validation error vs test error\n",
    "### Novelties\n",
    "* inserire novelties sulla back-prop: quick-prop, R-prop\n",
    "* momentum -> OBBLIGATORIO \n",
    "* early stopping\n",
    "* learning rate variabile/adam \n",
    "* standarditation e normalization (FACOLTATIVO FORSE)\n",
    "# Cose da fare secondo le (!) Micheli\n",
    "* **Try a number of random starting configurations** (e.g. 5-10 or more training runs or trials) -> una configurazione è nr di layers, nr unità per layer, inzializzazione pesi e bias (quindi togliere magari il seed). Useful for the project: take the mean results (mean of errors) and look to variance to evaluate your model and then, if you like to have only 1 response:  you can choose the solution giving lowest (penalized) validation error (or the median) or you can take advantage of different end points by an average (committee) response: mean of the outputs/voting\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT3nSmTsZ9NP"
   },
   "source": [
    "# Notation conventions\n",
    "* **net** = $X \\cdot W+b$, $\\quad X$: input matrix, $\\quad W$: weights matrix, $\\quad b$: bias array;\n",
    "* **number of examples** = $l$ ;\n",
    "* **number of features** = $n$ ;\n",
    "* **input_size** : for the layer $i$ -> $k_{i-1}$ : number of the units of the previous layer $i-1$ ;\n",
    "* **outputz_size** : for the layer $i$ -> $k_{i}$ : number of the units of the current layer $i$;\n",
    "* **output_value** : $o_i=f(net_i)$ for layer $i$, where $f$ is the activation function.\n",
    "* **number of labels** = $d$ for each example -> $l \\ \\textrm{x}\\ d$ matrix. \\\n",
    "dim(**labels**) = dim(**predictions**) = dim(**targets**).\n",
    "\n",
    "### Input Layer $L_0$ with $k_0$ units :\n",
    "* input_size = $n$;\n",
    "* output_size = $k_0$;\n",
    "* net = $X \\cdot W +b$, $\\quad X$ : $l \\ \\textrm{x} \\ n$ matrix, $\\quad W: n \\ \\textrm{x} \\ k_0$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_0$ array; \\\n",
    "$⇒$ net: $l \\ \\textrm{x} \\ k_0$ matrix.\n",
    "\n",
    "### Generic Layer $L_i$ with $k_i$ units :\n",
    "* input_size = $k_{i-1}$ ;\n",
    "* output_size = $k_i$ ;\n",
    "* net = $X \\cdot W+b$, $\\quad X$ : $l \\ \\textrm{x} \\ k_{i-1}$ matrix, $\\quad W: k_{i-1} \\ \\textrm{x} \\ k_i$ matrix, $\\quad b = 1 \\ \\textrm{x} \\ k_i$ array ; \\\n",
    "$⇒$ net : $l \\ \\textrm{x} \\ k_i$ matrix .\n",
    "\n",
    "### Online vs mini-batch version:\n",
    "* online version: $l' = 1$ example;\n",
    "* mini-batch version: $l' =$ number of examples in the mini-batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7La5v5wwHVP"
   },
   "source": [
    "# Activation functions\n",
    "Definition of the activation functions and their derivatives.\n",
    "* **Hidden layers**:\n",
    "  * **ReLU**: computationally efficient, resilient to vanishing gradient problem, suffers if net < 0;\n",
    "  * **Leaky ReLU**: better than ReLU in case of convergence problems thanks to non null output for net < 0;\n",
    "    * $0<$ alpha $<<1$: if alpha is too small, the gradient could be negligable;\n",
    "  * **ELU**: same as Leaky ReLU, the best in term of performances but the worst in term of computational costs;\n",
    "  * **tanh**: very useful if data are distributed around 0, but tends to saturate to -1 or +1 in other cases;\n",
    "* **Output layer**:\n",
    "  * **Regression problem**: Linear output;\n",
    "  * **Binary classification**: Sigmoid, converts values to 0 or 1 via threshold while being differentiable in 0, unlike e.g. the sign function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qv1wwlgWsFM8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(net):\n",
    "    return 1 / (1 + np.exp(-net))\n",
    "\n",
    "def d_sigmoid(net):\n",
    "    return np.exp(-net) / (1 + np.exp(-net))**2\n",
    "\n",
    "def tanh(net):\n",
    "    return np.tanh(net)\n",
    "\n",
    "def d_tanh(net):\n",
    "    return 1 - (np.tanh(net))**2\n",
    "\n",
    "\"\"\"   DA RIVEDERE\n",
    "\n",
    "def softmax(net):\n",
    "    return np.exp(net) / np.sum(np.exp(net), axis = 1, keepdims=True)\n",
    "\n",
    "def softmax_derivative(net):\n",
    "\n",
    "    # batch_size is the number of the rows in the matrix net; current_neuron_size is the number of the columns\n",
    "    batch_size, current_neuron_size = net.shape\n",
    "\n",
    "    # initialization of Jacobian tensor: each example in the batch (batch_size) is the input to current_neuron_size neurons,\n",
    "    # for each neuron we compute current_neuron_size derivatives with respect to the other neurons and itself. This results in a\n",
    "    # batch_size x current_neuron_size x current_neuron_size tensor.\n",
    "    jacobians = np.zeros((batch_size, current_neuron_size, current_neuron_size))\n",
    "\n",
    "    for i in range(batch_size): # for each example i in the batch\n",
    "        s = net[i].reshape(-1, 1)  # creation of a column vector of dimension current_neuron_size x 1, s contains all the features of\n",
    "                                   # the example i\n",
    "        jacobians[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobians\n",
    "\"\"\"\n",
    "\n",
    "def softplus(net):\n",
    "    return np.log(1 + np.exp(net))\n",
    "\n",
    "def d_softplus(net):\n",
    "    return np.exp(net) / (1 + np.exp(net))\n",
    "\n",
    "def linear(net):\n",
    "    return net\n",
    "\n",
    "def d_linear(net):\n",
    "    return 1\n",
    "\n",
    "def ReLU(net):\n",
    "    return np.maximum(net, 0)\n",
    "\n",
    "def d_ReLU(net):\n",
    "    return 1 if(net>=0) else 0\n",
    "\n",
    "def leaky_relu(net, alpha):\n",
    "    return np.maximum(net, alpha*net)\n",
    "\n",
    "def d_leaky_relu(net, alpha):\n",
    "    return 1 if(net>=0) else alpha\n",
    "\n",
    "def ELU(net):\n",
    "    return net if(net>=0) else np.exp(net)-1\n",
    "\n",
    "ELU = np.vectorize(ELU)\n",
    "\n",
    "def d_ELU(net):\n",
    "    return 1 if(net>=0) else np.exp(net)\n",
    "\n",
    "d_ELU = np.vectorize(d_ELU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjNvxQOLsFM-"
   },
   "source": [
    "# Loss/Error functions:\n",
    "Definition of loss/error functions and their derivatives. \\\n",
    "For each derivative we omit a minus from the computation because it's included later in the computation of the learning rule:\n",
    "* **mean_squared_error**;\n",
    "* **mean_euclidian_error**;\n",
    "* **huber_loss**: used when there are expected big and small errors due to outliers or noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8Q7LVpTswQAh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.sum((y_true - y_pred)**2)\n",
    "\n",
    "def d_mean_squared_error(y_true, y_pred):\n",
    "    return 2 * (y_true - y_pred)  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def mean_euclidian_error(y_true, y_pred):\n",
    "    return np.sqrt(np.sum((y_true - y_pred)**2))\n",
    "\n",
    "def d_mean_euclidian_error(y_true, y_pred):\n",
    "    return (y_true - y_pred) / np.sqrt(np.sum((y_true - y_pred)**2))  # we'd get a minus but it's included in the computation of the learning rule\n",
    "\n",
    "def huber_loss(y_true, y_pred, delta):\n",
    "    return 0.5 * (y_true - y_pred)**2 if(np.abs(y_true-y_pred)<=delta) else delta * np.abs(y_true - y_pred) - 0.5 * delta**2\n",
    "\n",
    "def d_huber_loss(y_true, y_pred, delta):\n",
    "    return y_true - y_pred if(np.abs(y_true-y_pred)<=delta) else delta * np.sign(y_true-y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAFcEjML9we1"
   },
   "source": [
    "#  class Layer\n",
    "**Constructor parameters:**\n",
    " * input_size: $k_{i-1}$;\n",
    " * output_size: $k_i$;\n",
    " * activation_function;\n",
    " * activation_derivative. \n",
    "\n",
    "**Constructor attributes:**\n",
    "* self.input_size = input_size;\n",
    "* self.output_size = output_size;\n",
    "* self.activation_function;\n",
    "* self.activation_derivative;\n",
    "* self.initialize_weights(): initialize weights and biases recalling the method initialize_weights every time an istance of the class Layer is created.\n",
    "\n",
    "**Methods :**\n",
    "\n",
    "* **initialize_weights**: initialize weights and biases\n",
    "  * attributes:\n",
    "    * self.weights: $k_{i-1} \\ \\textrm{x} \\ k_i$ matrix. Initialized extracting randomly from a uniform distribution [-1/a, 1/a], where a = $\\sqrt{k_{i-1}}$;\n",
    "    * self.biases: $1 \\ \\textrm{x} \\ k_i$ array. Initialized to an array of zeros;\n",
    "    \n",
    "* **forward_layer**: allows to compute the output of the layer for a given input.\n",
    "  * parameter:\n",
    "    * input_array: matrix $X$ (see above for the case $L_0$ or $L_i$).\n",
    "  * attributes:\n",
    "    * self.input: input_array;\n",
    "    * self.net: net matrix $X \\cdot W + b$ (see above for the case $L_0$ or $L_i$).\n",
    "  * return -> output = $f(net)$, where $f$ is the activation function. $f(net)$ has the same dimensions of $net$.\n",
    "  \n",
    "* **backward_layer**: computes the gradient loss and updates the weights by the learning rule for the single layer.\n",
    "  * parameters:\n",
    "    * d_Ep: target_value $-$ output_value, element by element: $l \\ \\textrm{x} \\ d$ matrix.\n",
    "    * learning_rate.\n",
    "  * return -> sum_delta_weights $= \\delta \\cdot W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8Ap9rLxLlNU",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\\n        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\\n\\n        # tikhonov and lasso implementation   \\n        if (reg_type=='tikhonov'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\\n        elif (reg_type=='lasso'):\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\\n        elif (reg_type=='elastic'): # lasso + tikhonov regularization\\n            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights) \\n        \\n        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\\n        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\\n        return sum_delta_weights\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, input_size, output_size, activation_function, activation_derivative):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_derivative = activation_derivative\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        self.weights = np.random.uniform(low=-1/np.sqrt(self.input_size), high=1/np.sqrt(self.input_size), size=(self.input_size, self.output_size))\n",
    "        self.biases = np.zeros((1, self.output_size))\n",
    "\n",
    "        self.velocity_weights = np.zeros_like(self.weights)\n",
    "        \n",
    "    def forward_layer(self, input_array):\n",
    "        self.input = input_array\n",
    "        self.net = np.dot(self.input, self.weights) + self.biases\n",
    "        output = self.activation_function(self.net)\n",
    "        return output\n",
    "    \n",
    "    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum):\n",
    "        weights_pred = self.weights + momentum * self.velocity_weights  # predicted weights. \n",
    "        self.net = np.dot(self.input, weights_pred) + self.biases  #  Net has been computed with respect to the predicted weights\n",
    "        delta = d_Ep * self.activation_derivative(self.net)  # Loss gradient\n",
    "        grad_weights = learning_rate * np.dot(self.input.T, delta)  # The gradient has been computed with respect to the predicted weights\n",
    "\n",
    "        # Tikhonov and lasso implementation\n",
    "        if reg_type == 'tikhonov':\n",
    "            grad_weights += 2 * Lambda_t * self.weights # learning rule - tikhonov regularization\n",
    "        elif reg_type == 'lasso':\n",
    "            grad_weights += Lambda_l * np.sign(self.weights) # learning rule - lasso regularization\n",
    "        elif reg_type == 'elastic':\n",
    "            grad_weights += 2 * Lambda_t * self.weights + Lambda_l * np.sign(self.weights) # lasso + tikhonov regularization\n",
    "\n",
    "        self.velocity_weights = learning_rate * momentum * self.velocity_weights + grad_weights  # Delta w new\n",
    "        self.weights -= self.velocity_weights  # Update of the weights\n",
    "\n",
    "        # Va fatto anche per il bias ????????????????????????\n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n",
    "\n",
    "'''\n",
    "    def backward_layer(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type):\n",
    "        delta = d_Ep * self.activation_derivative(self.net) # loss gradient\n",
    "\n",
    "        # tikhonov and lasso implementation   \n",
    "        if (reg_type=='tikhonov'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights # learning rule - tikhonov regularization\n",
    "        elif (reg_type=='lasso'):\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - Lambda_l*np.sign(self.weights) # learning rule - lasso regularization\n",
    "        elif (reg_type=='elastic'): # lasso + tikhonov regularization\n",
    "            self.weights += learning_rate * np.dot(self.input.T, delta) - 2*Lambda_t*self.weights - Lambda_l*np.sign(self.weights) \n",
    "        \n",
    "        self.biases += learning_rate * np.sum(delta, axis = 0, keepdims = True) # learning rule for the biases\n",
    "        sum_delta_weights = np.dot(delta, self.weights.T) # loss gradient for hidden layer\n",
    "        return sum_delta_weights\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rQI2lNkn83H"
   },
   "source": [
    "# class NeuralNetwork\n",
    "**Constructor attributes**:\n",
    " * self.layers: an empty list that will contain the layers.\n",
    "\n",
    "**Methods**:\n",
    "\n",
    "* **data_split**: splits the input data into two sets: training & validation set, test set.\n",
    "   * parameters:\n",
    "     * x_tot: total data given as input;\n",
    "     * target: total data labels given as input;\n",
    "     * test_split: percentile of test set with respect to the total data.\n",
    "    * return->:\n",
    "      * x_train_val: training & validation set extracted from input data;\n",
    "      * target_train_val: training & validation set labels;\n",
    "      * x_test_val: test set extracted from input data;\n",
    "      * target_test_val: test set for input data labels.\n",
    "\n",
    "     \n",
    "* **add_layer**: appends a layer to the empty list self.layers\n",
    "   * parameter:\n",
    "     * layer: the layer appended to the list self.layers.\n",
    "\n",
    "* **forward**: iterates the layer.forward_layer method through each layer in the list self.layers\n",
    "  * parameter:\n",
    "    * input: $X$ matrix for layer $L_0$, $o_{i-1}$ for layer $L_i$.\n",
    "  * return -> input = $o_i$ for layer $L_i$.\n",
    "  \n",
    "* **backward**: iterates from the last layer to the first layer the layer.backward_layer method, thus updating the weights and the biases for each layer.\n",
    "  * parameters:\n",
    "    * d_Ep;\n",
    "    * learning_rate.\n",
    "\n",
    "* **reinitialize_weights**: re-initializes the weights layer-by-layer in case of need, e.g. with k-fold cross validation after each cycle.\n",
    "\n",
    "* **train_val_setup**: stores the outputs of training and validation into arrays (retrieved by the iteration of forward propagation + backpropagation), computes the training loss and the validation error.\n",
    "  * parameters:\n",
    "    * x_train: set of the original dataset used for training;\n",
    "    * target_train: labels corresponding to the training set;\n",
    "    * x_val: set of the original dataset used for validation;\n",
    "    * target_val: labels corresponding to the validation set;\n",
    "    * epochs: number of iterations of the forward propagation + backpropagation algorithms; hyperparameter;\n",
    "    * learning_rate: determines the step size of the learning algorithm. Warning: has to be tuned keeping in mind gradient explosion, unsmoothness of the learning rate, velocity of the convergence, etc...; hyperparameter;\n",
    "    * loss_function: hyperparameter;\n",
    "    * loss_function_derivative: hyperparameter;\n",
    "    * batch_size: number of examples included in each batch. If batch_size = 1 the **online algorithm** is applied (data is processed example-by-example), else the **mini-batch algorithm** is applied with batches of size batch_size; hyperparameter.\n",
    "\n",
    "* **train_val**: actual training and validation process.\n",
    "  * parameters:\n",
    "    * x_train_val;\n",
    "    * target_train_val;\n",
    "    * K: number of splits of the training & validation set. If K = 1 validation is performed as hold-out validation, else K is the number of folds in the K-fold cross validation; hyperparameter;\n",
    "    * epochs;\n",
    "    * learning_rate;\n",
    "    * loss_function;\n",
    "    * loss_function_derivative;\n",
    "    * batch_size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EmtKxOEhn91Q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def data_split(self, x_tot, target, test_split):\n",
    "        num_samples = x_tot.shape[0] # the total number of the examples in input = the number of rows in the x_tot matrix\n",
    "        test_size = int(num_samples * test_split) # the number of the examples in the test set        \n",
    "\n",
    "        x_test = x_tot[:test_size]\n",
    "        target_test = target[:test_size]\n",
    "        x_train_val = x_tot[test_size:]\n",
    "        target_train_val = target[test_size:]\n",
    "\n",
    "        return x_train_val, target_train_val, x_test, target_test\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward_layer(input)\n",
    "        return input\n",
    "    \n",
    "    def backward(self, d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_Ep = layer.backward_layer(d_Ep, learning_rate, Lambda_t, Lambda_l, reg_type, momentum)\n",
    "\n",
    "    def reinitialize_weights(self):\n",
    "        for layer in self.layers:\n",
    "            layer.initialize_weights() # does it layer-by-layer         \n",
    "\n",
    "    def train_val_setup(self, x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size):\n",
    "        train_error_epoch = np.zeros(epochs)\n",
    "        val_error_epoch = np.zeros(epochs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_tr_loss = 0 # initialization of the training loss; errors will be added epoch-by-epoch\n",
    "            epoch_val_loss = 0\n",
    "\n",
    "            # shuffling training data before splitting it into batches.\n",
    "            # done in order to avoid reinforcing neurons in the same way\n",
    "            # in different epochs due to invisible patterns in the data\n",
    "            train_indices = np.arange(x_train.shape[0])\n",
    "            np.random.shuffle(train_indices)\n",
    "            x_train = x_train[train_indices] # PROVARE SE VIENE SHUFFOLATO LO SHUFFOLATO O SE VIENE SHUFFOLATO OGNI VOLTA L'ORIGINALE\n",
    "            target_train = target_train[train_indices]\n",
    "            \n",
    "            # if batch_size=1 we get the online version, \n",
    "            # else we get mini-batch version with batches of size batch_size\n",
    "            for i in range(0, x_train.shape[0], batch_size): # the step of the for cycle is batch_size.\n",
    "                                                             # Even if the number of examples is not divisible \n",
    "                                                             # for batch_size the last, smaller batch is processed anyway\n",
    "                x_batch = x_train[i:i+batch_size]\n",
    "                target_batch = target_train[i:i+batch_size]\n",
    "\n",
    "                # forward propagation\n",
    "                predictions = self.forward(x_batch) # calculates the output of the training data\n",
    "                # computing loss and gradient\n",
    "                loss = loss_function(target_batch, predictions)\n",
    "                loss_gradient = loss_function_derivative(target_batch, predictions)\n",
    "                epoch_tr_loss += np.sum(loss)\n",
    "\n",
    "                # Backward pass\n",
    "                self.backward(loss_gradient, learning_rate, Lambda_t, Lambda_l, reg_type, momentum)\n",
    "\n",
    "            # validation\n",
    "            val_predictions = self.forward(x_val) # calculates the output of the validation date\n",
    "            val_loss = loss_function(target_val, val_predictions)\n",
    "            epoch_val_loss = np.mean(val_loss) # the validation error is computed on all validation set after the training is finished. \n",
    "                                               # the computation of the validation error at the end of the epoch is the standard for all NN\n",
    "            # Store average errors for the epoch\n",
    "            train_error_epoch[epoch] = epoch_tr_loss / x_train.shape[0]\n",
    "            val_error_epoch[epoch] = epoch_val_loss\n",
    "\n",
    "        return train_error_epoch, val_error_epoch\n",
    "\n",
    "\n",
    "    def train_val(self, x_train_val, target_train_val, K, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size):\n",
    "        num_samples = x_train_val.shape[0]\n",
    "        fold_size = num_samples // K\n",
    "\n",
    "        # error storage for averaging\n",
    "        avg_train_error_epoch = np.zeros(epochs)\n",
    "        avg_val_error_epoch = np.zeros(epochs)\n",
    "        \n",
    "        if K==1: # hold-out validation\n",
    "            train_indices = np.arange(0, int(0.75*num_samples)) # training set is 75% of the training & validation set\n",
    "            val_indices = np.setdiff1d(np.arange(num_samples), train_indices) # setdiff1d is the set difference between the first and the second set\n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices] # definition of training set with matching targets\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices] # definition of validation set with matching targets           \n",
    "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size\n",
    "            ) # computation of errors via train_val_setup method\n",
    "            return train_error_epoch, val_error_epoch\n",
    "            \n",
    "\n",
    "        for k in range(K):\n",
    "            # creating fold indices\n",
    "            val_indices = np.arange(k * fold_size, (k + 1) * fold_size) # creation of an array of indices with len = fold_size.\n",
    "                                                                        # It contains the indices of the examples used in validation set for\n",
    "                                                                        # this fold.\n",
    "            train_indices = np.setdiff1d(np.arange(num_samples), val_indices) # creation of an array of indices with len = num_samples - \n",
    "                                                                              # len(val_indices). It contains the indices of all the examples\n",
    "                                                                              # but the ones used in the validation set for this fold. \n",
    "                                                                              # It corresponds to the training set for the current fold. \n",
    "            x_train, target_train = x_train_val[train_indices], target_train_val[train_indices]\n",
    "            x_val, target_val = x_train_val[val_indices], target_train_val[val_indices]\n",
    "\n",
    "            # re-initializing weights for each fold\n",
    "            self.reinitialize_weights()\n",
    "\n",
    "            # training on the current fold\n",
    "            train_error_epoch, val_error_epoch = self.train_val_setup(\n",
    "            x_train, target_train, x_val, target_val, epochs, learning_rate, Lambda_t, Lambda_l, reg_type, momentum, loss_function, loss_function_derivative, batch_size\n",
    "            )\n",
    "\n",
    "            # accumulating errors for averaging, we have the errors for each epoch summed for all the folds\n",
    "            avg_train_error_epoch += train_error_epoch\n",
    "            avg_val_error_epoch += val_error_epoch\n",
    "\n",
    "            print(f\"Fold {k+1} completed.\")\n",
    "\n",
    "        # averaging errors across all folds\n",
    "        avg_train_error_epoch /= K\n",
    "        avg_val_error_epoch /= K\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Avg Training Loss: {train_error_epoch[epoch]}, Avg Validation Loss: {val_error_epoch[epoch]}\")\n",
    "\n",
    "        return avg_train_error_epoch, avg_val_error_epoch\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT0LTc5_oFD2"
   },
   "source": [
    "# Unit Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cp0g8MgT3hhO",
    "outputId": "b78a9bed-2010-4273-fcef-d34a0603d40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 completed.\n",
      "Fold 2 completed.\n",
      "Fold 3 completed.\n",
      "Fold 4 completed.\n",
      "Fold 5 completed.\n",
      "Epoch 1/1000, Avg Training Loss: 0.916605301018193, Avg Validation Loss: 153.77555638447595\n",
      "Epoch 2/1000, Avg Training Loss: 0.7329204966991335, Avg Validation Loss: 123.82491205815421\n",
      "Epoch 3/1000, Avg Training Loss: 0.5985666656844897, Avg Validation Loss: 102.34715149727533\n",
      "Epoch 4/1000, Avg Training Loss: 0.5012533129861843, Avg Validation Loss: 86.9482223520544\n",
      "Epoch 5/1000, Avg Training Loss: 0.43134127050085147, Avg Validation Loss: 75.85746383750224\n",
      "Epoch 6/1000, Avg Training Loss: 0.3808623918271358, Avg Validation Loss: 67.9204471880814\n",
      "Epoch 7/1000, Avg Training Loss: 0.34365376271621434, Avg Validation Loss: 62.241447523946974\n",
      "Epoch 8/1000, Avg Training Loss: 0.3172478001427556, Avg Validation Loss: 58.16691105163946\n",
      "Epoch 9/1000, Avg Training Loss: 0.29795618060047574, Avg Validation Loss: 55.26829162459332\n",
      "Epoch 10/1000, Avg Training Loss: 0.28403791970301656, Avg Validation Loss: 53.21688479369527\n",
      "Epoch 11/1000, Avg Training Loss: 0.27386188082558904, Avg Validation Loss: 51.765003746991184\n",
      "Epoch 12/1000, Avg Training Loss: 0.266800887876209, Avg Validation Loss: 50.73015726675722\n",
      "Epoch 13/1000, Avg Training Loss: 0.2615005820026402, Avg Validation Loss: 50.01243498115598\n",
      "Epoch 14/1000, Avg Training Loss: 0.2577396855494074, Avg Validation Loss: 49.513950369004235\n",
      "Epoch 15/1000, Avg Training Loss: 0.25497366686146383, Avg Validation Loss: 49.168320169921444\n",
      "Epoch 16/1000, Avg Training Loss: 0.25295427352594246, Avg Validation Loss: 48.93050623236665\n",
      "Epoch 17/1000, Avg Training Loss: 0.2515402053341915, Avg Validation Loss: 48.770817721607564\n",
      "Epoch 18/1000, Avg Training Loss: 0.2506199130977558, Avg Validation Loss: 48.665631981450275\n",
      "Epoch 19/1000, Avg Training Loss: 0.24958905285504485, Avg Validation Loss: 48.59600728483434\n",
      "Epoch 20/1000, Avg Training Loss: 0.2491566026224209, Avg Validation Loss: 48.554184054635805\n",
      "Epoch 21/1000, Avg Training Loss: 0.2486550277987301, Avg Validation Loss: 48.53059599257557\n",
      "Epoch 22/1000, Avg Training Loss: 0.24856378408812865, Avg Validation Loss: 48.51934282113778\n",
      "Epoch 23/1000, Avg Training Loss: 0.2481907409680142, Avg Validation Loss: 48.516594293762566\n",
      "Epoch 24/1000, Avg Training Loss: 0.24793207096008188, Avg Validation Loss: 48.51746423289407\n",
      "Epoch 25/1000, Avg Training Loss: 0.2480665157592888, Avg Validation Loss: 48.522596443460095\n",
      "Epoch 26/1000, Avg Training Loss: 0.24787232641289833, Avg Validation Loss: 48.5296379748404\n",
      "Epoch 27/1000, Avg Training Loss: 0.24792926736851242, Avg Validation Loss: 48.537036328009656\n",
      "Epoch 28/1000, Avg Training Loss: 0.24763696711023048, Avg Validation Loss: 48.54325243176329\n",
      "Epoch 29/1000, Avg Training Loss: 0.24792722980584997, Avg Validation Loss: 48.550365857016466\n",
      "Epoch 30/1000, Avg Training Loss: 0.2476403261986459, Avg Validation Loss: 48.556115174635934\n",
      "Epoch 31/1000, Avg Training Loss: 0.2477198898506817, Avg Validation Loss: 48.56279909201281\n",
      "Epoch 32/1000, Avg Training Loss: 0.24776323246567475, Avg Validation Loss: 48.56783143381786\n",
      "Epoch 33/1000, Avg Training Loss: 0.24776082263298618, Avg Validation Loss: 48.57219659521749\n",
      "Epoch 34/1000, Avg Training Loss: 0.24768958531384488, Avg Validation Loss: 48.577736803613334\n",
      "Epoch 35/1000, Avg Training Loss: 0.24755086186509298, Avg Validation Loss: 48.582017265791166\n",
      "Epoch 36/1000, Avg Training Loss: 0.24770857270889374, Avg Validation Loss: 48.58655870622128\n",
      "Epoch 37/1000, Avg Training Loss: 0.24768126347118444, Avg Validation Loss: 48.58873728210298\n",
      "Epoch 38/1000, Avg Training Loss: 0.24767199284913627, Avg Validation Loss: 48.59190722916733\n",
      "Epoch 39/1000, Avg Training Loss: 0.24763464313292502, Avg Validation Loss: 48.59504073851332\n",
      "Epoch 40/1000, Avg Training Loss: 0.24796701311774907, Avg Validation Loss: 48.59627463879951\n",
      "Epoch 41/1000, Avg Training Loss: 0.2476562271643089, Avg Validation Loss: 48.59759725481327\n",
      "Epoch 42/1000, Avg Training Loss: 0.24765537767388132, Avg Validation Loss: 48.598652084773995\n",
      "Epoch 43/1000, Avg Training Loss: 0.2476182269488423, Avg Validation Loss: 48.59875356892758\n",
      "Epoch 44/1000, Avg Training Loss: 0.24780114834250302, Avg Validation Loss: 48.59970095570161\n",
      "Epoch 45/1000, Avg Training Loss: 0.24752522249722506, Avg Validation Loss: 48.60138930774065\n",
      "Epoch 46/1000, Avg Training Loss: 0.2476261831661915, Avg Validation Loss: 48.60201152777289\n",
      "Epoch 47/1000, Avg Training Loss: 0.24769480674270572, Avg Validation Loss: 48.603388988270595\n",
      "Epoch 48/1000, Avg Training Loss: 0.24763171994404976, Avg Validation Loss: 48.60281917120571\n",
      "Epoch 49/1000, Avg Training Loss: 0.24769766343790212, Avg Validation Loss: 48.604999379764934\n",
      "Epoch 50/1000, Avg Training Loss: 0.24761623489713458, Avg Validation Loss: 48.60635250581328\n",
      "Epoch 51/1000, Avg Training Loss: 0.247472248623772, Avg Validation Loss: 48.606088050703086\n",
      "Epoch 52/1000, Avg Training Loss: 0.24756164193334013, Avg Validation Loss: 48.60700582252883\n",
      "Epoch 53/1000, Avg Training Loss: 0.247749516449489, Avg Validation Loss: 48.607797591834725\n",
      "Epoch 54/1000, Avg Training Loss: 0.24780034965445735, Avg Validation Loss: 48.60687422756\n",
      "Epoch 55/1000, Avg Training Loss: 0.24769456944942853, Avg Validation Loss: 48.60707338161294\n",
      "Epoch 56/1000, Avg Training Loss: 0.2475980793408997, Avg Validation Loss: 48.60663698702382\n",
      "Epoch 57/1000, Avg Training Loss: 0.2478569568309903, Avg Validation Loss: 48.607227201324974\n",
      "Epoch 58/1000, Avg Training Loss: 0.24782437152317838, Avg Validation Loss: 48.60668144383956\n",
      "Epoch 59/1000, Avg Training Loss: 0.24789589993323274, Avg Validation Loss: 48.60740180064623\n",
      "Epoch 60/1000, Avg Training Loss: 0.2476044316064913, Avg Validation Loss: 48.606866972580676\n",
      "Epoch 61/1000, Avg Training Loss: 0.24764765146429732, Avg Validation Loss: 48.60603940552921\n",
      "Epoch 62/1000, Avg Training Loss: 0.24773173470912904, Avg Validation Loss: 48.607917712496295\n",
      "Epoch 63/1000, Avg Training Loss: 0.24760182695771213, Avg Validation Loss: 48.60739078768531\n",
      "Epoch 64/1000, Avg Training Loss: 0.24776163596772968, Avg Validation Loss: 48.608622776892375\n",
      "Epoch 65/1000, Avg Training Loss: 0.24779909596409191, Avg Validation Loss: 48.609109983172814\n",
      "Epoch 66/1000, Avg Training Loss: 0.24779634027456296, Avg Validation Loss: 48.60922281666289\n",
      "Epoch 67/1000, Avg Training Loss: 0.247695634476432, Avg Validation Loss: 48.60896598766036\n",
      "Epoch 68/1000, Avg Training Loss: 0.24763935632418627, Avg Validation Loss: 48.60918640237968\n",
      "Epoch 69/1000, Avg Training Loss: 0.24744361608707158, Avg Validation Loss: 48.60908770423829\n",
      "Epoch 70/1000, Avg Training Loss: 0.24779157539187793, Avg Validation Loss: 48.60920373313356\n",
      "Epoch 71/1000, Avg Training Loss: 0.24773585793543393, Avg Validation Loss: 48.60713083433674\n",
      "Epoch 72/1000, Avg Training Loss: 0.24770933974561604, Avg Validation Loss: 48.60764319281791\n",
      "Epoch 73/1000, Avg Training Loss: 0.247636032305016, Avg Validation Loss: 48.608218520175114\n",
      "Epoch 74/1000, Avg Training Loss: 0.2476549180156254, Avg Validation Loss: 48.60867364287741\n",
      "Epoch 75/1000, Avg Training Loss: 0.24781747432354473, Avg Validation Loss: 48.609104076268835\n",
      "Epoch 76/1000, Avg Training Loss: 0.24784434906663147, Avg Validation Loss: 48.610462133181414\n",
      "Epoch 77/1000, Avg Training Loss: 0.247623341413109, Avg Validation Loss: 48.61073197269279\n",
      "Epoch 78/1000, Avg Training Loss: 0.2476092734216227, Avg Validation Loss: 48.610236570316246\n",
      "Epoch 79/1000, Avg Training Loss: 0.2478996729209041, Avg Validation Loss: 48.60986793544622\n",
      "Epoch 80/1000, Avg Training Loss: 0.24783521294733857, Avg Validation Loss: 48.61131949279869\n",
      "Epoch 81/1000, Avg Training Loss: 0.24782889705759095, Avg Validation Loss: 48.61235377053144\n",
      "Epoch 82/1000, Avg Training Loss: 0.24748427931860476, Avg Validation Loss: 48.61151809474417\n",
      "Epoch 83/1000, Avg Training Loss: 0.24757626210009345, Avg Validation Loss: 48.61085782627002\n",
      "Epoch 84/1000, Avg Training Loss: 0.2475355546321947, Avg Validation Loss: 48.60969329136546\n",
      "Epoch 85/1000, Avg Training Loss: 0.24765380225866138, Avg Validation Loss: 48.60893434257899\n",
      "Epoch 86/1000, Avg Training Loss: 0.2477006596587095, Avg Validation Loss: 48.60871162134677\n",
      "Epoch 87/1000, Avg Training Loss: 0.24753346438029294, Avg Validation Loss: 48.60787805796937\n",
      "Epoch 88/1000, Avg Training Loss: 0.2479382635737101, Avg Validation Loss: 48.608183899091024\n",
      "Epoch 89/1000, Avg Training Loss: 0.24755547755415264, Avg Validation Loss: 48.60817023044143\n",
      "Epoch 90/1000, Avg Training Loss: 0.24766654776686237, Avg Validation Loss: 48.607963262682205\n",
      "Epoch 91/1000, Avg Training Loss: 0.24792065131616942, Avg Validation Loss: 48.60907478020366\n",
      "Epoch 92/1000, Avg Training Loss: 0.2477334064588788, Avg Validation Loss: 48.610066473833115\n",
      "Epoch 93/1000, Avg Training Loss: 0.24763942232871705, Avg Validation Loss: 48.610364643779306\n",
      "Epoch 94/1000, Avg Training Loss: 0.2474406649050591, Avg Validation Loss: 48.610726443423744\n",
      "Epoch 95/1000, Avg Training Loss: 0.2476634369358678, Avg Validation Loss: 48.610842909398706\n",
      "Epoch 96/1000, Avg Training Loss: 0.2477791015941363, Avg Validation Loss: 48.609421189232776\n",
      "Epoch 97/1000, Avg Training Loss: 0.24746611315355613, Avg Validation Loss: 48.60863339657647\n",
      "Epoch 98/1000, Avg Training Loss: 0.2478082280422521, Avg Validation Loss: 48.609644370719764\n",
      "Epoch 99/1000, Avg Training Loss: 0.24771943493342763, Avg Validation Loss: 48.60944583459977\n",
      "Epoch 100/1000, Avg Training Loss: 0.24768889449763376, Avg Validation Loss: 48.60869744073894\n",
      "Epoch 101/1000, Avg Training Loss: 0.2475829276021728, Avg Validation Loss: 48.60858606893864\n",
      "Epoch 102/1000, Avg Training Loss: 0.24776080598626707, Avg Validation Loss: 48.60910838304793\n",
      "Epoch 103/1000, Avg Training Loss: 0.24765063518093078, Avg Validation Loss: 48.60868088704075\n",
      "Epoch 104/1000, Avg Training Loss: 0.24778598837718466, Avg Validation Loss: 48.608233010151196\n",
      "Epoch 105/1000, Avg Training Loss: 0.24782338039038154, Avg Validation Loss: 48.609415372642694\n",
      "Epoch 106/1000, Avg Training Loss: 0.24756714155946902, Avg Validation Loss: 48.60853825351323\n",
      "Epoch 107/1000, Avg Training Loss: 0.2476627473188386, Avg Validation Loss: 48.608297053451786\n",
      "Epoch 108/1000, Avg Training Loss: 0.24758900388334723, Avg Validation Loss: 48.60840592922696\n",
      "Epoch 109/1000, Avg Training Loss: 0.2475747301280788, Avg Validation Loss: 48.61010392561485\n",
      "Epoch 110/1000, Avg Training Loss: 0.2475376194541784, Avg Validation Loss: 48.60997064103895\n",
      "Epoch 111/1000, Avg Training Loss: 0.24733409093794972, Avg Validation Loss: 48.60798151240533\n",
      "Epoch 112/1000, Avg Training Loss: 0.2476697505914705, Avg Validation Loss: 48.608528728052455\n",
      "Epoch 113/1000, Avg Training Loss: 0.24759836509414154, Avg Validation Loss: 48.60938467852361\n",
      "Epoch 114/1000, Avg Training Loss: 0.24779283091661625, Avg Validation Loss: 48.60977349879481\n",
      "Epoch 115/1000, Avg Training Loss: 0.24784143077498896, Avg Validation Loss: 48.60974229625648\n",
      "Epoch 116/1000, Avg Training Loss: 0.24757658001157012, Avg Validation Loss: 48.61021422320003\n",
      "Epoch 117/1000, Avg Training Loss: 0.2477321122074406, Avg Validation Loss: 48.610861147327654\n",
      "Epoch 118/1000, Avg Training Loss: 0.24783425343094703, Avg Validation Loss: 48.61081098909558\n",
      "Epoch 119/1000, Avg Training Loss: 0.2479367969076693, Avg Validation Loss: 48.6114952650411\n",
      "Epoch 120/1000, Avg Training Loss: 0.2479506249139841, Avg Validation Loss: 48.613143116943476\n",
      "Epoch 121/1000, Avg Training Loss: 0.24752369071823108, Avg Validation Loss: 48.61343706052384\n",
      "Epoch 122/1000, Avg Training Loss: 0.24767493491655035, Avg Validation Loss: 48.6130383564658\n",
      "Epoch 123/1000, Avg Training Loss: 0.2478384496518433, Avg Validation Loss: 48.6137053992031\n",
      "Epoch 124/1000, Avg Training Loss: 0.2475874003923684, Avg Validation Loss: 48.61332330019253\n",
      "Epoch 125/1000, Avg Training Loss: 0.24797683102996915, Avg Validation Loss: 48.61305259103115\n",
      "Epoch 126/1000, Avg Training Loss: 0.24752233910844879, Avg Validation Loss: 48.61300737973292\n",
      "Epoch 127/1000, Avg Training Loss: 0.24767756638653016, Avg Validation Loss: 48.61417539196744\n",
      "Epoch 128/1000, Avg Training Loss: 0.24791114368349454, Avg Validation Loss: 48.614125335225296\n",
      "Epoch 129/1000, Avg Training Loss: 0.24777826246347168, Avg Validation Loss: 48.61462462065358\n",
      "Epoch 130/1000, Avg Training Loss: 0.2479997777862421, Avg Validation Loss: 48.61315144671613\n",
      "Epoch 131/1000, Avg Training Loss: 0.2475048442888209, Avg Validation Loss: 48.612640854195035\n",
      "Epoch 132/1000, Avg Training Loss: 0.24756441929537665, Avg Validation Loss: 48.61230937056119\n",
      "Epoch 133/1000, Avg Training Loss: 0.24801591390259753, Avg Validation Loss: 48.613111506211645\n",
      "Epoch 134/1000, Avg Training Loss: 0.2477527847781785, Avg Validation Loss: 48.61391934721031\n",
      "Epoch 135/1000, Avg Training Loss: 0.24758455379050645, Avg Validation Loss: 48.615133871368776\n",
      "Epoch 136/1000, Avg Training Loss: 0.2477523218896851, Avg Validation Loss: 48.61593521908502\n",
      "Epoch 137/1000, Avg Training Loss: 0.24790408403706313, Avg Validation Loss: 48.615200099073064\n",
      "Epoch 138/1000, Avg Training Loss: 0.2476298681151032, Avg Validation Loss: 48.61553198062488\n",
      "Epoch 139/1000, Avg Training Loss: 0.24768386526711456, Avg Validation Loss: 48.6166751579667\n",
      "Epoch 140/1000, Avg Training Loss: 0.24773783446936337, Avg Validation Loss: 48.61650080389129\n",
      "Epoch 141/1000, Avg Training Loss: 0.24786021003660452, Avg Validation Loss: 48.61844321613281\n",
      "Epoch 142/1000, Avg Training Loss: 0.2475798343865822, Avg Validation Loss: 48.61763065465887\n",
      "Epoch 143/1000, Avg Training Loss: 0.24776819532299132, Avg Validation Loss: 48.61996576865403\n",
      "Epoch 144/1000, Avg Training Loss: 0.2478357460895982, Avg Validation Loss: 48.62005729487779\n",
      "Epoch 145/1000, Avg Training Loss: 0.24783023302879484, Avg Validation Loss: 48.62018324180341\n",
      "Epoch 146/1000, Avg Training Loss: 0.2479424364786932, Avg Validation Loss: 48.622476473668186\n",
      "Epoch 147/1000, Avg Training Loss: 0.24774964177603956, Avg Validation Loss: 48.62218835109413\n",
      "Epoch 148/1000, Avg Training Loss: 0.2474387765821458, Avg Validation Loss: 48.62206870632433\n",
      "Epoch 149/1000, Avg Training Loss: 0.24766370838553337, Avg Validation Loss: 48.62118303117916\n",
      "Epoch 150/1000, Avg Training Loss: 0.24742255912508587, Avg Validation Loss: 48.62115792901443\n",
      "Epoch 151/1000, Avg Training Loss: 0.24762861296372513, Avg Validation Loss: 48.621649767523294\n",
      "Epoch 152/1000, Avg Training Loss: 0.24776020103645757, Avg Validation Loss: 48.62216992364077\n",
      "Epoch 153/1000, Avg Training Loss: 0.2475819556980147, Avg Validation Loss: 48.6200043722163\n",
      "Epoch 154/1000, Avg Training Loss: 0.24793824483017965, Avg Validation Loss: 48.62322051384057\n",
      "Epoch 155/1000, Avg Training Loss: 0.2475212465236711, Avg Validation Loss: 48.62332143785396\n",
      "Epoch 156/1000, Avg Training Loss: 0.24768681649070737, Avg Validation Loss: 48.6222932857866\n",
      "Epoch 157/1000, Avg Training Loss: 0.24778788570815544, Avg Validation Loss: 48.6231910882589\n",
      "Epoch 158/1000, Avg Training Loss: 0.24762535091197382, Avg Validation Loss: 48.621391039811485\n",
      "Epoch 159/1000, Avg Training Loss: 0.2479339164700252, Avg Validation Loss: 48.62350271942776\n",
      "Epoch 160/1000, Avg Training Loss: 0.2476176861270416, Avg Validation Loss: 48.62329557105225\n",
      "Epoch 161/1000, Avg Training Loss: 0.24778566516235553, Avg Validation Loss: 48.62408458890315\n",
      "Epoch 162/1000, Avg Training Loss: 0.24784574390067385, Avg Validation Loss: 48.62342611832538\n",
      "Epoch 163/1000, Avg Training Loss: 0.24761579732544792, Avg Validation Loss: 48.62445793602268\n",
      "Epoch 164/1000, Avg Training Loss: 0.2479160935201653, Avg Validation Loss: 48.62482208791586\n",
      "Epoch 165/1000, Avg Training Loss: 0.24748753050443395, Avg Validation Loss: 48.621954908564376\n",
      "Epoch 166/1000, Avg Training Loss: 0.2479994326105791, Avg Validation Loss: 48.623292212496764\n",
      "Epoch 167/1000, Avg Training Loss: 0.24811087911008214, Avg Validation Loss: 48.62660217171\n",
      "Epoch 168/1000, Avg Training Loss: 0.24782190151610492, Avg Validation Loss: 48.627523150665\n",
      "Epoch 169/1000, Avg Training Loss: 0.24765904743319794, Avg Validation Loss: 48.627081557355915\n",
      "Epoch 170/1000, Avg Training Loss: 0.2475055469404536, Avg Validation Loss: 48.62739016370994\n",
      "Epoch 171/1000, Avg Training Loss: 0.24793979674673444, Avg Validation Loss: 48.62824215248288\n",
      "Epoch 172/1000, Avg Training Loss: 0.24776760783838508, Avg Validation Loss: 48.62843893013411\n",
      "Epoch 173/1000, Avg Training Loss: 0.24755575968729843, Avg Validation Loss: 48.62787544914647\n",
      "Epoch 174/1000, Avg Training Loss: 0.24785400039164615, Avg Validation Loss: 48.62712417864388\n",
      "Epoch 175/1000, Avg Training Loss: 0.24785450599724335, Avg Validation Loss: 48.627671520284416\n",
      "Epoch 176/1000, Avg Training Loss: 0.24728134183087758, Avg Validation Loss: 48.6271849452623\n",
      "Epoch 177/1000, Avg Training Loss: 0.24780598573666798, Avg Validation Loss: 48.62733198417517\n",
      "Epoch 178/1000, Avg Training Loss: 0.24759440639720023, Avg Validation Loss: 48.62680854896341\n",
      "Epoch 179/1000, Avg Training Loss: 0.24793135620013843, Avg Validation Loss: 48.62711558177279\n",
      "Epoch 180/1000, Avg Training Loss: 0.2478686275282737, Avg Validation Loss: 48.628732818435424\n",
      "Epoch 181/1000, Avg Training Loss: 0.2478345047949708, Avg Validation Loss: 48.62986584013858\n",
      "Epoch 182/1000, Avg Training Loss: 0.24754957663272031, Avg Validation Loss: 48.62986490506835\n",
      "Epoch 183/1000, Avg Training Loss: 0.2475244885429505, Avg Validation Loss: 48.62900051910631\n",
      "Epoch 184/1000, Avg Training Loss: 0.24781444716826329, Avg Validation Loss: 48.629547223330476\n",
      "Epoch 185/1000, Avg Training Loss: 0.24778991169745965, Avg Validation Loss: 48.62960675795818\n",
      "Epoch 186/1000, Avg Training Loss: 0.24759119859491213, Avg Validation Loss: 48.63162917123791\n",
      "Epoch 187/1000, Avg Training Loss: 0.2476362432279396, Avg Validation Loss: 48.63349193021128\n",
      "Epoch 188/1000, Avg Training Loss: 0.2477500130776511, Avg Validation Loss: 48.63461397871981\n",
      "Epoch 189/1000, Avg Training Loss: 0.2473714537421642, Avg Validation Loss: 48.63418748284319\n",
      "Epoch 190/1000, Avg Training Loss: 0.24774263738455535, Avg Validation Loss: 48.63306234492694\n",
      "Epoch 191/1000, Avg Training Loss: 0.247774597602021, Avg Validation Loss: 48.6341648565873\n",
      "Epoch 192/1000, Avg Training Loss: 0.24769326158303087, Avg Validation Loss: 48.63450405579988\n",
      "Epoch 193/1000, Avg Training Loss: 0.24764531075253768, Avg Validation Loss: 48.63472165854587\n",
      "Epoch 194/1000, Avg Training Loss: 0.24777179026420534, Avg Validation Loss: 48.63503967276645\n",
      "Epoch 195/1000, Avg Training Loss: 0.24754465896847186, Avg Validation Loss: 48.63555647987134\n",
      "Epoch 196/1000, Avg Training Loss: 0.24756745420543766, Avg Validation Loss: 48.63322823711099\n",
      "Epoch 197/1000, Avg Training Loss: 0.24782888420143515, Avg Validation Loss: 48.63418048466532\n",
      "Epoch 198/1000, Avg Training Loss: 0.2476566233964931, Avg Validation Loss: 48.63476033033747\n",
      "Epoch 199/1000, Avg Training Loss: 0.24798654436255826, Avg Validation Loss: 48.635847284600416\n",
      "Epoch 200/1000, Avg Training Loss: 0.2477987281733951, Avg Validation Loss: 48.63590839994952\n",
      "Epoch 201/1000, Avg Training Loss: 0.2478458190375746, Avg Validation Loss: 48.63623069014304\n",
      "Epoch 202/1000, Avg Training Loss: 0.24775153136216255, Avg Validation Loss: 48.635089007379555\n",
      "Epoch 203/1000, Avg Training Loss: 0.24774650836434084, Avg Validation Loss: 48.634238784103665\n",
      "Epoch 204/1000, Avg Training Loss: 0.24773426707371066, Avg Validation Loss: 48.63447467310631\n",
      "Epoch 205/1000, Avg Training Loss: 0.24794348920344816, Avg Validation Loss: 48.636631295375366\n",
      "Epoch 206/1000, Avg Training Loss: 0.24783917607488548, Avg Validation Loss: 48.637475110746145\n",
      "Epoch 207/1000, Avg Training Loss: 0.24765619108958858, Avg Validation Loss: 48.63911935491904\n",
      "Epoch 208/1000, Avg Training Loss: 0.2475323491509826, Avg Validation Loss: 48.638887908223964\n",
      "Epoch 209/1000, Avg Training Loss: 0.2478067883858624, Avg Validation Loss: 48.6395490128904\n",
      "Epoch 210/1000, Avg Training Loss: 0.24758685804801597, Avg Validation Loss: 48.6392632162714\n",
      "Epoch 211/1000, Avg Training Loss: 0.2475931368755827, Avg Validation Loss: 48.63989145056702\n",
      "Epoch 212/1000, Avg Training Loss: 0.24775673710947568, Avg Validation Loss: 48.64038085979032\n",
      "Epoch 213/1000, Avg Training Loss: 0.24742841035585791, Avg Validation Loss: 48.64121195609111\n",
      "Epoch 214/1000, Avg Training Loss: 0.2476052610960765, Avg Validation Loss: 48.64094893951475\n",
      "Epoch 215/1000, Avg Training Loss: 0.24781234085717, Avg Validation Loss: 48.64136384076288\n",
      "Epoch 216/1000, Avg Training Loss: 0.2478143253207271, Avg Validation Loss: 48.64026777597705\n",
      "Epoch 217/1000, Avg Training Loss: 0.2478568114945603, Avg Validation Loss: 48.640787090656794\n",
      "Epoch 218/1000, Avg Training Loss: 0.24766967075361015, Avg Validation Loss: 48.640775323988464\n",
      "Epoch 219/1000, Avg Training Loss: 0.24783009318490323, Avg Validation Loss: 48.64132283286272\n",
      "Epoch 220/1000, Avg Training Loss: 0.24780412268209562, Avg Validation Loss: 48.641202511651954\n",
      "Epoch 221/1000, Avg Training Loss: 0.2479986002641762, Avg Validation Loss: 48.64121794864057\n",
      "Epoch 222/1000, Avg Training Loss: 0.24801769716871874, Avg Validation Loss: 48.64162376213315\n",
      "Epoch 223/1000, Avg Training Loss: 0.2476831011055244, Avg Validation Loss: 48.64115740767548\n",
      "Epoch 224/1000, Avg Training Loss: 0.24754318817473986, Avg Validation Loss: 48.641635260609306\n",
      "Epoch 225/1000, Avg Training Loss: 0.24807964077618194, Avg Validation Loss: 48.64318701294651\n",
      "Epoch 226/1000, Avg Training Loss: 0.2478444035822566, Avg Validation Loss: 48.64373248744298\n",
      "Epoch 227/1000, Avg Training Loss: 0.24790273748286837, Avg Validation Loss: 48.64431653160423\n",
      "Epoch 228/1000, Avg Training Loss: 0.24768720559837895, Avg Validation Loss: 48.644545046763454\n",
      "Epoch 229/1000, Avg Training Loss: 0.24776570834977776, Avg Validation Loss: 48.644988972012705\n",
      "Epoch 230/1000, Avg Training Loss: 0.24777807929170645, Avg Validation Loss: 48.64727723341622\n",
      "Epoch 231/1000, Avg Training Loss: 0.24781988687239223, Avg Validation Loss: 48.648879525405036\n",
      "Epoch 232/1000, Avg Training Loss: 0.24797222068036934, Avg Validation Loss: 48.649120676887364\n",
      "Epoch 233/1000, Avg Training Loss: 0.24763864698632623, Avg Validation Loss: 48.64955949940004\n",
      "Epoch 234/1000, Avg Training Loss: 0.2478036223562782, Avg Validation Loss: 48.650689667657986\n",
      "Epoch 235/1000, Avg Training Loss: 0.2475053589962647, Avg Validation Loss: 48.65060634944008\n",
      "Epoch 236/1000, Avg Training Loss: 0.24748470179308332, Avg Validation Loss: 48.65028932181056\n",
      "Epoch 237/1000, Avg Training Loss: 0.24776376764320474, Avg Validation Loss: 48.649524345791804\n",
      "Epoch 238/1000, Avg Training Loss: 0.24752336984383988, Avg Validation Loss: 48.64838751673963\n",
      "Epoch 239/1000, Avg Training Loss: 0.24785232697963905, Avg Validation Loss: 48.651013593884414\n",
      "Epoch 240/1000, Avg Training Loss: 0.24811481772715133, Avg Validation Loss: 48.651531417049696\n",
      "Epoch 241/1000, Avg Training Loss: 0.24789877352404643, Avg Validation Loss: 48.6501449020545\n",
      "Epoch 242/1000, Avg Training Loss: 0.24779569382624833, Avg Validation Loss: 48.652455319682225\n",
      "Epoch 243/1000, Avg Training Loss: 0.24793682944024795, Avg Validation Loss: 48.65370562522354\n",
      "Epoch 244/1000, Avg Training Loss: 0.24736799379630545, Avg Validation Loss: 48.65471010086034\n",
      "Epoch 245/1000, Avg Training Loss: 0.24748518724576374, Avg Validation Loss: 48.65346708338808\n",
      "Epoch 246/1000, Avg Training Loss: 0.24822566603483104, Avg Validation Loss: 48.654507114054546\n",
      "Epoch 247/1000, Avg Training Loss: 0.24767746191791148, Avg Validation Loss: 48.654982860591\n",
      "Epoch 248/1000, Avg Training Loss: 0.24789030908179777, Avg Validation Loss: 48.65550897579282\n",
      "Epoch 249/1000, Avg Training Loss: 0.2472598382980406, Avg Validation Loss: 48.65438733898305\n",
      "Epoch 250/1000, Avg Training Loss: 0.24769870380670753, Avg Validation Loss: 48.65493980742385\n",
      "Epoch 251/1000, Avg Training Loss: 0.24793625564671207, Avg Validation Loss: 48.65588347077403\n",
      "Epoch 252/1000, Avg Training Loss: 0.24791006871108473, Avg Validation Loss: 48.65610392187195\n",
      "Epoch 253/1000, Avg Training Loss: 0.2478585611541147, Avg Validation Loss: 48.65774180607898\n",
      "Epoch 254/1000, Avg Training Loss: 0.2480357793411315, Avg Validation Loss: 48.65729691216096\n",
      "Epoch 255/1000, Avg Training Loss: 0.2479481048501831, Avg Validation Loss: 48.657800065784215\n",
      "Epoch 256/1000, Avg Training Loss: 0.24791889198313638, Avg Validation Loss: 48.6608764018321\n",
      "Epoch 257/1000, Avg Training Loss: 0.24793269749777916, Avg Validation Loss: 48.66084345777717\n",
      "Epoch 258/1000, Avg Training Loss: 0.2483109477400685, Avg Validation Loss: 48.66476134853842\n",
      "Epoch 259/1000, Avg Training Loss: 0.24743409691149107, Avg Validation Loss: 48.665180811758574\n",
      "Epoch 260/1000, Avg Training Loss: 0.2475386358805052, Avg Validation Loss: 48.66612977718069\n",
      "Epoch 261/1000, Avg Training Loss: 0.24776762738007294, Avg Validation Loss: 48.66580242164923\n",
      "Epoch 262/1000, Avg Training Loss: 0.24761721211565252, Avg Validation Loss: 48.66457351003679\n",
      "Epoch 263/1000, Avg Training Loss: 0.2480002347996783, Avg Validation Loss: 48.6650409206067\n",
      "Epoch 264/1000, Avg Training Loss: 0.24794427023049115, Avg Validation Loss: 48.66607188187504\n",
      "Epoch 265/1000, Avg Training Loss: 0.24770435584173572, Avg Validation Loss: 48.66614605623728\n",
      "Epoch 266/1000, Avg Training Loss: 0.24818451474219944, Avg Validation Loss: 48.66696196548397\n",
      "Epoch 267/1000, Avg Training Loss: 0.2477727363914487, Avg Validation Loss: 48.666999770119524\n",
      "Epoch 268/1000, Avg Training Loss: 0.24788481707064153, Avg Validation Loss: 48.66748086104995\n",
      "Epoch 269/1000, Avg Training Loss: 0.2476541413027551, Avg Validation Loss: 48.668836478639534\n",
      "Epoch 270/1000, Avg Training Loss: 0.24786308927806097, Avg Validation Loss: 48.67158443602773\n",
      "Epoch 271/1000, Avg Training Loss: 0.2478013857286015, Avg Validation Loss: 48.67223078191392\n",
      "Epoch 272/1000, Avg Training Loss: 0.2481701135834389, Avg Validation Loss: 48.67605162405623\n",
      "Epoch 273/1000, Avg Training Loss: 0.24768059499413872, Avg Validation Loss: 48.67671369939495\n",
      "Epoch 274/1000, Avg Training Loss: 0.24833743907785355, Avg Validation Loss: 48.6781622970155\n",
      "Epoch 275/1000, Avg Training Loss: 0.2474042285452308, Avg Validation Loss: 48.678307991240246\n",
      "Epoch 276/1000, Avg Training Loss: 0.24767693711547697, Avg Validation Loss: 48.67719509194391\n",
      "Epoch 277/1000, Avg Training Loss: 0.2477339193657804, Avg Validation Loss: 48.67912123442595\n",
      "Epoch 278/1000, Avg Training Loss: 0.24773784215981437, Avg Validation Loss: 48.67850992722895\n",
      "Epoch 279/1000, Avg Training Loss: 0.2474983422479186, Avg Validation Loss: 48.678787978065024\n",
      "Epoch 280/1000, Avg Training Loss: 0.24817934868146252, Avg Validation Loss: 48.682109446828754\n",
      "Epoch 281/1000, Avg Training Loss: 0.24754969476917105, Avg Validation Loss: 48.68126427645188\n",
      "Epoch 282/1000, Avg Training Loss: 0.24822288276800475, Avg Validation Loss: 48.684230527668895\n",
      "Epoch 283/1000, Avg Training Loss: 0.24779478674614935, Avg Validation Loss: 48.68413365826777\n",
      "Epoch 284/1000, Avg Training Loss: 0.24763618108016763, Avg Validation Loss: 48.68337424024928\n",
      "Epoch 285/1000, Avg Training Loss: 0.2484713210824667, Avg Validation Loss: 48.686521270362704\n",
      "Epoch 286/1000, Avg Training Loss: 0.2478058728831877, Avg Validation Loss: 48.68731862328752\n",
      "Epoch 287/1000, Avg Training Loss: 0.24769633319593246, Avg Validation Loss: 48.68638097313551\n",
      "Epoch 288/1000, Avg Training Loss: 0.24802517062877605, Avg Validation Loss: 48.68740618086276\n",
      "Epoch 289/1000, Avg Training Loss: 0.24816583680917734, Avg Validation Loss: 48.68886158313346\n",
      "Epoch 290/1000, Avg Training Loss: 0.24832947799578123, Avg Validation Loss: 48.69210507681502\n",
      "Epoch 291/1000, Avg Training Loss: 0.2481858126565113, Avg Validation Loss: 48.69476353124131\n",
      "Epoch 292/1000, Avg Training Loss: 0.2478985163919006, Avg Validation Loss: 48.69429556329716\n",
      "Epoch 293/1000, Avg Training Loss: 0.2475101330111663, Avg Validation Loss: 48.692807674403966\n",
      "Epoch 294/1000, Avg Training Loss: 0.24772371019388342, Avg Validation Loss: 48.694689198049645\n",
      "Epoch 295/1000, Avg Training Loss: 0.24787739337606898, Avg Validation Loss: 48.69437928828854\n",
      "Epoch 296/1000, Avg Training Loss: 0.2478512573242979, Avg Validation Loss: 48.69483901713352\n",
      "Epoch 297/1000, Avg Training Loss: 0.24817466926978693, Avg Validation Loss: 48.696971932126836\n",
      "Epoch 298/1000, Avg Training Loss: 0.24757914430039613, Avg Validation Loss: 48.69593022276264\n",
      "Epoch 299/1000, Avg Training Loss: 0.24841566721490727, Avg Validation Loss: 48.698640838670826\n",
      "Epoch 300/1000, Avg Training Loss: 0.24827225482452167, Avg Validation Loss: 48.700537399425215\n",
      "Epoch 301/1000, Avg Training Loss: 0.24765904765775512, Avg Validation Loss: 48.70082740010867\n",
      "Epoch 302/1000, Avg Training Loss: 0.24762822561687378, Avg Validation Loss: 48.70045873828592\n",
      "Epoch 303/1000, Avg Training Loss: 0.24781151428700318, Avg Validation Loss: 48.70278918817138\n",
      "Epoch 304/1000, Avg Training Loss: 0.24798672638670538, Avg Validation Loss: 48.703699012787446\n",
      "Epoch 305/1000, Avg Training Loss: 0.2477661275384623, Avg Validation Loss: 48.70417167554277\n",
      "Epoch 306/1000, Avg Training Loss: 0.2475613319857416, Avg Validation Loss: 48.7031518275863\n",
      "Epoch 307/1000, Avg Training Loss: 0.2475271713087643, Avg Validation Loss: 48.7028948296046\n",
      "Epoch 308/1000, Avg Training Loss: 0.24776548810191007, Avg Validation Loss: 48.70213583432267\n",
      "Epoch 309/1000, Avg Training Loss: 0.24841619856319258, Avg Validation Loss: 48.70514997594732\n",
      "Epoch 310/1000, Avg Training Loss: 0.24802147121962104, Avg Validation Loss: 48.706045035171876\n",
      "Epoch 311/1000, Avg Training Loss: 0.24795640751441467, Avg Validation Loss: 48.707580695374865\n",
      "Epoch 312/1000, Avg Training Loss: 0.2479973090588122, Avg Validation Loss: 48.709351191650654\n",
      "Epoch 313/1000, Avg Training Loss: 0.2479788220521786, Avg Validation Loss: 48.709917965777365\n",
      "Epoch 314/1000, Avg Training Loss: 0.24841279981953895, Avg Validation Loss: 48.71217785759141\n",
      "Epoch 315/1000, Avg Training Loss: 0.2477263092955894, Avg Validation Loss: 48.71137964488719\n",
      "Epoch 316/1000, Avg Training Loss: 0.24817698977562908, Avg Validation Loss: 48.71083702860037\n",
      "Epoch 317/1000, Avg Training Loss: 0.24805146906691153, Avg Validation Loss: 48.713024029653184\n",
      "Epoch 318/1000, Avg Training Loss: 0.24820865654136032, Avg Validation Loss: 48.71454860116626\n",
      "Epoch 319/1000, Avg Training Loss: 0.24825964506961753, Avg Validation Loss: 48.717746874998994\n",
      "Epoch 320/1000, Avg Training Loss: 0.24751208675249953, Avg Validation Loss: 48.71686608657155\n",
      "Epoch 321/1000, Avg Training Loss: 0.24767638781090898, Avg Validation Loss: 48.71699663107768\n",
      "Epoch 322/1000, Avg Training Loss: 0.24771591114079466, Avg Validation Loss: 48.719135962269284\n",
      "Epoch 323/1000, Avg Training Loss: 0.24837083359718357, Avg Validation Loss: 48.719552007365465\n",
      "Epoch 324/1000, Avg Training Loss: 0.24803303346350902, Avg Validation Loss: 48.721420659002526\n",
      "Epoch 325/1000, Avg Training Loss: 0.24764488357643785, Avg Validation Loss: 48.72379333760901\n",
      "Epoch 326/1000, Avg Training Loss: 0.24737794868134644, Avg Validation Loss: 48.72551628420413\n",
      "Epoch 327/1000, Avg Training Loss: 0.24816151175058265, Avg Validation Loss: 48.72638658320872\n",
      "Epoch 328/1000, Avg Training Loss: 0.24804692748486232, Avg Validation Loss: 48.72899677549069\n",
      "Epoch 329/1000, Avg Training Loss: 0.24755675433853383, Avg Validation Loss: 48.727230026513794\n",
      "Epoch 330/1000, Avg Training Loss: 0.24832877845670676, Avg Validation Loss: 48.730257383063474\n",
      "Epoch 331/1000, Avg Training Loss: 0.24783095765060392, Avg Validation Loss: 48.72968756821972\n",
      "Epoch 332/1000, Avg Training Loss: 0.2476683288858797, Avg Validation Loss: 48.73208895768099\n",
      "Epoch 333/1000, Avg Training Loss: 0.24823829568779632, Avg Validation Loss: 48.73232351454056\n",
      "Epoch 334/1000, Avg Training Loss: 0.2477458894486376, Avg Validation Loss: 48.730624822495336\n",
      "Epoch 335/1000, Avg Training Loss: 0.24844061880026125, Avg Validation Loss: 48.73226530233219\n",
      "Epoch 336/1000, Avg Training Loss: 0.24800496213399104, Avg Validation Loss: 48.73315682813947\n",
      "Epoch 337/1000, Avg Training Loss: 0.24825263966732625, Avg Validation Loss: 48.733645197735505\n",
      "Epoch 338/1000, Avg Training Loss: 0.24754609892415605, Avg Validation Loss: 48.734027892651056\n",
      "Epoch 339/1000, Avg Training Loss: 0.24817165982512548, Avg Validation Loss: 48.73672510293231\n",
      "Epoch 340/1000, Avg Training Loss: 0.24833306643468248, Avg Validation Loss: 48.73815964658023\n",
      "Epoch 341/1000, Avg Training Loss: 0.2469994194826128, Avg Validation Loss: 48.73634785740023\n",
      "Epoch 342/1000, Avg Training Loss: 0.24780358682311415, Avg Validation Loss: 48.739350393173005\n",
      "Epoch 343/1000, Avg Training Loss: 0.24778721216795915, Avg Validation Loss: 48.73968283887618\n",
      "Epoch 344/1000, Avg Training Loss: 0.2482119179943349, Avg Validation Loss: 48.73997417291024\n",
      "Epoch 345/1000, Avg Training Loss: 0.2477395102898938, Avg Validation Loss: 48.74034721422703\n",
      "Epoch 346/1000, Avg Training Loss: 0.24744060064166512, Avg Validation Loss: 48.741647690702806\n",
      "Epoch 347/1000, Avg Training Loss: 0.2483217507117993, Avg Validation Loss: 48.74264958725977\n",
      "Epoch 348/1000, Avg Training Loss: 0.24770543252018298, Avg Validation Loss: 48.74402951511816\n",
      "Epoch 349/1000, Avg Training Loss: 0.24866053652262213, Avg Validation Loss: 48.74720771658541\n",
      "Epoch 350/1000, Avg Training Loss: 0.24838436132209105, Avg Validation Loss: 48.74753861840116\n",
      "Epoch 351/1000, Avg Training Loss: 0.24780624743435226, Avg Validation Loss: 48.7474626026039\n",
      "Epoch 352/1000, Avg Training Loss: 0.24798402602515593, Avg Validation Loss: 48.7505064045385\n",
      "Epoch 353/1000, Avg Training Loss: 0.24880660876955776, Avg Validation Loss: 48.75375773261334\n",
      "Epoch 354/1000, Avg Training Loss: 0.24820104570003476, Avg Validation Loss: 48.754972992459855\n",
      "Epoch 355/1000, Avg Training Loss: 0.24779521321428252, Avg Validation Loss: 48.75637414315342\n",
      "Epoch 356/1000, Avg Training Loss: 0.24794611655968915, Avg Validation Loss: 48.75844096683994\n",
      "Epoch 357/1000, Avg Training Loss: 0.24807944386941366, Avg Validation Loss: 48.75922161103177\n",
      "Epoch 358/1000, Avg Training Loss: 0.24756359947969714, Avg Validation Loss: 48.759527858781766\n",
      "Epoch 359/1000, Avg Training Loss: 0.24876904252392748, Avg Validation Loss: 48.76268423842621\n",
      "Epoch 360/1000, Avg Training Loss: 0.2477393216240246, Avg Validation Loss: 48.76431853084121\n",
      "Epoch 361/1000, Avg Training Loss: 0.24848880421995592, Avg Validation Loss: 48.76636591336152\n",
      "Epoch 362/1000, Avg Training Loss: 0.2476395971804539, Avg Validation Loss: 48.76662710333564\n",
      "Epoch 363/1000, Avg Training Loss: 0.24777693269788936, Avg Validation Loss: 48.767899839805104\n",
      "Epoch 364/1000, Avg Training Loss: 0.2475864213777946, Avg Validation Loss: 48.76733673300025\n",
      "Epoch 365/1000, Avg Training Loss: 0.2476678743815334, Avg Validation Loss: 48.766819288395936\n",
      "Epoch 366/1000, Avg Training Loss: 0.2479092889718123, Avg Validation Loss: 48.76850167959245\n",
      "Epoch 367/1000, Avg Training Loss: 0.24794836347556956, Avg Validation Loss: 48.769642278440855\n",
      "Epoch 368/1000, Avg Training Loss: 0.24847171458667588, Avg Validation Loss: 48.77177069059145\n",
      "Epoch 369/1000, Avg Training Loss: 0.24816727513560594, Avg Validation Loss: 48.7733473566106\n",
      "Epoch 370/1000, Avg Training Loss: 0.24783088615461513, Avg Validation Loss: 48.77512972910782\n",
      "Epoch 371/1000, Avg Training Loss: 0.24775641887671754, Avg Validation Loss: 48.774719105276574\n",
      "Epoch 372/1000, Avg Training Loss: 0.2482356045883999, Avg Validation Loss: 48.77563363344046\n",
      "Epoch 373/1000, Avg Training Loss: 0.24775108043305039, Avg Validation Loss: 48.776850951591854\n",
      "Epoch 374/1000, Avg Training Loss: 0.248161700784876, Avg Validation Loss: 48.77982715232959\n",
      "Epoch 375/1000, Avg Training Loss: 0.24741778416856053, Avg Validation Loss: 48.780028120336695\n",
      "Epoch 376/1000, Avg Training Loss: 0.2479909583827277, Avg Validation Loss: 48.7832291780751\n",
      "Epoch 377/1000, Avg Training Loss: 0.24787579262427734, Avg Validation Loss: 48.78294598329913\n",
      "Epoch 378/1000, Avg Training Loss: 0.24756522018831792, Avg Validation Loss: 48.78523503146914\n",
      "Epoch 379/1000, Avg Training Loss: 0.24809552996610798, Avg Validation Loss: 48.786469476096435\n",
      "Epoch 380/1000, Avg Training Loss: 0.24845486385724094, Avg Validation Loss: 48.78907404089698\n",
      "Epoch 381/1000, Avg Training Loss: 0.24786529203643132, Avg Validation Loss: 48.78887435975289\n",
      "Epoch 382/1000, Avg Training Loss: 0.24856925510289202, Avg Validation Loss: 48.790444019185735\n",
      "Epoch 383/1000, Avg Training Loss: 0.2477722536740025, Avg Validation Loss: 48.79214009503875\n",
      "Epoch 384/1000, Avg Training Loss: 0.24820205479635393, Avg Validation Loss: 48.79275111933663\n",
      "Epoch 385/1000, Avg Training Loss: 0.24817244869234062, Avg Validation Loss: 48.79472653306557\n",
      "Epoch 386/1000, Avg Training Loss: 0.24817637514258428, Avg Validation Loss: 48.7965298825668\n",
      "Epoch 387/1000, Avg Training Loss: 0.2476326720698458, Avg Validation Loss: 48.7969242489158\n",
      "Epoch 388/1000, Avg Training Loss: 0.24901197575171394, Avg Validation Loss: 48.801694420837705\n",
      "Epoch 389/1000, Avg Training Loss: 0.24786415874660364, Avg Validation Loss: 48.802245701050545\n",
      "Epoch 390/1000, Avg Training Loss: 0.24767047079690716, Avg Validation Loss: 48.80010606582594\n",
      "Epoch 391/1000, Avg Training Loss: 0.24787432795912223, Avg Validation Loss: 48.801462093934205\n",
      "Epoch 392/1000, Avg Training Loss: 0.2485389061364711, Avg Validation Loss: 48.80539303180025\n",
      "Epoch 393/1000, Avg Training Loss: 0.24800165459493456, Avg Validation Loss: 48.806153938211864\n",
      "Epoch 394/1000, Avg Training Loss: 0.2482535412961091, Avg Validation Loss: 48.80788180414923\n",
      "Epoch 395/1000, Avg Training Loss: 0.24886098561766232, Avg Validation Loss: 48.811031583271685\n",
      "Epoch 396/1000, Avg Training Loss: 0.24773238976549947, Avg Validation Loss: 48.81193769663707\n",
      "Epoch 397/1000, Avg Training Loss: 0.24888553882563896, Avg Validation Loss: 48.81567053868621\n",
      "Epoch 398/1000, Avg Training Loss: 0.24792844684050586, Avg Validation Loss: 48.81725517264759\n",
      "Epoch 399/1000, Avg Training Loss: 0.2475723923204612, Avg Validation Loss: 48.81862480449894\n",
      "Epoch 400/1000, Avg Training Loss: 0.2483249183621441, Avg Validation Loss: 48.82026708263747\n",
      "Epoch 401/1000, Avg Training Loss: 0.24799091873615006, Avg Validation Loss: 48.82362181441559\n",
      "Epoch 402/1000, Avg Training Loss: 0.24814138208239916, Avg Validation Loss: 48.8250466097023\n",
      "Epoch 403/1000, Avg Training Loss: 0.24905125302748363, Avg Validation Loss: 48.82759186179899\n",
      "Epoch 404/1000, Avg Training Loss: 0.24786082862620465, Avg Validation Loss: 48.827776118899926\n",
      "Epoch 405/1000, Avg Training Loss: 0.24786374415661708, Avg Validation Loss: 48.82824803487164\n",
      "Epoch 406/1000, Avg Training Loss: 0.24766045135191164, Avg Validation Loss: 48.826200880913156\n",
      "Epoch 407/1000, Avg Training Loss: 0.24924106940810895, Avg Validation Loss: 48.828949441863834\n",
      "Epoch 408/1000, Avg Training Loss: 0.2481269947565507, Avg Validation Loss: 48.8300685645785\n",
      "Epoch 409/1000, Avg Training Loss: 0.24777864411276945, Avg Validation Loss: 48.83045568774309\n",
      "Epoch 410/1000, Avg Training Loss: 0.24886740567743326, Avg Validation Loss: 48.83200170561892\n",
      "Epoch 411/1000, Avg Training Loss: 0.24799430631888988, Avg Validation Loss: 48.83270258591139\n",
      "Epoch 412/1000, Avg Training Loss: 0.24783055682321262, Avg Validation Loss: 48.834965382142556\n",
      "Epoch 413/1000, Avg Training Loss: 0.24887645950472131, Avg Validation Loss: 48.83713966897558\n",
      "Epoch 414/1000, Avg Training Loss: 0.24765526237565033, Avg Validation Loss: 48.8397148717751\n",
      "Epoch 415/1000, Avg Training Loss: 0.24890159567462092, Avg Validation Loss: 48.84492891992325\n",
      "Epoch 416/1000, Avg Training Loss: 0.2479794510861104, Avg Validation Loss: 48.84679960246214\n",
      "Epoch 417/1000, Avg Training Loss: 0.248630054964537, Avg Validation Loss: 48.84846002742366\n",
      "Epoch 418/1000, Avg Training Loss: 0.2480305390249142, Avg Validation Loss: 48.85099946714442\n",
      "Epoch 419/1000, Avg Training Loss: 0.24859969707883484, Avg Validation Loss: 48.85426029424269\n",
      "Epoch 420/1000, Avg Training Loss: 0.24808944423952764, Avg Validation Loss: 48.85738846406107\n",
      "Epoch 421/1000, Avg Training Loss: 0.24864150004156027, Avg Validation Loss: 48.86083606396356\n",
      "Epoch 422/1000, Avg Training Loss: 0.24803383071792623, Avg Validation Loss: 48.861262689730566\n",
      "Epoch 423/1000, Avg Training Loss: 0.24809613826090351, Avg Validation Loss: 48.86280405306621\n",
      "Epoch 424/1000, Avg Training Loss: 0.24779245297284241, Avg Validation Loss: 48.86529359063874\n",
      "Epoch 425/1000, Avg Training Loss: 0.2481969254380965, Avg Validation Loss: 48.868980601068046\n",
      "Epoch 426/1000, Avg Training Loss: 0.2482226301798068, Avg Validation Loss: 48.87165344661414\n",
      "Epoch 427/1000, Avg Training Loss: 0.24858275602150617, Avg Validation Loss: 48.873817893027734\n",
      "Epoch 428/1000, Avg Training Loss: 0.2480676101363727, Avg Validation Loss: 48.874822048164816\n",
      "Epoch 429/1000, Avg Training Loss: 0.24846426743499317, Avg Validation Loss: 48.877391456177726\n",
      "Epoch 430/1000, Avg Training Loss: 0.2481737070294443, Avg Validation Loss: 48.87966325841278\n",
      "Epoch 431/1000, Avg Training Loss: 0.24753314061141446, Avg Validation Loss: 48.87836616949188\n",
      "Epoch 432/1000, Avg Training Loss: 0.24795604988098408, Avg Validation Loss: 48.87815436771487\n",
      "Epoch 433/1000, Avg Training Loss: 0.24945463423051578, Avg Validation Loss: 48.88167935538222\n",
      "Epoch 434/1000, Avg Training Loss: 0.24891883742738272, Avg Validation Loss: 48.88682544999904\n",
      "Epoch 435/1000, Avg Training Loss: 0.2479099946119133, Avg Validation Loss: 48.887264772362\n",
      "Epoch 436/1000, Avg Training Loss: 0.249086360973641, Avg Validation Loss: 48.89062758205344\n",
      "Epoch 437/1000, Avg Training Loss: 0.24841853209005194, Avg Validation Loss: 48.89075383890554\n",
      "Epoch 438/1000, Avg Training Loss: 0.24813192009020482, Avg Validation Loss: 48.89256856368486\n",
      "Epoch 439/1000, Avg Training Loss: 0.24877920833874012, Avg Validation Loss: 48.89555128437642\n",
      "Epoch 440/1000, Avg Training Loss: 0.24863475962122603, Avg Validation Loss: 48.89830915232426\n",
      "Epoch 441/1000, Avg Training Loss: 0.24846902454961534, Avg Validation Loss: 48.89977961347548\n",
      "Epoch 442/1000, Avg Training Loss: 0.24763559983771316, Avg Validation Loss: 48.90138419833604\n",
      "Epoch 443/1000, Avg Training Loss: 0.248076536927058, Avg Validation Loss: 48.903695035282\n",
      "Epoch 444/1000, Avg Training Loss: 0.24757049060003788, Avg Validation Loss: 48.90503015058784\n",
      "Epoch 445/1000, Avg Training Loss: 0.2483947015551669, Avg Validation Loss: 48.90655776859648\n",
      "Epoch 446/1000, Avg Training Loss: 0.24824213258447603, Avg Validation Loss: 48.90799917528436\n",
      "Epoch 447/1000, Avg Training Loss: 0.2487917004971016, Avg Validation Loss: 48.910284186923334\n",
      "Epoch 448/1000, Avg Training Loss: 0.24817657758454678, Avg Validation Loss: 48.91204954538261\n",
      "Epoch 449/1000, Avg Training Loss: 0.2475856757886822, Avg Validation Loss: 48.914427733358536\n",
      "Epoch 450/1000, Avg Training Loss: 0.2482720290756023, Avg Validation Loss: 48.91759963524042\n",
      "Epoch 451/1000, Avg Training Loss: 0.24803316108726242, Avg Validation Loss: 48.918025715307444\n",
      "Epoch 452/1000, Avg Training Loss: 0.24902743260654656, Avg Validation Loss: 48.922231519863374\n",
      "Epoch 453/1000, Avg Training Loss: 0.24807164002700818, Avg Validation Loss: 48.92422344312329\n",
      "Epoch 454/1000, Avg Training Loss: 0.24890042993014572, Avg Validation Loss: 48.929017454179515\n",
      "Epoch 455/1000, Avg Training Loss: 0.24754617147559155, Avg Validation Loss: 48.92962857717296\n",
      "Epoch 456/1000, Avg Training Loss: 0.24975468577559076, Avg Validation Loss: 48.933874052431214\n",
      "Epoch 457/1000, Avg Training Loss: 0.24871412823418734, Avg Validation Loss: 48.936980067568435\n",
      "Epoch 458/1000, Avg Training Loss: 0.24839104266262, Avg Validation Loss: 48.9405275348367\n",
      "Epoch 459/1000, Avg Training Loss: 0.24856863281535574, Avg Validation Loss: 48.94414646523158\n",
      "Epoch 460/1000, Avg Training Loss: 0.24862837682956496, Avg Validation Loss: 48.95019516057314\n",
      "Epoch 461/1000, Avg Training Loss: 0.24838332214513278, Avg Validation Loss: 48.95259465048504\n",
      "Epoch 462/1000, Avg Training Loss: 0.2476629389406898, Avg Validation Loss: 48.951116359252126\n",
      "Epoch 463/1000, Avg Training Loss: 0.24947069664581922, Avg Validation Loss: 48.95499756725185\n",
      "Epoch 464/1000, Avg Training Loss: 0.2497961527030764, Avg Validation Loss: 48.95970698817477\n",
      "Epoch 465/1000, Avg Training Loss: 0.2494010663092453, Avg Validation Loss: 48.962945448081214\n",
      "Epoch 466/1000, Avg Training Loss: 0.2495438961592279, Avg Validation Loss: 48.967284756233575\n",
      "Epoch 467/1000, Avg Training Loss: 0.24851813873453701, Avg Validation Loss: 48.970116853248484\n",
      "Epoch 468/1000, Avg Training Loss: 0.2484567156859513, Avg Validation Loss: 48.97328874380049\n",
      "Epoch 469/1000, Avg Training Loss: 0.24829489518377215, Avg Validation Loss: 48.97523597291675\n",
      "Epoch 470/1000, Avg Training Loss: 0.24915031229833567, Avg Validation Loss: 48.97720880557548\n",
      "Epoch 471/1000, Avg Training Loss: 0.24856402046084544, Avg Validation Loss: 48.980521947424975\n",
      "Epoch 472/1000, Avg Training Loss: 0.2484089389113797, Avg Validation Loss: 48.9840484533417\n",
      "Epoch 473/1000, Avg Training Loss: 0.24875913161353125, Avg Validation Loss: 48.9885601338142\n",
      "Epoch 474/1000, Avg Training Loss: 0.24884206392965877, Avg Validation Loss: 48.99294535477132\n",
      "Epoch 475/1000, Avg Training Loss: 0.2492269186886923, Avg Validation Loss: 48.99601830646574\n",
      "Epoch 476/1000, Avg Training Loss: 0.24843524307532988, Avg Validation Loss: 48.99913417115286\n",
      "Epoch 477/1000, Avg Training Loss: 0.24929034599924604, Avg Validation Loss: 49.002497703963115\n",
      "Epoch 478/1000, Avg Training Loss: 0.24940539978922266, Avg Validation Loss: 49.006676671102404\n",
      "Epoch 479/1000, Avg Training Loss: 0.24826746255198187, Avg Validation Loss: 49.00891489410517\n",
      "Epoch 480/1000, Avg Training Loss: 0.24762740448761505, Avg Validation Loss: 49.008285540392464\n",
      "Epoch 481/1000, Avg Training Loss: 0.24907962259100025, Avg Validation Loss: 49.010534794800144\n",
      "Epoch 482/1000, Avg Training Loss: 0.2482481985943922, Avg Validation Loss: 49.01231564481654\n",
      "Epoch 483/1000, Avg Training Loss: 0.24798543970050396, Avg Validation Loss: 49.01436255730522\n",
      "Epoch 484/1000, Avg Training Loss: 0.24890988977877926, Avg Validation Loss: 49.016442693103414\n",
      "Epoch 485/1000, Avg Training Loss: 0.24757054462898082, Avg Validation Loss: 49.018418442719984\n",
      "Epoch 486/1000, Avg Training Loss: 0.2489205207204406, Avg Validation Loss: 49.02210932187134\n",
      "Epoch 487/1000, Avg Training Loss: 0.2498907706664911, Avg Validation Loss: 49.02875411341113\n",
      "Epoch 488/1000, Avg Training Loss: 0.24835658205617844, Avg Validation Loss: 49.03383908708949\n",
      "Epoch 489/1000, Avg Training Loss: 0.24863496044215144, Avg Validation Loss: 49.03844385650005\n",
      "Epoch 490/1000, Avg Training Loss: 0.24902789558870883, Avg Validation Loss: 49.04183410466388\n",
      "Epoch 491/1000, Avg Training Loss: 0.24904329062902966, Avg Validation Loss: 49.04602592952545\n",
      "Epoch 492/1000, Avg Training Loss: 0.24891517225209855, Avg Validation Loss: 49.050630295969796\n",
      "Epoch 493/1000, Avg Training Loss: 0.24758422411593783, Avg Validation Loss: 49.05240759097722\n",
      "Epoch 494/1000, Avg Training Loss: 0.24847187799579423, Avg Validation Loss: 49.054660622650495\n",
      "Epoch 495/1000, Avg Training Loss: 0.2482875844372151, Avg Validation Loss: 49.0566143717637\n",
      "Epoch 496/1000, Avg Training Loss: 0.24967688599526716, Avg Validation Loss: 49.061520069002235\n",
      "Epoch 497/1000, Avg Training Loss: 0.249264993275425, Avg Validation Loss: 49.06456844322443\n",
      "Epoch 498/1000, Avg Training Loss: 0.25019014146080126, Avg Validation Loss: 49.07025466343757\n",
      "Epoch 499/1000, Avg Training Loss: 0.24871685666717291, Avg Validation Loss: 49.07193005241746\n",
      "Epoch 500/1000, Avg Training Loss: 0.24866760174217656, Avg Validation Loss: 49.07495189078226\n",
      "Epoch 501/1000, Avg Training Loss: 0.24771056626253857, Avg Validation Loss: 49.073872015173286\n",
      "Epoch 502/1000, Avg Training Loss: 0.24917113644474229, Avg Validation Loss: 49.07911668326878\n",
      "Epoch 503/1000, Avg Training Loss: 0.2493812312314291, Avg Validation Loss: 49.084749960066695\n",
      "Epoch 504/1000, Avg Training Loss: 0.24857458999267532, Avg Validation Loss: 49.08866317495914\n",
      "Epoch 505/1000, Avg Training Loss: 0.24870769369002438, Avg Validation Loss: 49.091738418695\n",
      "Epoch 506/1000, Avg Training Loss: 0.2501703554017947, Avg Validation Loss: 49.09732522613631\n",
      "Epoch 507/1000, Avg Training Loss: 0.25035993540092477, Avg Validation Loss: 49.102234518937465\n",
      "Epoch 508/1000, Avg Training Loss: 0.24882253918634295, Avg Validation Loss: 49.107147847086424\n",
      "Epoch 509/1000, Avg Training Loss: 0.2479638749689912, Avg Validation Loss: 49.10808111272227\n",
      "Epoch 510/1000, Avg Training Loss: 0.24931234975950173, Avg Validation Loss: 49.11224461418635\n",
      "Epoch 511/1000, Avg Training Loss: 0.24960525469658953, Avg Validation Loss: 49.11788342191656\n",
      "Epoch 512/1000, Avg Training Loss: 0.24943296320503758, Avg Validation Loss: 49.123096544207776\n",
      "Epoch 513/1000, Avg Training Loss: 0.24845759466457212, Avg Validation Loss: 49.12692330178081\n",
      "Epoch 514/1000, Avg Training Loss: 0.2493675143656333, Avg Validation Loss: 49.13153657684819\n",
      "Epoch 515/1000, Avg Training Loss: 0.24881860284237076, Avg Validation Loss: 49.13491796868702\n",
      "Epoch 516/1000, Avg Training Loss: 0.24884586441143455, Avg Validation Loss: 49.136544157812764\n",
      "Epoch 517/1000, Avg Training Loss: 0.2491436181422451, Avg Validation Loss: 49.14022368622127\n",
      "Epoch 518/1000, Avg Training Loss: 0.24872969963809038, Avg Validation Loss: 49.14743989650924\n",
      "Epoch 519/1000, Avg Training Loss: 0.24968953460820345, Avg Validation Loss: 49.15177535893925\n",
      "Epoch 520/1000, Avg Training Loss: 0.24997209969166861, Avg Validation Loss: 49.15710213207379\n",
      "Epoch 521/1000, Avg Training Loss: 0.248759918277273, Avg Validation Loss: 49.161669591820186\n",
      "Epoch 522/1000, Avg Training Loss: 0.24991113037115575, Avg Validation Loss: 49.16597663260396\n",
      "Epoch 523/1000, Avg Training Loss: 0.24948937850116315, Avg Validation Loss: 49.17115295973744\n",
      "Epoch 524/1000, Avg Training Loss: 0.24949886385951675, Avg Validation Loss: 49.17759180480685\n",
      "Epoch 525/1000, Avg Training Loss: 0.25064301359819124, Avg Validation Loss: 49.18309200135275\n",
      "Epoch 526/1000, Avg Training Loss: 0.24876723092513933, Avg Validation Loss: 49.1877287909042\n",
      "Epoch 527/1000, Avg Training Loss: 0.2501813977127551, Avg Validation Loss: 49.19199768327015\n",
      "Epoch 528/1000, Avg Training Loss: 0.24911668887245397, Avg Validation Loss: 49.19437975934749\n",
      "Epoch 529/1000, Avg Training Loss: 0.24984429428419466, Avg Validation Loss: 49.19836850865839\n",
      "Epoch 530/1000, Avg Training Loss: 0.24885028411731494, Avg Validation Loss: 49.19869268872888\n",
      "Epoch 531/1000, Avg Training Loss: 0.2494834273883361, Avg Validation Loss: 49.20067609174817\n",
      "Epoch 532/1000, Avg Training Loss: 0.2488569457459797, Avg Validation Loss: 49.20428700177355\n",
      "Epoch 533/1000, Avg Training Loss: 0.2500045553258384, Avg Validation Loss: 49.21114345298281\n",
      "Epoch 534/1000, Avg Training Loss: 0.24881438167151995, Avg Validation Loss: 49.21400842039921\n",
      "Epoch 535/1000, Avg Training Loss: 0.24843850544587348, Avg Validation Loss: 49.217458219671826\n",
      "Epoch 536/1000, Avg Training Loss: 0.2493148021775819, Avg Validation Loss: 49.22171724102027\n",
      "Epoch 537/1000, Avg Training Loss: 0.2489962922383144, Avg Validation Loss: 49.227433162973824\n",
      "Epoch 538/1000, Avg Training Loss: 0.25120975775616106, Avg Validation Loss: 49.23542733403265\n",
      "Epoch 539/1000, Avg Training Loss: 0.2479855456573861, Avg Validation Loss: 49.23852868266378\n",
      "Epoch 540/1000, Avg Training Loss: 0.2503028688278017, Avg Validation Loss: 49.245088474100285\n",
      "Epoch 541/1000, Avg Training Loss: 0.24849506083397052, Avg Validation Loss: 49.249120642208766\n",
      "Epoch 542/1000, Avg Training Loss: 0.24905966212353928, Avg Validation Loss: 49.253853282114974\n",
      "Epoch 543/1000, Avg Training Loss: 0.25017099482113087, Avg Validation Loss: 49.25988741333897\n",
      "Epoch 544/1000, Avg Training Loss: 0.2505766671885253, Avg Validation Loss: 49.26512458215466\n",
      "Epoch 545/1000, Avg Training Loss: 0.24743477142415884, Avg Validation Loss: 49.26683635121856\n",
      "Epoch 546/1000, Avg Training Loss: 0.2503737582376775, Avg Validation Loss: 49.27143175460552\n",
      "Epoch 547/1000, Avg Training Loss: 0.24990066251250298, Avg Validation Loss: 49.277376611933455\n",
      "Epoch 548/1000, Avg Training Loss: 0.2516859784541424, Avg Validation Loss: 49.28727213192461\n",
      "Epoch 549/1000, Avg Training Loss: 0.2481612259144168, Avg Validation Loss: 49.29176340872588\n",
      "Epoch 550/1000, Avg Training Loss: 0.24924946740931933, Avg Validation Loss: 49.29802632518296\n",
      "Epoch 551/1000, Avg Training Loss: 0.24872528336796015, Avg Validation Loss: 49.30342979214853\n",
      "Epoch 552/1000, Avg Training Loss: 0.25096425054285987, Avg Validation Loss: 49.311657446154925\n",
      "Epoch 553/1000, Avg Training Loss: 0.25135916459087543, Avg Validation Loss: 49.32065059664386\n",
      "Epoch 554/1000, Avg Training Loss: 0.2495992597293985, Avg Validation Loss: 49.32594275459922\n",
      "Epoch 555/1000, Avg Training Loss: 0.2501868347604716, Avg Validation Loss: 49.33062628043568\n",
      "Epoch 556/1000, Avg Training Loss: 0.2489410709083496, Avg Validation Loss: 49.33426564347524\n",
      "Epoch 557/1000, Avg Training Loss: 0.2512473472835958, Avg Validation Loss: 49.34113999884403\n",
      "Epoch 558/1000, Avg Training Loss: 0.2494926143523305, Avg Validation Loss: 49.34760066467034\n",
      "Epoch 559/1000, Avg Training Loss: 0.24920021680289495, Avg Validation Loss: 49.35196830280416\n",
      "Epoch 560/1000, Avg Training Loss: 0.24834162640819435, Avg Validation Loss: 49.353636375255036\n",
      "Epoch 561/1000, Avg Training Loss: 0.2510741107924675, Avg Validation Loss: 49.36049088918479\n",
      "Epoch 562/1000, Avg Training Loss: 0.2502375181442848, Avg Validation Loss: 49.3693529388772\n",
      "Epoch 563/1000, Avg Training Loss: 0.25022835770304824, Avg Validation Loss: 49.377791563023514\n",
      "Epoch 564/1000, Avg Training Loss: 0.24933066810119872, Avg Validation Loss: 49.38273370648105\n",
      "Epoch 565/1000, Avg Training Loss: 0.24857118921800864, Avg Validation Loss: 49.385789564926036\n",
      "Epoch 566/1000, Avg Training Loss: 0.25005460146487235, Avg Validation Loss: 49.39245844538674\n",
      "Epoch 567/1000, Avg Training Loss: 0.2502721660211539, Avg Validation Loss: 49.401443349882584\n",
      "Epoch 568/1000, Avg Training Loss: 0.25046682628904743, Avg Validation Loss: 49.409062718219275\n",
      "Epoch 569/1000, Avg Training Loss: 0.24785708566624431, Avg Validation Loss: 49.411479450679984\n",
      "Epoch 570/1000, Avg Training Loss: 0.2516980204001991, Avg Validation Loss: 49.41723534376207\n",
      "Epoch 571/1000, Avg Training Loss: 0.24901937687488038, Avg Validation Loss: 49.420383202515765\n",
      "Epoch 572/1000, Avg Training Loss: 0.2515717110847937, Avg Validation Loss: 49.4280087263768\n",
      "Epoch 573/1000, Avg Training Loss: 0.2507100511352187, Avg Validation Loss: 49.43585682750448\n",
      "Epoch 574/1000, Avg Training Loss: 0.25085909855494337, Avg Validation Loss: 49.44281196964072\n",
      "Epoch 575/1000, Avg Training Loss: 0.25032823604176957, Avg Validation Loss: 49.449363634581374\n",
      "Epoch 576/1000, Avg Training Loss: 0.2502166117587106, Avg Validation Loss: 49.45533837075735\n",
      "Epoch 577/1000, Avg Training Loss: 0.25153626250111566, Avg Validation Loss: 49.46327432242251\n",
      "Epoch 578/1000, Avg Training Loss: 0.2503537219373378, Avg Validation Loss: 49.47187267468634\n",
      "Epoch 579/1000, Avg Training Loss: 0.24888669053538398, Avg Validation Loss: 49.478179428185406\n",
      "Epoch 580/1000, Avg Training Loss: 0.2520706033008456, Avg Validation Loss: 49.4865799856213\n",
      "Epoch 581/1000, Avg Training Loss: 0.25121226346637493, Avg Validation Loss: 49.494616937099714\n",
      "Epoch 582/1000, Avg Training Loss: 0.25099920757465805, Avg Validation Loss: 49.50235256543246\n",
      "Epoch 583/1000, Avg Training Loss: 0.2494297222547842, Avg Validation Loss: 49.50886190236676\n",
      "Epoch 584/1000, Avg Training Loss: 0.25054103319319, Avg Validation Loss: 49.51513753351874\n",
      "Epoch 585/1000, Avg Training Loss: 0.2510141672240848, Avg Validation Loss: 49.52160156069057\n",
      "Epoch 586/1000, Avg Training Loss: 0.25050080841038846, Avg Validation Loss: 49.52919125780487\n",
      "Epoch 587/1000, Avg Training Loss: 0.2527490851599977, Avg Validation Loss: 49.54107561590444\n",
      "Epoch 588/1000, Avg Training Loss: 0.2506369421838741, Avg Validation Loss: 49.55138878681933\n",
      "Epoch 589/1000, Avg Training Loss: 0.24897557623052066, Avg Validation Loss: 49.557004795418706\n",
      "Epoch 590/1000, Avg Training Loss: 0.25039456499896623, Avg Validation Loss: 49.56329050319114\n",
      "Epoch 591/1000, Avg Training Loss: 0.24999651514564872, Avg Validation Loss: 49.57052076941005\n",
      "Epoch 592/1000, Avg Training Loss: 0.25067398729383983, Avg Validation Loss: 49.57489688410017\n",
      "Epoch 593/1000, Avg Training Loss: 0.2514011488726992, Avg Validation Loss: 49.58296934922592\n",
      "Epoch 594/1000, Avg Training Loss: 0.25109385556281816, Avg Validation Loss: 49.5911961520023\n",
      "Epoch 595/1000, Avg Training Loss: 0.25182906092904617, Avg Validation Loss: 49.600727356034085\n",
      "Epoch 596/1000, Avg Training Loss: 0.2502448514208139, Avg Validation Loss: 49.60528851194822\n",
      "Epoch 597/1000, Avg Training Loss: 0.2527091791579105, Avg Validation Loss: 49.61284291572311\n",
      "Epoch 598/1000, Avg Training Loss: 0.24605732641471853, Avg Validation Loss: 49.61377219796962\n",
      "Epoch 599/1000, Avg Training Loss: 0.25182007613212937, Avg Validation Loss: 49.62032412911115\n",
      "Epoch 600/1000, Avg Training Loss: 0.2514530183441427, Avg Validation Loss: 49.631613198171976\n",
      "Epoch 601/1000, Avg Training Loss: 0.2516002792956326, Avg Validation Loss: 49.64277480675356\n",
      "Epoch 602/1000, Avg Training Loss: 0.25105166601292955, Avg Validation Loss: 49.65165806120886\n",
      "Epoch 603/1000, Avg Training Loss: 0.2506663009744681, Avg Validation Loss: 49.659127150791804\n",
      "Epoch 604/1000, Avg Training Loss: 0.2511924692316061, Avg Validation Loss: 49.66723645340785\n",
      "Epoch 605/1000, Avg Training Loss: 0.251720061174951, Avg Validation Loss: 49.676343507455584\n",
      "Epoch 606/1000, Avg Training Loss: 0.2501946378978311, Avg Validation Loss: 49.68581766735802\n",
      "Epoch 607/1000, Avg Training Loss: 0.24946757756782997, Avg Validation Loss: 49.694079923669506\n",
      "Epoch 608/1000, Avg Training Loss: 0.2516061512535281, Avg Validation Loss: 49.7047849594053\n",
      "Epoch 609/1000, Avg Training Loss: 0.24982737127078225, Avg Validation Loss: 49.71554326872764\n",
      "Epoch 610/1000, Avg Training Loss: 0.25083934041323414, Avg Validation Loss: 49.72434771224347\n",
      "Epoch 611/1000, Avg Training Loss: 0.25241139917839633, Avg Validation Loss: 49.732736736031676\n",
      "Epoch 612/1000, Avg Training Loss: 0.2513059991755465, Avg Validation Loss: 49.74372475693987\n",
      "Epoch 613/1000, Avg Training Loss: 0.25186664415190885, Avg Validation Loss: 49.75570287560936\n",
      "Epoch 614/1000, Avg Training Loss: 0.251200802867176, Avg Validation Loss: 49.7667346947206\n",
      "Epoch 615/1000, Avg Training Loss: 0.25198204321332485, Avg Validation Loss: 49.77809927095889\n",
      "Epoch 616/1000, Avg Training Loss: 0.2522704368854586, Avg Validation Loss: 49.78572885458621\n",
      "Epoch 617/1000, Avg Training Loss: 0.2505961834621302, Avg Validation Loss: 49.79406577422175\n",
      "Epoch 618/1000, Avg Training Loss: 0.25289063384899935, Avg Validation Loss: 49.80502582916188\n",
      "Epoch 619/1000, Avg Training Loss: 0.2514864622155529, Avg Validation Loss: 49.81001762102737\n",
      "Epoch 620/1000, Avg Training Loss: 0.24943266126259758, Avg Validation Loss: 49.81178921417998\n",
      "Epoch 621/1000, Avg Training Loss: 0.24863698182095717, Avg Validation Loss: 49.81682001361602\n",
      "Epoch 622/1000, Avg Training Loss: 0.251116093931723, Avg Validation Loss: 49.82261515903943\n",
      "Epoch 623/1000, Avg Training Loss: 0.2509100698758548, Avg Validation Loss: 49.83280886327974\n",
      "Epoch 624/1000, Avg Training Loss: 0.2528264512858127, Avg Validation Loss: 49.845761206456196\n",
      "Epoch 625/1000, Avg Training Loss: 0.24960498841437675, Avg Validation Loss: 49.85199903954738\n",
      "Epoch 626/1000, Avg Training Loss: 0.2505389422187354, Avg Validation Loss: 49.85550959613826\n",
      "Epoch 627/1000, Avg Training Loss: 0.24936450561406948, Avg Validation Loss: 49.86312177153822\n",
      "Epoch 628/1000, Avg Training Loss: 0.2529270423913531, Avg Validation Loss: 49.87553098534947\n",
      "Epoch 629/1000, Avg Training Loss: 0.25294423730942045, Avg Validation Loss: 49.887033076256536\n",
      "Epoch 630/1000, Avg Training Loss: 0.2543168935034471, Avg Validation Loss: 49.89781354932842\n",
      "Epoch 631/1000, Avg Training Loss: 0.2549552370935842, Avg Validation Loss: 49.91194963670873\n",
      "Epoch 632/1000, Avg Training Loss: 0.2508604641791805, Avg Validation Loss: 49.924259461070534\n",
      "Epoch 633/1000, Avg Training Loss: 0.25384133269608716, Avg Validation Loss: 49.939998921035915\n",
      "Epoch 634/1000, Avg Training Loss: 0.2505574985375858, Avg Validation Loss: 49.95119072013959\n",
      "Epoch 635/1000, Avg Training Loss: 0.25101138528609934, Avg Validation Loss: 49.96207613103529\n",
      "Epoch 636/1000, Avg Training Loss: 0.25084546320793083, Avg Validation Loss: 49.96903105295648\n",
      "Epoch 637/1000, Avg Training Loss: 0.25436907180684776, Avg Validation Loss: 49.98210351858121\n",
      "Epoch 638/1000, Avg Training Loss: 0.2524338758700883, Avg Validation Loss: 49.99298657905939\n",
      "Epoch 639/1000, Avg Training Loss: 0.25147874066999865, Avg Validation Loss: 50.00346041278194\n",
      "Epoch 640/1000, Avg Training Loss: 0.2530315797613292, Avg Validation Loss: 50.01586618274264\n",
      "Epoch 641/1000, Avg Training Loss: 0.2541990949036335, Avg Validation Loss: 50.03182309879469\n",
      "Epoch 642/1000, Avg Training Loss: 0.249273049540504, Avg Validation Loss: 50.04031082376975\n",
      "Epoch 643/1000, Avg Training Loss: 0.2503023120231941, Avg Validation Loss: 50.04999820234269\n",
      "Epoch 644/1000, Avg Training Loss: 0.25452271659673026, Avg Validation Loss: 50.060336982022235\n",
      "Epoch 645/1000, Avg Training Loss: 0.2533699174872776, Avg Validation Loss: 50.077075346356786\n",
      "Epoch 646/1000, Avg Training Loss: 0.2521587313340129, Avg Validation Loss: 50.086266942570425\n",
      "Epoch 647/1000, Avg Training Loss: 0.2532075808565574, Avg Validation Loss: 50.10395869614645\n",
      "Epoch 648/1000, Avg Training Loss: 0.251289576106088, Avg Validation Loss: 50.11459358773248\n",
      "Epoch 649/1000, Avg Training Loss: 0.2525236918110629, Avg Validation Loss: 50.12586507470837\n",
      "Epoch 650/1000, Avg Training Loss: 0.24872297061620177, Avg Validation Loss: 50.133683621003655\n",
      "Epoch 651/1000, Avg Training Loss: 0.2536888759859023, Avg Validation Loss: 50.14637606040243\n",
      "Epoch 652/1000, Avg Training Loss: 0.25666508396337284, Avg Validation Loss: 50.1638604923333\n",
      "Epoch 653/1000, Avg Training Loss: 0.2526302685926194, Avg Validation Loss: 50.17947183284383\n",
      "Epoch 654/1000, Avg Training Loss: 0.2525057015523793, Avg Validation Loss: 50.193023096109094\n",
      "Epoch 655/1000, Avg Training Loss: 0.2526276115021547, Avg Validation Loss: 50.19970396825098\n",
      "Epoch 656/1000, Avg Training Loss: 0.2538894517034736, Avg Validation Loss: 50.216299202181375\n",
      "Epoch 657/1000, Avg Training Loss: 0.24924481522056827, Avg Validation Loss: 50.22152201461948\n",
      "Epoch 658/1000, Avg Training Loss: 0.25172140731812304, Avg Validation Loss: 50.23178275286129\n",
      "Epoch 659/1000, Avg Training Loss: 0.25389512545507975, Avg Validation Loss: 50.24868273373841\n",
      "Epoch 660/1000, Avg Training Loss: 0.25255593296905837, Avg Validation Loss: 50.25897026752159\n",
      "Epoch 661/1000, Avg Training Loss: 0.25299433487931866, Avg Validation Loss: 50.27352734806615\n",
      "Epoch 662/1000, Avg Training Loss: 0.2529870135335965, Avg Validation Loss: 50.28108464140911\n",
      "Epoch 663/1000, Avg Training Loss: 0.25481104063040994, Avg Validation Loss: 50.29477445887666\n",
      "Epoch 664/1000, Avg Training Loss: 0.2540431253934768, Avg Validation Loss: 50.31864004434719\n",
      "Epoch 665/1000, Avg Training Loss: 0.252411403263285, Avg Validation Loss: 50.327490446736824\n",
      "Epoch 666/1000, Avg Training Loss: 0.25325939526637165, Avg Validation Loss: 50.341470638993\n",
      "Epoch 667/1000, Avg Training Loss: 0.25414020655753966, Avg Validation Loss: 50.35647428108891\n",
      "Epoch 668/1000, Avg Training Loss: 0.2513517080306668, Avg Validation Loss: 50.367219961616385\n",
      "Epoch 669/1000, Avg Training Loss: 0.2586065290151243, Avg Validation Loss: 50.38963236158659\n",
      "Epoch 670/1000, Avg Training Loss: 0.25212399820219206, Avg Validation Loss: 50.407449474760156\n",
      "Epoch 671/1000, Avg Training Loss: 0.2528759188153963, Avg Validation Loss: 50.41861045884613\n",
      "Epoch 672/1000, Avg Training Loss: 0.2582392766697789, Avg Validation Loss: 50.43490454053243\n",
      "Epoch 673/1000, Avg Training Loss: 0.25276463453373404, Avg Validation Loss: 50.44896036924674\n",
      "Epoch 674/1000, Avg Training Loss: 0.25741953394887684, Avg Validation Loss: 50.47430119155446\n",
      "Epoch 675/1000, Avg Training Loss: 0.25521236945167436, Avg Validation Loss: 50.48696207602464\n",
      "Epoch 676/1000, Avg Training Loss: 0.25428550654467463, Avg Validation Loss: 50.49912717502653\n",
      "Epoch 677/1000, Avg Training Loss: 0.2560860012653977, Avg Validation Loss: 50.51242041685868\n",
      "Epoch 678/1000, Avg Training Loss: 0.25409981529403247, Avg Validation Loss: 50.5300802191623\n",
      "Epoch 679/1000, Avg Training Loss: 0.2555317656704045, Avg Validation Loss: 50.54364094309573\n",
      "Epoch 680/1000, Avg Training Loss: 0.25359207776499587, Avg Validation Loss: 50.55889908828132\n",
      "Epoch 681/1000, Avg Training Loss: 0.2557651707458871, Avg Validation Loss: 50.579297079331084\n",
      "Epoch 682/1000, Avg Training Loss: 0.25541157497504385, Avg Validation Loss: 50.604663922943615\n",
      "Epoch 683/1000, Avg Training Loss: 0.2507021950017511, Avg Validation Loss: 50.61840808698136\n",
      "Epoch 684/1000, Avg Training Loss: 0.2551147730477342, Avg Validation Loss: 50.622541997688394\n",
      "Epoch 685/1000, Avg Training Loss: 0.2530236151295188, Avg Validation Loss: 50.636825654646714\n",
      "Epoch 686/1000, Avg Training Loss: 0.2534957187889442, Avg Validation Loss: 50.65383452178527\n",
      "Epoch 687/1000, Avg Training Loss: 0.25337640085159213, Avg Validation Loss: 50.66958584886886\n",
      "Epoch 688/1000, Avg Training Loss: 0.25603963052224815, Avg Validation Loss: 50.68710404142003\n",
      "Epoch 689/1000, Avg Training Loss: 0.2554131668588822, Avg Validation Loss: 50.70418120498622\n",
      "Epoch 690/1000, Avg Training Loss: 0.25403020408098215, Avg Validation Loss: 50.71829256918449\n",
      "Epoch 691/1000, Avg Training Loss: 0.2561490821270354, Avg Validation Loss: 50.737366817583045\n",
      "Epoch 692/1000, Avg Training Loss: 0.25558414973375665, Avg Validation Loss: 50.75816340643823\n",
      "Epoch 693/1000, Avg Training Loss: 0.2538976080946509, Avg Validation Loss: 50.780705219913855\n",
      "Epoch 694/1000, Avg Training Loss: 0.2552534904651511, Avg Validation Loss: 50.79499113121949\n",
      "Epoch 695/1000, Avg Training Loss: 0.2553801129730697, Avg Validation Loss: 50.812626154110305\n",
      "Epoch 696/1000, Avg Training Loss: 0.2541817945035635, Avg Validation Loss: 50.821777573767704\n",
      "Epoch 697/1000, Avg Training Loss: 0.2540758471736509, Avg Validation Loss: 50.84332044872434\n",
      "Epoch 698/1000, Avg Training Loss: 0.2587278913686578, Avg Validation Loss: 50.86375478914097\n",
      "Epoch 699/1000, Avg Training Loss: 0.25448509721417095, Avg Validation Loss: 50.885744498502156\n",
      "Epoch 700/1000, Avg Training Loss: 0.25971977048745887, Avg Validation Loss: 50.89834297099866\n",
      "Epoch 701/1000, Avg Training Loss: 0.2576824004190372, Avg Validation Loss: 50.925175527710095\n",
      "Epoch 702/1000, Avg Training Loss: 0.2587739064644281, Avg Validation Loss: 50.956496903401955\n",
      "Epoch 703/1000, Avg Training Loss: 0.25595931123569265, Avg Validation Loss: 50.982798271386415\n",
      "Epoch 704/1000, Avg Training Loss: 0.2538324484390404, Avg Validation Loss: 50.99604333578338\n",
      "Epoch 705/1000, Avg Training Loss: 0.25751809107066354, Avg Validation Loss: 51.01113638067359\n",
      "Epoch 706/1000, Avg Training Loss: 0.25672237013802396, Avg Validation Loss: 51.029134549358595\n",
      "Epoch 707/1000, Avg Training Loss: 0.25542866848146384, Avg Validation Loss: 51.045140971094554\n",
      "Epoch 708/1000, Avg Training Loss: 0.25258807393282295, Avg Validation Loss: 51.062210063349355\n",
      "Epoch 709/1000, Avg Training Loss: 0.25561396507354234, Avg Validation Loss: 51.075066605344695\n",
      "Epoch 710/1000, Avg Training Loss: 0.2547927362122751, Avg Validation Loss: 51.08974382658185\n",
      "Epoch 711/1000, Avg Training Loss: 0.25741787872590516, Avg Validation Loss: 51.11379889531827\n",
      "Epoch 712/1000, Avg Training Loss: 0.2583774408515429, Avg Validation Loss: 51.13838832524377\n",
      "Epoch 713/1000, Avg Training Loss: 0.25987070326845246, Avg Validation Loss: 51.15105460590193\n",
      "Epoch 714/1000, Avg Training Loss: 0.258457655996905, Avg Validation Loss: 51.178355346027615\n",
      "Epoch 715/1000, Avg Training Loss: 0.2525925086257925, Avg Validation Loss: 51.200661695328364\n",
      "Epoch 716/1000, Avg Training Loss: 0.2573113680111814, Avg Validation Loss: 51.21521159181218\n",
      "Epoch 717/1000, Avg Training Loss: 0.2541519450859277, Avg Validation Loss: 51.24029023528645\n",
      "Epoch 718/1000, Avg Training Loss: 0.2533153384350147, Avg Validation Loss: 51.24735961241609\n",
      "Epoch 719/1000, Avg Training Loss: 0.26040213250033434, Avg Validation Loss: 51.274993519278524\n",
      "Epoch 720/1000, Avg Training Loss: 0.2602863172221152, Avg Validation Loss: 51.30266487823123\n",
      "Epoch 721/1000, Avg Training Loss: 0.2576603123081997, Avg Validation Loss: 51.316937092365016\n",
      "Epoch 722/1000, Avg Training Loss: 0.2576556648350837, Avg Validation Loss: 51.347655707191066\n",
      "Epoch 723/1000, Avg Training Loss: 0.25750745921218765, Avg Validation Loss: 51.37356819343691\n",
      "Epoch 724/1000, Avg Training Loss: 0.25831583044182205, Avg Validation Loss: 51.40009998357664\n",
      "Epoch 725/1000, Avg Training Loss: 0.25839139277926293, Avg Validation Loss: 51.4155516315776\n",
      "Epoch 726/1000, Avg Training Loss: 0.2577956448154362, Avg Validation Loss: 51.451874600017575\n",
      "Epoch 727/1000, Avg Training Loss: 0.2598863041918472, Avg Validation Loss: 51.46476920097306\n",
      "Epoch 728/1000, Avg Training Loss: 0.25602774194953204, Avg Validation Loss: 51.489421358110405\n",
      "Epoch 729/1000, Avg Training Loss: 0.25937045075315246, Avg Validation Loss: 51.50864873824481\n",
      "Epoch 730/1000, Avg Training Loss: 0.2640359673593271, Avg Validation Loss: 51.53897742058835\n",
      "Epoch 731/1000, Avg Training Loss: 0.25666522502184824, Avg Validation Loss: 51.58422840466747\n",
      "Epoch 732/1000, Avg Training Loss: 0.2570193797912596, Avg Validation Loss: 51.58182566007579\n",
      "Epoch 733/1000, Avg Training Loss: 0.2609963926767069, Avg Validation Loss: 51.60717002987956\n",
      "Epoch 734/1000, Avg Training Loss: 0.26153167794312276, Avg Validation Loss: 51.64330405038578\n",
      "Epoch 735/1000, Avg Training Loss: 0.25690602536579293, Avg Validation Loss: 51.65837668216648\n",
      "Epoch 736/1000, Avg Training Loss: 0.2619347119663691, Avg Validation Loss: 51.69269577204713\n",
      "Epoch 737/1000, Avg Training Loss: 0.260041491151429, Avg Validation Loss: 51.71160168498689\n",
      "Epoch 738/1000, Avg Training Loss: 0.25977940032326957, Avg Validation Loss: 51.74747949583687\n",
      "Epoch 739/1000, Avg Training Loss: 0.26465809230372633, Avg Validation Loss: 51.771781722820826\n",
      "Epoch 740/1000, Avg Training Loss: 0.25975455057559976, Avg Validation Loss: 51.79932195762537\n",
      "Epoch 741/1000, Avg Training Loss: 0.2551573570363504, Avg Validation Loss: 51.82263524736802\n",
      "Epoch 742/1000, Avg Training Loss: 0.2576209011753723, Avg Validation Loss: 51.85390098830108\n",
      "Epoch 743/1000, Avg Training Loss: 0.2606371927594339, Avg Validation Loss: 51.891279364430694\n",
      "Epoch 744/1000, Avg Training Loss: 0.258067239946733, Avg Validation Loss: 51.9268350973497\n",
      "Epoch 745/1000, Avg Training Loss: 0.25854022864929027, Avg Validation Loss: 51.935310929702744\n",
      "Epoch 746/1000, Avg Training Loss: 0.26403202797845415, Avg Validation Loss: 51.969045830868126\n",
      "Epoch 747/1000, Avg Training Loss: 0.26419539997605723, Avg Validation Loss: 51.99991342914335\n",
      "Epoch 748/1000, Avg Training Loss: 0.26045743828568524, Avg Validation Loss: 52.01868004684449\n",
      "Epoch 749/1000, Avg Training Loss: 0.25986478284639924, Avg Validation Loss: 52.058392635593464\n",
      "Epoch 750/1000, Avg Training Loss: 0.26344777026129496, Avg Validation Loss: 52.06904677427851\n",
      "Epoch 751/1000, Avg Training Loss: 0.2596204340637182, Avg Validation Loss: 52.10681326677936\n",
      "Epoch 752/1000, Avg Training Loss: 0.265373858521276, Avg Validation Loss: 52.13805840942256\n",
      "Epoch 753/1000, Avg Training Loss: 0.2571795287018146, Avg Validation Loss: 52.183757164913786\n",
      "Epoch 754/1000, Avg Training Loss: 0.26047431755178785, Avg Validation Loss: 52.188634589488274\n",
      "Epoch 755/1000, Avg Training Loss: 0.2634394323951266, Avg Validation Loss: 52.22180348248688\n",
      "Epoch 756/1000, Avg Training Loss: 0.25860847917558927, Avg Validation Loss: 52.25103831666099\n",
      "Epoch 757/1000, Avg Training Loss: 0.26642916746000656, Avg Validation Loss: 52.27661925097462\n",
      "Epoch 758/1000, Avg Training Loss: 0.26374083272684073, Avg Validation Loss: 52.330880017430765\n",
      "Epoch 759/1000, Avg Training Loss: 0.26621944514433404, Avg Validation Loss: 52.36263916446103\n",
      "Epoch 760/1000, Avg Training Loss: 0.2590889753269292, Avg Validation Loss: 52.40071601333571\n",
      "Epoch 761/1000, Avg Training Loss: 0.2657520869264819, Avg Validation Loss: 52.432098082499095\n",
      "Epoch 762/1000, Avg Training Loss: 0.2597190920740232, Avg Validation Loss: 52.44767619520386\n",
      "Epoch 763/1000, Avg Training Loss: 0.26499594822273037, Avg Validation Loss: 52.480553481590526\n",
      "Epoch 764/1000, Avg Training Loss: 0.25865800026370306, Avg Validation Loss: 52.50172641339845\n",
      "Epoch 765/1000, Avg Training Loss: 0.2627103635112936, Avg Validation Loss: 52.54920688108574\n",
      "Epoch 766/1000, Avg Training Loss: 0.26112899193476413, Avg Validation Loss: 52.57334991051077\n",
      "Epoch 767/1000, Avg Training Loss: 0.2627181155220727, Avg Validation Loss: 52.59487197500517\n",
      "Epoch 768/1000, Avg Training Loss: 0.2679592426215899, Avg Validation Loss: 52.661787454400795\n",
      "Epoch 769/1000, Avg Training Loss: 0.26547666126427627, Avg Validation Loss: 52.686547997639735\n",
      "Epoch 770/1000, Avg Training Loss: 0.26553563648639633, Avg Validation Loss: 52.71056832752296\n",
      "Epoch 771/1000, Avg Training Loss: 0.2606264081062828, Avg Validation Loss: 52.750780162598716\n",
      "Epoch 772/1000, Avg Training Loss: 0.26115034930751413, Avg Validation Loss: 52.792841967189005\n",
      "Epoch 773/1000, Avg Training Loss: 0.26614502082529545, Avg Validation Loss: 52.83833360309392\n",
      "Epoch 774/1000, Avg Training Loss: 0.26975584865945296, Avg Validation Loss: 52.861667966967566\n",
      "Epoch 775/1000, Avg Training Loss: 0.2643736443141183, Avg Validation Loss: 52.90188760973673\n",
      "Epoch 776/1000, Avg Training Loss: 0.26447318011018495, Avg Validation Loss: 52.91368614203331\n",
      "Epoch 777/1000, Avg Training Loss: 0.25886320261375095, Avg Validation Loss: 52.96212215094731\n",
      "Epoch 778/1000, Avg Training Loss: 0.26899128977103803, Avg Validation Loss: 52.98032421576536\n",
      "Epoch 779/1000, Avg Training Loss: 0.26479910315558297, Avg Validation Loss: 53.06096346135962\n",
      "Epoch 780/1000, Avg Training Loss: 0.26646776531370153, Avg Validation Loss: 53.087958721822076\n",
      "Epoch 781/1000, Avg Training Loss: 0.2650739727784838, Avg Validation Loss: 53.1091417190935\n",
      "Epoch 782/1000, Avg Training Loss: 0.261919073476896, Avg Validation Loss: 53.14854271946183\n",
      "Epoch 783/1000, Avg Training Loss: 0.26914022180628155, Avg Validation Loss: 53.17089865536276\n",
      "Epoch 784/1000, Avg Training Loss: 0.26936423437601914, Avg Validation Loss: 53.23128338366074\n",
      "Epoch 785/1000, Avg Training Loss: 0.2670322845648038, Avg Validation Loss: 53.25897943195751\n",
      "Epoch 786/1000, Avg Training Loss: 0.2657386186673765, Avg Validation Loss: 53.33074402315037\n",
      "Epoch 787/1000, Avg Training Loss: 0.26542167002024486, Avg Validation Loss: 53.37730348193436\n",
      "Epoch 788/1000, Avg Training Loss: 0.2703075810777607, Avg Validation Loss: 53.40116031691088\n",
      "Epoch 789/1000, Avg Training Loss: 0.26177056681932015, Avg Validation Loss: 53.42652335230386\n",
      "Epoch 790/1000, Avg Training Loss: 0.2715141917674716, Avg Validation Loss: 53.44604363366884\n",
      "Epoch 791/1000, Avg Training Loss: 0.2619165312592855, Avg Validation Loss: 53.501800593751724\n",
      "Epoch 792/1000, Avg Training Loss: 0.2699725137492594, Avg Validation Loss: 53.53667620153927\n",
      "Epoch 793/1000, Avg Training Loss: 0.2663869272926852, Avg Validation Loss: 53.56805563302617\n",
      "Epoch 794/1000, Avg Training Loss: 0.2646123600783537, Avg Validation Loss: 53.63236961426944\n",
      "Epoch 795/1000, Avg Training Loss: 0.26790168504355355, Avg Validation Loss: 53.68343449955156\n",
      "Epoch 796/1000, Avg Training Loss: 0.2689697273811614, Avg Validation Loss: 53.702023639667836\n",
      "Epoch 797/1000, Avg Training Loss: 0.26658278133814245, Avg Validation Loss: 53.74232615603657\n",
      "Epoch 798/1000, Avg Training Loss: 0.2635526386473299, Avg Validation Loss: 53.79403513920716\n",
      "Epoch 799/1000, Avg Training Loss: 0.2652571270414584, Avg Validation Loss: 53.85222968866732\n",
      "Epoch 800/1000, Avg Training Loss: 0.2702196400627686, Avg Validation Loss: 53.862264661819076\n",
      "Epoch 801/1000, Avg Training Loss: 0.26798132751003106, Avg Validation Loss: 53.90921965547552\n",
      "Epoch 802/1000, Avg Training Loss: 0.2761038306945887, Avg Validation Loss: 53.95965237249392\n",
      "Epoch 803/1000, Avg Training Loss: 0.2777100845549772, Avg Validation Loss: 54.009090081134154\n",
      "Epoch 804/1000, Avg Training Loss: 0.2706072988795054, Avg Validation Loss: 54.04989059339381\n",
      "Epoch 805/1000, Avg Training Loss: 0.2655845124737509, Avg Validation Loss: 54.12234460043322\n",
      "Epoch 806/1000, Avg Training Loss: 0.2694002432207352, Avg Validation Loss: 54.16461284266745\n",
      "Epoch 807/1000, Avg Training Loss: 0.27044874263215307, Avg Validation Loss: 54.22261644250169\n",
      "Epoch 808/1000, Avg Training Loss: 0.2723879527233645, Avg Validation Loss: 54.23315584627146\n",
      "Epoch 809/1000, Avg Training Loss: 0.27432646618882045, Avg Validation Loss: 54.33828870396145\n",
      "Epoch 810/1000, Avg Training Loss: 0.27419652996753635, Avg Validation Loss: 54.357663912247205\n",
      "Epoch 811/1000, Avg Training Loss: 0.2686019176914175, Avg Validation Loss: 54.402073051400926\n",
      "Epoch 812/1000, Avg Training Loss: 0.26669045241336137, Avg Validation Loss: 54.49179194464054\n",
      "Epoch 813/1000, Avg Training Loss: 0.2728900041849813, Avg Validation Loss: 54.51969374491395\n",
      "Epoch 814/1000, Avg Training Loss: 0.27573024009020897, Avg Validation Loss: 54.54650807846809\n",
      "Epoch 815/1000, Avg Training Loss: 0.269729259097534, Avg Validation Loss: 54.573629407976696\n",
      "Epoch 816/1000, Avg Training Loss: 0.2717259746871383, Avg Validation Loss: 54.61568997034832\n",
      "Epoch 817/1000, Avg Training Loss: 0.2714934707328236, Avg Validation Loss: 54.66116180267813\n",
      "Epoch 818/1000, Avg Training Loss: 0.2682600513175894, Avg Validation Loss: 54.74913556181223\n",
      "Epoch 819/1000, Avg Training Loss: 0.2684186438449714, Avg Validation Loss: 54.774108125515006\n",
      "Epoch 820/1000, Avg Training Loss: 0.2770552004589216, Avg Validation Loss: 54.852500012402935\n",
      "Epoch 821/1000, Avg Training Loss: 0.2704785518930128, Avg Validation Loss: 54.90428952275814\n",
      "Epoch 822/1000, Avg Training Loss: 0.2738715307060869, Avg Validation Loss: 54.93284708236014\n",
      "Epoch 823/1000, Avg Training Loss: 0.28072264145054104, Avg Validation Loss: 54.99225413998086\n",
      "Epoch 824/1000, Avg Training Loss: 0.2786416655946457, Avg Validation Loss: 55.08424119355296\n",
      "Epoch 825/1000, Avg Training Loss: 0.27685434469373754, Avg Validation Loss: 55.11609396424102\n",
      "Epoch 826/1000, Avg Training Loss: 0.27099457375839925, Avg Validation Loss: 55.1579030131213\n",
      "Epoch 827/1000, Avg Training Loss: 0.2776187815504847, Avg Validation Loss: 55.21697569221644\n",
      "Epoch 828/1000, Avg Training Loss: 0.2717779869639566, Avg Validation Loss: 55.28576602617872\n",
      "Epoch 829/1000, Avg Training Loss: 0.2755544049829787, Avg Validation Loss: 55.33855623871591\n",
      "Epoch 830/1000, Avg Training Loss: 0.273613449756293, Avg Validation Loss: 55.34227081229409\n",
      "Epoch 831/1000, Avg Training Loss: 0.27257097111824374, Avg Validation Loss: 55.39080724198563\n",
      "Epoch 832/1000, Avg Training Loss: 0.27421500893287054, Avg Validation Loss: 55.529422791863674\n",
      "Epoch 833/1000, Avg Training Loss: 0.2729984572723363, Avg Validation Loss: 55.58930394610444\n",
      "Epoch 834/1000, Avg Training Loss: 0.2744131572536691, Avg Validation Loss: 55.607043164353\n",
      "Epoch 835/1000, Avg Training Loss: 0.27004777177792916, Avg Validation Loss: 55.66479517481147\n",
      "Epoch 836/1000, Avg Training Loss: 0.2815060527777067, Avg Validation Loss: 55.68542683032415\n",
      "Epoch 837/1000, Avg Training Loss: 0.2817090527014794, Avg Validation Loss: 55.80803695964143\n",
      "Epoch 838/1000, Avg Training Loss: 0.2759244225178784, Avg Validation Loss: 55.79340815554305\n",
      "Epoch 839/1000, Avg Training Loss: 0.2794814753212759, Avg Validation Loss: 55.915897784924375\n",
      "Epoch 840/1000, Avg Training Loss: 0.27906166974048635, Avg Validation Loss: 55.9386232891934\n",
      "Epoch 841/1000, Avg Training Loss: 0.2768901478563364, Avg Validation Loss: 56.00571953561251\n",
      "Epoch 842/1000, Avg Training Loss: 0.2804515606980588, Avg Validation Loss: 56.07977926555549\n",
      "Epoch 843/1000, Avg Training Loss: 0.27544969586153667, Avg Validation Loss: 56.14740194026912\n",
      "Epoch 844/1000, Avg Training Loss: 0.28341315503742726, Avg Validation Loss: 56.21718280188091\n",
      "Epoch 845/1000, Avg Training Loss: 0.28336963508638435, Avg Validation Loss: 56.29008167921229\n",
      "Epoch 846/1000, Avg Training Loss: 0.28477228964916823, Avg Validation Loss: 56.31141065703965\n",
      "Epoch 847/1000, Avg Training Loss: 0.28215141354345297, Avg Validation Loss: 56.38148950496189\n",
      "Epoch 848/1000, Avg Training Loss: 0.2830190353761429, Avg Validation Loss: 56.468928112549854\n",
      "Epoch 849/1000, Avg Training Loss: 0.27436779268443795, Avg Validation Loss: 56.559661076080715\n",
      "Epoch 850/1000, Avg Training Loss: 0.2873278806574735, Avg Validation Loss: 56.60461658765068\n",
      "Epoch 851/1000, Avg Training Loss: 0.28331849493037126, Avg Validation Loss: 56.72783588129368\n",
      "Epoch 852/1000, Avg Training Loss: 0.2800862931308384, Avg Validation Loss: 56.783927133830204\n",
      "Epoch 853/1000, Avg Training Loss: 0.2809890840825814, Avg Validation Loss: 56.83896345678755\n",
      "Epoch 854/1000, Avg Training Loss: 0.29113438801517055, Avg Validation Loss: 56.842399886319775\n",
      "Epoch 855/1000, Avg Training Loss: 0.281809966021594, Avg Validation Loss: 56.9922317212077\n",
      "Epoch 856/1000, Avg Training Loss: 0.2801651083504192, Avg Validation Loss: 57.029276499887885\n",
      "Epoch 857/1000, Avg Training Loss: 0.2811302792626787, Avg Validation Loss: 57.019536994054434\n",
      "Epoch 858/1000, Avg Training Loss: 0.27988056601845324, Avg Validation Loss: 57.12178696964516\n",
      "Epoch 859/1000, Avg Training Loss: 0.27728175864330606, Avg Validation Loss: 57.250897777923406\n",
      "Epoch 860/1000, Avg Training Loss: 0.27862727478840044, Avg Validation Loss: 57.34764328377197\n",
      "Epoch 861/1000, Avg Training Loss: 0.2845354457452883, Avg Validation Loss: 57.36371735241657\n",
      "Epoch 862/1000, Avg Training Loss: 0.2797397987411366, Avg Validation Loss: 57.43088264654164\n",
      "Epoch 863/1000, Avg Training Loss: 0.29103084347996083, Avg Validation Loss: 57.603843478738966\n",
      "Epoch 864/1000, Avg Training Loss: 0.28285451607115725, Avg Validation Loss: 57.6092737364577\n",
      "Epoch 865/1000, Avg Training Loss: 0.28362629828026253, Avg Validation Loss: 57.643534626279845\n",
      "Epoch 866/1000, Avg Training Loss: 0.28685936824642794, Avg Validation Loss: 57.753040192699096\n",
      "Epoch 867/1000, Avg Training Loss: 0.2936454274541856, Avg Validation Loss: 57.86710552280242\n",
      "Epoch 868/1000, Avg Training Loss: 0.29149438986145015, Avg Validation Loss: 57.918513697745695\n",
      "Epoch 869/1000, Avg Training Loss: 0.2834894733540223, Avg Validation Loss: 57.93951038103803\n",
      "Epoch 870/1000, Avg Training Loss: 0.28459558552237196, Avg Validation Loss: 57.94920490138282\n",
      "Epoch 871/1000, Avg Training Loss: 0.28419998335828806, Avg Validation Loss: 58.219542958705546\n",
      "Epoch 872/1000, Avg Training Loss: 0.2904712691923151, Avg Validation Loss: 58.21262454409079\n",
      "Epoch 873/1000, Avg Training Loss: 0.29458711619899886, Avg Validation Loss: 58.2518609652586\n",
      "Epoch 874/1000, Avg Training Loss: 0.2871571016492026, Avg Validation Loss: 58.28795157491055\n",
      "Epoch 875/1000, Avg Training Loss: 0.28519521397123704, Avg Validation Loss: 58.46464385442509\n",
      "Epoch 876/1000, Avg Training Loss: 0.28636278832188433, Avg Validation Loss: 58.541238463404625\n",
      "Epoch 877/1000, Avg Training Loss: 0.2933986490514739, Avg Validation Loss: 58.650472594619714\n",
      "Epoch 878/1000, Avg Training Loss: 0.2829002719096447, Avg Validation Loss: 58.79472270243623\n",
      "Epoch 879/1000, Avg Training Loss: 0.2890854806586466, Avg Validation Loss: 58.82011238194115\n",
      "Epoch 880/1000, Avg Training Loss: 0.2987836124684487, Avg Validation Loss: 58.70582282474879\n",
      "Epoch 881/1000, Avg Training Loss: 0.2969306186075618, Avg Validation Loss: 58.85292360161528\n",
      "Epoch 882/1000, Avg Training Loss: 0.2957306658979716, Avg Validation Loss: 59.04443984852892\n",
      "Epoch 883/1000, Avg Training Loss: 0.2999988746532394, Avg Validation Loss: 59.12891417114258\n",
      "Epoch 884/1000, Avg Training Loss: 0.29646391791130017, Avg Validation Loss: 59.14583861368791\n",
      "Epoch 885/1000, Avg Training Loss: 0.29226948198983477, Avg Validation Loss: 59.39897828058204\n",
      "Epoch 886/1000, Avg Training Loss: 0.29363297211561346, Avg Validation Loss: 59.3384152230651\n",
      "Epoch 887/1000, Avg Training Loss: 0.29562865561485446, Avg Validation Loss: 59.3742725840787\n",
      "Epoch 888/1000, Avg Training Loss: 0.29804862842189667, Avg Validation Loss: 59.42305514789874\n",
      "Epoch 889/1000, Avg Training Loss: 0.3002026023523339, Avg Validation Loss: 59.56019166977866\n",
      "Epoch 890/1000, Avg Training Loss: 0.2957355058880284, Avg Validation Loss: 59.713306224662745\n",
      "Epoch 891/1000, Avg Training Loss: 0.2909016286772628, Avg Validation Loss: 59.76137724475835\n",
      "Epoch 892/1000, Avg Training Loss: 0.29662246927893954, Avg Validation Loss: 60.01379119025362\n",
      "Epoch 893/1000, Avg Training Loss: 0.2958255293874641, Avg Validation Loss: 60.12634837248118\n",
      "Epoch 894/1000, Avg Training Loss: 0.29847547791304, Avg Validation Loss: 60.16934505394652\n",
      "Epoch 895/1000, Avg Training Loss: 0.2968468717389658, Avg Validation Loss: 60.11784677536531\n",
      "Epoch 896/1000, Avg Training Loss: 0.29683025722240847, Avg Validation Loss: 60.24220321071061\n",
      "Epoch 897/1000, Avg Training Loss: 0.3066456625686176, Avg Validation Loss: 60.323439728273144\n",
      "Epoch 898/1000, Avg Training Loss: 0.2989591001570775, Avg Validation Loss: 60.43144183998815\n",
      "Epoch 899/1000, Avg Training Loss: 0.3018330003039874, Avg Validation Loss: 60.48587811106104\n",
      "Epoch 900/1000, Avg Training Loss: 0.29802600809540203, Avg Validation Loss: 60.75035467719252\n",
      "Epoch 901/1000, Avg Training Loss: 0.2992006321666647, Avg Validation Loss: 60.9419841083948\n",
      "Epoch 902/1000, Avg Training Loss: 0.30145339870947835, Avg Validation Loss: 60.91201646054448\n",
      "Epoch 903/1000, Avg Training Loss: 0.3069495628840124, Avg Validation Loss: 60.868681575839915\n",
      "Epoch 904/1000, Avg Training Loss: 0.30227479374310035, Avg Validation Loss: 61.03800161016683\n",
      "Epoch 905/1000, Avg Training Loss: 0.30500415871892467, Avg Validation Loss: 61.24416454148678\n",
      "Epoch 906/1000, Avg Training Loss: 0.29958966106844026, Avg Validation Loss: 61.293336931437565\n",
      "Epoch 907/1000, Avg Training Loss: 0.30033225087786436, Avg Validation Loss: 61.552054685663805\n",
      "Epoch 908/1000, Avg Training Loss: 0.3037743816326822, Avg Validation Loss: 61.637225254309584\n",
      "Epoch 909/1000, Avg Training Loss: 0.30628323243300476, Avg Validation Loss: 61.64868551625206\n",
      "Epoch 910/1000, Avg Training Loss: 0.30482125720804787, Avg Validation Loss: 61.812708273002244\n",
      "Epoch 911/1000, Avg Training Loss: 0.2951786638347896, Avg Validation Loss: 61.87788260299581\n",
      "Epoch 912/1000, Avg Training Loss: 0.30846048595689085, Avg Validation Loss: 61.880099837143746\n",
      "Epoch 913/1000, Avg Training Loss: 0.3074081890054845, Avg Validation Loss: 61.919159719739035\n",
      "Epoch 914/1000, Avg Training Loss: 0.30363472765505434, Avg Validation Loss: 62.153639964078835\n",
      "Epoch 915/1000, Avg Training Loss: 0.3056736065617738, Avg Validation Loss: 62.33337620959877\n",
      "Epoch 916/1000, Avg Training Loss: 0.30704380198389364, Avg Validation Loss: 62.129731817806665\n",
      "Epoch 917/1000, Avg Training Loss: 0.30605859519653655, Avg Validation Loss: 62.49523357823392\n",
      "Epoch 918/1000, Avg Training Loss: 0.30447107968690507, Avg Validation Loss: 62.55340519883425\n",
      "Epoch 919/1000, Avg Training Loss: 0.3083737050779598, Avg Validation Loss: 62.765125640589176\n",
      "Epoch 920/1000, Avg Training Loss: 0.3078130389636703, Avg Validation Loss: 62.8716767938961\n",
      "Epoch 921/1000, Avg Training Loss: 0.30852695243612854, Avg Validation Loss: 62.911730225210945\n",
      "Epoch 922/1000, Avg Training Loss: 0.3158029964410123, Avg Validation Loss: 62.98267349824954\n",
      "Epoch 923/1000, Avg Training Loss: 0.3114815822837221, Avg Validation Loss: 63.15031155504192\n",
      "Epoch 924/1000, Avg Training Loss: 0.30742985091697944, Avg Validation Loss: 63.15828149328611\n",
      "Epoch 925/1000, Avg Training Loss: 0.30491055919087184, Avg Validation Loss: 63.30735620278471\n",
      "Epoch 926/1000, Avg Training Loss: 0.31189333096109206, Avg Validation Loss: 63.41360016019713\n",
      "Epoch 927/1000, Avg Training Loss: 0.3172067217727759, Avg Validation Loss: 63.57022485354474\n",
      "Epoch 928/1000, Avg Training Loss: 0.32023708521191124, Avg Validation Loss: 63.64858611826297\n",
      "Epoch 929/1000, Avg Training Loss: 0.3149660853273978, Avg Validation Loss: 63.826569483124956\n",
      "Epoch 930/1000, Avg Training Loss: 0.31584732936308474, Avg Validation Loss: 63.97519922954244\n",
      "Epoch 931/1000, Avg Training Loss: 0.31727863538726303, Avg Validation Loss: 64.28089393341867\n",
      "Epoch 932/1000, Avg Training Loss: 0.3131024094454186, Avg Validation Loss: 64.1825463400411\n",
      "Epoch 933/1000, Avg Training Loss: 0.31256999395286694, Avg Validation Loss: 64.38834294658425\n",
      "Epoch 934/1000, Avg Training Loss: 0.30974838649398384, Avg Validation Loss: 64.42251971872527\n",
      "Epoch 935/1000, Avg Training Loss: 0.32664123194800204, Avg Validation Loss: 64.53973453540569\n",
      "Epoch 936/1000, Avg Training Loss: 0.32955532353832645, Avg Validation Loss: 65.13972780192377\n",
      "Epoch 937/1000, Avg Training Loss: 0.31993066063531367, Avg Validation Loss: 64.77840348183697\n",
      "Epoch 938/1000, Avg Training Loss: 0.32811816223538093, Avg Validation Loss: 64.90894553189825\n",
      "Epoch 939/1000, Avg Training Loss: 0.31181904852571435, Avg Validation Loss: 65.20594840728246\n",
      "Epoch 940/1000, Avg Training Loss: 0.3258830262276964, Avg Validation Loss: 65.02991098356102\n",
      "Epoch 941/1000, Avg Training Loss: 0.3156909523566491, Avg Validation Loss: 65.335430933994\n",
      "Epoch 942/1000, Avg Training Loss: 0.31894845638292096, Avg Validation Loss: 65.74495136079234\n",
      "Epoch 943/1000, Avg Training Loss: 0.32253840762450325, Avg Validation Loss: 65.72843851669995\n",
      "Epoch 944/1000, Avg Training Loss: 0.31975979763203144, Avg Validation Loss: 65.91075800571826\n",
      "Epoch 945/1000, Avg Training Loss: 0.3228085954422276, Avg Validation Loss: 65.86222752804146\n",
      "Epoch 946/1000, Avg Training Loss: 0.32584780596125085, Avg Validation Loss: 66.0151203432572\n",
      "Epoch 947/1000, Avg Training Loss: 0.33596520552632614, Avg Validation Loss: 66.03361468392511\n",
      "Epoch 948/1000, Avg Training Loss: 0.3183393724437247, Avg Validation Loss: 66.22529078074825\n",
      "Epoch 949/1000, Avg Training Loss: 0.3393464022307782, Avg Validation Loss: 66.49441796026396\n",
      "Epoch 950/1000, Avg Training Loss: 0.33529258727887173, Avg Validation Loss: 66.59091045482276\n",
      "Epoch 951/1000, Avg Training Loss: 0.3340647455445722, Avg Validation Loss: 66.77054278903334\n",
      "Epoch 952/1000, Avg Training Loss: 0.3160891833249103, Avg Validation Loss: 66.8844921557181\n",
      "Epoch 953/1000, Avg Training Loss: 0.3331315322081053, Avg Validation Loss: 67.09008132266851\n",
      "Epoch 954/1000, Avg Training Loss: 0.3297225404957983, Avg Validation Loss: 67.04054930788016\n",
      "Epoch 955/1000, Avg Training Loss: 0.3280197931082141, Avg Validation Loss: 67.34280324130972\n",
      "Epoch 956/1000, Avg Training Loss: 0.3321487301817989, Avg Validation Loss: 67.42289008297712\n",
      "Epoch 957/1000, Avg Training Loss: 0.3305447557756701, Avg Validation Loss: 67.42577029411571\n",
      "Epoch 958/1000, Avg Training Loss: 0.328186565706287, Avg Validation Loss: 67.85893111976387\n",
      "Epoch 959/1000, Avg Training Loss: 0.32888639709251527, Avg Validation Loss: 67.76401666175832\n",
      "Epoch 960/1000, Avg Training Loss: 0.3353934571622378, Avg Validation Loss: 68.16496060212879\n",
      "Epoch 961/1000, Avg Training Loss: 0.33999158920844313, Avg Validation Loss: 68.05815235915222\n",
      "Epoch 962/1000, Avg Training Loss: 0.34304505061476676, Avg Validation Loss: 68.43551231397336\n",
      "Epoch 963/1000, Avg Training Loss: 0.3363218546792997, Avg Validation Loss: 68.40521984521921\n",
      "Epoch 964/1000, Avg Training Loss: 0.3383143577876257, Avg Validation Loss: 68.70188475316877\n",
      "Epoch 965/1000, Avg Training Loss: 0.3436703459512794, Avg Validation Loss: 68.91236822656512\n",
      "Epoch 966/1000, Avg Training Loss: 0.33420147508368747, Avg Validation Loss: 69.23736828174854\n",
      "Epoch 967/1000, Avg Training Loss: 0.34461311488551183, Avg Validation Loss: 68.99000998680967\n",
      "Epoch 968/1000, Avg Training Loss: 0.3524445671509263, Avg Validation Loss: 69.38310404855564\n",
      "Epoch 969/1000, Avg Training Loss: 0.34492100513720453, Avg Validation Loss: 69.63190784703829\n",
      "Epoch 970/1000, Avg Training Loss: 0.33561411941410846, Avg Validation Loss: 69.6847537366459\n",
      "Epoch 971/1000, Avg Training Loss: 0.3500646405778904, Avg Validation Loss: 69.8461419415608\n",
      "Epoch 972/1000, Avg Training Loss: 0.3566924805438589, Avg Validation Loss: 69.67887786938168\n",
      "Epoch 973/1000, Avg Training Loss: 0.34947586379584616, Avg Validation Loss: 69.85255353567464\n",
      "Epoch 974/1000, Avg Training Loss: 0.3432307061362663, Avg Validation Loss: 70.40706204716727\n",
      "Epoch 975/1000, Avg Training Loss: 0.3473846633458102, Avg Validation Loss: 70.57574929629382\n",
      "Epoch 976/1000, Avg Training Loss: 0.3339311476609439, Avg Validation Loss: 70.83442736840547\n",
      "Epoch 977/1000, Avg Training Loss: 0.3507925376307211, Avg Validation Loss: 70.86013870887197\n",
      "Epoch 978/1000, Avg Training Loss: 0.3518005704993988, Avg Validation Loss: 71.10863782494549\n",
      "Epoch 979/1000, Avg Training Loss: 0.3584654465039515, Avg Validation Loss: 71.48694382902953\n",
      "Epoch 980/1000, Avg Training Loss: 0.35820564358951024, Avg Validation Loss: 71.03299977526011\n",
      "Epoch 981/1000, Avg Training Loss: 0.35425095687953984, Avg Validation Loss: 71.08861338232167\n",
      "Epoch 982/1000, Avg Training Loss: 0.3417457673950128, Avg Validation Loss: 71.80198283328409\n",
      "Epoch 983/1000, Avg Training Loss: 0.3463761700858617, Avg Validation Loss: 71.59616219072502\n",
      "Epoch 984/1000, Avg Training Loss: 0.3468500160120174, Avg Validation Loss: 71.76856891326354\n",
      "Epoch 985/1000, Avg Training Loss: 0.3553218233023341, Avg Validation Loss: 71.89676514618574\n",
      "Epoch 986/1000, Avg Training Loss: 0.35677130462763457, Avg Validation Loss: 72.13795268390807\n",
      "Epoch 987/1000, Avg Training Loss: 0.3494510999292726, Avg Validation Loss: 72.23066761207241\n",
      "Epoch 988/1000, Avg Training Loss: 0.3599558324100706, Avg Validation Loss: 72.94468432800784\n",
      "Epoch 989/1000, Avg Training Loss: 0.367759288474046, Avg Validation Loss: 73.1005233988472\n",
      "Epoch 990/1000, Avg Training Loss: 0.3467919935821826, Avg Validation Loss: 73.27249058310287\n",
      "Epoch 991/1000, Avg Training Loss: 0.34889179195184566, Avg Validation Loss: 73.88507416075826\n",
      "Epoch 992/1000, Avg Training Loss: 0.3637999831802619, Avg Validation Loss: 73.69184216252808\n",
      "Epoch 993/1000, Avg Training Loss: 0.3599139298988536, Avg Validation Loss: 73.96953732494855\n",
      "Epoch 994/1000, Avg Training Loss: 0.3611916503047926, Avg Validation Loss: 73.82933498174694\n",
      "Epoch 995/1000, Avg Training Loss: 0.35441831520820716, Avg Validation Loss: 74.10483852382933\n",
      "Epoch 996/1000, Avg Training Loss: 0.35301947529808575, Avg Validation Loss: 74.18787945803143\n",
      "Epoch 997/1000, Avg Training Loss: 0.3566464499736881, Avg Validation Loss: 74.27536116552017\n",
      "Epoch 998/1000, Avg Training Loss: 0.3598739002401307, Avg Validation Loss: 74.22589849850489\n",
      "Epoch 999/1000, Avg Training Loss: 0.3625928352925149, Avg Validation Loss: 74.91657319133324\n",
      "Epoch 1000/1000, Avg Training Loss: 0.3738912850431073, Avg Validation Loss: 74.64111271729982\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test\n",
    "np.random.seed(42)\n",
    "\n",
    "x_tot = np.random.rand(1000, 15)\n",
    "target = np.random.rand(1000, 3)\n",
    "\n",
    "layer_one = Layer(15, 8, ELU, d_ELU)\n",
    "layer_two = Layer(8, 5, ELU, d_ELU)\n",
    "layer_out = Layer(5, 3, linear, d_linear)\n",
    "\n",
    "NN = NeuralNetwork()\n",
    "NN.add_layer(layer_one)\n",
    "NN.add_layer(layer_two)\n",
    "NN.add_layer(layer_out)\n",
    "\n",
    "# Parametri di training\n",
    "K = 5\n",
    "epochs = 1000\n",
    "learning_rate = 0.0001 # online smaller learning_rate, minibatch greater learning_rate, to compare online vs minibatch we need to divide\n",
    "                       # learning_rate of minibatch for number of the examples in the minibatch\n",
    "batch_size = 100\n",
    "Lambda_t = 0.1\n",
    "Lambda_l = 0.1\n",
    "momentum = 0.7 # totalmente a caso\n",
    "\n",
    "# Cross-validation\n",
    "train_error, val_error = NN.train_val(x_tot, target, K, epochs, learning_rate, Lambda_t, Lambda_l, 'elastic', momentum, mean_squared_error, d_mean_squared_error, batch_size)\n",
    "\n",
    "# Plot degli errori\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_error, label='Training Error')\n",
    "plt.plot(val_error, label='Validation Error')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
