
 Best configuration after Hyperband n: 1 
Batch Size: 1
Validation Error: 2.796164867453027
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 2 
Batch Size: 40
Validation Error: 2.8102292290463686
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 3 
Batch Size: 40
Validation Error: 2.907745634004013
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 4 
Batch Size: 40
Validation Error: 2.9413935525882393
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 5 
Batch Size: 1
Validation Error: 3.1524295035584045
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 6 
Batch Size: 1
Validation Error: 3.1628986026613766
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 7 
Batch Size: 40
Validation Error: 3.197981044074724
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 8 
Batch Size: 1
Validation Error: 3.2024929609456123
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 9 
Batch Size: 40
Validation Error: 3.2329732045495447
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 10 
Batch Size: 1
Validation Error: 3.2552511003955518
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 11 
Batch Size: 40
Validation Error: 3.2666878830668784
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 12 
Batch Size: 40
Validation Error: 3.2749130732663487
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 13 
Batch Size: 1
Validation Error: 3.3122770562761175
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 14 
Batch Size: 1
Validation Error: 3.340738990550368
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 15 
Batch Size: 40
Validation Error: 3.3505362007356068
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 16 
Batch Size: 1
Validation Error: 3.4192431088896575
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 17 
Batch Size: 40
Validation Error: 3.483015789162856
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.0001
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 18 
Batch Size: 1
Validation Error: 3.4920461531763807
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 19 
Batch Size: 1
Validation Error: 3.53471873963217
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 20 
Batch Size: 1
Validation Error: 3.538260756067774
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 21 
Batch Size: 40
Validation Error: 3.588100594445715
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.0001
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 22 
Batch Size: 1
Validation Error: 3.6424235094052526
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 23 
Batch Size: 1
Validation Error: 3.6541467246819446
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 64
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 1e-05
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 1e-05
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 1e-05
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 1e-05
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 24 
Batch Size: 40
Validation Error: 3.6690436667553854
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.0001
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 25 
Batch Size: 1
Validation Error: 3.672738062770589
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 26 
Batch Size: 1
Validation Error: 3.7111760502254314
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.33
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 27 
Batch Size: 1
Validation Error: 3.7419368038297938
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 64
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 64
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 28 
Batch Size: 1
Validation Error: 3.8055009465846354
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0.66
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 29 
Batch Size: 1
Validation Error: 3.807001219612092
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-06
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 30 
Batch Size: 40
Validation Error: 3.8107976509442354
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0.0001
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------
