
 Best configuration after Hyperband n: 1 
Batch Size: 1
Validation Error: 1.9247700218558539
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 2 
Batch Size: 1
Validation Error: 2.1072735670511227
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 3 
Batch Size: 1
Validation Error: 2.1099397593960036
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 4 
Batch Size: 1
Validation Error: 2.1938666450564197
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 5 
Batch Size: 1
Validation Error: 2.248950888261697
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 6 
Batch Size: 1
Validation Error: 2.380099608787723
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 7 
Batch Size: 1
Validation Error: 2.4226171810681736
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 8 
Batch Size: 1
Validation Error: 2.4838557454838104
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 9 
Batch Size: 1
Validation Error: 2.5605513223317575
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 10 
Batch Size: 1
Validation Error: 2.576793851435694
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 11 
Batch Size: 1
Validation Error: 2.6400259571956446
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 12 
Batch Size: 1
Validation Error: 2.6976927263081327
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 13 
Batch Size: 1
Validation Error: 2.6985380556909733
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 14 
Batch Size: 1
Validation Error: 2.7260661361486163
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 15 
Batch Size: 1
Validation Error: 2.801492485281906
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 16 
Batch Size: 1
Validation Error: 2.80398813127598
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 17 
Batch Size: 1
Validation Error: 2.8155056057399532
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 18 
Batch Size: 1
Validation Error: 2.8439254219113566
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 19 
Batch Size: 1
Validation Error: 2.8443414640064213
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 20 
Batch Size: 1
Validation Error: 2.849070120850578
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 21 
Batch Size: 1
Validation Error: 2.851757726012721
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 22 
Batch Size: 1
Validation Error: 2.852202975844417
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 23 
Batch Size: 1
Validation Error: 2.945542174707543
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 24 
Batch Size: 1
Validation Error: 2.949580664904771
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: leaky_relu
    d_activation_function: d_leaky_relu
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 25 
Batch Size: 1
Validation Error: 2.9612900076984796
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 26 
Batch Size: 40
Validation Error: 2.9713920918547077
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 1
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 27 
Batch Size: 40
Validation Error: 2.9757557365950467
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 28 
Batch Size: 1
Validation Error: 3.008475123104949
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 32
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 29 
Batch Size: 40
Validation Error: 3.043814142077456
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 1e-05
  alpha: 0
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------

 Best configuration after Hyperband n: 30 
Batch Size: 1
Validation Error: 3.0463793764993183
NN Details:
=== Neural Network Details ===

Regularizer Configuration:
  Lambda: 0
  alpha: 0.5
  reg_type: elastic

Layers Configuration:
    Layer 1:
    dim_prev_layer: 12
    dim_layer: 32
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 2:
    dim_prev_layer: 32
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 3:
    dim_prev_layer: 128
    dim_layer: 128
    activation_function: tanh
    d_activation_function: d_tanh
    Layer 4:
    dim_prev_layer: 128
    dim_layer: 3
    activation_function: linear
    d_activation_function: d_linear

Optimizers Configuration:
  Optimizer 1:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 2:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 3:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None
  Optimizer 4:
    opt_type: NAG
    learning_rate: 0.0001
    momentum: 0.9
    beta_1: None
    beta_2: None
    epsilon: None

--------------------------------------------------
